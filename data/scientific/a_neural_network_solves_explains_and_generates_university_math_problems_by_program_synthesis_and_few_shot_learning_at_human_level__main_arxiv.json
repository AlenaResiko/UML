{
  "authors": [
    "Iddo Drori",
    "Sarah Zhang",
    "Reece Shuttleworth",
    "Leonard Tang",
    "Albert Lu",
    "Elizabeth Ke",
    "Kevin Liu",
    "Linda Chen",
    "Sunny Tran",
    "Newman Cheng",
    "Roman Wang",
    "Nikhil Singh",
    "Taylor L. Patti",
    "Jayson Lynch",
    "Avi Shporer",
    "Nakul Verma",
    "Eugene Wu",
    "Gilbert Strang"
  ],
  "date_published": "2021-12-31",
  "raw_tex": "\\documentclass[9pt,twocolumn,twoside,lineno]{pnas-new}\n Use the lineno option to display guide line numbers if required.\n \\usepackage{newtxtext,newtxmath}\n\\usepackage{amsmath}\n\\usepackage{longtable}\n\\usepackage{xcolor}\n\n\\captionsetup*[longtable]{labelfont+{small},textfont+{small,sf,bf},skip10pt,positionabove}\n booktabs provide nice spacing, but rule widths and distances need fixing\n\\setlength{\\heavyrulewidth}{0.5pt}\n\\setlength{\\lightrulewidth}{0.5pt}\n\\setlength{\\aboverulesep}{1.5pt}\n\\setlength{\\belowrulesep}{1.5pt}\n\\setlength{\\belowbottomsep}{10pt}\n\\AtBeginEnvironment{longtable}{\n\\sffamily\\fontsize{7.5}{10}\\selectfont\n}\n\n\\newcommand{\\gray}[1]{\\textcolor{gray}{#1}}\n\\newcommand{\\red}[1]{\\textcolor{red}{#1}}\n\\newcommand{\\ewu}[1]{\\red{#1\\xspace}}\n\\newcommand{\\comment}[1]{\\textcolor{blue}{COMMENT: #1\\xspace}}\n\n\\templatetype{pnasresearcharticle}   Choose template \n {pnasresearcharticle}   Template for a two-column research article\n {pnasmathematics}   Template for a one-column mathematics article\n {pnasinvited}   Template for a PNAS invited submission\n\n\\title{A Neural Network Solves, Explains, and Generates University Math Problems by Program Synthesis and Few-Shot Learning at Human Level}\n\n Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author\n\\author[1,a,b]{Iddo Drori}\n\\author[a]{Sarah Zhang}\n\\author[a]{Reece Shuttleworth}\n\\author[c]{Leonard Tang}\n\\author[a]{Albert Lu}\n\\author[a]{Elizabeth Ke}\n\\author[a]{Kevin Liu}\n\\author[a]{Linda Chen}\n\\author[a]{Sunny Tran}\n\\author[b]{Newman Cheng}\n\\author[b]{Roman Wang}\n\\author[a]{Nikhil Singh}\n\\author[c]{Taylor L. Patti}\n\\author[d]{Jayson Lynch}\n\\author[a]{Avi Shporer}\n\\author[b]{Nakul Verma}\n\\author[b]{Eugene Wu}\n\\author[a]{Gilbert Strang}\n\n\\affil[a]{Massachusetts Institute of Technology}\n\\affil[b]{Columbia University}\n\\affil[c]{Harvard University}\n\\affil[d]{University of Waterloo}\n\n Please give the surname of the lead author for the running footer\n\\leadauthor{Drori} \n\n Please add a significance statement to explain the relevance of your work\n\\significancestatement{We demonstrate that a neural network automatically solves, explains, and generates university-level problems from the largest MIT mathematics courses at a human level. Our methods combine three innovations: (1) using recent neural networks pre-trained on text and fine-tuned on code rather than pre-trained on text, (2) few-shot learning synthesizing programs that correctly solve course problems automatically, and (3) a pipeline to solve questions, explain solutions, and generate new questions indistinguishable by students from course questions. Our work is the first to solve university-level mathematics courses and improves upon state-of-the-art increasing automatic accuracy on randomly sampled questions on a benchmark by order of magnitude. Implications for higher education include the new roles of AI in automated course evaluation and content generation.}\n\n Please include corresponding author, author contribution, and author declaration information\n\n\\authorcontributions{\n1. Wrote the paper.\n2. Created figures.\n3. Provided ideas.\n4. Curated datasets.\n5. Generated questions.\n6. Generated solutions.\n7. Run GPT-3 and Codex.\n8. Executed programs.\n9. Evaluated solutions.\n10. Analyzed data.\n11. Wrote code.\n12. Designed survey.\nI.D. 1,2,3,4,5,6,7,8,9,10,11,12;\nS.Z. 1,2,4,6,7,8,9,10,11;\nR.S. 1,2,4,5,6,7,8,9,10,11;\nL.T. 1,4,5,6,7,8,9;\nA.L. 4,6,7,8,9,11;\nE.K. 4,7,8,9;\nK.L. 1,4,5,6,8,9;\nL.C. 4,6,7,8,9;\nS.T. 1,4,5,6,7,8,9;\nN.C. 4,5,6,7,8,9;\nR.W. 4,5,6,7,8,9;\nN.S. 1,2,3,10,11,12;\nT.L.P. 1,2,3,7,8,9;\nJ.L. 1,3,4,5,6,7,8,9;\nA.S. 1,10;\nN.V. 1,3,10,12;\nE.W. 1,2,3,12;\nG.S. 1,3}\n\n\\authordeclaration{There are no competing interests.}\n\\equalauthors{\\textsuperscript{1}A.O.(Author One) contributed equally to this work with A.T. (Author Two) (remove if not applicable).}\n\\correspondingauthor{\\textsuperscript{1}Iddo Drori. E-mail: idrori@mit.edu}\n\n At least three keywords are required at submission. Please provide three to five keywords, separated by the pipe symbol.\n\\keywords{Neural networks $|$ Mathematics courses $|$ Answering, explaining, and generating questions} \n\n\\begin{abstract}\nWe demonstrate that a neural network pre-trained on text and fine-tuned on code solves mathematics course problems, explains solutions, and generates new questions at a human level. We automatically synthesize programs using few-shot learning and OpenAI's Codex transformer and execute them to solve course problems at 81\\ automatic accuracy. We curate a new dataset of questions from MIT's largest mathematics courses (Single Variable and Multivariable Calculus, Differential Equations, Introduction to Probability and Statistics, Linear Algebra, and Mathematics for Computer Science) and Columbia University's Computational Linear Algebra. We solve questions from a MATH dataset (on Prealgebra, Algebra, Counting and Probability, Intermediate Algebra, Number Theory, and Precalculus), the latest benchmark of advanced mathematics problems designed to assess mathematical reasoning. We randomly sample questions and generate solutions with multiple modalities, including numbers, equations, and plots. The latest GPT-3 language model pre-trained on text automatically solves only 18.8\\ of these university questions using zero-shot learning and 30.8\\ using few-shot learning and the most recent chain of thought prompting. In contrast, program synthesis with few-shot learning using Codex fine-tuned on code generates programs that automatically solve 81\\ of these questions. Our approach improves the previous state-of-the-art automatic solution accuracy on the benchmark topics from 8.8\\ to 81.1\\. We perform a survey to evaluate the quality and difficulty of generated questions. This work is the first to automatically solve university-level mathematics course questions at a human level and the first work to explain and generate university-level mathematics course questions at scale, a milestone for higher education.\n\\end{abstract}\n\n\\dates{This manuscript was compiled on \\today}\n\\doi{\\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}\n\n\\usepackage{listings}\n\\usepackage{color}\n\n\\definecolor{dkgreen}{rgb}{0,0.6,0}\n\\definecolor{gray}{rgb}{0.5,0.5,0.5}\n\\definecolor{mauve}{rgb}{0.58,0,0.82}\n\n\\lstset{languagePython,\n   aboveskip1mm,\n   belowskip1mm,\n   showstringspacesfalse,\n   columnsflexible,\n   basicstyle{\\small\\ttfamily},\n   numbersnone,\n   numberstyle\\tiny\\color{gray},\n   keywordstyle\\color{blue},\n   commentstyle\\color{dkgreen},\n   stringstyle\\color{mauve},\n   breaklinestrue,\n   breakatwhitespacetrue,\n   tabsize3,\n   framenone\n}\n\n\\begin{document}\n\n\\maketitle\n\\thispagestyle{firststyle}\n\\ifthenelse{\\boolean{shortarticle}}{\\ifthenelse{\\boolean{singlecolumn}}{\\abscontentformatted}{\\abscontent}}{}\n\n\\section*{Introduction}\n\n\\begin{figure*}[!htb]\n   \\centering\n   \\includegraphics[width0.9\\textwidth]{math.pdf}\n   \\caption{We apply a neural network, OpenAI Codex, to solve, explain, and generate mathematics problems. We randomly sample the input math problems from MIT and Columbia University courses and the MATH dataset (left). We use zero-shot and few-shot learning to automatically generate programs that solve $81\\$ of the questions. We then use Codex to explain the generated programs. The generated programs can output diverse forms of answers, like printing a numerical answer or generating a plot (right). For example, in Calculus: the volume generated by rotating the finite 2-dimensional region bounded by two 2-dimensional graphs about the plotted axis (top right); in Differential Equations: the Lorenz strange attractor (middle right); In Linear Algebra: the geometry of the singular value decomposition (SVD) (middle right). An example of Codex's ability to produce line-by-line explanations of synthesized programs is demonstrated for a problem from Introduction to Probability and Statistics (bottom right).} \n   \\label{fig:teaser}\n\\end{figure*}\n\n\\dropcap{U}ntil this work; it was widely believed that neural networks could not solve advanced mathematics problems \\cite{choi20217}. However, the previous unsuccessful studies used only text-based pre-training. We now demonstrate that a neural network, OpenAI Codex, that is pre-trained on text \\textit{and} fine-tuned on code automatically answers $81\\$ of university-level mathematics problems by program synthesis using few-shot learning.\n\nFigure \\ref{fig:teaser} illustrates several example problems: computing the volume generated by rotating the graph of a single variable function around an axis, computing the Lorenz attractor and its projection, and computing and demonstrating the geometry of a singular value decomposition (SVD). For the first time, we show that a single machine learning model can solve these example problems and solve a wide variety of mathematics courses at scale.\n\n\\subsection*{Related Work}\nTransformers are deep learning architectures based only on attention mechanisms \\citep{vaswani2017attention} that do not use recurrent neural networks or convolutional neural networks. Transformer-based language models have enjoyed tremendous success across various natural language processing (NLP) tasks, including zero-shot and few-shot language tasks \\cite{brown2020language}. However, these models have largely failed to solve math problems \\cite{hendrycks2020measuring,MATH,rae2021scaling}. In particular, previous work using transformers, such as GPT-3 \\cite{brown2020language}, has failed to solve mathematics problems because the transformers were pre-trained on text alone. Using few-shot learning and chain of thought (CoT) prompting \\citep{kojima2022stepbystep} improves the mathematical reasoning ability of GPT-3; however, without code, GPT-3 with few-shot learning and CoT still fails on university-level mathematics problems and the MATH benchmark.\n\nPre-training a transformer is computationally expensive and often involves vast amounts of unlabeled data. The most common optimization objectives for pre-training language models are (1) masked word prediction: predicting a random deleted word in a sentence or predicting the next word, or (2) classifying whether two sentences follow each other. This computationally expensive step is usually done once, followed by a relatively fast fine-tuning step. In fine-tuning, the pre-trained model is tuned using a specific dataset or task.\n\nThis work demonstrates that OpenAI's Codex \\cite{chen2021evaluating}, a transformer that has been \\emph{pre-trained on text} and then \\emph{fine-tuned on code}, generates programs (i.e., conducts program synthesis) that solve math problems at scale and, with few-shot learning, automatically solves $81\\$ of the math course problems.\n\nPrevious work has seen modest success on simpler or specialized mathematics problem benchmarks. Techniques based on co-training output to verify \\cite{shen2021generate, cobbe2021training} or predict expression trees \\cite{xie2019goal,wu2020knowledge,qin2020semantically,zhang2020graph,li2020graph,liang2021mwp}, such as MAWPS and Math23k, are able to solve elementary school-level math problems with over $81\\$ accuracy. However, these approaches do not extend to high-school, math Olympiad, or university-level courses. Co-training paired with graph neural networks (GNNs) to predict arithmetic expression trees is able to solve university-level problems in Machine Learning \\cite{tran2021solving} with up to $95\\$ accuracy. However, that work is limited to numeric answers and overfits a specific course, which does not generalize to other courses.\n\n\\begin{figure*}[!htb]\n   \\centering\n   \\includegraphics[width0.8\\textwidth]{sankey.pdf}\n   \\caption{We select a random sample of questions from each course or topic that do not contain input images or require proofs. A language model pre-trained on text (GPT-3 text-davinci-002) automatically solves only $18\\$ (for courses) and $25.5\\$ (for the MATH benchmark topics) of these questions. In contrast, using zero-shot learning with a network pre-trained on text and fine tuned on code (OpenAI Codex code-davinci-002), we synthesize programs that automatically solve $71\\$ (for courses) and $72.2\\$ (for the MATH benchmark topics) of the questions. Using the same network but using few-shot learning, we automatically solve $81\\$ (for courses) and $81.1\\$ (for the MATH benchmark topics) of the questions. We use the nearest embedded zero-shot questions and their synthesized code for few-shot learning. The remaining $19\\$ of the course questions and $18.9\\$ of MATH benchmark topic questions are manually prompted to solve the question.\n   } \n   \\label{fig:few-shot}\n\\end{figure*}\n\n\\subsection*{Major Contributions}\nOur main contribution, as shown in Figure \\ref{fig:few-shot}, is demonstrating that a single neural network model, OpenAI Codex, automatically solves $81\\$ of randomly selected university-level mathematics problems (from six MIT mathematics courses and one Columbia University course) by using program synthesis and few-shot learning. We also automatically explain the solutions and generate new questions, a process requiring only seconds per problem. The courses are listed in Table \\ref{tab:dataset-examples}. We randomly sample 25 questions per course, and the problems are solved as-is or with minor contextual information that is automatically applied. The neural network outputs an executable program that answers the problem when prompted with the question. Furthermore, our method explains the solutions and generates new problems nearly indistinguishable from human-written problems.\n\nThis methodology increases the solution accuracy on the MATH benchmark \\cite{MATH} from $8.8\\$ accuracy using previous state-of-the-art methods to $81.1\\$ accuracy using automatic few-shot learning. The MATH benchmark measures the mathematical problem-solving ability of neural network models with challenging problems sourced from high school math competitions, such as the AMC 10\\footnote{American Mathematics Competitions}, AMC 12, and AIME\\footnote{American Invitational Mathematics Examination}.\n\nThe methods we propose are simple and broadly applicable. The first is using a transformer model pre-trained on text \\emph{and fine-tuned on code} so that it is adept at synthesizing programmatic solutions. The second is to use zero-shot learning of the questions as-is or automatically added contextual information about the problem or program. The third is to use few-shot learning based on question--code pairs of similar questions that have been solved, found by using the cosine similarity of the question embeddings. In our work, we use the latest version of OpenAI Codex \\cite{chen2021evaluating}: code-davinci-002.\n\n\\begin{table*}[!htb]\n\\small\n\\centering\n\\begin{tabular}{|l|p{3cm}|p{9cm}|p{3cm}|}\n   \\hline\n   \\textbf{ID} &\n   \\textbf{Course} & \\textbf{Question} & \\textbf{Solution} \\\\\n   \\hline\n   1 & 18.01\\newline Single Variable Calculus & A bacteria population is $4000$ at time $t   0$ and its rate of growth is $1000 * 2^t$ bacteria per hour after $t$ hours. What is the population after one hour? & $4000 + \\frac{1000}{\\log(2)}$ \\\\\n   \\hline\n   2 & 18.02\\newline Multi-variable Calculus & Describe the graph of the function $f$: \\newline \n   $f(x,y)   10 - \\sqrt{x^2+y^2}$ & \\includegraphics[height22mm]{18.02-q9-codex-output-plot.pdf}\\\\\n   \\hline\n   3 & 18.03\\newline Differential Equations & Find general solutions of the differential equations. If an initial condition is given, find the corresponding particular solution. Throughout, primes denote derivatives with respect to $x$. $y' + y   2$, $y(0)   0$ & $y(x)   2(1-e^{-x})$ \\\\\n   \\hline\n   4 & 18.05\\newline Introduction to Probability and Statistics & Calculate the probability of getting a three-of-a-kind poker hand. & $0.021128$ \\\\\n   \\hline\n   5 & 18.06\\newline Linear Algebra & Find a combination $x_1w_1+x_2w_2+x_3w_3$ that gives the zero vector with $x_1   1$. $w_1$ is the vector (1;2;3). $w_2$ is the vector $(4;5;6)$. $w_3$ is the vector $(7;8;9)$. & $x_1   1, x_2   -2, x_3   1$ \\\\\n   \\hline\n   6 & 6.042\\newline Mathematics for\\newline Computer Science & Find a number $x \\in \\{0, 1, . . . , 112\\}$ such that $11x \\equiv 1$ (mod $113$). & $72$ \\\\\n   \\hline\n   7 & COMS3251\\newline Computational\\newline\n   Linear Algebra & Given a d-dimensional non-zero vector $v$, compute the rank of the matrix $vv'$ & $1$\\\\\n   \\hline\n   8 & MATH\\newline Prealgebra & What is the greatest common factor of $84$, $112$ and $210$? & $14$ \\\\\n   \\hline\n   9 & MATH\\newline Algebra & Let $N,O$ be functions such that $N(x)   2\\sqrt{x}$, and $O(x)   x^2$. What is $N(O(N(O(N(O(3))))))$? & $24$ \\\\\n   \\hline\n   10 & MATH\\newline Number Theory & How many four-digit numbers whose digits add up to $9$ are divisible by $11$? & $0$ \\\\\n   \\hline\n   11 & MATH\\newline Counting and Probability & A standard six-sided fair die is rolled four times. The probability that the product of all four numbers rolled is a perfect square is $\\tfrac{m}{n}$, where $m$ and $n$ are relatively prime positive integers. Find $m+n$. & $187$ \\\\\n   \\hline\n   12 & MATH\\newline Intermediate Algebra & Given that $x^2 + y^2   14x + 6y + 6,$ find the largest possible value of $3x + 4y.$ & $73$ \\\\\n   \\hline\n   13 & MATH\\newline Precalculus & If the six solutions of $x^6-64$ are written in the form $a+bi$, where $a$ and $b$ are real, find the product of those solutions with $a>0.$ & $4$ \\\\\n   \\hline\n\\end{tabular}\n\\vspace{8pt}\n\\caption{Example questions and solutions from six MIT courses (18.01, 18.02, 18.03, 18.05, 18.06, 6.042), one Columbia University course (COMS3251), and six topics from the MATH dataset. The solutions can contain numerical answers, equations, plots, or other modalities.}\n\\label{tab:dataset-examples}\n\\end{table*}\n\n\\section*{Methods}\n\\label{sec:methods}\n\n\\begin{figure*}[!htb]\n   \\centering\n   \\includegraphics[width0.91\\textwidth]{panels.pdf}\n   \\caption{Example pipelines automatically solve questions from MIT mathematics courses and explain the solutions. 18.01 Single Variable Calculus Zero-Shot example: Given a question and the automatically generated prefix ``using SymPy,'' Codex is prompted and outputs a program. Running the program results in equations that are the correct answer. The program is then fed to Codex again with an automatic prompt, resulting in a generated code explanation. 18.02 Multivariable Calculus Few-Shot example: Given a question, the prefix ``write a program using SymPy'' is automatically generated. The question is embedded with the other zero-shot questions in the course. The nearest zero-shot question and its corresponding code are used as a few-shot example. The few-shot example pair and the input question are fed into Codex, which generates a program that solves the question. The question, program, and prompt for explanation are fed into Codex to generate the explanation. 18.03 Differential Equations Zero-Shot example: In this example, the answer is both a vector and a plot. 18.05 Introduction to Probability and Statistics Zero-Shot example: Given the question, a probabilistic program is generated by adding ``using simulation'' to the prompt. 18.06 Linear Algebra Zero-Shot example: The output answer is the correct vector.}\n   \\label{fig:panels}\n\\end{figure*}\n\n\\begin{figure*}[!htb]\n   \\centering\n   \\includegraphics[width1\\textwidth]{results.pdf}\n   \\caption{Comparison of the automatic solve rates on (A) MIT math courses and a Columbia University course and on (B) MATH benchmark dataset. The latest OpenAI GPT-3 (text-davinci-002), a transformer pre-trained on text, achieves on the MIT math courses (A) of $18.8\\$ with zero-shot, and $30.8\\$ with few-shot and CoT, and on the MATH benchmark (B) $25.5\\$ with zero-shot, and $42.2\\$ with few-shot and CoT. In contrast, program synthesis with using the latest OpenAI Codex (code-davinci-002), a transformer pre-trained on text and fine-tuned on code, achieve automatic solve rates on the MIT math courses (A) $71.1\\$ with zero-shot learning and $81.1\\$ with few-shot learning, and on the MATH benchmark (B) $72.2\\$ with zero-shot learning and $81.1\\$ with few-shot learning.}\n   \\label{fig:accuracy}\n\\end{figure*}\n\n\n\\subsection*{Dataset}\nWe randomly sample 25 questions from each of the seven courses: MIT's 18.01 Single Variable Calculus, 18.02 Multivariable Calculus, 18.03 Differential Equations, 18.05 Introduction to Probability and Statistics, 18.06 Linear Algebra, 6.042 Mathematics for Computer Science, and Columbia University's COMS3251 Computational Linear Algebra. For the MATH dataset, we randomly sample 15 questions from six topics in the dataset (Algebra, Counting \\& Probability, Intermediate Algebra, Number Theory, Prealgebra, and Precalculus). We validate that our results are not merely overfitting training data by solving questions from a new Computational Linear Algebra course COMS3251 which is unavailable online and was unseen by Codex when trained. We automatically obtain correct answers for $81\\$ of the randomly sampled university math course questions and $81.1\\$ of the MATH benchmark questions. Before this work, the previous state-of-the-art on this benchmark was $8.8\\$ \\cite{hendrycks2020measuring}.\n\n\\subsection*{Workflow}\nOur method takes a course problem as input and synthesizes a program that, when run, outputs the solution. Figure \\ref{fig:accuracy} compares the percent of automatically solved questions for each course using our zero-shot learning and few-shot learning approaches with the latest GPT-3 (text-davinci-002) and Codex (code-davinci-002) versions. The error bars on the totals are standard errors.\n\nFigures \\ref{fig:panels} show examples of automatic workflows for solving course questions and generating explanations using Codex. The panels show the original question, the automatic augmentation with context, the resulting synthesized program, the executed output answer that is the solution, and the explanation of the solution program. Questions are given to Codex either as-is or by automatically adding minor context, as described below. The output answer may be of numerous modalities. In the examples featured in Figure \\ref{fig:panels}, the output answers are an equation (18.01), a Boolean value (18.02), a plot (18.03), and a numerical value (18.05), and a vector (18.03 and 18.06).\n\n\\subsection*{Automatic Contextualization}\n\n\\paragraph{Programming Language Context.}\nBest results are obtained when the Codex prompt specifies that a program should be written and specifies which programming language should be used. We add the text ``write a program'' before the question and focus on the Python programming language by placing the text within Pythonic triple quotes like a docstring.\n\n\\paragraph{Library Context.}\nLikewise, the best results are obtained when the Codex prompt specifies which programming package should be used. For instance, we may add the Python library SymPy as context (see Figure \\ref{fig:panels} top panel 18.01), specifying that the program synthesized to solve the problem should use this package.\n\nFigure \\ref{fig:library-imports} shows the Python programming packages used by each course. Each colored stacked bar represents the number of questions in the class using that package. All courses use NumPy and Sympy. Matplotlib is used in classes with questions that require plotting. Around half of the courses use math, random, and SciPy. The usage patterns of these courses are incorporated automatically in our approach, as we only specify SymPy or plot-related imports; these other package imports are automatically synthesized.\n\n\\begin{figure}[hb!]\n   \\centering\n   \\includegraphics[width.5\\textwidth]{course-imports.pdf}\n   \\caption{Imported Python programming libraries by course: NumPy is used by nearly all courses. Matplotlib is used in courses with questions that involve plotting. Sympy is used by most of the courses, and SciPy by half of the courses.}\n   \\label{fig:library-imports}\n\\end{figure}\n\n\n\\subsection*{Automatic Zero-Shot and Few-Shot Learning}\nZero-shot learning synthesizes a program from the original question or the automatically augmented question without examples. This method automatically solves $71\\$ of the questions. Next, we describe the few-shot learning process in detail: If the question is not solved, we do the following: we embed all the questions using OpenAI's \\textit{text-similarity-babbage-001} embedding engine, which embeds the questions onto a 2,048-dimensional space. Then, we calculate the most similar solved questions to the unsolved question from within its course using the cosine similarity of the embeddings. We take the most similar question and its corresponding code and use these as few-shot examples for the new question. If the generated code does not output the correct answer, we add another solved question--code pair, using the next similar solved question each time. We found that using up to five examples for few-shot learning works well in practice, increasing the total number of questions automatically solved from $71\\$ using zero-shot learning to $81\\$ using few-shot learning. Figure \\ref{fig:panels}   (18.02) demonstrates few-shot learning. \n Given the original question, the nearest embedded question is found among the question that has been zero-shot. This question--code pair and the automatic input is given to Codex, resulting in generated code that correctly solves the problem.\n\n\\subsection*{Simulation}\nFigure \\ref{fig:panels} (18.05) shows an example from Probability and Statistics where the question is turned into a probabilistic programming task that generates simulations in order to compute an empirical statistic.\n\n\\subsection*{Manual Prompt Modification}\n\n\\paragraph{Question Tidying.}\n\nWhile $81\\$ of the question is automatically solved by zero-shot and few-shot learning, $19\\$ of the questions may require manual editing to be solved by Codex. These questions may be vague or contain redundant information (e.g., reference movie characters or current events) and require tidying to extract the essence of the question. Question tidying primarily involves removing redundant information, breaking down long sentence structures into smaller components, and converting prompts into a programming format.\n\n\\paragraph{Interaction for Visualization.}\nAnother form of manual prompting occurs when an answer involves a plot and requires multiple steps to generate a visually pleasing and clear plot. These special cases, which are among the remaining $19\\$ of the questions, require interactively prompting Codex until reaching the desired visualizations.\n\n\\subsection*{Automatic Explanation}\nExplanations are generated automatically using the question, the code generated by Codex when prompted with the question, and a prompt consisting of three quotes followed by the text ``Here is what the above code is doing:\n1.''. This prompt is given after both the question and the generated code since the code may be a lossy representation of the question. The result is a step-by-step explanation of the solution code given to Codex.\n\n\\subsection*{Question Generation and their Human Evaluation}\nWe also use Codex to generate new questions for each course. This is done by creating a numbered list of human-written questions from each class. This list is cut off after a random number of questions, and the result is used to prompt Codex to generate the next question. This process is repeated to create many new questions for each course.\n Questions may be generated based on the course topic, and given student performance may also be generated based on difficulty level.\n\nTo evaluate the generated questions, we survey MIT students who have taken these courses or their equivalents to compare the quality and difficulty of machine-generated questions with human-written questions for each of the courses.\\footnote{The IRB that approved the survey is MIT IRB Exempt Id E-3792. The survey was optional and included informed consent, with the following description: ``We are conducting a survey to assess the quality and difficulty of automatically generated questions for STEM courses. You will be presented with a series of blocks consisting of questions, either human-written (taken from an actual course) or generated with machine learning, but you will not be told the source of a given question. For each question, you will be asked (a) whether you think the question is human-written or machine-generated, (b) whether the question is appropriate for the given course, and finally, (c) how you would rate the difficulty of the question. Please carefully read each question and answer to the best of your ability''.}\nWe randomly sampled five original, human-written questions and five generated questions for each of the six MIT courses. Students are asked to read these ten questions per course in the survey, mixed and presented randomly. \n\nFor each of the 60 questions, the students are asked three survey questions: (1) ``Is the question human-written or machine-generated?'', (2) ``Is the question appropriate or not appropriate for the specific course?'', and (3) ``What is the question's difficulty level on a scale between 1 (easiest) and 5 (hardest)?'' An example of this survey format is given in Figure \\ref{fig:survey-question}. The students are asked to provide their ratings and not solve the questions. The survey is conducted online and anonymously.\n\n\\begin{figure*}[!htb]\n   \\centering\n   \\includegraphics[width0.8\\textwidth]{survey-question.pdf}\n   \\caption{Student survey example question: For each of 60 questions, students are asked if (1) the question is human-written or machine-generated, (2) the question is appropriate or inappropriate for the course, and (3) to rate the difficulty level of each question on a scale between 1 (easiest) and 5 (hardest).}\n   \\label{fig:survey-question}\n\\end{figure*}\n\n\\section*{Results}\n\\label{sec:results}\n\n\\begin{figure*}[!ht]\n   \\centering\n   \\includegraphics[width0.9\\textwidth]{survey-results.pdf}\n   \\caption{Student survey results: Panel A compares the level of difficulty of human-written questions and questions generated by our approach for each course based on the student ratings. The plot shows the means of the difficulty ratings between 1 (easiest) and 5 (hardest) and their $95\\$ confidence intervals. Panel B shows the percentage of human-written and machine-generated questions rated as appropriate and not appropriate for the course. Panel C shows the percentage of human-written questions rated as human-written or machine-generated (left) and the percentage of machine-generated questions rated as human-written or machine-generated (right).}\n   \\label{fig:survey-results}\n\\end{figure*}\n\n\\subsection*{Questions Solved}\nWe solve 265 questions, 213 of them automatically, as described in the Supplementary Information. These 265 questions include 25 randomly sampled questions from each of the seven courses (18.01/18.02/18.03/18.05/18.06/6.042/COMS3251) and 15 randomly sampled questions for each of the six topics in the MATH dataset (Prealgebra/Algebra/Number Theory/Counting and Probability/Intermediate Algebra/Precalculus). The breakdown of automatic solve rate by zero-shot and few-shot learning using Codex as compared with GPT-3 and GPT-3 with CoT is shown in Figure \\ref{fig:accuracy}. Programs involve step-by-step commands; therefore, CoT is inherent in programs.\n\n\\subsection*{Visualization of Embedded Questions}\nWe embed the 175 mathematics course questions onto a 2,048-dimensional space using OpenAI's \\textit{text-similarity-babbage-001} embedding engine, which captures semantic similarity between texts. We then use uniform manifold approximation and projection (UMAP) \\cite{mcinnes2018umap} to reduce the dimensionality of the 175 question embeddings to two. Figure \\ref{fig:umap}, the plot of these two dimensions, shows that the embedded questions are clustered by course topics. We see clusters of questions representing linear algebra from MIT's 18.06 Linear Algebra and Columbia's COMS3251 Computational Linear Algebra on the top right. On the left side, we see a collection of the questions representing calculus from MIT's 18.01 Single Variable Calculus, 18.02 Multivariable Calculus, and 18.03 Differential Equations. On the bottom right, we see a cluster of the questions from MIT's 18.05 Introduction to Probability and Statistics and 6.042 Mathematics for Computer Science, covering probability and statistics.\n\n\\begin{figure*}[!ht]\n   \\centering\n   \\includegraphics[width0.6\\textwidth]{MIT-math-Umap.pdf}\n   \\caption{Visualization of embeddings of course questions: We embed the course questions into a 2,048-dimensional space using OpenAI's \\textit{text-similarity-babbage-001} embedding engine, which captures semantic similarity between texts. We then use uniform manifold approximation and projection to reduce the dimensionality to two. This shows distinctive clusters based on topics. We see clusters of questions from MIT's 18.06 Linear Algebra and Columbia's COMS3251 Computational Linear Algebra on the top right. On the left side, we see a cluster of the questions from MIT's 18.01, 18.02, and 18.03. On the bottom right, we see a cluster of the questions from MIT's 18.05 Introduction to Probability and Statistics and 6.042 Mathematics for Computer Science, covering probability and statistics. }\n   \\label{fig:umap}\n\\end{figure*}\n\n\\subsection*{Automatically Generating New Questions}\nWe generate new questions for each course and topic by prompting Codex with numbered human-written questions to generate the next question automatically. Specifically, we create prompts of 25 randomly selected problems for which Codex generates correct answers, remove the questions after a randomly chosen question in the list, and have Codex complete the next new question. We present 130 new questions generated by Codex in the Supplementary Information to demonstrate this capability. These include ten new questions for each of the seven courses and each of the six MATH topics. Table~\\ref{tab:newall} shows one generated question for each class and MATH topic. Generating a question takes less than a second. We can generate an arbitrarily large number of questions, demonstrating that this is a practical and effective method for creating new course content. \n\n\\begin{table*}[!htb]\n\\small\n\\centering\n\\begin{tabular}{|l|p{3cm}|p{5.6cm}|p{5.6cm}|c|}\n\\hline\n\\textbf{ID} &\n\\textbf{Course} & \\textbf{Machine-generated question} & \\textbf{Most similar human-written question} & \\textbf{Similarity}\\\\\n\\hline\n1 & 18.01\\newline Single-Variable Calculus & Find the area of the region bounded by the curve and the x-axis. $y   x^2\\sin(x), 0\\leq x \\leq \\pi$ & Find the area of the region under the given curve from $1$ to $2$. $y   (x^2+1)/(3x-x^2)$ & $0.61$\n\\\\\n\\hline\n2 & 18.02\\newline Multi-Variable Calculus & Find $a \\times b$. $a   \\langle 9,-2,1 \\rangle$, $b   \\langle -2,1,1 \\rangle$ & Find $a \\times b$. $a   \\langle 5,-1,-2 \\rangle$, $b   \\langle -3,2,4 \\rangle$ & $0.87$\n\\\\\n\\hline\n3 & 18.03\\newline Differential Equations & Use the method of separable variables to solve the initial-value problem $\\frac{dy}{dx}   5e^x, y(2)12 \\text{ when } x2$ & Separate variables and use partial fractions to solve the initial value problems. Use either the exact solution or a computer-generated slope field to sketch the graphs of several solutions of the given differential equation, and highlight the indicated particular solution. \\newline $f'(x)3f(x)(5-f(x)), f(0)8$ & $0.21$\n\\\\\n\\hline\n4 & 18.05\\newline Introduction to Probability and Statistics & Let $X$ be a uniformly distributed random variable over the interval $[0, 1)$. Find $\\mathbb{E}[X^2]$ & Let $X$ be the result of rolling a fair 4-sided die. Let $Y$ be the result of rolling a fair 6-sided die. You win $2X$ dollars if $X>Y$ and lose 1 dollar otherwise. After playing this game 60 times, what is your expected total gain? & $0.29$\n\\\\ \n\\hline\n5 & 18.06\\newline Linear Algebra & Write a Matlab code to determine if the given matrix $A[1,1;4,4]$ is positive semidefinite and if it is negative semidefinite. & Find $A'A$ if the columns of $A$ are unit vectors, all mutually perpendicular. & $0.21$\n\\\\\n\\hline\n6 & 6.042\\newline Mathematics for\\newline Computer Science & A student is taking a test consisting of $n$ multiple-choice questions. Each question has five possible answers, and only one is correct. The student knows that the probability that any particular question is answered correctly is $\\frac{1}{5}$. Let $X$ be the number of questions answered correctly by the student. What is $\\mathbb{E}(X)$?   & \nMIT students sometimes delay laundry for a few days. Assume all random values described below are mutually independent. A busy student must complete three problem sets before doing laundry. Each problem set requires 1 day with probability $\\frac{2}{3}$ and 2 days with probability $\\frac{1}{3}$. Let $B$ be the number of days a busy student delays laundry. What is $\\mathbb{E}(B)$? & $0.47$\n\\\\\n\\hline\n7 & COMS3251\\newline Computational\\newline &\nFind a combination of the vectors $\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}$ that gives the vector $\\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix}$. & Find a combination of the vectors $\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}$ that give the zero vector. & $0.90$\n\\\\\n\\hline\n8 & MATH\\newline Pre-Algebra & How many four-digit positive integers are there with hundreds digit $2$? & How many four-digit positive integers are there with thousands digit $2?$ & $0.90$\n\\\\\n\\hline\n9 & MATH\\newline Algebra & Find the distance between the points $(0,0)$ and $(3,4)$. & Find the distance between the points $(0,4)$ and $(3,0)$. & $0.99$\n\\\\\n\\hline\n10 & MATH\\newline Number Theory & Find the smallest positive integer $n$ such that $n^2$ is divisible by $2^{10}$ and $n^3$ is divisible by $3^{10}$. & How many four-digit numbers whose digits add up to $9$ are divisible by $11$? & $0.25$\n\\\\\n\\hline\n11 & MATH\\newline Counting and Probability & How many ways are there to divide a set of $10$ objects into two sets of equal size? & Compute $\\dbinom{8}{4}$. & $0.12$\n\\\\\n\\hline\n12 & MATH\\newline Intermediate Algebra & Let $x$ and $y$ be positive real numbers such that $x^2 + y^2   1.$ Find the maximum value of $xy$. & Given that $x^2 + y^2   14x + 6y + 6,$ find the largest possible value of $3x + 4y$. & $0.59$\n\\\\\n\\hline\n13 & MATH\\newline Precalculus & Let $A$ be the matrix $\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}$ Find the determinant of $A^2 + A^3.$ & If $\\det(A)   2$ and $\\det(B)   12,$ then find $\\det (AB).$ & $0.41$\n\\\\\n\\hline\n\\end{tabular}\n\\vspace{8pt}\n\\caption{Examples of new questions generated automatically by Codex for each course and the most similar question from its course.}\n\\label{tab:newall}\n\\end{table*}\n\n\\subsection*{Student Survey Results}\nFifteen participants completed our survey, answering questions about all 60 questions, taking a median of 40 minutes. Figure \\ref{fig:survey-results} summarizes the results of the student survey comparing human-written and machine-generated questions. Panel A compares the difficulty level of human-written questions and the machine-generated questions for each course based on the student ratings. The plot shows the means of the difficulty ratings between 1 (easiest) and 5 (hardest) and their $95\\$ confidence intervals. Panel B shows the percentage of human-written and machine-generated questions rated by students as appropriate or not appropriate for the courses. Panel C shows the percentage of human-written questions rated as human-written or machine-generated (left) and the percentage of machine-generated questions rated as human-written or machine-generated (right).\n\nSummarizing the student survey results:\n\\begin{itemize}[itemsep0em]\n   \\item Survey participants rated our machine-generated and human-written questions to be similar in difficulty within confidence intervals.\n   \\item Survey participants rated human-written questions slightly more appropriate for the courses than machine-generated ones.\n   \\item Survey participants rated human-written questions more likely to be human-written as shown on the left side of Panel C. Survey participants rated machine-generated questions equally likely to be machine-generated and human-written as shown on the right side of Panel C.\n\\end{itemize}\n\n\\subsection*{Human Level}\nWith our methodology, Codex reaches human performance levels in both the contexts of solving existing questions and generating new content. We achieve $81\\$ automatic accuracy in solving mathematics course problems at MIT and Columbia, comparable to typical student performance on these problem sets in our MIT and Columbia University courses. Furthermore, we automatically generate new questions that are indistinguishable to students from human-written course questions.\n\n\\subsection*{Implementation Details}\nWe make our data and code publicly available.\\footnote{Data and code: \\href{https://github.com/idrori/mathQ}{https://github.com/idrori/mathQ}}\nWe use the latest version of OpenAI's GPT-3 \\textit{text-davinci-002} and Codex \\textit{codex-davinci-002} engines for all of our experiments. We fix all Codex's hyperparameters to be the same for all solution and explanation experiments to yield deterministic and reproducible results. Specifically, top P, which controls diversity, is set to $0$ and sampling temperature, which controls randomness, is also set to $0$. The frequency and presence penalties are set to $0$, and we do not halt on any stop sequences. We allow diversity and randomness for all new question generation experiments by setting the top P and temperature to $0.1$. Each prompt is structured as a Python documentation comment surrounded by triple quotations and line breaks. We evaluate the solution by running the generated program using a Python interpreter. Evaluations are considered correct if the printed output or the value returned by the generated program is the correct solution.\n\nFew-shot learning prompts are structured as follows: for each question--code examples being used, we insert the question in a docstring on the following available line, have a line break, and then insert the code on the following lines. After all the examples, we insert the target question at the end in the same way as described above and prompt Codex.\n\nChain of thought (CoT) prompts for GPT-3 are implemented by adding the text ``Let's think step by step.'' \\cite{kojima2022stepbystep} after the few-shot questions and answers, and the new question.\n\n\n Lean is a functional programming language for mathematical proofs. We handle proofs by few-shot learning a lean program corresponding to the question using question--lean pairs. Once we have the corresponding lean program, we break it down into pairs of statements and prove each pair by generating proof tactics.\n\n\n\\subsection*{Types of Problems the Model Cannot Solve}\n\\label{subsec:challenges}\nThere are a few different types of problems the model is incapable of solving: (1) any problem for which the question is in the form of an image or other non-text modality; (2) questions with solutions that require proofs; and (3) problems that are computationally intractable, such as factoring very large primes. This last category is not expected in any math course assignment, as students themselves would also be unable to answer them. That being said, many questions that students can answer have generalizations that are computationally intractable.\n\n\\section*{Conclusion}\n\\label{sec:Conclusion}\nWe demonstrate that few-shot learning and program synthesis using OpenAI Codex is able to solve, explain, and generate university-level mathematics problems at a human level. In contrast, previous methods using transformers only pre-trained on text, such as GPT-3, fail on these tasks. We verify that our strong results are not overfitting the training data by solving a new course that is not available online. We also generate and analyze new problem sets. The success of this work confirms that programs serve as a good representation and computation environment for solving math problems. Since our approach requires no additional training, it is easily scalable. This work addresses significant pedagogical challenges, bringing substantial benefits to higher education like curriculum design and analysis tools and automatic content generation.\n\nWe show that neural network synthesis with modern programming languages is more dynamic and widely applicable than expression trees and likely solves a broader range of problems. Although any finite computation could be expressed as a sufficiently large expression tree, one may see an arbitrarily large expansion in the size of the expression tree needed, as opposed to a Turing-complete language. This flexibility is bolstered by the massive corpus of existing programs, which eclipses the number of labeled expression trees available. Program outputs are also inherently more human-readable, as the ability to use abstraction, modularity, and high-level logic leads to more explicit illustrations of the path to a solution. Furthermore, program synthesis can convey logical deductions directly through explanatory comments and function and variable names. In particular, we see such descriptive text and derivations in a number of the Codex outputs. The unification of such formal and informal language is an inherent advantage of our methodology. We emphasize that the results may be complex and multi-modal. For example, by using packages such as Matplotlib, we can produce graphs of equations. This advanced and unique ability is time-consuming for humans and offers a significant pedagogical benefit. \n\nIn summary, we automatically solve, explain, and generate university-level mathematics course questions in real-time at a human level. Students rated machine-generated questions as equally likely to have been human-written as machine-generated. Students also rated machine-generated questions as similarly difficult to human-written questions and most appropriate for their respective courses. Finally, we have succeeded in scaling up this work to over thirty STEM courses across 13 departments in science and engineering schools at MIT and Ivy League universities, with excellent results.\n\n\n\\bibliography{bibliography}\n\n\\onecolumn\n\\appendix\n\\tableofcontents\n\\input{solution-18.01}\\newpage\n\\input{solution-18.02}\\newpage\n\\input{solution-18.03}\\newpage\n\\input{solution-18.05}\\newpage\n\\input{solution-18.06}\\newpage\n\\input{solution-6.042}\\newpage\n\\input{solution-COMS3251}\\newpage\n\\input{solution-MATH-Prealgebra}\\newpage\n\\input{solution-MATH-Algebra}\\newpage\n\\input{solution-MATH-Number-Theory}\\newpage\n\\input{solution-MATH-Counting-and-Probability}\\newpage\n\\input{solution-MATH-Intermediate-Algebra}\\newpage\n\\input{solution-MATH-Precalculus}\\newpage\n\n\\input{generation-18.01}\n\\input{generation-18.02}\n\\input{generation-18.03}\n\\input{generation-18.05}\n\\input{generation-18.06}\n\\input{generation-6.042}\n\\input{generation-COMS3251}\n\\input{generation-MATH-Algebra}\n\\input{generation-MATH-Counting-and-Probability}\n\\input{generation-MATH-Intermediate-Algebra}\n\\input{generation-MATH-Number-Theory}\n\\input{generation-MATH-Prealgebra}\n\\input{generation-MATH-Precalculus}\n\n\\end{document}\n",
  "title": "A Neural Network Solves, Explains, and Generates University Math Problems by Program Synthesis and Few-Shot Learning at Human Level"
}
