{
  "authors": [
    "Fei-Tzin Lee",
    "Chris Kedzie",
    "Nakul Verma",
    "Kathleen McKeown"
  ],
  "date_published": "2021-11-27",
  "raw_tex": "\\section{Content Selection Model}\n\\label{app:content}\nAs our classification model we use a feedforward layer attached to a Graph Attention Network, or GAT \\cite{velickovic2018graph}, that uses a modified the scaled dot-product attention function of the Transformer architecture \\cite{vaswani-etal-attention} that is masked according to the undirected AMR graph structure. We use two layers, a single attention head, 256-dimensional intermediate representations, and the ReLU activation function. We implement our model in PyTorch. Node parameters in the GAT layers are initialized from a unit normal distribution; the classification layer is initialized with the PyTorch default.   FL: ...I actually don't know what this is. oh well\n in the future, i have found that using an initializer is super helpful! you live you learn you retrain \n FL: yeah it definitely helped a lot for the GAT, i don't know why i didn't think to try it for the feedforward layer. ah well. next time\n FL: live, learn, get a restraining order against neural nets,\n\nThe input representations for each node consist of the summed word embeddings for all words in its concept label, as well as four discrete features: in-degree, out-degree, sentence index and number of occurrences in the text. The representation of every node at each graph layer is recalculated as an attention-weighted sum over the representations of itself and its neighbors in the AMR graph. The final node representation is passed to the single-layer feedforward classification component.\n\nWe train our classifier with the Adam optimizer with a learning rate of .001 and batch size of 1 for a maximum of 128 epochs, using an early stopping routine that halts training after training loss decreases by any amount between epochs or development loss has not increased within the last three epochs.\n\nUsing Transformer-style self-attention (masked to respect the AMR graph), the learnable parameters for the GAT are a projection matrix for keys, queries and values for each layer. As we use two layers, an intermediate representation size of 256, and 304-dimensional inputs, the total number of parameters for the GAT is therefore $2 \\times (256 \\times 304 + 304 \\times 256 + 304 \\times 304)$. The binary classification layer has $304 \\times 2$ parameters.\n\nA single training run of this model over the full proxy report dataset typically runs for around 30 epochs (with early stopping) and takes approximately three minutes on a single Tesla V100 GPU.",
  "title": "An analysis of document graph construction methods for AMR summarization"
}
