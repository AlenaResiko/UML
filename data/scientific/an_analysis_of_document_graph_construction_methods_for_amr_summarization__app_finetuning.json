{
  "authors": [
    "Fei-Tzin Lee",
    "Chris Kedzie",
    "Nakul Verma",
    "Kathleen McKeown"
  ],
  "date_published": "2021-11-27",
  "raw_tex": "\\section{Finetuning setup for BART generator}\n\\label{app:finetuning}\n\\fnote{added for EMNLP.}\n\nFor each merge strategy, we start with a vanilla pretrained BART-large model \nand finetune it for four epochs \\fnote{specify other hyperparameters?}\nwith a learning rate of .00003\non the generation task where the input is linearized selected AMR and the output is the summary text. (We originally considered a range of up to 64 epochs, but found that validation performance according to automatic metrics \\fnote{specify?}\npeaked before epoch 10 in all cases, usually between epochs 3-5.) \\cknote{For the future you would typically do early stopping per model on the validation set in case one model benefits from slighty more or less epochs. I think this is ok for now as the summary generation is not the main focus.}\n\nThe linearized AMR input is generated as follows: given a merged document graph and the output of a node selection model on that graph, i.e., a binary label for each node of the graph indicating whether that node is to be included in the summary or not, we linearize each sentence in sequence, keeping track of all nodes that have been seen before to avoid repetition. To linearize each sentence, we start at the original root node of that sentence (i.e., one of the children of the artificial root node of the document graph) and perform a depth-first traversal from that node. Nodes that we touch that are labeled 1 are added to the sequence; nodes that we touch that are labeled 0 are not added to the sequence unless they have a descendant labeled 1. Nodes that we have already touched are added to the sequence again, but their subtrees are not traversed again.\n\\fnote{do i need to write up another algorithm for this?} \\cknote{I think what you have here is good but if there's time add an algo -- definitely for the final draft.}\nThe final sequence is converted to a string in PENMAN format.\n\nWe present the pseudocode for the linearization procedure in Algorithm \\ref{alg:linearize}.\n\n\\begin{algorithm}\n\\SetAlgoLined\n\\KwIn{A merged document graph $G   (V, E)$; labels $L$ for every node $n \\in V$, with $L(n) \\in \\{0, 1\\}$; original sentence roots $r_1, r_2, ..., r_s \\in V$.}\n// Initialize sequence\\\\\nseq $\\gets []$\\\\\ntouched $\\gets []$\\\\\n\\For{$i \\in [s]$}{\n   // Traverse subtree of sentence root in depth-first order\\\\\n   \\For{$n \\in$ descendant subtree of $r_i$}{\n   \\uIf{n $\\in$ touched}{\n   append $n$ to seq\\\\\n   skip recursion and return to parent\n   }\n   \\uElseIf{L(n)   1 or a descendant of n is labeled 1}{\n   append $n$ to seq\\\\\n   continue recursion in depth-first order\n   }\n   \\uElse{\n   skip recursion and return to parent\n   }\n   }\n}\n\\Return{seq}\\\\\n\\caption{AMR linearization}\n\\label{alg:linearize}\n\\end{algorithm}\n\n\n\\section{Sample generated summaries}\n\\label{app:generation}\n\\fnote{maybe i should merge this with the finetuning section?? it feels like we have a second paper on generation in the appendix at this point} \\cknote{:) Yeah I think you could merge for sure.}\n\nKM shoudl elaborate on this. I am. \nAlthough it is not the focus of our work, we generate text summaries from the subgraphs selected in the node selection phase with combined merging.\n\nFor each merge strategy, we trained BART on linearized AMR for its selected subgraphs and the associated text summaries, and used this model to generate a text summary for each document. We provide examples of a few such summaries using the combined merge strategy in \\autoref{tab:generated}.\n\n\\begin{table*}[]\n   \\centering\n   \\begin{tabular}{p{7cm}|p{7cm}}\n   \\toprule\n   Gold summary & Generated summary output\\\\\n   \\midrule\n   Afghanistan's counter-narcotic police have confiscated 1.2 metric tons (1.3 tons) of opium during a raid. In 2007 Afghanistan accounted for 93 percent of the world's opium supply. & Afghanistan's Interior Ministry announced the confiscation of more than a ton of opium from a drug trafficker and the detention of a drug smuggler in the southernmost province of Helmand. The drug was smuggled into Afghanistan from Pakistan.\\\\ \\hline\n   Russia proposes cooperation with India and China to increase security around Afghanistan to block drug supplies. Afghanistan is 1 of the world's major opium producers and supplies Western markets through transit countries such as Russia. & Russia's Foreign Minister Lavrov stated that the Russian government will increase security at the borders with Afghanistan and Tajikistan in order to decrease the flow of illegal drugs from Afghanistan to the world market. The Russian government stated that Afghanistan is the world's largest producer of opium and the largest supplier of heroin to Russia.\\\\ \\hline\n   Russian Federation President Dmitry Medvedev promised on May 15, 2008 to provide funding for Russia's nuclear missile program. & Russian President Dmitry Medvedev made his first public appearance since being sworn in as the new President of the Russian Federation on November 9, 2008. The Russian President stated that the Russian military is ready to respond to any threat from the West.\\\\ \\hline\n   \\bottomrule\n   \\end{tabular}\n   \\caption{Sample generated summary output from BART given our selected content as input.}\n   \\label{tab:generated}\n\\end{table*}",
  "title": "An analysis of document graph construction methods for AMR summarization"
}
