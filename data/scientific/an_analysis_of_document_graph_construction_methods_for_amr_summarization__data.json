{
  "authors": [
    "Fei-Tzin Lee",
    "Chris Kedzie",
    "Nakul Verma",
    "Kathleen McKeown"
  ],
  "date_published": "2021-11-27",
  "raw_tex": "\\section{Data}\n\\subsection{AMR Corpus}\nWe use release 3.0 of the AMR annotation corpus \\cite{amrbank}, which consists of sections from a variety of domains including blogs, Aesop's Fables, Wikipedia, and newswire text. We specifically focus on the proxy report section, which is the only section in the corpus with AMR for document-summary pairs rather than standalone documents. Each document in this section is a news article paired with a human-written abstractive summary, or ``proxy report\". As in the other sections of the corpus, both documents and summaries are annotated with AMR graphs for each sentence.\n\nThe proxy section contains 298 document-summary pairs in the training set, one of which we discard because the associated summary is empty; 35 in the development set; and 33 in the test set. The average document length is 16.9 sentences, with a standard deviation of 9.1; the average summary length is 1.6 sentences.\n\nThe AMR graph for each sentence is a directed graph with labels on nodes and edges. Every node in the graph is associated with a concept that may be either a noun or a verb sense, and is assigned a text label representing that concept. Edges are directed according to the relation they represent, and are labeled with one of a fixed set of relation types between nodes.\n\n\\subsection{Annotations}\n\\label{sec:annotations}\nOur annotations are pairings between document and summary nodes labeling co-reference between those nodes.\n\nWe annotated 50 document-summary pairs in total, including the full 33 documents from the test set as well as an additional 17 from the development set. We recruited ten students in total as annotators, all undergraduate or graduate students with a background in NLP, nine of whom are native English speakers. Each document was assigned to two annotators who first performed the annotation task independently, then worked together to adjudicate disagreements. A sample of the final adjudicated labels and the IDs of the associated documents in the original corpus are provided as supplementary data. We include further annotation details in \\autoref{app:interface}, and full guidelines for the annotation task in the supplementary material.\n\n\\paragraph{Agreement and adjudication}\nAs traditional agreement metrics such as Cohen's $\\kappa$ are not applicable to our setting, which may be viewed as a binary multi-label classification problem, we instead compute two set-based metrics for annotator agreement over the set of nodes which received at least one alignment from either annotator. The first is based on exact match: the label sets given by two annotators are considered a match if they are precisely identical. The second is the softer notion of Jaccard similarity between the two label sets.\n\nThe preliminary round of annotations achieved .487 exact match agreement and .511 average Jaccard similarity, both of which are far higher than the expected values from random annotation, given that each document node may be aligned to on the order of 20-30 summary entities. However, as our annotations are designed to be a relatively small but high-quality evaluation set, we performed an adjudication round in which the pair of annotators assigned to each document worked together to resolve any disagreements, yielding a finalized set of adjudicated gold labels for each node.",
  "title": "An analysis of document graph construction methods for AMR summarization"
}
