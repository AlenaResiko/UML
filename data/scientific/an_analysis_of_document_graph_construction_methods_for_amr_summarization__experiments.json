{
  "authors": [
    "Fei-Tzin Lee",
    "Chris Kedzie",
    "Nakul Verma",
    "Kathleen McKeown"
  ],
  "date_published": "2021-11-27",
  "raw_tex": "\\section{Experiments}\n\\label{sec:experiments}\nIn our experiments we evaluate the merge methods laid out in the previous section. We additionally consider an unmerged baseline where sentences are simply joined at the root by an artificial root node.\n\nWe evaluate these merge styles in two ways: we compare the proposed merge clusters directly against those induced by our human annotations using coreference metrics (node merging); and we evaluate the summary nodes predicted by a classifier trained on each style of merged graph against our annotated gold alignments with precision, recall and F-measure over the unmerged sentence nodes aligned to reference summary   nodes (node selection).\n\n\\subsection{Data handling and merging}\nWe use the TAMR aligner \\cite{liu-etal-2018-amr} to generate node-to-text alignments. We use the Penman \\cite{goodman-2020-penman} and NetworkX packages in Python to read and store the resulting AMR graph structures, respectively. For text coreference we use SpanBERT-large \\cite{joshi-etal-2020-spanbert}.\nRegardless of merge method, we also join sentences at the root with an artificial root node when forming the document graph.\n\n\\subsection{Evaluating merge clusters}\nEach merge strategy produces a partition of the\nsentence-level AMR graph. Given this partition, we may consider the associated co-reference cluster for each node of the graph to be the set of other nodes which occur within the same element of the partition, and can evaluate these clusters against those we derive from the annotations.\n\nAlthough our collected annotations are for document-summary alignment rather than document merging, it is simple to infer merge clusters from these alignments: every document node can be said to be co-referent with all other document nodes that are aligned to one of the same summary nodes. In this manner we can collect a set of gold merge clusters that we may evaluate our automatic clusters against. We note that this approach does mean that any nodes not present in the summary will be treated as unmerged, but for summarization purposes we are only interested in \nsummary-aligned nodes.\n\n\\input{img/tab_merge}\n\nAlthough the co-reference task we are interested in is over nodes rather than text, it shares with text co-reference the objective of determining which semantic objects appear in the same cluster. We thus evaluate these clusters using two textual co-reference metrics, $B^3$ \\cite{Bagga98algorithmsfor} and LEA \\cite{moosavi2016lea}, implemented over our AMR data structure. We do not use importance weighting for the latter.\n\n\\subsection{Node selection}\nIn the node selection phase, the merged or unmerged document graph produced in the previous step is given as input to a node classifier which learns to predict a binary label for each node indicating whether it should be present in the summary. Here we follow prior work by using label-based alignment between documents and summaries to generate the training labels - that is, a document node is assigned a ``gold\" label of 1 for training purposes if there is at least one node in the summary with the same concept label, and 0 otherwise. This is an imperfect method for the same reasons that apply to the merging stage; while these noisy labels may be sufficient for training, we therefore need an external benchmark to objectively evaluate content selection with respect to the true summary.\n\nThus, we use our annotations as gold standard here as well: we consider the true label of a node to be 1 if it is aligned to at least one node in the summary in our annotations, and 0 otherwise. To avoid introducing confounding effects by counting merged nodes once in a merged graph but multiple times in the unmerged graph, we propagate the labels predicted on each node in a merged document graph back to the original sentence nodes that formed it, and compute precision, recall and F-measure against the gold labels induced by the alignments over the unmerged nodes.\n\nWe use the same GAT architecture and training procedure to perform node classification on all merge strategies. The details of both are described in \\autoref{app:content}.\n\n\\subsection{Generation}\nFinally, to generate summary text from the nodes selected with each merge strategy, we linearize the extracted nodes of the graph into a sequential format. This linearized AMR is then passed to a pretrained BART-large model which has been finetuned to generate the summary text from such linearized input. We include further details on the linearization and finetuning procedure in \\autoref{app:finetuning}.\n\n\nWe compare the generated output from each merge strategy using three kinds of automatic metrics (ROUGE \\citep{lin2004rouge}, METEOR \\citep{banerjee-lavie-2005-meteor}, and MoverScore \\citep{zhao-etal-2019-moverscore}), as well as with a human evaluation designed to investigate the influence of our proposed merge strategies on person-related output, ranking each set of summaries on three criteria (fluency, salience and faithfulness with regards to information about humans). We provide sample summary outputs in \\autoref{app:finetuning}, and report details of the human evaluation setup in \\autoref{app:humaneval}.",
  "title": "An analysis of document graph construction methods for AMR summarization"
}
