{
  "authors": [
    "Fei-Tzin Lee",
    "Chris Kedzie",
    "Nakul Verma",
    "Kathleen McKeown"
  ],
  "date_published": "2021-11-27",
  "raw_tex": "\\newcommand{\\concepts}{\\mathcal{V}}\n\\newcommand{\\relations}{\\mathcal{E}}\n\\newcommand{\\snodes}{V}\n\\newcommand{\\sedges}{E}\n\\newcommand{\\innodes}{V_:}\n\\newcommand{\\inedges}{E_:}\n\\newcommand{\\doc}{X}\n\\newcommand{\\dsize}{n}\n\\newcommand{\\mergestrat}{M}\n\\newcommand{\\inspace}{\\mathcal{X}}\n\\newcommand{\\outspace}{???}\n\\newcommand{\\dnodes}{\\Lambda}\n\\newcommand{\\dedges}{\\xi}\n\\newcommand{\\mnode}{\\nu}\n\\newcommand{\\cluster}{C}\n\\newcommand{\\sumnodes}{S}\n\\newcommand{\\sumedges}{R}\n\\newcommand{\\nlabel}{y}\n\\newcommand{\\prednlabel}{\\hat{\\nlabel}}\n\\newcommand{\\clf}{f}\n\\newcommand{\\params}{\\theta}\n\n\\section{Methods}\n\\label{sect:methods}\n\nFollowing \\citet{liu-etal-2015-toward-abstractive}, we model AMR summarization as a three\nstage pipeline (depicted in \\autoref{fig:pipeline}) of \\textit{(i)} sentence graph combination, \\textit{(ii)} content selection, and \\textit{(iii)} summary text generation.\n\n\\paragraph{Sentence Graph Combination} The input to the pipeline is an ordered sequence of $\\dsize$ disjoint \nAMR graphs,   $\\left(\\snodes_1, \\sedges_1\\right),\n\\ldots,\n\\left(\\snodes_\\dsize,\\sedges_\\dsize   \\right)$, where $\\snodes_i$ and $\\sedges_i$\nare the set of nodes and edges respectively of the AMR graph representation of sentence $i$.\\footnote{Technically, the $\\snodes_i$ are multi-sets since they \ncan contain two or more   distinct instances of the same concept (this happens in \\autoref{fig:merge}, where there are multiple instances of \\textit{person}, \\textit{country}, and other concepts).} A node $v \\in \\snodes_i$ corresponds to an instance of an AMR concept and an edge $(v,v^\\prime)$ exists in $\\sedges_i$ if $v^\\prime$ occupies a role in the sentence with respect to $v$ (e.g., if $v^\\prime$ is the ARG0 of $v$). In this stage of the pipeline, the disjoint sentence graphs are connected into a connected graph $\\left(\\dnodes, \\dedges \\right)$ representing the semantics of the entire document. We describe several ways of automatically performing sentence graph merging in \\autoref{sec:merging}. \n\n\\paragraph{Node Selection} We develop a model to identify a summary subgraph $\\left(\\sumnodes, \\sumedges\\right) \\subseteq \\left(\\dnodes, \\dedges\\right)$ corresponding to the AMR graph of the summary text. We treat this as a node-level binary classification task, predicting for each node $v\\in\\dnodes$ whether to include it in $\\sumnodes$ or not. Noisy training labels for this task are inferred from the summary graphs by assigning a label of 1 to a document node if there is at least one summary node with the same concept label, and 0 otherwise. We train a graph attention network (GAT) \\cite{velickovic2018graph} to perform the classification. The edges of the summary subgraph are determined implicitly by the summary nodes (i.e., $\\sumedges   \\left\\{(v,v^\\prime) \\in \\dedges | v,v^\\prime \\in \\sumnodes   \\right\\}).$ Additional details about the GAT model can be found in \\autoref{app:content}.\n\n\\paragraph{Summary Text Generation} \nFinally, a natural language generation (NLG) model is tasked with \nmapping $\\left(\\sumnodes, \\sumedges\\right)$ to a natural language summary. We fine-tune BART \\citep{lewis-etal-2020-bart} on the training split of the proxy corpus to generate summary text from a linearization of the selected nodes.\n\n\\subsection{Sentence Graph Combination}\n\\label{sec:merging}\n\n\\input{img/fig_example}\n\nWe assume each sentence of each input document is annotated with its own AMR graph.\\footnote{In this work we use the gold AMRs graphs provided by the AMR corpus but they could in principal be provided by an AMR parser.} This means that while multiple references to the same instance of a concept are annotated as such within sentences (e.g., if the same person is mentioned multiple times in a sentence,\nthey will only have a single \\textit{person} node in the corresponding AMR graph), \nthey are not annotated for co-reference across sentences.\n\nPerforming content selection directly \non this disjoint collection of AMR graphs is not\nideal. First, under a strict interpretation of AMR, distinct nodes correspond to distinct instances, which forces the content selection model to consider each mention\nof a concept in isolation, making it difficult to\ncapture how the same instance of a concept might\nparticipate in many other events in a document. \n\nWe hypothesize that an ideal document AMR \ngraph would contain a single node \nfor each distinct instance of a concept mentioned\nin the sentence AMR graphs. In other words, \nnodes that were co-referent across sentence AMRs\nwould be ``merged'' into a single node in the document AMR. This merged node would inherit the incoming and outgoing edges to \nother concepts in the original AMRs, and   thus\nfrequently occuring instances in the document \nwould take on greater graph centrality in the document AMR.\nCentrality is a useful feature for summarization using lexically constructed graphs \\cite{erkan04lexrank}, and we suspect this to hold for semantically constructed graphs as well.\n\nWe explore three methods of merging nodes to produce a document AMR graph, which we refer to as Concept Merging, Person Merging, and Combined Merging.\\footnote{We also considered evaluating the merge strategy outlined in \\citet{dohare-etal-2018-unsupervised}, but were unable to retrieve the node merges from their published code, and so we do not compare against their method in this paper.}\n\n\\paragraph{Concept Merging}\n\\citet{liu-etal-2015-toward-abstractive} merge any sentence-level nodes into a single node in\nthe document graph if they share the same AMR concept regardless of whether they are different instances. \nFormally, \n\\citeauthor{liu-etal-2015-toward-abstractive} partition the sentence nodes into disjoint sets $\\cluster_j$ such that $ \\bigcup_{i1}^\\dsize \\snodes_i   \\bigcup_{j1}^m \\cluster_j$ and\n$\\operatorname{concept}(c)   \\operatorname{concept}(c^\\prime)$ for\nany $c,c^\\prime \\in \\cluster_j$.\nFor each $\\cluster_j$, a single node $\\mnode$ of the same concept is created in   \n$\\dnodes$.   while the nodes in $\\cluster$ are not added to $\\dnodes.$ \nFor any $\\mnode_i,\\mnode_j \\in \\dnodes$ with corresponding concept clusters $\\cluster_i$ and $\\cluster_j$ respectively, there exists an edge $(\\mnode_i, \\mnode_j) \\in \\dedges$\nin the document graph if there exists a pair of \nsentence nodes $c_i \\in \\cluster_i$ and   $c_j \\in \\cluster_j$ with an edge $(c_i, c_j) \\in \\bigcup_{k1}^n \\sedges_k$.\n\nThe primary flaw of this strategy is the fact that, with some \nexceptions (due to an initial step collapsing name and date nodes,\nwhich we elaborate upon in \\autoref{sec:error-analysis}),\nthe resulting document AMR has no way of distinguishing between multiple instances of the \nsame type of concept. For example, in many cases, nodes representing different instances of \nthe \\textit{person} concept will be merged across sentences without regard to whether those nodes actually refer to the same person or not. As an example, see the Concept Merging example in \\autoref{fig:merge} where two different people (the Indian and Chinese foreign ministers)\nare collapsed to a single person node.\n\n\\paragraph{Person Merging}\nOur first strategy for merging is inspired by the observation that many of the merge errors in concept merging involve incorrect merging of nodes that occur in the descendant subgraph of a specific instance of some entity -- for example, identifying nodes that are attached to a particular person. To address this issue, we take a person-focused approach to merging as follows: \n\\begin{enumerate}\n\\item Identify the subgraph consisting of all descendant nodes of every node representing the \\textit{person} concept.\n\\item Identify the text spans that correspond to each of these nodes using automatic AMR-to-text alignment.\n\\item Use a textual co-reference system to determine whether any of the text spans corresponding to each of any two sub-graphs are co-referent; and if so, determine those two sub-graphs to be co-referent.\n\\item For any two co-referent person sub-graphs, merge any nodes across the two sub-graphs with the same node label.\n\\end{enumerate}\n   We will refer to these graphs as ``person-merged.'' An example of person merging can be found in \\autoref{fig:merge}, where both distinct instances of the \\textit{person} concept along with their sub-trees are preserved.   \n\n\\paragraph{Combined Merging}\nOur final merge strategy combines both Concept and Person merging. In this joint strategy, we first perform Person Merging. We then perform concept-merging on all remaining nodes of the sentence graphs (but do not perform an initial name or date collapse), excluding any person nodes or their subtrees that have already been merged in the Person Merging phase.\nDetailed pseudocode for the combined merging method can be found in \\autoref{app:person}.",
  "title": "An analysis of document graph construction methods for AMR summarization"
}
