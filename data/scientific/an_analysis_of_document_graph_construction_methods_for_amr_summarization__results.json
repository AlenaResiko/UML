{
  "authors": [
    "Fei-Tzin Lee",
    "Chris Kedzie",
    "Nakul Verma",
    "Kathleen McKeown"
  ],
  "date_published": "2021-11-27",
  "raw_tex": "\\section{Results}\nIn this section we report results for node merging and node selection. For node selection, we report the average of precision, recall and F-score over five classifier training runs in order to control for random variation in the model initialization. For statistical significance   we use the Approximate Randomization Test \\cite{riezler05}.\n\n\\subsection{Node merging}\nWe report precision, recall and F-score for the $B^3$ and LEA metrics over the node clusters induced by person merging, concept merging and combined merging, as compared to the gold annotated clusters (see \\autoref{tab:merge_scores}).\n\nInterestingly, combined merging strongly outperforms both person and concept merging on nearly all metrics. While it is in line with expectations that cluster precision would be higher for our method than for concept merging, as it is more selective about which nodes may be merged, it is surprising that our method also yields substantially higher recall. We discuss this phenomenon in \\autoref{sec:analysis-person}.\n\n\\subsection{Node selection}\n\\input{tab_content_selection.tex}\n\nWe evaluate the unmerged and concept merging baselines, as well as both our combined merging strategy and person merging alone. Results are presented in   \\autoref{tab:content_selection}. Combined merging's performance in node clustering seems to effectively carry over to node selection, where it continues to outperform concept merging and person merging in both precision and recall.\n\n\\subsection{Generation}\n\\begin{table*}[]\n   \\centering\n   \\begin{tabular}{c|c c c c c}\n   \\toprule\n   Merge & ROUGE-1 & ROUGE-2 & ROUGE-L & METEOR & MoverScore \\\\\n   \\midrule\n   Unmerged & 0.339 & 0.098 & 0.230 & 0.250 & 0.240 \\\\\n   Concept & 0.284 & 0.066 & 0.210 & 0.175 & 0.211 \\\\\n   Combined & 0.305 & 0.071 & 0.210 & 0.180 & 0.224 \\\\ \\midrule\n   Seq-to-seq BART (CNN) & 0.417 & 0.237 & 0.330 & 0.288 & 0.390 \\\\\n   \\bottomrule\n   \\end{tabular}\n   \\caption{Scores on automatic metrics for each merge strategy, and the pretrained sequence-to-sequence BART-large model finetuned on CNN/DailyMail. Reported ROUGE numbers are f-scores.}\n   \\label{tab:generation}\n\\end{table*}\n\n\\begin{table}[]\n   \\centering\n   \\begin{tabular}{c|c c c}\n   \\toprule\n   Merge & Fluency & Salience & Faithfulness \\\\\n   \\midrule\n   Unmerged & 20 (8) & \\textbf{21 (8)} & 16 (6) \\\\\n   Concept & 21 (10) & 15 (6) & 15 (5) \\\\\n   Combined & \\textbf{22 (8)} & 18 (3) & \\textbf{17 (7)} \\\\\n   \\bottomrule\n   \\end{tabular}\n   \\caption{Number of test set documents for which each merge strategy was ranked first by at least one annotator (both annotators) on each criterion. The test set contains 33 documents, but the numbers do not sum to 33 because ties are permitted.}\n   \\label{tab:generation-human}\n\\end{table}\n\nWe report the performance of our basic finetuned BART models when given as input the linearized selected AMR from the unmerged, concept, and combined merge strategies on\na range of automatic metrics in \\autoref{tab:generation}.\\footnote{While we also provide the scores for the pretrained sequence-to-sequence BART model finetuned on CNN/DailyMail as a performance ceiling, we note that our AMR-to-text BART models are not intended to provide summary output competitive with end-to-end BART, but are rather a diagnostic tool to investigate the effects that the different merge strategies have upon the final generated output. Our AMR-to-text models were finetuned on an extremely small amount of data and should not be considered fully pretrained for this task; a significant amount of further work on tuning would need to be done in order to make a comparison between AMR-to-text and sequence-to-sequence BART models.} We report the performance of our finetuned BART models with the same three merge strategies under human evaluation for fluency, salience and faithfulness in \\autoref{tab:generation-human}.\n\nWhile BART outputs using the unmerged strategy appear superior under all automatic metrics, the human evaluation reveals a different array of strengths, in which unmerged outputs are rated the best in terms of salience the most often by far, but the combined merge strategy actually yields the most faithful outputs in general.",
  "title": "An analysis of document graph construction methods for AMR summarization"
}
