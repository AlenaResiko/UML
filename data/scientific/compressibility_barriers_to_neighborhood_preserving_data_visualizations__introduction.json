{
  "authors": [
    "Szymon Snoeck",
    "Noah Bergam",
    "Nakul Verma"
  ],
  "date_published": "2025-08-09",
  "raw_tex": "\nVisualizing a 10,000-point, 1,000-dimensional dataset in a two-dimensional plot is a bold pursuit. It is also a common practice across natural and social scientific research literature, from biology to economics to physics, where colorful UMAP and t-SNE plots have become a standard feature of data analysis \\citep{kobak2019art, dimitriadis2018t, han2021resource}. In light of the well-known impossibility of low-distortion metric embeddings in constant dimensions (see e.g.\\ Chapter 15 of \\citet{matousek2013lectures}), the putative theoretical justification of these extreme dimension reduction procedures is that they need not preserve all minutiae \nthe detail \nof the input data. Instead, the argument goes, highlighting only the most basic structures, like local neighborhoods of points or cluster partitions\n, is enough for most exploratory data analysis purposes. We study the conditions under which such structure-preserving embeddings are possible. \n\nWe demonstrate that, in many scenarios of practical interest, it is impossible to embed a point cloud\nin \\emph{any} metric space (let alone a Euclidean space) of constant dimension while preserving neighborhood structure. \\textcolor{red}{Surprisingly, the embedding dimension required to satisfy even our weakest notion of structure preservation (see Definition 1) has the same \\(\\log(n)\\) \\todo{which exact result is this?} scaling as the much stronger notion of approximate isometric embedding \\citep{matousek2013lectures}. Therefore, the conventional wisdom underlying the effectiveness of many data visualization methods is mistaken.}{\\color{red} [say something about *almost all* point clouds cannot be nicely embedded?]} \n\nThis result calls into question the efficacy of manifold learning methods (like Laplacian Eigenmaps, t-SNE, and UMAP), which construct low-dimensional embeddings   \\st{based on the similarity structure of the input nearest neighbor graph.} \\textcolor{red}{by trying to recreate the input nearest neighbor graph in the output space.} \nthat are meant to illustrate the key structures in the input.\n\n\\todo[inline]{Add some commentary inline with the following idea:It is well known that globally preserving the structure of graphs in an embedding requires $\\Theta(\\log n)$ dimensions. For example, \\cite{Matou\u0161ekchapter15} shows that approximately-isometric embedding graphs in Euclidean spaces requires $\\Omega(\\log n)$ dimensions. Thus data-visualization algorithms which embed into constant dimensions have no hope of preserving the global structure of the data. Instead, researchers justify the apparent success of data-visualization algorithms by arguing that they are able to preserve local structure which allows them to peusdo preserve global structure such as cluster or manifold structure. We show that even if we dramatically reduce the requirements on the embedding to only focus on preserving local structure (i.e. distances of length 1) and not even requiring recoverability, we still get bounds that are nearly $\\Omega(\\log n)$ \\textit{for nearly all input graphs}. Therefore, the conventional wisdom that preserving only local structure is much easier than global structure is mistaken. This result present an unconditional boundary for data visualizations since it disproves their theoretical justification that local structure can be preserved in low dimensional spaces. }\n\n\ngenerally work by (1) constructing a high-dimensional nearest neighbor graph of the input points and (2) finding a low-dimensional embedding which optimally respects the similarity structure of that graph.\n\\todo[inline]{What is local cluster structure? Also I feel like we should emphasize t-SNE and UMAP as algorithms that create high dim nn graphs and then try to preserve them in 2-3 dimensions.}\n\n\nOur analysis begins by re-framing low-dimensional data visualization in terms of embedding the neighbor graphs of the input point cloud into metric spaces. Let $V$ be a size-\\(n\\) set representing our data points of interest. Let $\\Gn(V)$ or simply $\\Gn$ for short denote the set of all $n$-vertex unweighted, undirected, simple graphs on \\(V\\). Let $(\\X, \\rho)$ be a target metric space of interest \\textcolor{red}{of interest} \nwith doubling dimension $d\\dim(\\X)$ (see Section \\ref{sec:prelims} for a definition). We think of a map $f:\\Gn \\rightarrow \\X^n$ as a \\textit{data-visualization algorithm} and \\(f(G)\\) as an \\textit{embedding} of a specific graph \\(G (V,E) \\in \\Gn\\) into $\\X$ via the algorithm \\(f\\).\nOut of convenience, for any \\(G (V,E) \\in \\Gn\\) and   $v\\in V$,\nwe write $f_G(v)$ to denote the point in $f(G)$ to which the vertex $v$ of $G$ is being mapped. If there is an edge between \\(u\\) and \\(v\\) in \\(G\\), we write \\(u\\sim_G v\\), or simply \\(u\\sim v\\) when clear from context \\st{if there is an edge between $u$ and $v$ in $G$}\n(symmetrically, we write \\(u\\not\\sim v\\) if there is no direct edge). \\todo{We can move definition of diameter to preliminaries as there is no longer a $\\beta$.}   \n\nWithin this framework, we study data visualization algorithms that are faithful to the neighborhood structure of input graphs, in the sense of keeping neighbors close and non-neighbors far. We define this desideratum below, and then study the minimum doubling dimension of the output metric space, \\(\\X\\), necessary to accommodate it. Since we are working with general metric spaces, we use the doubling dimension of \\(\\X\\), denoted \\(\\dim(\\X)\\), as our benchmark. \n\nWe consider an embedding faithful if it separates ``dissimilar'' vertices while keeping ``similar'' vertices close together. We model vertex similarity in terms of (i) edge-connectedness (denoted as \\(v\\sim w\\) for \\(v,w\\in V\\)) and (ii) path-connectedness (denoted as \\(v\\leftrightarrow{} w\\)). For \\(v,w \\in V\\) edge-connected, we write \\(v\\sim w\\). For \\(v,w \\in V\\) path-connected, we write \\(v\\leftrightarrow{} w\\). \n\n\\todo[inline]{Add recoverable definition.}\nMaybe first present recoverable definition or neighbors only definition. Then ease the reader into adding (3) to the definition. Basically explain the definition in words and why it makes sense before giving it to the reader, See Dasgupta's context before definition of seperate clusters in section 2.1 of Learning Mixture of Gaussians. \n\nOur notion of faithfulness to the input graph is defined as follows.\n\n\\begin{definition}\\label{def:preserve}\n   Fix $\\alpha \\geq 0$. A data visualization algorithm \\(f: \\Gn \\to \\mathcal{X}^n\\) is said to \\(\\alpha\\)-preserve \\(G\\in \\Gn\\) if there exists $rr_G > 0$ (the neighborhood threshold) such that for all distinct $u,v \\in V$,\n   \\begin{enumerate}\n   \\item[(1)] $ u \\sim v \\implies \\rho(f_G(v), f_G(w)) < r$, \\hfill (neighbor proximity)\n   \\item[(2)] $ u \\nsim v \\implies \\rho(f_G(v), f_G(w)) \\geq \\alpha\\cdot r $.   \\hfill (non-neighbor separation)\n   \\end{enumerate}\n   WITH \\(S\\) set-wise preservation\n   We say \\(f\\) \\(\\alpha\\)-preserves \\(S\\subseteq \\Gn\\) if it \\(\\alpha\\)-preserves each \\(G\\in S\\). \\todo{remove S}\n\\end{definition}\n\nNote that when $\\alpha \\geq 1$, one can recover the input graph \\(G\\) from the embedding \\(f(G)\\) by simply drawing an edge between any two vertices embedded within a distance of \\(r\\).   We call this special case of preservability \\textit{recoverability}, and we notice that, depending on the setting, this distinction can have a sharp impact on the difficulty of preservability. , and we find in the course of our study that, despite being qualitatively much stronger than preservability, it is not necessarily much harder to satisfy\\todo{is this still true? given new/refined results..}\nFurthermore, preservability (including the special case of recoverability) stands out from other notions of structure preserving embeddings, such as ordinal embeddings, low-distortion embeddings, etc., due to its strictly local nature; it only requires the neighbor structure to be preserved, while non-neighbor structure can be distorted arbitrarily. \nInterestingly, while it seems like the locality of \\(\\alpha\\)-preservation should make it easier to satisfy from a compressibility perspective, For example, an approximately isometric embedding of the shortest path metric of an $n$-clique requires $\\Omega(\\log (n))$ dimensions in Euclidean space \\citep{ALON200331, larsen2017optimality}, whereas, for any $\\alpha \\geq 0$, it takes 0 dimensions to $\\alpha$-preserve any clique graph since it can be embedded as a single point.\nwe find that \\(\\alpha\\)-preserving a typical graph even in general metrics is as hard as near-isometric embedding of points in \\(\\ell_2\\) (typically considered very rigid for embedding purposes) in that both require \\(\\Omega(\\log n)\\) dimensions \\citep{larsen2017optimality}. \n\nroughly as hard as finding a near-isometric embedding , in that both require \\(\\Omega(\\log n)\\) dimensions in general.\nHowever, the dimension lower bounds for preservability \\textit{in general metric spaces} have the same \\(\\Omega(\\log n)\\) scaling as \\(\\ell_2\\) dimension reduction (in the sense of Johnson-Lindenstrauss). \n\n\n\\textcolor{red}{It turns out that $\\alpha$-preservability is \\emph{not} a weaker criterion to satisfy and also requires $\\Theta(\\log(n))$ dimensions in general. More surprisingly, we find that this requirement jumps to   $\\Theta(n)$ dimensions for embeddings into normed spaces! <need a transition sentence to next para>}\n\n\nWe are interested in characterizing the minimum dimension necessary to \\(\\alpha\\)-preserve a graph\\textcolor{red}{or a set of graphs}   SET WISE\n. This is formalized in what we call the \\(\\alpha\\)-preservation dimension. \n\n SET WISE PRESERVATION \\todo[inline]{Remove $S\\subseteq \\Gn$ since we only ever use $\\dim_\\alpha(G,\\mathbb{X}).$}\n\\begin{definition}[\\(\\alpha\\)-preservation dimension]\n   Fix $\\alpha \\geq 0$ and $n \\in \\mathbb{N}$. Let $S \\subseteq \\Gn$ be nonempty and \n   $\\mathbb{X}$ be a collection of metric spaces of interest. The \\(\\alpha\\)-preservation dimension of \\(G\\) in \\(\\mathbb{X}\\) is given by\\footnote{In the degenerate case when no such \\(f\\) exists, \\(\\dim_\\alpha(G,\\mathbb{X})\\) is undefined. {\\color{red} Changed to infinite from undefined because then whenever $\\dim_\\alpha$ does not exist lower bounds hold true trivially and upper bounds are always constructive so doesn't mess that up either.}\n   }\n\\begin{equation}\\label{eq:alpha_pres}\n   \\dim_{\\alpha}(S, \\mathbb{X}) : \\min \\{ \\dim(\\X) \\ | \\ \\X \\in \\mathbb{X} \\text{ and there exists $f:S \\to \\X^n$ which $\\alpha$-preserves $S$}\\}.   \n   \\dim_{\\alpha}(G, \\mathbb{X}) : \\min \\{ \\dim(\\X) \\ | \\ \\X \\in \\mathbb{X} \\text{ and there exists $f:\\Gn \\to \\X^n$ which $\\alpha$-preserves $G$}\\}.   \n   \\end{equation}\n   In other words, \\st{it is the smallest dimension such that all graphs in $S$ are $\\alpha$-preservable in some metric space $\\X\\in \\mathbb{X}$.} \n   it is the smallest \\(d\\in \\mathbb{N}\\) such that there exists a metric space $\\X\\in \\mathbb{X}$ of doubling dimension \\(d\\) that $\\alpha$-preserves $G$. all graphs in $S$.\n   If $\\mathbb{X}$ is the collection of all possible metric spaces, we shorten the above to $\\dim_{\\alpha}(G)$.   , and if $S \\{G\\}$ (i.e.\\ a single graph), we write $\\dim_\\alpha(G)$.\n\\end{definition}\n\n\nOur notion of preservation dimension can be understood as a natural generalization of a well-studied graph invariant known as \\emph{sphericity} \\citep{MAEHARA198455}. The sphericity of a graph \\(G\\) is the minimum dimension \\(d\\) such that the vertices of \\(G\\) can be distinctly embedded into a \\(d\\)-dimensional Euclidean space such that the embedded vertices are distance at most \\(1\\) if and only if they are edge-connected. Preservation dimension relaxes this notion by the constraints of sphericity in the sense that it \n(1) parametrizing the separation between neighbors versus non-neighbors via \\(\\alpha\\) (see Figure \\ref{fig:neighbor_overlap}) (and therefore allows for non-neighbors to be closer than neighbors when \\(\\alpha < 1\\)) (2) allows for non-injective mappings of vertices, \nand (2) allowing for embeddings into general metric spaces. In developing this generalization of sphericity, we obtain a more fine-grained understanding of structure-preserving metric embeddings of graphs.   Unlike sphericity, our notion of visualization dimension parametrizes the distinction between neighbor versus non-neighbor separation via \\(\\alpha\\).\\todo{Big difference btw sphericity and preservation: Add explanation that non-neighbors and neighbors can exist in same area for $\\alpha <1$ and add back in the figure.} Furthermore, it allows for embeddings into arbitrary metric spaces rather than Euclidean space. Finally, and perhaps most importantly, it allows for us to quantify the difficulty of embedding families of graphs in a robust manner.\\todo{I do not see why this is a distinction between sphericity and a $\\alpha$-preservation since there are also results of that nature for sphericity.} \n\n\\begin{figure}\n   \\centering\n   \\includegraphics[width0.4\\linewidth]{img/preservation_ideal.pdf}\n   \\includegraphics[width0.4\\linewidth]{img/preservation_general.pdf}\n   \\caption{An \\(f\\)-embedding of a vertex \\(v \\in V\\). Left: non-neighbor and neighbor relations of \\(v\\) are recoverable by thresholding at \\(r\\) \\((\\alpha \\geq 1)\\). Right: the more relaxed case of preservation where overlap is allowed \\((\\alpha < 1)\\).\n   Ideally, we are embedding with $\\alpha \\geq 1$ which implies strict neighbor separation. However, strict separation may not always be possible hence we allow for $\\alpha \\leq 1$. Note that sphericity corresponds to $\\alpha   1$. \n   }\n   \\label{fig:neighbor_overlap}\n\\end{figure}\nHow easy it \\(\\alpha\\)-preservation? Consider the problem of embedding a complete graph. This can be done trivially by sending all vertices to the same point in \\(\\X\\). A graph with no edges, meanwhile, can be Ideally, one would like to preserve all neighbor and non-neighbor relations when embedding a graph. However, this may not always be possible so we introduce a weakened version of preservation where we allow some of the $(u,v)$ pairs to not be preserved:\n\n\\begin{definition}[approximate preservation]\\label{def:approxpreserve}   Fix $\\alpha \\geq 0$, $M \\in \\mathbb{N}$. We say that \\(G   (V,E)\\in \\Gn\\) is \\((\\alpha, M)\\)-preservable (or   \\((\\alpha, M)\\)-preserved by some $f: \\Gn \\to \\X^n$) if there exists $G'   (V, E')$ with $|E \\Delta E'| \\leq M$ that is $\\alpha$-preservable in $\\X$. \\todo{Need to define $\\includegraphics{img/alphabetagraph.pdf}$?}\\end{definition}\n\n\\pagebreak\n\nOur main results are as follows. \n\\begin{itemize}\n   \\item \\textbf{Preservation in General Metrics}. Though certain kinds of graphs are easily \\(\\alpha\\)-preserved in constant dimensions (e.g.\\ cliques, cycles, paths, etc.), we show that these ``easy'' graphs comprise a vanishing fraction of all graphs: an overwhelming fraction of \\(G\\in \\Gn\\) require \\(\\dim_\\alpha(G)   \\Omega\n   \\big(\\log(n) / \\log(\\frac{8}{\\alpha})\\big)\\), see Theorem \\ref{thm:NeighborPreservingMain_1}(i). Even if we consider only constant-degree regular graphs, a natural model for neighborhood connectivity, the situation is similar: an overwhelming fraction of such graphs require $\\Omega\\big(\\log(n)/(\\log (\\frac{\\log (n)}{\\alpha}))\\big)$ dimensions, see Theorem \\ref{thm:NeighborPreservingMain_1}(ii). We conclude with a full characterization of \\(\\alpha\\)-preservation for \\textit{all} constant-diameter graphs, see Corollary \\ref{cor:metric_ub_typical}, which highlights a key difference between the cases of recoverability and non-recoverability. \\todo{last sentence super awkward} \\todo{why is there a k?} dimensions to \\(\\alpha\\)-preserve, see Theorem \\ref{thm:NeighborPreservingMain_2}. These lower bounds are surprisingly similar to the more stringent case of near-isometric embeddings. These results are significant because, despite the fact that \\(\\alpha\\)-preservation is typically much easier than isometric embedding, the dimension lower bounds for both types of embedding are roughly the same.   Even a graph sampled from a stochastic block model (an approximate partition graph, in some sense) typically has visualization dimension scaling with \\(\\log n\\), with high probability. \n   \\item \\textbf{Preservation in Normed Spaces}. When we insist on embedding graphs into normed spaces, typical \\(G\\in \\Gn\\) become exponentially harder to \\((\\alpha>1)\\)-preserve, whereas typical \\(G\\in \\Gnk\\) . \n   the neighborhood recoverability landscape changes dramatically: an overwhelming fraction of \\(G\\in \\Gn\\) require ${\\dim_{(\\alpha >1)}(G)   \\Omega(n/\\log(\\frac{8}{\\alpha-1}))}$ in \\emph{any} normed space, see Theorem \\ref{thm:polyMain}. For Euclidean spaces, we can improve this to ${\\dim_{(\\alpha 1)}(G, \\ell_2)   \\Omega(n)}$, . Furthermore in $\\ell_2$, \n   and as \\(\\alpha\\) exceeds \\(1\\), a phase-change phenomenon occurs: below a certain threshold \\(\\alpha\\)-preservation can be achieved in dimension depending on the graph's spectral properties, and beyond this threshold \\(\\alpha\\)-preservation may not be possible, see Proposition \\ref{prop:chiGBasedUpperBound}. Meanwhile, \\(k\\)-regular graphs do not suffer from a \\(\\Omega(n)\\) recoverability barrier in normed spaces; \\(O(k^2 \\log n)\\) dimensions suffice even in \\(\\ell_2\\), see Proposition \\ref{prop:kregularUpperBound}.\n   \n   \n   becomes impossible past some threshold on \\(\\alpha\\) based on the graph's spectral properties (see Proposition \\ref{prop:chiGBasedUpperBound}). Fortunately, $k$-regular graphs fare better: $O(k\\log n)$-dimensional $\\alpha$-preservation is possible even in $\\ell_2$, see Proposition \\ref{prop:kregularUpperBound}.\n   \\textcolor{red}{talk about phase transition in $\\ell_p$ spaces and implications in Euclidean}\n   \\todo{complete properly}\n   for typical \\(G\\in \\Gn\\), \\(\\dim_\\alpha(G,\\mathbb{L})   \\Theta(n)\\), whereas for typical \\(G\\in \\Gnk\\) for \\(kO(1)\\), \\(\\dim_\\alpha(G,\\mathbb{L})   \\Theta(\\log n)\\). \n   See Theorem \\ref{thm:polyMain}.\n   Specifically,   \\((\\alpha \\geq 1)\\)-preservation for normed space\n   \\((\\alpha < 1)\\)-preservation continues to require \\(\\Omega(\\log n)\\) dimensions whereas \n   Specifically for \\(p\\in [1,\\infty)\\), an overwhelming fraction of graphs \\(G\\in \\Gn\\) require \\(\\dim_{\\alpha}(G, \\mathbb{L}_p)   \\Omega\\left({n}/{\\log\\left(\\frac{1}{\\log(\\alpha)/\\log(n)+1/p}\\right)}\\right)\\), where \\(\\mathbb{L}_p   \\{\\ell_p^d: d\\in\\mathbb{N}\\}\\), see Theorem \\ref{thm:family_lb}. Meanwhile,   \\(k\\)-regular graphs require \\(\\Omega( k\\log n /\\log(\\frac{\\log n}{\\alpha - 1}))\\), see Theorem \\ref{thm:family_lb}. \\todo{resolve with new polynomial result} We prove that, for \\(\\alpha > 1\\), the preservation dimension of a set of graphs in a normed space scales with the number of graphs being embedded. If we let \\(\\V\\) denote the set of all normed spaces, we show that \\(\\dim_{\\alpha}(\\Gn, \\V)   \\Omega(n)\\) while \\(\\dim_{\\alpha}(\\Gnk, \\V)   \\Omega(k\\log n)\\), where \\(\\Gnk\\) denotes the set of \\(k\\)-regular graphs. We find tight and near-tight upper bounds, respectively, for these quantities. \n   \\todo[inline]{Update below point to reflect no manifolds.}\n\\pagebreak\n   \\item \\textbf{Preservation of Clustered Data}. We study the preservation dimension of graphs generated from a planted partition model. We show that, with high probability, a neighborhood graph of points sampled $d$-dimensional submanifold requires $\\Omega(d / \\log(8/\\alpha))$ dimensions to \\(\\alpha\\)-preserve, see Corollary \\ref{XXX}. For data sampled from a \\(k\\)-cluster distribution, \n   We find that, with high probability, \\(\\alpha\\)-preservation requires \\(\\Omega\\Big(\\frac{ (1-\\xi)\\log(n) + \\xi\\log(k)   }{\\log (8/\\alpha)}\\Big)\\) dimensions in general metrics, where \\(\\xi\\) is a suitable measure of the cluster salience, see Theorem \\ref{thm:planted_partition}. Furthermore, if we insist on \\(\\alpha > 1\\), we are hit with a lower bound of \\(\\Omega(\\log(n)/ \\log(\\frac{1}{\\alpha - 1}))\\) \\(\\Omega(\\frac{\\log n}{\\log(\\frac{1}{\\alpha-1})})\\)\n   regardless of cluster salience. \n   \\todo{rephrase the whole para}\n   graphs sampled from two settings of practical interest: high-dimensional point cloud data and planted partition models. We show that a neighborhood graph of points sampled from a \\(d\\)-dimensional Euclidean subspace of \\(\\mathbb{R}^D\\) requires \\(\\Theta(d)\\). If even a small amount of full-dimensional noise is added to these points, then the visualization dimension jumps to \\(\\Theta(D)\\).\\todo{skip noise} These lower bounds continue to hold even if we allow for the output visualization to misrepresent a certain fraction of edges.\\textcolor{red}{We prove analogous results for data sampled from a \\(d\\)-dimensional submanifold. }\n\n\\end{itemize}\n\n\n\n\n\\textbf{Approximate Isometry Implies Preservation}: At its core, $(\\alpha, \\beta)$-preservation is a local form of structure preservation. For the sake of comparison, consider another form of structure preservation: approximate-isometry. In approximate-isometry, all distances, as measured by the \\textcolor{red}{shortest path metric}, must be approximately preserved in the embedding. Hence, information regarding the distance between far away points must be captured in the embedding so it is preserving the global structure of the graph. On the other hand, $(\\alpha, \\beta)$-preservation only requires that the smallest distances in the graph stay small and allows all other distances to be whatever so long as they are not too small. Thus, an $(\\alpha, \\beta)$-preserving embedding is only capturing the local information in the graph. \n\n\n\\textcolor{purple}{\nExperimental ``removing $r$'' definition:\n\\begin{definition} Fix $\\alpha \\geq 0, \\beta \\in [0,1]$. We say\n   \\(S\\subseteq \\Gn\\) is \\((\\alpha,\\beta)\\)-\\textbf{preservable} in dimension \\(d\\) if there exists a metric space $(\\X,\\rho)$ with $\\dim(\\X)   d$ and \\(f: \\Gn \\to \\mathcal{X}^n\\) such that for all $ G   (V,E) \\in S$ with connected components $G_1, \\dots, G_k$ and $\\forall v,u\\in V$:\n   \\begin{enumerate}\n   \\item[(1)] \\(v \\not\\sim u \\implies \\rho(f_G(v), f_G(u)) \\geq \\alpha \\cdot \\sup_{w \\sim z} \\rho(f_G(w), f_G(z)) \\) \\todo{Biggest issue is that on one hand is $\\sup$ over $\\sim$ and otherhand is $\\sup$ over $\\nleftrightarrow$}\n   \\item[(2)] \\(v \\nleftrightarrow u \\implies \\rho(f_G(v), f_G(u)) \\geq \\beta \\cdot \\sup_{w \\leftrightarrow z} \\rho(f_G(w), f_G(z)) \\) \n   \\end{enumerate}\n   If only $(1)$ holds then we say that $S$ is $\\alpha$-\\textbf{neighbor-preservable} in dimension $d$. If only $(2)$ holds then $S$ is $\\beta$-\\textbf{cluster-preservable} in dimension $d$.\n\\end{definition}}\n\nIf $\\alpha, \\beta   \\theta(1)$, we may omit $\\alpha, \\beta$ and in place just say \\textit{neighbor-preservable} or \\textit{cluster-preservable}.\n\n\n\n\n\\todo[inline]{Justify adding $\\diam(f_G(V))$ to definition 1 + Add discussion about recoverablility (plus add figure). Goal should be to prime the reader for $\\epsilon$-neighbors   recoverable and not $k-$nn   recoverable. In other words something like: a graph $G$ is recoverable if given an embedding $f(G) \\subseteq \\X$ and some $R > 0$, one can rebuild the graph from the embedding by connecting each point to all other points with distance less than $R$. Note this is equivalent to $\\alpha$-neighbor-preserving with $\\alpha \\geq 1$. Thus our notion of $\\alpha$-neighbor-preserving is in general strictly weaker than recoverablility.}\n\n\nThese results present a formidable barrier for existing data visualization algorithms, which typically embed input points in two- or three-dimensional Euclidean space. Per the lower bounds presented in this paper, such visualizations are doomed to misrepresent a portion of the neighborhood structure. This should be of great concern to practitioners who use these algorithms for data analysis and hypothesis generation with the expectation that they reliably reveal the neighborhood structure of high dimensional datasets. \\textcolor{red}{non-obvious \u2013\u2013 why not just messing up a single edge?}. \n \n\nHence, they cannot preserve the similarity structure of the input under any of the naturally desirable definitions in multiple scenarios. \n\n\n",
  "title": "Compressibility Barriers to Neighborhood-Preserving Data Visualizations"
}
