{
  "authors": [
    "Szymon Snoeck",
    "Noah Bergam",
    "Nakul Verma"
  ],
  "date_published": "2025-08-09",
  "raw_tex": "Our analysis of \\(\\alpha\\)-preservation brings insights and techniques from the graph embedding literature to bear on the problem of data visualization.\n\n\n\\subsection{Metric Embeddings of Graphs}\n\nOur analysis of \\(\\alpha\\)-preservation connects with and in some cases unifies various perspectives in the study of metric embeddings (of graphs) and data visualization.\n\n\\todo[inline]{SECTION NEEDS WORK: Things to comment on about preservation: unlike ordinal embeddings, approximate isometry, etc, it focuses only on preserving local information.}\n\nRepresenting graphs faithfully in metric spaces is of intense interest in computer science. There are different techniques and limitations for this endeavor depending on what metric one embeds into and what structure the embedding is supposed to preserve. For instance, if one seeks to embed a graph into Euclidean space in such a way that reflects its cluster structure, then spectral clustering is a standard choice \\citep{von2007tutorial}, and it provably preserves sufficiently prominent clusters in the input \\citep{ng2001spectral}. If, on the other hand, one seeks a low-distortion embedding (see Definition \\ref{def:distortion}) of some metric induced by a graph (such as the shortest path metric), one can use the famous result by \\citet{bourgain} that guarantees an \\(O(\\log n)\\)-distortion embedding into $O(\\log n)$-dimensional Euclidean space. If one insists on arbitrarily good average distortion-\\(D\\) embeddings   into normed spaces, \\citet{naor2021average} showed that \\(n^{\\Omega(1/D)}\\) dimensions is necessary in general, with constant-degree expanders providing the hard instances. The situation improves if one is willing to assume some intrinsic structure; indeed, if the input metric (derived from the graph) has doubling dimension $d$, then an $O(d)$-dimensional Euclidean embedding exists with $\\mathrm{polylog}(n)$-distortion \\citep{AbrahamIntrinsicDimension} and an $O(d\\log \\log n)$-dimensional Euclidean embedding exists with $o(\\log n)$ distortion \\citep{chan2010ultra}.\n\n then an $O(d\\log \\log n)$-dimensional Euclidean embedding exists with $o(\\log n)$ distortion \\citep{chan2010ultra}.\n\n\n\n\\textcolor{red}{\\st{There is} A more refined result \\st{of this flavor} in similar vein \\st{by xxx which} states that, if the input metric has doubling dimension \\(d\\), then it is embeddable in \\st{embeds into} \\(O(d\\log\\log n)\\)-dimensional Euclidean space with \\(o(\\log n)\\) distortion \\citep{chan2010ultra}.}\\todo{this doesnt transition well into the next sentence because euclidean space is a normed space and one is a positive result the other is negative. slight repharse is needed}\n\n\nOne could also seek out an embedding which preserves graph \\emph{neighborhoods}, in the sense that edge-connected vertices are mapped as neighbors and non-edge-connected vertices are mapped as non-neighbors (for some suitable sense of neighborhood). One of the first studies in the direction was by \\citet{erdos1965dimension}, who define the dimension of a graph as the minimum \\(d\\in\\mathbb{N}\\) such that there exists an injection \\(f:V\\rightarrow\\mathbb{R}^d\\) for which \\(u\\sim v \\implies \\|f(u)-f(v)\\|_2   1\\) for all \\(u,v\\in V\\). In this setting, the distances between non-edge connected vertices is irrelevant.\nOne can make the distinction between neighbor and non-neighbor either based on an absolute threshold (``radius neighborhood'') or a relative threshold (``\\(k\\)-nearest-neighbors''). \\todo{dont talk about these two cases} Our notion of \\(\\alpha\\)-preservation uses an absolute neighbor threshold.   Usually, two types of neighborhood relations are studied: (1) \\(\\epsilon\\)-radius neighborhoods and (2) \\(k\\)-nearest-neighborhoods. \nOne interesting line of work in this direction studies \\emph{sphericity}.\n\\citet{MAEHARA198455} later introduced a threshold-based notion of graph dimension known as sphericity, where the condition on the embedding becomes which is defined to be the minimum \\(d\\in \\mathbb{N}\\) such that there exists an injective map \\(f\\) from \\(V\\) to \\(\\mathbb{R}^d\\) where \n\\(u\\sim v \\iff \\|f(u)- f(v)\\|_2 < 1\\). Since Euclidean dimension is only a constant factor off from doubling dimension, it is not hard to see that \\(\\dim_\\alpha(G)\\leq O(\\sph(G))\\) for all \\(\\alpha \\leq 1\\). \n\\citet{MAEHARA198455} showed that for all \\(G\\in \\Gn\\) except the complete graph, \\(\\sph(G)\\leq n - \\kappa(G)\\), where \\(\\kappa(n)\\) is the size of the largest clique in \\(G\\). \n\\citet{reiterman1989embeddings} showed somewhat strikingly that for \\(n\\geq 37\\), all but \\((1-1/n)\\)-fraction of graphs have sphericity \\(\\geq n/15\\). Further results have lower-bounded sphericity in terms of spectral properties of the graph adjacency matrix \\citep{bilu2004monotonemapssphericitybounded}. More recently, \\citet{bhattacharjee2020relations} developed multiple notions of dimension for \\emph{directed} graphs, motivated by the recent surge in the interest of sequential data (e.g.\\ natural language). They relate these notions of embeddability to fundamental graph properties like cyclicity and eigenspectra.   Unlike sphericity, our notion of \\(\\alpha\\)-preservation can consider non-Euclidean embeddings.\nFurthermore, \\(\\alpha\\)-preservation uses separate thresholds to embed neighbors versus non-neighbors (where \\(\\alpha\\) parametrizes the ratio between these thresholds)\\todo{unnecessary sentence, simply say alpha allows for more fine-grained...}. \nUnfortunately, optimal sphericity embeddings require potentially exponential precision to be written down \\citep{kang2011sphere}. One of the benefits of \\(\\alpha\\)-preservation is that \\(\\alpha\\) bounded away from \\(1\\) avoids these precision issues.   Though sphericity and \\(\\alpha\\)-preservation have a similar flavor, our results for \\(\\alpha\\)-preservation reveal more subtleties about how the output metric space or the separation between neighbor and non-neighbor thresholds affects the difficulty of embedding with an absolute neighborhood threshold. \n\n\\textcolor{red}{this is kind of vague, could we somehow sketch/define their notion?}\n\n\\citet{indyk2007nearest} study a notion of local structure-preserving embedding that is motivated by approximate nearest neighbor search. They show that for input metrics with constant aspect ratio (i.e.\\ the diameter over the smallest interpoint distance) or constant doubling dimension, one can achieve efficient \\((1+\\epsilon)\\)-approximate \\todo{+/-}\nnearest neighbor Euclidean embedding in $\\Omega(1/\\epsilon^2)$ dimensions.\\footnote{One should note that an analogous statement for \\((1\\pm \\epsilon)\\)-isometry for points with constant doubling dimension is known not to hold \\citep{alon2003problems}.}. \nAn alternative approach to this problem, for Euclidean inputs, is to use the local Johnson-Lindenstrauss mapping of \nIn a similar vein, \\citet{bartal2011dimensionality} develop a local Johnson-Lindenstrauss-type result which embeds into \\(\\Omega(\\log k /\\epsilon^2)\\)-dimensional Euclidean space and promise low distortion between any input point and its \\(k\\)-nearest-neighbors. \n\n\\citet{bhattacharjee2020relations} study an interesting variation focused on neighborhood relationships in \\textit{directed} graphs. Their primary motivation is inspired by recent surge in the study of sequential data (e.g.\\ natural language). They connect successful embeddability with properties of the input graph.   \n\nneighborhood preservation of \\emph{directed} graphs. Their notion of \\(\\delta\\)-robustness is essentially the directed graph analogue to our notion of \\((\\alpha > 1)\\)-preservation, in the sense that they parametrize the separation between neighbors and non-neighbors.\n\\todo{dont really like the last few paras :/}\n\\footnote{on the practical side, LMNN and Dasgupta}\nLARGE MARGINS NEAREST NEIGHBORS\n\nThis notions are inspired by word embedding literature, where it is desired that ordered relations between words are preserved geometrically. \nThey show, for instance, that the SVD of a rank-\\(k\\) adjacency matrix with largest singular value \\(\\sigma_1\\) yields a \\((1,1+\\frac{1}{\\sigma_1})\\) neighbor-separating \\todo{? do you mean cluster-preserving?} embedding in \\(\\mathbb{R}^k\\) (to translate their result into the terminology of this paper). Our paper studies graph embedding in a more general setting and considers \n\n\nother NLP word embedding geometry related citations, if needed:\n\\citep{gittens2017skip}\n\\citep{arora2020contextual}\n\n\\todo[inline]{Add discussion of following papers:\\cite{MAEHARA198455} --> OG sphericity paper + $\\Omega(n)$ UB + for all $m > 0$, $\\exists$ G of size $~2^m$ such that $G$ has sphericity $m$ + sphericity is stronger condition then $\\alpha   1$. \\cite{bilu2004monotonemapssphericitybounded} --> most ordinal embeddings of discrete metric spaces takes $n$ dimensions (Does this imply an individual style result) + links paper that proves sphericity of $K_{n,n}$ is at least $n$ + attempts to construct more examples of graphs with sphericity $\\Omega(n)$ + [Embeddings of graphs in euclidean spaces] prove that for most graphs sphericity is greater than $n/15$.}\n\n\\subsection{Data Visualization and Other Applications}\n\nData visualization is a type of extreme dimension reduction that is focused on producing two- or three-dimensional\noutputs which emphasize cluster or local neighborhood structure.\nData visualization is essentially extreme dimension reduction.\\todo{liked the previous phrasing a lot better (see March 20 rendering)} Broadly speaking, the goal is to take data high-dimensional datasets and produce two- or three-dimensional plots, amenable to the human eye, which say \\textit{something} about the input. \nStandard (linear) dimension reduction methods like classical multi-dimensional scaling (MDS) and random projections are generally not well-suited for this purpose: when forced into ultra-low dimensions(lower than the intrinsic dimension of the data)\n, these embeddings tend to destroy salient structures and display ``artifacts'' unrelated to the intrinsic structure of the data \\citep{dasgupta2006concentration, diaconis2008horseshoes}.\nA similar phenomenon can be said of many popular manifold learning methods like Locally Linear Embedding or Laplacian Eigenmaps : the normalization conditions of these methods put strange restrictions on their success\n\\citep{perraul2013non, goldberg2008manifold, venna2010information}.\n\n\nt-SNE \\citep{van2008visualizing} and UMAP \\citep{mcinnes2018umap}, on the other hand, have gained widespread popularity across the general scientific literature for their seemingly remarkable ability to visualize salient structure in high-dimensional data. These methods seem to satisfy a basic and sometimes practical desire to see the structure in data, but there are fundamental   \\todo{Add explanation that these methods build graphs over the input and try to preserve them in the output.}\n\\citet{shaham2017stochastic}, \\cite{linderman2019clustering}, and \\citet{arora2018analysis} were the first works showing that, for sufficiently well-clustered inputs, t-SNE does indeed output the desired cluster visualization. These results corroborate t-SNE's apparent ability to tease out global cluster structure. What about local neighborhood structure? \\citet{im2018stochastic} (building on the precision-recall framework of \\citet{venna2010information}) and \\citet{chari2023specious} provide some practical evidence that t-SNE and UMAP are less attuned to faithfully revealing neighborhoods. \\st{neighborhoods}\\textcolor{red}{manifold or hierarchical structure}.   \\citet{}.\\todo{perhaps cite pactor as well?} and \\todo{discuss/resolve the connection with arora and sarkar possibility results}\n\nIf one seeks to embed \\textit{labelled} data (for downstream prediction as well as visualization), large-margin nearest neighbors is a canonical linear\\footnote{Similar nonlinear techniques often used in practice include contrastive learning \\citep{contrastivelearning} and Siamese networks \\citep{siamesenetworks}.} technique from the Mahalonobis metric learning literature \\citep{weinberger2009distance}. This method aims to alter the original representation of the data such that the nearest neighbor of any point have the same label, while differently-labelled points are separated with a   ``margin''. \nThey seek to map neighboring (same-label) points closer together than non-neighboring (differently-labelled) points in such a way that they are separated by a large margin. \n This margin or gap is akin to our notion of separability between neighbors and non-neighbors for \\((\\alpha > 1)\\)-preservation (see Figure \\ref{fig:neighbor_overlap}, left).\n\nThe common thread between these data visualzations\n\nOur results can be viewed as answering \nA fundamental question in the backdrop of these studies is: what is the minimum embedding dimension necessary to preserve local neighborhoods? Our work addresses this unifying question in a very general setting, demonstrating when and how the embedding dimension must scale with key properties of the input data (e.g.\\ number of data points, connectivity and neighborhood structure, etc.).   with the number of data points and the connectivity/neighborhood structure that may be present in the input data. \\textcolor{red}{edit for readability: emphasize data parameters}   for these algorithms to be successful in their respective goals?   \n\nneeded to successfully achieve their respective goals?\n\nOur work seeks to provide theoretical foundations for data visualization.\n\\todo{para above/below contradict}\n\nThese techniques are barely understood from a theoretical standpoint, in part due to the fact that effective data visualization is usually not a well-defined objective. \\citet{arora2018analysis} provided a compelling definition and showed that t-SNE on well clustered inputs yields an appropriate constant-dimensional visualization of the cluster structure with high probability.\n\nCITE VENNA paper (information retrieval); local neighborhood prcision-recall. provides a framework to evaluate these kinds of embeddings. practically studies the local purity of the neighborhoods by a IR technique, for t-SNE and other embedding techniques. We are theoretically analyzing a special case of this precision-recall, seeing what is and is not possible.\n\nLARGE MARGIN NEAREST \n\n\\st{two-dimensional t-SNE embeddings of high-dimensional inputs generated by mixture of well-separated log-concave distributions successfully visualize the input with high probability. Their definition of data visualization essentially enforces that well-separated spherical clusters in the input are maintained in the output. This paper attempts a more general view of data visualization, in particular allowing for non-spherical clusters.}\\todo{Add to discussion}\n\n\n\\todo[inline]{para below should be later in the text in the discussion section. At this point the comments made will not be understood/appreciated by the reader}\n\n\\textcolor{red}{DONT PUT IT IN THE CONTEXT OF OUR RESULTS. Indeed, there are theoretical guarantees showing that 2D t-SNE embeddings of high-dimensional inputs generated by mixture of well-separated log-concave distributions display visual cluster structure \\citep{arora2018analysis}. Note that their result holds for \\(k \\ll n^{1/5}\\), implicit to their result is the idea that the minimum distance between clusters (as a fraction of the data diameter) decreases as \\(k\\) grows (in their result, \\(k\\) can be as large as \\(n^{1/5}\\)). Note that our definitions of cluster-preservation enforces that this minimum distance (it can be thought of as a notion of precision or aspect ratio) is a constant, which in turn forces the dimension to grow.}\n",
  "title": "Compressibility Barriers to Neighborhood-Preserving Data Visualizations"
}
