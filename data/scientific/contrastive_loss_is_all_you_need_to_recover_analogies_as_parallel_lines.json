{
  "authors": [
    "Narutatsu Ri",
    "Fei-Tzin Lee",
    "Nakul Verma"
  ],
  "date_published": "2023-06-14",
  "raw_tex": "\\pdfoutput1\n\n\\documentclass[11pt]{article}\n\\usepackage{ACL2023}\n\\usepackage{times}\n\\usepackage{latexsym}\n\\usepackage[T1]{fontenc}\n\\usepackage[utf8]{inputenc}\n\\usepackage{microtype}\n\\usepackage{inconsolata}\n\\usepackage{svg}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{subcaption}\n\\usepackage[english]{babel}\n\\usepackage{amsmath}\n\\usepackage{float}\n\\usepackage{graphicx}\n\\usepackage{amsmath,amssymb,pgf}\n\\usepackage{hyperref}\n\\usepackage{color, colortbl}\n\n\\newcommand*{\\parallelogramm}{\n   \\rlap{\\rotatebox{-30}{\\rule[.05ex]{.4pt}{.77em}}}\n   \\kern.04em\n   \\rlap{\\kern.36em\\raisebox{0.649519052835em}{\\rule{.6em}{.4pt}}}\n   \\rule{.6em}{.4pt}\\kern-.04em\n   \\rotatebox{-30}{\\rule[.05ex]{.4pt}{.77em}}}\n\n\\newtheorem{theorem}{Theorem}\n\\newtheorem{lemma}[theorem]{Lemma}\n\\newtheorem{claim}[theorem]{Claim}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\graphicspath{ {images} }\n\\setlength{\\belowcaptionskip}{-8pt}\n\n\\definecolor{Gray}{gray}{0.85}\n\n\\title{Contrastive Loss is All You Need to Recover Analogies as Parallel Lines}\n\n\\definecolor{fl}{HTML}{00C8FF}\n\\newcommand\\fnote[1]{\\textcolor{fl}{#1}}\n\n\\definecolor{er}{HTML}{ff0000}\n\\newcommand\\enote[1]{\\textcolor{er}{#1}}\n\n\\author{Narutatsu Ri \\\\\n   Columbia University \\\\\n   \\texttt{wl2787@columbia.edu} \\\\\\And\n   Fei-Tzin Lee \\\\\n   Columbia University \\\\\n   \\texttt{feitzin@cs.columbia.edu} \\\\\\And\n   Nakul Verma \\\\\n   Columbia University \\\\\n   \\texttt{verma@cs.columbia.edu} \\\\}\n\n\\begin{document}\n\\maketitle\n\\begin{abstract}\n\nWhile static word embedding models are known to represent linguistic analogies as parallel lines in high-dimensional space, the underlying mechanism as to why they result in such geometric structures remains obscure. We find that an elementary contrastive-style optimization employed over distributional information performs competitively with popular word embedding models on analogy recovery tasks, while achieving dramatic speedups in training time. Further, we demonstrate that a contrastive loss is sufficient to create these parallel structures in word embeddings, and establish a precise relationship between the co-occurrence statistics and the geometric structure of the resulting word embeddings.\n\\end{abstract}\n\n\\section{Introduction}\nStatic word embeddings take inspiration from the distributional hypothesis \\cite{Firth1957} and assign vector representations to words based on co-occurrence statistics. Such embeddings are known to implicitly encode syntactic and semantic analogies as parallelogram-type structures (\\citealp{Mikolov2013EfficientEO, mikolov-etal-2013-linguistic}). This discovery inspired a series of theoretical investigations \\cite{NIPS2014_feab05aa, gittens-etal-2017-skip, https://doi.org/10.48550/arxiv.1901.09813, ethayarajh-etal-2019-towards}. \n\nRecent studies reconsider whether analogies are indeed represented as parallelograms in the embedding space \\cite{schluter-2018-word, linzen-2016-issues, https://doi.org/10.48550/arxiv.2102.11749}, and propose a weaker notion of viewing analogies as parallel \\textit{lines} \\cite{arora-etal-2016-latent} as a more appropriate model (cf.\\ Figure \\ref{fig:parallelograms_trapezoids_example}). While this claim is shown to hold empirically for popular word embeddings \\cite{fournier-etal-2020-analogies}, few analyze the theoretical underpinnings of this phenomenon. \n\n\\begin{figure}[t!]\n\\centering\n\\includegraphics[width0.50\\textwidth]{vis1}\n\\small{\\caption{\\label{fig:parallelograms_trapezoids_example} \nVisualization of analogies as parallelograms and as parallel lines. For the quadruple \\texttt{``man, woman, king, queen\"}, two analogy relations coincide (\\texttt{``man:woman   king:queen\"} representing gender and \\texttt{``man:king   woman:queen\"} representing royalty). In contrast, the quadruple \\texttt{``run, running, scream screaming\"} contains only one analogy relation (\\texttt{``run:running   scream:screaming\"} representing present participle). Representing analogies as lines relaxes the geometric requirements on the analogy structure.}}\n\\end{figure}\n\nIn this paper, we present a remarkable observation that a simple contrastive-style optimization \\cite{1467314} performs just as well as highly-optimized versions of popular word embeddings while achieving 50$\\times$ speedup in training time. Our work theoretically analyzes the precise conditions under which this optimization procedure can recover analogies as parallel lines. We further investigate the extent to which real-world data satisfies these conditions, and the contrastive loss recovers such parallel structures.\n\nIn Section \\ref{sec:related_work}, we review recent literature on the theory of word embeddings. Sections \\ref{sec:cwm} and \\ref{sec:analysis} present our contrastive learning objective and its analysis. Section \\ref{sec:experiments} showcases the performance of our approach on analogy-based benchmarks.\\footnote{Code can be found at \\url{https://github.com/narutatsuri/cwm}.} \n\n\\section{Related Work}\\label{sec:related_work}\n\\textbf{Analogies as Parallelograms.} \\citet{gittens-etal-2017-skip} study the parallelogram phenomenon by analyzing analogies as a relation between paraphrases. \\citet{https://doi.org/10.48550/arxiv.1901.09813} extend this line of work and show that analogies are captured as parallelograms when the vectors are linear projections of the \\emph{Pointwise Mutual Informaton} (PMI) matrix. \\citet{ethayarajh-etal-2019-towards} further generalize Gittens' result by introducing the \\textit{co-occurrence shifted pointwise mutual information} (csPMI)\\footnote{csPMI$(a, b)   $ PMI$(a, b) + \\log p(a, b)$.} and analyze the conditions on the csPMI for which parallelograms emerge. \n\n\\textbf{Analogies as Parallel Lines.} To the best of our knowledge, the only theoretical work that explores analogies more generally as parallel lines is by \\citet{arora2019latent}, who propose that analogies are encoded as such when the inner products between embeddings weakly recover the PMI of word co-occurrence statistics. We take an alternate approach and show that a contrastive-style optimization suffices to encode analogies as parallel lines.\n\n\\section{The Contrastive Word Model (CWM)\\label{sec:cwm}}\nContrastive learning methods are based on an intuitive yet powerful idea that pulling similar items closer together while pushing dissimilar items away significantly improves model performance. \n\nWe can employ the same push-pull dynamics in word embeddings by placing the vector representations of words that co-occur closer together than those of words that do not. We call this the Contrastive Word Model (CWM), detailed below.\n\n\\subsection{Notation \\& Formulation}\nGiven a training corpus, we denote the vocabulary as $W$. We aim to learn a $D$-dimensional vector representation $v_w$ for each word $w$ in the vocabulary. The collection of all these vectors is denoted by $V   \\{v_1,\\ldots, v_{|W|}\\}$. We refer to the length-normalized version of a vector ${v}$ as $\\hat{v}$. \n\nLet $\\#(i)$ be the occurrence count of word $i$ and $\\#(i, j)$ the co-occurrence count (for a context window of size $\\Delta$) of words $i$ and $j$ in the training corpus. We denote \\emph{window words} as words that co-occur with a reference \\emph{center word} (these are reminiscent of the target and context words in \\citealp{mikolov-etal-2013-linguistic}), and \\emph{negative window words} as words that do not co-occur with the center word. The center-, window-, and negative window words are denoted as $c, w, w'$ respectively. Let $D_{c, w}$ be the set of negative window words for fixed $c, w$. \nWe define the CWM objective as:\n\\begin{align*}\n\\sum_{c \\in W} \\! \\sum_{w \\in W} \\! \\#(c, w) \\cdot \\!\\!\\!\\!\\!\\!   \\sum_{w' \\in D_{c, w}} \\!\\!\\!\\! \\Big[m - \\underbrace{\\hat{v}_c \\cdot \\hat{v}_w}_{\\textup{pull}} + \\underbrace{\\hat{v}_c \\cdot \\hat{v}_{w'}}_{\\textup{push}}\\Big]_{_+},\n\\end{align*}\nwhere $[\\cdot]_{_+}$ is the hinge function and $m$ is a tunable hyperparameter. \n\nTo better understand our proposed loss, consider its effect on a fixed center word $c$. The difference between the terms $\\hat{v}_c \\cdot \\hat{v}_w$ and $\\hat{v}_c \\cdot \\hat{v}_{w'}$ encourages the \\emph{angle} between vectors $v_c$ and $v_w$ to be smaller than that between $v_c$ and $v_{w'}$ by at least a margin of $m$. The hinge function neutralizes the loss once the vectors satisfy the desired relationship. Such max-margin type losses among triples are well investigated in metric learning literature \\cite{NIPS2005_a7f592ce}. \n\n\\subsection{Relation to Popular Word Embeddings}\nInterestingly, popular word embedding models such as Skip-gram \\cite{Mikolov2013EfficientEO} and GloVe \\cite{pennington-etal-2014-glove} can be viewed as implicitly employing a push-pull action similar to CWM. Consider Skip-gram's objective: for a fixed pair of co-occurring words $c$ and $w$, the model updates the word vector $v_c$ as:\n\\begin{align}\\label{eq:sgns_update}\nv_c^{\\textup{new}}   & v_c^{\\textup{old}} + \\underbrace{\\left(1-\\frac{e^{v_w ^\\intercal u_{c'}}}{\\sum_{w' \\in W} e^{v_w ^\\intercal u_{w'}}} \\right)}_{\\textup{pull}}v_w\\\\\n&- \\underbrace{\\mathbb{E}_{w'\\sim W}[v_{w'}]}_{\\textup{push}}\\nonumber + \\textup{additional terms}.\n\\end{align}\nHere, the word $c'$ (and its target vector $u_{c'}$) co-occurs with both $c$ and $w$, encouraging all of them to be mapped together (pull), whereas the negative term pushes away randomly sampled words $w'$ from $c$. See Appendix \\ref{app:sgns_update} for a derivation.\n\nThe GloVe objective, on the other hand, performs a series of updates on $c, w$, and $w'$ as:\n\\begin{align}\\label{eq:glove_update}\n\\textup{pull } &\n\\Bigg\\{ \n\\begin{matrix}\nv_c^{\\textup{new}} & v_c^{\\textup{old}} + g(c, c')u_{c'}\\\\\nv_w^{\\textup{new}} & v_w^{\\textup{old}} + g(w, c')u_{c'}\n\\end{matrix} \\\\\n\\textup{push } &\n\\Big\\{ v_{w'}^{\\textup{new}} \\;\\;\\; v_{w'}^{\\textup{old}} - g(w', c')u_{c'}, \\nonumber\n\\end{align}\nwhere $g(\\cdot, \\cdot)$ always returns a positive value. Notice that the \\emph{positive} contribution of $g$ in the first two updates encourages $v_c$ and $v_w$ to be closer together (pull), while the \\emph{negative} contribution to the $v_{w'}$ update encourages it to be pushed away. See Appendix \\ref{app:glove_update} for a derivation.\n\nWe believe that part of the success of these word embedding models is due to their implicit push-pull dynamics. Hence, a natural question to consider is what happens when one purely optimizes for the push-pull action alone.\n\n\\section{Analysis\\label{sec:analysis}}\nIn this section, we provide a theoretical justification for the emergence of analogies as parallel lines when we optimize for the CWM objective. \n\nConsider the expression for word vectors $v_c\\in V$ that minimizes the global objective:\n Add \n\\begin{align} \\label{eq:optimal_embeddings}\n\\!\\!\\!\\!{v}_c   \\rho_c \\Bigg( \\! \\sum_{w \\in W} \\!\\! \\Big(\\frac{\\#(c, w)}{\\#(c)} {\\hat{v}}_w\\Big) - \\!\\!\\!\\! \\underset{w'\\sim U(W)}{\\mathbb{E}} \\!\\! \\left[\\hat{v}_{w'}\\right] \\Bigg),\n\\end{align}\nwhere $\\rho_c \\in \\mathbb{R}$ is a constant dependent on $c$. \nIn essence, $v_c$ is the difference between the weighted average of the window words and the mean of all word vectors. See Appendix \\ref{app:superposition} for derivation.\n\nUnder Eq.\\ \\eqref{eq:optimal_embeddings}, we consider the conditions that word co-occurrence statistics need to satisfy for a set of words $a, b, c, d$ to form parallel geometric structures. \n\n\\begin{theorem}\\label{thm:main_theorem}\nFor any quadruple of words $a, b, c, d \\in W$, if there exists a constant $\\zeta \\in \\mathbb{R}$ where the co-occurrence statistics satisfy the condition: $\\quad\\forall w \\in W$\n Add \\!\n\\begin{equation}\\label{eq:2}\n\\!\\!\\!\\!\\resizebox{.9\\hsize}{!}{$\n\\left(\\frac{\\#(a, w)}{\\#(a)} - \\frac{\\#(b, w)}{\\#(b)}\\right)\\!\\!\\!\\bigg/\\!\\!\\!\\left(\\frac{\\#(c, w)}{\\#(c)} - \\frac{\\#(d, w)}{\\#(d)}\\right) : \\zeta,$}\n\\end{equation}\nthen the corresponding word vectors satisfy the property:\n$$\n{\\hat{v}}_a - {\\hat{v}}_b   \\zeta\\left({\\hat{v}}_c - {\\hat{v}}_d \\right).\n$$\n\\end{theorem}\nNote that Theorem \\ref{thm:main_theorem} establishes a direct relationship between word co-occurrence statistics---which are solely derived from the training corpus---and the geometric structure of the word embedding. \n\nFor a given quadruple $a, b, c, d\\in W$ (regardless of whether they form an analogy), the existence of $\\zeta$ induces parallel structures between $\\hat{v}_a, \\hat{v}_b, \\hat{v}_c, \\hat{v}_d$. \nIf such a $\\zeta$ exists and is equal to $1$, then $\\hat{v}_b - \\hat{v}_a   \\hat{v}_d - \\hat{v}_c$ and the quadruple forms a parallelogram. When $\\zeta \\neq 1$, then the difference vectors $\\hat{v}_b - \\hat{v}_a$ and $\\hat{v}_d - \\hat{v}_c$ are mainly parallel, inducing a trapezoidal structure among $\\hat{v}_a, \\hat{v}_b, \\hat{v}_c, \\hat{v}_d$ (cf.\\ Figure \\ref{fig:parallelograms_trapezoids_example}). \n\nOne would expect that the co-occurrence statistics of real data conform with the existence of such a $\\zeta$ value for analogy quadruples, whereas $\\zeta$ does not exist for random quadruples. This is empirically investigated in Section \\ref{sec:analogies_and_lines}. The relationship between the value of $\\zeta$ and the resulting parallelogram structure (parallelogram vs.\\ trapezoid) is empirically verified in Section \\ref{sec:zeta_parallelograms}. \n\n\\section{Experiments}\\label{sec:experiments}\n\nWe first compare the performance of CWM to that of other popular word embedding methods on analogy recovery (Section \\ref{sec:results-analogy}). We then empirically verify the degree to which our assumptions regarding co-occurrences hold on real data (Section \\ref{sec:analogies_and_lines}) as well as the relation between $\\zeta$ and the parallelogram structure (Section \\ref{sec:zeta_parallelograms}).\n\n\\subsection{Data and Training Procedure}\nWe use the 03/2023 version of Wikimedia Downloads dump \\cite{wikidump} and train CWM for a single pass over the corpus using $\\Delta   5$ and $m0.2$ (chosen via cross validation from the range $0.1\\sim1$). For comparison, we also train Skip-gram with Negative Sampling (SGNS) and GloVe over the same corpus with the default parameter settings provided by \\citet{mikolov-etal-2013-linguistic} and \\citet{pennington-etal-2014-glove} respectively.\n\nWe utilize the BATS analogy dataset \\citep{gladkova-etal-2016-analogy} for all analogy related tasks. For all word embeddings, we use dimension $D300$ and the vectors are length-normalized to follow practical conventions \\cite{mikolov-etal-2013-linguistic}. Training was done on 256 instances of AMD EPYC 7763 64-Core Processor machine.\n\n\\begin{table}[t]\n\\centering\n\\begin{tabular}{l|cc|cc}\n\\hline\n& \\multicolumn{2}{|c|}{Analogies} & \\multicolumn{2}{c}{Training}\\\\\n\\hline\n\\textbf{Model} & \\textbf{PCS} & \\textbf{MSM} & \\textbf{Time (hrs)} & \\textbf{Speedup}\\\\\n\\hline\n\\rowcolor{Gray}\nCWM & \\textbf{0.677} & \\textbf{0.469}& \\textbf{0.59} & \\textbf{49$\\times$}\\\\\nSGNS & 0.675 & 0.433 & 29.27 & 1$\\times$\\\\\nGloVe & 0.667 & 0.423 & 30.71 & 0.91$\\times$\\\\\n\\hline\n\\end{tabular}\n\\small{\\caption{\\label{table:metric_performances} Performances for word embedding models. CWM refers to our contrastive word model. SGNS refers to Skip-gram with negative sampling. Best numbers are bolded.}}\n\\end{table}\n\n\\subsection{Analogy Recovery}\\label{sec:results-analogy}\nTo assess the degree to which word embeddings encode analogies as lines consistently in the embedding space, we use two intuitive metrics proposed by \n\\citet{fournier-etal-2020-analogies}:\npropose two intuitive metrics: \\fnote{``we use two intuitive metrics proposed by Fournier et al.\"?} \nthe \\textit{Pairing Consistency Score} (PCS) and \\textit{Mean Similarity Measure} (MSM). PCS assesses analogy alignment precision (the number of non-analogy offsets incorrectly aligned with true analogy offsets), while MSM measures absolute alignment. \n\nTable \\ref{table:metric_performances} shows the relative performance of popular word embeddings. Notice that our method performs $7\\$ better than popular word embeddings on the MSM metric, indicating that the word vectors learned by CWM exhibit higher alignment among analogy quadruples than Skip-gram and GloVe. CWM's performance on the PCS metric indicates that parallel lines are not erroneously encoded for non-analogy words. For completeness, see Appendix \\ref{app:analogy_parallelograms} for parallelogram recovery performances (previous literature questions the validity of the standard evaluation method).\n\n\\begin{figure}[t]\n   \\vspace{-1em}\n   \\centering\n   \\includegraphics[width0.50\\textwidth]{claim1_plot}\n   \\small{\\caption{\\label{fig:zeta_concentration} Cosine similarities between co-occurrence vectors $\\vec{C}_{a, b}$ and $\\vec{C}_{c, d}$ for words $a, b, c, d$ from uniformly sampled word quadruples (Random), shuffled analogy pairs (Shuffled), and true analogy pairs (Analogy).}}\n\\end{figure}\n\n\\subsection{Existence of $\\zeta$ and Analogies\\label{sec:analogies_and_lines}}\nTheorem \\ref{thm:main_theorem} provides insight into the conditions required for CWM to induce parallel lines in the learned word vectors, but these conditions are not specific to analogy word pairs. Thus, the question remains: does $\\zeta$ exist only when a quadruple forms an analogy?\n\nHere, we study the level at which the co-occurrence statistics of analogy and non-analogy pairs satisfy the condition in Theorem \\ref{thm:main_theorem}. To assess the existence of $\\zeta$, consider the vectors $\\vec{C}_{a, b}, \\vec{C}_{c, d} \\in \\mathbb{R}^{|W|}$ (derived purely from co-occurrence counts): \n\\begin{align*}\n\\resizebox{.9\\hsize}{!}{$\n\\vec{C}_{a, b}   \\Bigg[\\left(\\frac{\\#(a, w_1)}{\\#(a)} - \\frac{\\#(b, w_1)}{\\#(b)}\\right), \\dots, \\left(\\frac{\\#(a, w_{|W|})}{\\#(a)} - \\frac{\\#(b, w_{|W|})}{\\#(b)}\\right)\\Bigg]$},\\\\\n\\resizebox{.9\\hsize}{!}{$\n\\vec{C}_{c, d}   \\Bigg[\\left(\\frac{\\#(c, w_1)}{\\#(c)} - \\frac{\\#(d, w_1)}{\\#(d)}\\right), \\dots, \\left(\\frac{\\#(c, w_{|W|})}{\\#(c)} - \\frac{\\#(d, w_{|W|})}{\\#(d)}\\right)\\Bigg]$}.\n\\end{align*}\n\\fnote{i actually like spelling it out like this but it's kind of hard to read}\nExistence of a $\\zeta$ where Eq.\\ \\eqref{eq:2} holds for $a, b, c, d$, implies that all entries in $\\vec{C}_{a, b}$ are equal to the corresponding entries in $\\vec{C}_{c, d}$ scaled by a factor of $\\zeta$. This indicates that when $\\zeta$ exists, $\\vec{C}_{a, b}$ and $\\vec{C}_{c, d}$ are collinear. Thus, we can approximate assessing the existence of $\\zeta$ by evaluating whether the cosine similarity between $\\vec{C}_{a, b}$ and $\\vec{C}_{c, d}$ is sufficiently high. \n\nWe consider three settings from which the quadruples are obtained: randomly sampled word quadruples, false shuffled analogies, and true analogies using the BATS dataset. We compute the distribution of cosine similarities for all quadruples from these settings. \n\nResults are shown in Figure \\ref{fig:zeta_concentration}. Observe that the cosine similarities of random and shuffled quadruples is significantly lower than that for analogy words. This indicates a positive association between $\\zeta$ and analogy word quadruples in real world corpora. \n\n Furthermore, it is noteworthy to point out that the BATS dataset contains analogies with ambiguous   \n we examine word quadruples $a, b, c, d$ that exhibit high and low values for $|\\cos(\\vec{C}_{a,b}, \\vec{C}_{c,d})|$. Results are shown in Table \\ref{table:good_and_bad_analogies}.   \n\n \\subsubsection{Ambiguous Analogies}\n\n\\begin{table}[t]\n\\centering\n\\scalebox{0.9}{\n\\begin{tabular}{l|c}\n\\hline \n\\textbf{Analogy Quadruple} & \\textbf{Sim.}\\\\\n\\hline\n{\\small\\texttt{fall:rise   under:over}} & 1.000\\\\\n{\\small\\texttt{prevent:preventing   follow:following}} & 0.9901 \\\\\n{\\small\\texttt{lancaster:lancashire   salford:manchester}} & 0.9812\\\\\n{\\small\\texttt{refer:referred   agree:agreed}} & 0.9740\\\\\n\\hline\n{\\small\\texttt{organized:arranged   dollars:bucks}} & 0.0006\\\\\n{\\small\\texttt{staircase:step   shilling:pence}} & 0.0006\\\\\n{\\small\\texttt{guitar:string   church:altar}} & 0.0004\\\\\n{\\small\\texttt{monkey:infant   fox:cub}} & 0.0001\\\\\n\\hline\n\\end{tabular}}\n\\small{\\caption{\\label{table:good_and_bad_analogies} Samples of analogy quadruples illustrating cosine similarity values between $\\vec{C}_{a,b}$ and $\\vec{C}_{c,d}$. \"Sim.\" denotes the value of $|\\!\\cos(\\vec{C}_{a,b}, \\vec{C}_{c,d})|$.}}\n\\end{table}\n\n These analogies allow for alternative valid replacements of certain words, thereby challenging the notion of expecting word vectors to accurately recover ambiguous analogies as parallel lines. For example, in the analogy $a:bc:d$, with $a$ \\texttt{sun}, $b$ \\texttt{yellow}, $c$ \\texttt{sea}, and $d$ \\texttt{blue}, it is important to acknowledge that alternative valid replacements exist for $b$ and $d$ (e.g., $b   $ \\texttt{red, orange}, \\dots, $d$ \\texttt{green}, \\dots).\\footnote{Multiple alternatives for $b$ and $d$ are included in the BATS dataset for fixed words $a$ and $c$ to address these ambiguities.} \nFurthermore, it is worth noting the presence of \"ambiguous\" analogies within the BATS dataset. These include\ndespite its widespread usage in analogy benchmark tests. Some examples are \nanalogies with valid alternative replacements (e.g.\\ \\texttt{sun:orange   sea:blue} can also be \\texttt{sun:red   sea:blue}), or analogies with unclear relationships (e.g.\\ lexicographic analogies such as \\texttt{father:dad   lady:madam}). We investigate whether the ambiguity of an analogy correlates with its cosine similarity between $\\vec{C}_{a,b}$ and $\\vec{C}_{c,d}$\nwhether such ambiguous analogies exhibit low values for $|\\cos(\\vec{C}_{a,b}, \\vec{C}_{c,d})|$ \nby sampling from analogy quadruples with high and low values of $|\\!\\cos(\\vec{C}_{a,b}, \\vec{C}_{c,d})|$. \n\nResults are shown in Table \\ref{table:good_and_bad_analogies}. Observe that analogy quadruples with high cosine similarity between $\\vec{C}_{a,b}$ and $\\vec{C}_{c,d}$ seems to demonstrate a clear relationships, whereas those with low cosine similarity exhibit weaker/ambiguous relationships. \n\n ambiguous analogies correspond to low cosine similarity for \n\n Additionally, we examine the analogy quadruples exhibiting high and low values of $|\\cos(\\vec{C}_{a,b}, \\vec{C}_{c,d})|$. Results are shown in Figure \\ref{table:good_and_bad_analogies}. \n\n\\subsection{$\\zeta$ and Geometric Structure\\label{sec:zeta_parallelograms}}\n\nWe now examine the effect of $\\zeta$ on the geometry of analogy word pairs. Recall that $\\zeta$ exists for quadruples where $|\\!\\cos(\\vec{C}_{a, b}, \\vec{C}_{c, d})|   1$. As this condition is unlikely to hold exactly on real data, we approximate $\\zeta$ with the ratio $\\hat \\zeta : \\|\\vec{C}_{a, b}\\|/\\|\\vec{C}_{c, d}\\|$ for quadruples with high cosine similarity (which we define as $|\\!\\cos(\\vec{C}_{a, b}, \\vec{C}_{c, d})|\\geq0.9$). We expect the word vectors to form parallelograms when $\\hat \\zeta \\approx 1$ ($0.95\\leq\\hat\\zeta\\leq1.05$), and form trapezoids otherwise. \n\nSpecifically, for each such quadruple, we compute the word $w$ that minimizes $\\|\\hat{v}_b - \\hat{v}_a + \\hat{v}_c - \\hat{v}_w\\|$ for parallelograms; ideally, $w$ should equal $d$. For trapezoids, we retrieve the word $w$ that maximizes the quantity $\\cos(\\hat{v}_b-\\hat{v}_a, \\hat{v}_w-\\hat{v}_c)$. If the word $d$ is among the top $k$ words, we deem the quadruple to satisfy the corresponding geometric structure. For both cases, we consider $k   1$ and $5$. \n\n\\begin{table}[t]\n\\centering\n\\begin{tabular}{c|cc}\n\\hline\n & $k1$   & $k5$\\\\\n\\hline \\\\[-1em]\n$\\hat \\zeta \\not\\approx 1$ & 0.800 \\small{(619/774)} & 0.862 \\small{(667/774)} \\\\\n\\hline \\\\[-1em]\n$\\hat \\zeta \\approx 1$ & 0.652 \\small{(137/210)} & 0.871 \\small{(183/210)} \\\\\n\\hline\n\\end{tabular}\n\\small{\\caption{\\label{table:zeta_recovery}Parallelogram/trapezoid recovery performances for different values of $\\hat\\zeta$. Parallelogram recovery for all analogy pairs is $0.27$ (see Table \\ref{table:performances_sup} in Appendix), indicating dramatic performance increase for the analogy subset where $\\hat\\zeta \\approx 1$.}}\n\\end{table}\n\nResults are shown in Table \\ref{table:zeta_recovery}. Observe that for $k5$, $87\\$ of the quadruples form parallelograms when $\\hat \\zeta \\approx 1$ (i.e.,\\ $0.95\\leq\\hat\\zeta\\leq1.05$), and $86\\$ of quadruples form trapezoid-type structures when $\\hat\\zeta \\not\\approx 1$. This validates our expectation that parallelograms and trapezoids indeed form when $\\hat\\zeta \\approx 1$ and $ \\hat\\zeta \\not\\approx 1$ respectively.\n\n\\section{Conclusion and Discussion}\\label{sec:conclusion}\nWe demonstrate that optimizing a contrastive-style objective over word co-occurrences is indeed sufficient to encode analogies as parallel lines. Our analysis (Theorem \\ref{thm:main_theorem}) sheds light on the inner workings of word embeddings:\\ parallel geometry is induced largely from word co-occurrence statistics for any push-pull model. Our work builds upon and generalizes previous literature that illuminates the underlying mechanisms governing the geometry of word embeddings. \n\nNote that while our results demonstrate the sufficiency of the push-pull mechanism for recovering analogies as parallel lines, it remains unclear whether push-pull is a necessary condition for this phenomenon. Investigating alternative mechanisms and their ability to achieve similar results would provide further insight into the relationship between word co-occurrence statistics and the recovery of analogies. \n\n\\bibliography{custom}\n\\bibliographystyle{acl_natbib}\n\n\\clearpage\n\\newpage\n\n\\appendix\n\\label{appendix}\n\\section{Proofs}\n\\subsection{Derivation of Eq.\\ \\eqref{eq:optimal_embeddings}}\\label{app:superposition}\n\nRecall that the global objective of CWM for the vocabulary $W$ and set of word vectors $V$ can be written as:\n\\begin{align*}\n& \\mathcal{L}(V)   \\sum_{c \\in W} \\sum_{w \\in W} \\#(c, w) \\\\\n& \\sum_{w' \\in D_{c, w}} \\Big[m -   \\hat{v}_c \\cdot \\hat{v}_w +   \\hat{v}_c \\cdot \\hat{v}_{w'} \\Big]_{_+},\n\\end{align*}\nwhere $D_{c, w}   \\{w' | w' \\sim U(W)\\}, |D_{c, w}|   k$ denotes the set of $k$ negative window words sampled uniformly from the vocabulary for each $c, w$ word pair and $U(W)$ denotes the uniform distribution over the vocabulary.\n\nFor fixed $c, w, w'$, consider the two cases where $m -   \\hat{v}_c \\cdot \\hat{v}_w +   \\hat{v}_c \\cdot \\hat{v}_{w'} > 0$ and $m - \\hat{v}_c \\cdot \\hat{v}_w +   \\hat{v}_c \\cdot \\hat{v}_{w'} \\leq 0$. As the word vectors are not updated for the latter case, we examine the former by taking the partial derivative of $\\mathcal{L}(V)$ with respect to ${v}_c$ and setting it to ${0}$:\n\\begin{align*}\n&{0}   -\\sum_{w \\in W}\\#(c, w) \\\\\n& \\sum_{w'\\in D_{c, w}} \\left( \\frac{{v}_w}{\\|{v}_c\\| \\|{v}_w\\|} - \\frac{{v}_{w'}}{\\|{v}_c\\| \\|{v}_{w'}\\|}\\right. \\\\\n& \\left. \\vphantom{\\frac{{v}_w}{\\|{v}_c\\| \\|{v}_w\\|} - \\frac{{v}_{w'}}{\\|{v}_c\\|   \\|{v}_{w'}\\|}}   + \\left(   \\frac{\\hat{v}_c \\cdot \\hat{v}_{w'}}{\\|{v}_c\\|^2} -   \\frac{(\\hat{v}_c \\cdot \\hat{v}_w)}{\\|{v}_c\\|^2} \\right) {v}_c \\right)\\\\\n\\Leftrightarrow & \\sum_{w \\in W}\\#(c, w) \\sum_{w'\\in D_{c, w}} \\frac{{v}_w}{\\|{v}_c\\| \\|{v}_w\\|} \\\\\n& -\\sum_{w \\in W}\\#(c, w) \\sum_{w'\\in D_{c, w}} \\frac{{v}_{w'}}{\\|{v}_c\\| \\|{v}_{w'}\\|} \\\\\n & \\sum_{w\\in W} \\#(c, w) \\sum_{w' \\in D_{c, w}}\\left( \\frac{{v}_c   {v}_w}{\\|{v}_c\\|^2   \\|{v}_w\\|} \\right.\\\\\n& \\left. - \\frac{{v}_c   {v}_{w'}}{\\|{v}_c\\|^2   \\|{v}_{w'}\\|} \\right) \\frac{{v}_c}{\\|{v}_c\\|}.\n\\end{align*}\nAs $\\sum_{w \\in W}\\#(c, w) \\sum_{w'\\in D_{c, w}} \\frac{{v}_{w'}}{\\|{v}_{w'}\\|}$ represents $\\sum_{w \\in W} \\#(c, w) \\cdot k   k \\cdot \\#(c)$ uniform i.i.d. draws from the vocabulary, the following holds for sufficiently large values of $k\\cdot \\#(c)$:\n\\begin{align*}\n\\sum_{w \\in W}\\#(c, w) &\\sum_{w'\\in D_{c, w}} \\frac{{v}_{w'}}{\\|{v}_{w'}\\|} \\\\\n&k \\#(c) \\mathbb{E}_{w' \\sim U(W)} \\left[ \\frac{{v}_{w'}}{\\|{v}_{w'}\\|} \\right].\n\\end{align*}\n\nSetting $\\mathbb{E}_{w' \\sim U(W)} \\left[ \\frac{{v}_{w'}}{\\|{v}_{w'}\\|} \\right]   {v}_p$ and dividing both sizes by $\\frac{k\\#(c)}{\\|v\\|}$, \n\\begin{align*}\n& \\sum_{w\\in W} \\frac{\\#(c, w)}{\\#(c)} \\frac{{v}_w}{\\|{v}_w\\|} - {v}_p\\\\\n&   \\left[\\frac{{v}_c}{\\|{v}_c\\|} \\left( \\sum_{w\\in W} \\frac{\\#(c, w)}{\\#(c)} \\frac{{v}_w}{\\|{v}_w\\|} - {v}_p \\right) \\right] \\odot \\frac{{v}_c}{\\|{v}_c\\|}.\n\\end{align*}\nSetting $\\sum_{w\\in W} \\frac{\\#(c, w)}{\\#(c)} \\frac{{v}_w}{\\|{v}_w\\|}   {v}_{p'}$ and \n$\n\\gamma_c   \\norm{\\frac{{v}_c}{\\|{v}_c\\|} \\left( \\sum_{w\\in W} \\frac{\\#(c, w)}{\\#(c)} \\frac{{v}_w}{\\|{v}_w\\|} - {v}_p \\right)},\n$\nthe above equation can be rewritten as:\n$$\n\\frac{{v}_c}{\\|{v}_c\\|}   \\frac{{v}_{p'}}{\\gamma_c} \\cdot \\frac{1}{\\norm {\\frac{{v}_c}{\\|{v}_c\\|} }} \\cdot \\frac{1}{\\cos \\theta}.\n$$\nwhere $\\theta$ indicates the angle between ${v}_{p'}$ and $\\frac{{v}_c}{\\|{v}_c\\|}$. \n\nAs $\\norm {\\frac{{v}_c}{\\|{v}_c\\|}}   1$, \n$$\n{v}_c   \\|{v}_c\\| \\cdot \\frac{{v}_{p'}}{\\gamma_c}\\cdot \\frac{1}{\\cos \\theta}\n$$\nNotice that ${v}_c \\parallel {v}_{p'}$ by the above construction , so $\\cos\\theta   1$. Thus, \n$$\n{v}_c   \\|{v}_c\\| \\cdot \\frac{{v}_{p'}}{\\gamma_c}   \\frac{\\alpha\\#(c)^{\\frac{1}{\\beta}}}{{\\gamma_c}} \\cdot {v}_{p'}. \\quad \\blacksquare\n$$\n\nThe second equality is derived from the empirically observed property $\\|{v}_c\\| \\propto \\#(c)^\\frac{1}{\\beta}$ for some constant $\\beta \\in \\mathbb{R}$, which is verified below. \n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width0.45\\textwidth]{A1}\n\\end{figure}\n\nInterestingly, a similar linear relationship is also observed in existing word embedding models \\cite{arora-etal-2016-latent}. \n\n\\subsection{Proof for Theorem \\ref{thm:main_theorem}}\\label{app:zeta_cooccurrence}\nUnder the assumption that Eq.\\ \\eqref{eq:optimal_embeddings} holds, we can write the expressions for ${\\hat{v}}_a - {\\hat{v}}_b, {\\hat{v}}_c - {\\hat{v}}_d$ as follows.\n\\begin{align*}\n{\\hat{v}}_a - {\\hat{v}}_b &   \\frac{1}{\\gamma_a}\\left( \\sum_{w \\in W} \\left(\\frac{\\#(a, w)}{\\#(a)} \\frac{{v}_w}{\\|{v}_w\\|}\\right) - {v}_p \\right) \\\\\n& - \\frac{1}{\\gamma_b}\\left( \\sum_{w \\in W} \\left(\\frac{\\#(b, w)}{\\#(b)} \\frac{{v}_w}{\\|{v}_w\\|}\\right) - {v}_p \\right).\n\\end{align*} \n\nUnder the assumption that $\\forall c \\in W: \\gamma_c   \\gamma$ for some $\\gamma \\in \\mathbb{R}$, \n\\begin{align*}\n{\\hat{v}}_a - {\\hat{v}}_b   \\frac{1}{\\gamma} \\sum_{w \\in W} \\left( \\frac{\\#(a, w)}{\\#(a)} -\\frac{\\#(b, w)}{\\#(b)} \\right)\\frac{{v}_w}{\\|{v}_w\\|}\n\\end{align*}\n\nUsing Eq.\\ \\eqref{eq:2}, \n\\begin{align*}\n{\\hat{v}}_a - {\\hat{v}}_b & \\frac{\\zeta}{\\gamma} \\sum_{w \\in W} \\left( \\frac{\\#(c, w)}{\\#(c)} -\\frac{\\#(d, w)}{\\#(d)} \\right)\\frac{{v}_w}{\\|{v}_w\\|}\\\\\n& \\zeta({\\hat{v}}_c - {\\hat{v}}_d). \\quad\\blacksquare\n\\end{align*}\n\nThe invariance of the value $\\gamma_c$ can be verified through randomly sampling 5000 words $c$ and computing the respectivve $\\gamma_c$. The resulting mean and variance are respectively $\\bar{\\gamma_c}5.626, \\text{Var}(\\gamma_c)0.033$, indicating a tight concentration around the mean. \n\n \\begin{figure}[ht]\n \\centering\n \\includegraphics[width0.45\\textwidth]{A2}\n \\end{figure}\n $\\gamma_c$ can be seen to tightly concentrate around the median. The mean and variance are respectively $\\bar{\\gamma_c}5.626, \\text{Var}(\\gamma_c)0.033$. \n\n\\subsection{Derivation of Eq.\\ (\\ref{eq:sgns_update})}\\label{app:sgns_update}\nHere, we show that vanilla Skip-gram with the cross-entropy loss where the target distribution is represented as a one-hot vector induces an implicit pulling action on co-occuring words and pushes away other words.\n\nFor a given context word $c$, the cross-entropy loss is:\n\\begin{align*}\nH(p(\\cdot|c), \\hat{p}(\\cdot|c))   -\\sum_{w\\in W} \\hat{p}(w|c)\\log p(w|c),\n\\end{align*}\nwhere $p(w|c)   \\frac{e^{v_c ^\\intercal u_w}}{\\sum_{w' \\in W} e^{v_c ^\\intercal u_{w'}} }$ denotes the predicted distribution by Skip-gram. $\\hat{p}(\\cdot|c)$ denotes the target distribution where:\n\\begin{align*}\n   \\forall w \\in W: \\hat{p}(w|c) \n   \\begin{cases}\n   1 & \\text{if $w$ is the target word}\\\\\n   0 & \\text{otherwise}\n   \\end{cases}   \n\\end{align*}\nBy construction of $\\hat{p}(w|c)$, each term in the sum of the cross-entropy loss reduces to:\n\\begin{align*}\n   &\\hat{p}(w|c)\\log p(w|c) \\\\\n   &\\begin{cases}\n   -\\log \\frac{e^{v_c ^\\intercal u_w}}{\\sum_{w' \\in W} e^{v_c ^\\intercal u_{w'}} } & \\text{if $w$ is the target word}\\\\\n   0 & \\text{otherwise}\n   \\end{cases}   \n\\end{align*}\n\nThus, for a fixed context word $c$ and target word $w$, the loss of Skip-gram reduces to:\n$$\n\\mathcal{L}_{\\textup{SGNS}}(c, w)   - \\log \\frac{e^{v_c ^\\intercal u_w}}{\\sum_{w' \\in W} e^{v_c ^\\intercal u_{w'}}}.\n$$ \n\nNow, consider two words $c, w$ that co-occur. Without loss of generality, if we assume $w$ appears prior to $c$ in the training corpus, Skip-gram first updates the context and target vectors of $w$ and $c$ respectively. Taking the gradient of $\\mathcal{L}_{\\textup{SGNS}}$ with respect to $v_a$ and $u_b$ for two co-occurring words $a, b\\in W$,\n\\begin{align}\n\\frac{\\partial \\mathcal{L}_{\\textup{SGNS}}}{\\partial v_a} & \\sum_{w \\in W} \\left(\\frac{e^{v_a ^\\intercal u_b}}{\\sum_{w' \\in W} e^{v_a ^\\intercal u_{w'}}} u_w\\right) - u_b\\label{eq:sgns_1},\\\\\n\\frac{\\partial \\mathcal{L}_{\\textup{SGNS}}}{\\partial u_b} & \\left( \\frac{e^{v_a ^\\intercal u_b}}{\\sum_{w' \\in W} e^{v_a ^\\intercal u_{w'}}} - 1\\right)v_a\\label{eq:sgns_2}.\n\\end{align}\n\nObserve that the gradients induce a pulling action between the vectors $v_a$ and $u_b$. \n\nDefine the set of words that lie between $c$ and $w$ in the training corpus as $C$. Notice that $c$ and $w$ will co-occur with $\\Delta-1$ words. Hence, for each word $c' \\in C   \\{c_1, ..., c_{w-1}\\}$, the gradient update in Eq.\\ (\\ref{eq:sgns_1}) and (\\ref{eq:sgns_2}) is applied to all word pairs $(w, c_1), ..., (w, c_{w-1})$ and $(c, c_1), ..., (c, c_{w-1})$. \n\nConsider the pulling action induced by the word pairs $(w, c_i)$ and $(b, c_i)$ for some $i \\in [\\Delta-1]$. As we first update the context and target vectors for $w$ and $c_i$, notice that \n\\begin{align}\nv_w^{\\textup{new}} & v_w + u_{c'} - \\sum_{x \\in W} \\left(\\frac{e^{v_w ^\\intercal u_{c'}}}{\\sum_{{w'} \\in W} e^{v_w ^\\intercal u_{w'}}} u_x\\right),\\nonumber\\\\\nu_{c'}^{\\textup{new}} & u_{c'} + \\left(1-\\frac{e^{v_w ^\\intercal u_{c'}}}{\\sum_{{w'} \\in W} e^{v_w ^\\intercal u_{w'}}} \\right)v_w\\label{eq:sgns_3}.\n\\end{align}\nSimilarly, if we now update the context and target vectors for $c$ and $c_i$, \n\\begin{align*}\nv_c^{\\textup{new}} & v_c + u_{c'}^{\\textup{new}} - \\sum_{x \\in W} \\left(\\frac{e^{v_c ^\\intercal u_{c'}}}{\\sum_{{w'} \\in W} e^{v_c ^\\intercal u_{w'}}} u_x\\right).\n\\end{align*}\nPlugging the expression for $u_c^{\\textup{new}}$ in Eq.\\ (\\ref{eq:sgns_3}), we get\n\\begin{align*}\nv_c^{\\textup{new}}   & v_c + \\left(1-\\frac{e^{v_w ^\\intercal u_{c'}}}{\\sum_{{w'} \\in W} e^{v_w ^\\intercal u_{w'}}} \\right)v_w + u_{c'} \\\\\n&-\\sum_{x \\in W} \\left(\\frac{e^{v_w ^\\intercal u_c}}{\\sum_{{w'} \\in W} e^{v_w ^\\intercal u_{w'}}} u_x\\right).\n\\end{align*}\nThe expression above indicates that $v_c$ is pulled towards $v_w$ implicitly and shifted closer to $u_{c'}$ explicitly in the update process while pushing away the weighted average of all word vectors. This update resembles the push-pull action in CWM. \n\n\\subsection{Derivation of Eq.\\ (\\ref{eq:glove_update})}\\label{app:glove_update}\nFor a fixed word pair $i,j$, GloVe's local objective is:\n\\begin{align*}\n\\mathcal{L}_{\\textup{GloVe}}(i, j)   f(X_{ij}) (v_i^\\intercal u_j + b_i + \\tilde{b}_j - \\log X_{ij}),\n\\end{align*}\nwhere $X_{ij}$ is the co-occurrence count of words $i$ and $j$, $f(X_{ij})$ is a weighting term, $b_i, \\tilde{b}_j$ are bias terms, and $v_i$, $u_j$ denote the word vector and context word vectors respectively (cf.\\ \\citealp{pennington-etal-2014-glove}). Typically, $f(X_{ij})$ is set to $\\min\\{(X_i/X_{\\textup{max}})^{\\alpha}, 1\\}$ where $X_i$ denotes the occurrence count of word $i$ and $X_{\\textup{max}}   100$. For the sake of demonstrating the pushing action in the gradient update, we consider a weighting function $f(X_{ij})   \\min\\{(X_i/X_{\\textup{max}})^{\\alpha} + \\epsilon, 1\\}$ for a arbitrarily small $\\epsilon > 0$. \n\nThe derivative of the local objective with respect to $v_i$ and $u_j$ are:\n\\begin{align}\n\\frac{\\partial \\mathcal{L}_{\\textup{GloVe}}}{\\partial v_i} & 2f(X_{ij}) (v_i^\\intercal u_j + b_i + \\tilde{b}_j - \\log X_{ij}) u_j,\\nonumber\\\\\n\\frac{\\partial \\mathcal{L}_{\\textup{GloVe}}}{\\partial u_j} & 2f(X_{ij}) (v_i^\\intercal u_j + b_i + \\tilde{b}_j - \\log X_{ij}) v_i. \\label{eq:glove_gradient}\n\\end{align}\nConsider two co-occurring words $c, w$ and a word $w'$ that co-occurs with neither. Then, there exists a word $c'$ that co-occurrs with $c$ and $w$ but does not co-occur with $w'$. Define $X_{c'w'}   0, X_{cc'}   \\omega_c, X_{wc'}   \\omega_w$ where $\\omega_c, \\omega_w\\in\\mathbb{N}$.\n\nWith Eq. \\eqref{eq:glove_gradient}, the updated vectors for $c, w, w'$ can be written as:\n\\begin{align*}\n&\\resizebox{\\hsize}{!}{$\nv_c^{\\textup{new}}   v_c^{\\textup{old}} + 2f(\\omega_c) (v_c^\\intercal u_{c'} + b_c + \\tilde{b}_{c'} - \\log \\omega_c) u_{c'}$}, \\\\\n&\\resizebox{1\\hsize}{!}{$\nv_w^{\\textup{new}}   v_w^{\\textup{old}} + 2f(\\omega_w) (v_w^\\intercal u_{c'} + b_w + \\tilde{b}_{c'} - \\log \\omega_w) u_{c'}$}, \\\\\n&\\resizebox{1\\hsize}{!}{$\nv_{w'}^{\\textup{new}}   v_{w'}^{\\textup{old}} + 2f(\\epsilon) (v_{w'}^\\intercal u_{c'} + b_{w'} + \\tilde{b}_{c'} - \\log \\epsilon) u_{c'} $}.\n\\end{align*}\nAs $\\forall i, j: f_{X_{ij}} > 0$, notice that \n\\begin{align*}\n(v_c^\\intercal u_{c'} + b_c + \\tilde{b}_{c'} - \\log \\omega_c) &< 0,\\\\\n(v_w^\\intercal u_{c'} + b_w + \\tilde{b}_{c'} - \\log \\omega_w) &< 0,\\\\\n(v_{w'}^\\intercal u_{c'} + b_{w'} + \\tilde{b}_{c'} - \\log \\epsilon) &> 0,\n\\end{align*}\nfor sufficiently large $\\omega_c$ and $\\omega_w$ and for sufficiently small $\\epsilon$. Setting $2\\cdot |f(X_{ij}) (v_i^\\intercal u_{j} + b_i + \\tilde{b}_{j} - \\log X_{ij})|   g(i, j)$, we see that\n\\begin{align}\nv_c^{\\textup{new}} & v_c^{\\textup{old}} + g(c, c')v_{c'},\\nonumber\\\\\nv_w^{\\textup{new}} & v_w^{\\textup{old}} + g(w, c')v_{c'}\\nonumber,\\\\\nv_{w'}^{\\textup{new}} & v_{w'}^{\\textup{old}} - g(w', c')v_{c'}. \\nonumber\n\\end{align}\nThis indicates that $v_c$ and $v_w$ will be pulled towards the context word vectors of words that $c$ and $w$ both co-occur with, while words that do not co-occur with $c$ and $w$ will be pushed away from $v_c$ and $v_w$. \n\n\\section{Supplementary Experiments}\n\n\\subsection{Metric Details}\\label{app:metrics}\nGiven a set of word pairs in an analogy $A\\{(a_1, b_1), (a_2, b_2), \\dots\\}$, PCS measures relative directional alignment by computing the separability of cosine similarities between true vector offsets $v_{a_i} - v_{b_i}$ and false offsets $v_{a_i} - v_{b_j}, i \\neq j$. Concretely, denoting the set of true and false offsets as $P$ and $N$ respectively, PCS computes the expectation of the ROC-AUC score between $P$ and a subset of the false vector offsets $N' \\subset N$ where $|P|   |N'|$:\n$$\n\\text{PCS}(A)   \\mathbb{E}_{N' \\sim U(N)} \\left[\\text{AUC}(P, N')\\right],\n$$\nwhere $U(N)$ denotes the uniform distribution over all false vector offsets. Typically, the expectation is approximated by sampling $s   50$ subsets.\n\nIn contrast, MSM represents the absolute alignment within analogies by computing the cosine similarities between all true vector offsets and the mean of the true offsets.   \n$$\n\\text{MSM}(A)   \\frac{1}{|P|} \\sum_{v_p \\in P} \\cos\\left(v_p, \\frac{1}{|P|} \\sum_{v_p \\in P} v_p\\right)\n$$\nA high value of MSM indicates alignment between true vector offsets. However, note that MSM is susceptible to scoring undesirable vector structures with high values (e.g.\\ when all vectors are collapsed onto one point in the embedding space, MSM $   1$).\n\n\\begin{table}[t]\n\\centering\n\\begin{tabular}{l|c|ccc}\n\\hline\n\\textbf{Model} & \\textbf{\\parallelogramm} & \\textbf{WordSim} & \\textbf{MEN} & \\textbf{SimLex}\\\\\n\\hline\n\\rowcolor{Gray}\nCWM & 0.27 & 0.66 & 0.73 & 0.34\\\\\nSGNS & 0.29 & 0.72 & 0.74 & 0.36 \\\\\nGloVe & 0.29 & 0.61 & 0.75 & 0.37\\\\\n\\hline\n\\end{tabular}\n\\small{\\caption{\\label{table:performances_sup} Performances for embedding models on parallelogram analogy recovery and word similarity tasks. $\\parallelogramm$ refers to parallelogram recovery task. For word similarity, reported values are Spearman's rank correlation between word similarity rankings of human annotators and cosine similarites computed from word vectors.}}\n\\end{table}\n\n\n\\subsection{Word Similarity}\\label{app:word-sim}\nWhile the analogy task is our primary focus, we evaluate CWM on other commonly used benchmarking tasks for completeness. To this end, we benchmark our model on WordSim353 \\cite{Finkelstein2002PlacingSI}, the MEN Test Collection \\cite{Bruni2014MultimodalDS}, and SimLex999 \\cite{hill-etal-2015-simlex}. \n\nOn all tasks, CWM performs comparably with existing models (Table \\ref{table:performances_sup}). We highlight that minor performance differences on word similarity tasks are negligible, as such benchmarks are built using human annotations and are subject to noise. Nevertheless, we believe further refinement of the CWM model is required to boost performance on various downstream tasks. \n\n\\subsection{Analogies as Parallelograms}\\label{app:analogy_parallelograms}\nWe also benchmark all models on the traditional parallelogram analogy recovery task using the BATS dataset. \n\nConcretely, given an analogy pair $a:bc:d$, we utilize the most common metric where we compute the $x$ that satisfies:\n$$\nx   \\min_{x \\in W\\setminus\\{a, b, c\\}} \\|{v_b} - {v_a} + {v_c} - {v_x}\\|,\n$$\nand compare whether $x   d$. \n\nResults from Table \\ref{table:performances_sup} indicate that CWM recovers analogies as parallelograms comparably to existing models. \n However, as mentioned in the main paper, note that various literature posit issues with measuring analogy recovery using the standard method. We further emphasize that performance metrics are included solely for the sake of completeness. \n\n\\end{document}\n",
  "title": "Contrastive Loss is All You Need to Recover Analogies as Parallel Lines"
}
