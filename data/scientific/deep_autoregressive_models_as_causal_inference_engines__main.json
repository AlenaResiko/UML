{
  "authors": [
    "Daniel Jiwoong Im",
    "Kevin Zhang",
    "Nakul Verma",
    "Kyunghyun Cho"
  ],
  "date_published": "2024-09-27",
  "raw_tex": "\\documentclass[10pt]{article}   For LaTeX2e\n \\usepackage{tmlr}   for tmlr\n If accepted, instead use the following line for the camera-ready submission:\n \\usepackage[accepted]{tmlr}\n To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:\n\\usepackage[preprint]{tmlr2}   for arxiv\n\n Optional math commands from https://github.com/goodfeli/dlbook_notation.\n\\input{math_commands.tex}\n\n\\usepackage{hyperref}\n\\usepackage{url}\n\\usepackage{skak}\n\\usepackage{mathtools}\n\\usepackage{tikz}   causal diagram\n\\usepackage{subcaption}\n\\usetikzlibrary{shapes,decorations,arrows,calc,arrows.meta,fit,positioning}\n\\tikzset{\n   -Latex,auto,node distance   0.8 cm and 0.8 cm,semithick,\n   state/.style {ellipse, draw, minimum width   1 cm},\n   point/.style   {circle, draw, inner sep0.04cm,fill,node contents{}},\n   el/.style   {inner sep2pt, alignleft, sloped}\n}\n\n\n\\title{Deep Autoregressive Models as Causal Inference Engines}\n\n Authors must not appear in the submitted version. They should be hidden\n as long as the tmlr package is used without the [accepted] or [preprint] options.\n Non-anonymous submissions will be rejected without review.\n\n\\author{\\name Daniel Jiwoong Im \\email ji641@nyu.edu \\\\\n   \\addr Center for Data Science \\\\\n   New York University\n   \\AND\n   \\name Kevin Zhang \\email kyz2005@columbia.edu \\\\\n   \\addr Computer Science \\\\\n   Columbia University\n   \\AND\n   \\name Nakul Verma \\email verma@cs.columbia.edu \\\\\n   \\addr Computer Science \\\\\n   Columbia University\n   \\AND\n   \\name Kyunghyun Cho \\email kyunghyun.cho@nyu.edu \\\\\n   \\addr Center for Data Science \\\\\n   New York University}\n\n The \\author macro works with any number of authors. Use \\AND \n to separate the names and addresses of multiple authors.\n\n\\newcommand{\\fix}{\\marginpar{FIX}}\n\\newcommand{\\new}{\\marginpar{NEW}}\n\\newcommand{\\bos}{$\\langle$\\texttt{start}$\\rangle$}\n\\newcommand{\\eos}{$\\langle$\\texttt{end}$\\rangle$}\n\\newcommand{\\PeerRead}{\\texttt{PeerRead} }\n\\newcommand{\\edit}[1]{#1}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp} \n\n\\def\\month{07}   Insert correct month for camera-ready version\n\\def\\year{2025}   Insert correct year for camera-ready version\n\\def\\openreview{\\url{https://openreview.net/forum?iduuREHPf2ll}}   Insert correct link to OpenReview for camera-ready version\n\\DeclareMathOperator{\\cido}{\\mathsf{do}}\n\n\\begin{document}\n\n\\maketitle\n\n \\vspace{3pt}\n\\begin{abstract}\n   Existing causal inference (CI) models are often restricted to data with low-dimensional confounders and singleton actions. We propose an autoregressive (AR) CI framework capable of handling complex confounders and sequential actions commonly found in modern applications. Our approach accomplishes this using {\\em sequencification}, which transforms data from an underlying causal diagram into a sequence of tokens. Sequencification not only accommodates training with data generated from a large class of DAGs, but also extends existing CI capabilities to estimate multiple causal quantities using a {\\em single} model. We can directly compute probabilities from interventional distributions, simplifying inference and improving outcome prediction accuracy. We demonstrate that an AR model adapted for CI is efficient and effective in various complex applications such as navigating mazes, playing chess endgames, and evaluating the impact of certain keywords on paper acceptance rates, where we consider causal queries beyond standard reinforcement learning-type questions.\n\\end{abstract}\n \\vspace{3pt}\n\n\\section{Introduction}\n\\label{sec:intro}\n\nModeling causal relationships is important across various fields for informed decision-making~\\citep{Holland1986, Cochran1973, Pearl2010}. However, existing causal inference (CI) algorithms are often limited by their inability to handle \\edit{sequential variable-length actions and high-dimensional covariates}~\\citep{Louizos2017, Kumor2021, Im2021, Lu2022, Zhong2022DESCNDE}. This work aims to address these shortcomings by designing a CI engine applicable to complex data involving high-dimensional variables.\n\n\\edit{Consider a medical setting where a doctor prescribes a sequence of treatments to the patient. In this scenario, the number of treatments that the patient undergoes is not necessarily fixed (e.g.\\ additional procedures may be assigned depending on patient response or two medications might be combined into one). As a result, the number of possible treatment sequences grows combinatorially. Moreover, the medical decision may be dependent on high-dimensional data describing the condition of the patient, such as text from electronic health records. For successful treatment effect estimation, we want a framework that can accommodate arbitrary-length action sequences for complex, high-dimensional confounding data. To tackle this problem, we propose using neural network-based autoregressive (AR) models for causal inference.}\n\n\\edit{Autoregressive (AR) models are a standard framework for learning conditional probability distributions and predicting values in sequential or time-series data. Neural network-based AR models are widely used in applications such as language modeling for next-token prediction and text generation~\\citep{Devlin2019BERTPO,radford2019language,touvron2023llama}. As demonstrated by large language models (LLMs), AR models can capture complex relationships and scale effectively to large datasets. Recent studies~\\citep{Gupta2023ContinualPO, Zhang2024TinyLlama, Xu2024Survey} show that fine-tuning pre-trained LLMs utilizes knowledge from an internet-scale text corpus, enhancing performance on various tasks.}\n\n We propose to leverage autoregressive (AR) models for estimating causal effects. As demonstrated by large language models (LLMs), AR models can capture complex relationships and scale effectively to large datasets. Recent studies~\\citep{Gupta2023ContinualPO, Zhang2024TinyLlama, Xu2024Survey} show that fine-tuning pre-trained LLMs utilizes knowledge from an internet-scale text corpus, enhancing performance on various tasks.\n\n We demonstrate that AR models serve as an effective and efficient statistical engine by treating observations as part of a data-generating process. To achieve this, we propose a method called {\\em sequencification} for representing data based on a known underlying causal diagram. A {\\em single} model trained on sequencified data can utilize learned statistical estimates to answer a variety of causal questions.   During inference, we can efficiently sample high-dimensional confounders and actions, enabling Monte Carlo estimation to approximate causal effects.\n\n We demonstrate that AR models can also be used to perform causal inference by turning observational data into a sequence following the underlying causal DAG. Although the causal structure is not explicitly provided, a neural network-based AR model can still learn the conditional probability distributions implied by the DAG. Moreover, because the AR model estimates all conditional distributions in the sequence, we can compute a variety of interventional distributions.\n\n\\edit{We demonstrate that AR models can also be employed for causal inference by transforming observational data into a sequence following an underlying causal ordering. To achieve this, we employ {\\em sequencification} for representing data based on a known causal diagram, specified as a directed acyclic graph (DAG). A neural network-based AR model can then learn the conditional probability distributions implied by the DAG. This allows for efficient sampling of sequential actions and high-dimensional confounders, enabling Monte Carlo estimation to approximate various causal effects. Furthermore, since the AR model learns all conditional distributions in the sequence, a {\\em single} model trained on sequencified data can be used to compute a wide range of interventional distributions.}\n\n\\edit{Our methodology is designed to handle variable-length sequential actions, combinatorially large action spaces, and high-dimensional confounders. These capabilities encompass a variety of common causal inference tasks, including average treatment effect (ATE) estimation, individual treatment effect (ITE) estimation, interventional distribution approximation, etc. To the best of our knowledge, all prior work can only accommodate fixed-length covariates and actions, and most evaluate on relatively low-dimensional data.}\n\nWe conduct empirical studies across a variety of exemplar tasks, such as navigating mazes, playing chess endgames, and evaluating the impact of specific keywords on paper acceptance rates. The experiments show that our framework can (1) infer causal effects involving high-dimensional variables, (2) generalize to unseen confounders and action sequences, and (3) leverage pre-trained LLMs to answer text-based causal questions. \n The results support the potential of AR models to solve a broader array of problems in CI.\n\n\\section{Related work}\n\\label{sec:related_work}\n\nPrevious work has explored the use of deep learning or AR models for estimating causal effects. Here, we provide a brief overview of the relevant studies.\n\n\\vspace{-3pt}\n\\paragraph{Sequencification for statistical engines.}\nVarious machine learning fields have used linearized representations for tasks. In natural language processing (NLP), linearization (which we refer to as sequencification) is used to convert a syntactic tree into a sequence for building language model-based parsers~\\citep{Vinyals2015, Liu-etal-2022-Autoregressive, Sheng-etal-2023-Unified}. In reinforcement learning (RL), an episode can be encoded as a sequence of states, actions, and rewards. An autoregressive model is then trained on these sequences to capture relationships among the variables~\\citep{Chen2021Decision, NeurIPS2021_099fe6b0}. These instances suggest that AR models trained on sequencified data can effectively learn statistical dependencies among multiple high-dimensional variables.\n\n\\vspace{-3pt}\n\\paragraph{Language models as causal engines.} Several works have used language models for high-dimensional CI tasks~\\citep{Feder2021CausalII, Egami2022make, pmlr-veitch20a}. A key challenge across these studies is satisfying positivity constraints~\\citep{D2021Overlap, Tu2022AnOO, Gui2022Causal}. Techniques from NLP, such as topic models~\\citep{Sridhar2019LDA, Mozer2020Matching}, latent variable models~\\citep{Keith2020Text}, and contextual embeddings~\\citep{pmlr-veitch20a}, have been used to produce low-dimensional embeddings that can satisfy positivity. In addition to using NLP techniques to reduce the dimensionality of observations, natural language can also serve as a proxy for observed confounders. For example, \\citet{Roberts2020Adjusting} apply a text-matching algorithm using contextual embeddings and topic models to estimate causal effects based on proxy texts. \n With sufficiently large models and text corpora, LLMs can also generalize to previously unseen knowledge~\\citep{Chowdhery2023JMLR}. \n Thus, we leverage pre-trained LLMs to capitalize on prior model knowledge and enhance the performance of statistical inference engines.\n\n\\vspace{-3pt}\n\\paragraph{Deep learning for causal engines.}\nVarious deep neural network architectures have been proposed for CI. Representation learning for CI often incorporates a regularization term that enhances generalization for counterfactual actions~\\citep{Shalit2017, Johansson2018, Wang2021}. Deep latent variable CI models learn stochastic latent variables to model potential outcomes using a richer family of distributions~\\citep{Louizos2017, Kocaoglu2017, Im2021}. \\edit{Normalizing flows have also been used for causal effect estimation. For example, \\citet{Melnychuk2022NormalizingFF} develop a doubly robust density estimator for potential outcomes by combining a nuisance and target normalizing flow.}\n\nFor deep autoregressive models, \\citet{Monti2020Autoregressive} introduce an AR flow model that learns an invertible density transformation between variables. Their approach enables direct computation of interventional and counterfactual distributions without the need for complex latent variable manipulations. \\citet{GarridoBorysovRichPereira2021} use neural AR density estimators~\\citep{PMLR-V15-Larochelle11A} to model causal mechanisms and predict causal effects using Pearl\u2019s do-calculus~\\citep{pearl2009causality}. \n\n\\edit{Other studies examine treatment effect estimation in more complex, multi-dimensional scenarios. \\citet{Frauen2024ModelagnosticMF} propose a model-agnostic learner for a sequence of actions over a fixed time horizon. An interesting line of work focuses on real-valued actions (e.g.\\ amount of medical dosage administered) for heterogeneous dose-response estimation using different neural-network architectures, including generative adversarial networks \\citet{Bica2020EstimatingTE}, varying coefficient models~\\cite{Nie2021VCNetAF}, and contrastive representation learning \\citet{Zhu2024ContrastiveBR}.}\n\n\\vspace{-3pt}\n\\paragraph{Causal Generative Models.} Another approach to CI models data as part of a generative process and learns a causal generative model. These methods typically parameterize relationships in a known causal diagram using neural networks. For causal effect identification and estimation, \\citet{Xia2023NeuralCM} use neural causal models trained via gradient-based optimization with a minimization-maximization objective. \\citet{Rahman2024ModularLO} introduce a modular learning framework for optimizing causal generative models in semi-Markovian settings by decomposing the data distribution into $c$-factors~\\citep{Tian2002AGI}. Their method can use pre-trained generative models to learn individual conditional distribution components.\n\n\\vspace{-3pt}\n\\paragraph{Existing limitations.} Previous works exhibit notable weaknesses compared to our approach. First, most methods are validated only on \\edit{relatively} low-dimensional variables (tens of dimensions) and \\edit{fixed-length} actions. In contrast, our AR model is designed to handle high-dimensional confounders and   \\edit{variable-length} series of actions where the number of possible sequences grows exponentially. \\edit{This makes it applicable to settings with high-dimensional data and arbitrarily long sequence of treatments.}\n This makes it applicable to causal diagrams with a large number of variables. \nSecond, prior studies often focus on estimating specific causal effects or learning generative models by optimizing individual components of the causal structure. We instead use a {\\em unified} end-to-end AR model that can estimate multiple causal queries. Third, we extend existing CI capabilities by leveraging pre-trained language models for settings where domain knowledge is essential for accurate inference (e.g.\\ NLP).\n\n\\section{Background}\n\\label{sec:background}\n\nThis section outlines the background knowledge of causal effect estimation and language models necessary for understanding our methodology.\n\n\\subsection{CI problem formulation}\nWe study interactions between the following set of variables: an observable confounder $X$, action $A$, and outcome $Y$. Causal relationships are represented as a directed acyclic graph (DAG), where edges denote direct effects (cf.\\ Figure~\\ref{fig:standard_causal_graph}). By applying the backdoor adjustment formula~\\citep{pearl2009causality}, we can compute the potential outcome resulting from an intervention on the action:\n\\begin{align}\n   \\label{eq:backdoor}\n   Y_a \\coloneqq \\mathbb{E}_Y[Y \\mid \\cido(Aa)] \\coloneqq \\sum_{x,y} y \\cdot   p(Yy \\mid Aa, Xx) p(Xx).\n\\end{align}\nThe notation $\\cido(A   a)$ represents an intervention on $A$, setting its value to $a$. Typically, the confounder $X$ is assumed to be low-dimensional to avoid computing density estimates $p(x)$ in high dimensions. Our goal is to model causal effects when typical assumptions in prior CI work are violated, including settings with complex confounders and combinatorially large action spaces.\n \\footnote{A combinatorially large action space refers to the number of possible action sequences.}\n\n\\edit{We make the following set of standard assumptions in causal inference: (1) unconfoundedness, which states that $Y_0, Y_1 \\indep A \\mid X$, and (2) positivity, which requires that $p(a, x, y) > 0$ for all triplets $(a,x,y)$.}\n\n\\begin{figure*}[t!]\n   \\centering\n   \\begin{minipage}{0.3\\textwidth}\n   \\centering\n   \\begin{tikzpicture}\n   \\node[state] (x) at (0,0) {$X$};\n   \\node[state] (y) [below right   of x] {$Y$};\n   \\node[state] (a) [below left   of x] {$A$};\n   \n   \\path[red, line width1pt] (x) edge (a);\n   \\path (x) edge (y);\n   \\path (a) edge (y);\n   \\end{tikzpicture}\n   \\subcaption{}\n   \\label{fig:standard_causal_graph}\n   \\end{minipage}\n   \\hspace{12pt}\n   \\begin{minipage}{0.3\\textwidth}\n   \\centering\n   \\begin{tikzpicture}\n   \\node[state] (x) at (0,0) {$X$};\n   \\node[state] (y) [below right   of x] {$Y$};\n   \\node[state] (a1) [below left   of x] {$A_1$};\n   \\node[state] (a2) [below   0.6cm of x] {$A_2$};\n   \n   \\path[red, line width1pt] (x) edge (a1);\n   \\path (x) edge (a2);\n   \\path (x) edge (y);\n   \\path (a1) edge (a2);\n   \\path (a1) edge[bend right45] (y);\n   \\path (a2) edge (y);\n   \\end{tikzpicture}\n   \\subcaption{}\n   \\end{minipage}\n   \\hspace{12pt}\n   \\begin{minipage}{0.3\\textwidth}\n   \\centering\n   \\begin{tikzpicture}\n   \\node[state] (x1) at (0,0) {$X_1$};\n   \\node[state] (x2) [right   of x1] {$X_2$};\n   \\node[state] (y) [below   of x2] {$Y$};\n   \\node[state] (a) [below   of x1] {$A$};\n   \n   \\path[red, line width1pt] (x1) edge (a);\n   \\path (x1) edge (x2);\n   \\path (x1) edge (y);\n   \\path[red, line width1pt] (x2) edge (a);\n   \\path (x2) edge (y);\n   \\path (a) edge (y);\n   \\end{tikzpicture}\n   \\subcaption{}\n   \\end{minipage}\n   \\caption{ \\small Causal diagrams illustrating interventions on the action (a), partial action (b), and when conditioning on a prefix of the confounders (c), where $X$, $A$, and $Y$ denote the confounders, actions, and outcome respectively. Bold red arrows indicate the pathways blocked by the corresponding intervention. Potential outcomes can be computed using the backdoor adjustment formula for each scenario.}\n   \\label{fig:causal_diagrams}\n\\end{figure*}\n\n\\subsection{Language models}\nLanguage models (LMs) are designed to predict and generate text by learning linguistic patterns from a training corpus. Let $\\mathbb{W}$ denote the vocabulary of the text corpus, which includes special \\bos \\space and \\eos \\space tokens. LMs estimate the probability of a sequence of tokens ${\\bf w}   (w_1,\\cdots, w_T) \\in \\mathbb{W}^T$ in an autoregressive manner, where $w_1   \\text{\\bos}$ and $w_T   \\text{\\eos}$. \n Let ${\\bf w}_{1:t}$ represent the first $t$ tokens in ${\\bf w}$. \nThe probability of the sequence ${\\bf w}$ is decomposed into the product of next-token probabilities: $p({\\bf w})   p(w_1) \\cdot p(w_2 \\mid w_1) p(w_3 \\mid w_1, w_2) \\cdots p(w_T \\mid w_1, \\ldots, w_{T-1})$.\n \\[ p({\\bf w})   p(\\text{\\bos}) \\cdot \\prod_{t1}^{T} p(w_t|{\\bf w}_{1:t-1}) \\cdot p(\\text{\\eos}|{\\bf w}). \\]\n\n\\section{Language models as statistical inference engines}\n\\label{sec:lm_stat_engine}\n \nAs suggested by Equation~\\ref{eq:backdoor}, CI requires accurate estimation of statistical quantities to calculate causal effects. In this section, we describe how an AR model can be adapted into a statistical inference engine for any DAG involving a set of confounders, actions, and outcomes using sequencification.\n\n\\subsection{Causal graphs}\nWe assume the underlying causal DAG $\\mathcal{G}   (\\mathcal{V}, \\mathcal{E})$ is known, where the vertices $V_i \\in \\mathcal{V}$ represent random variables and the edges $E_{i\\to j} \\in \\mathcal{E}$ denote conditional dependencies. \\edit{Moreover, the graph satisfies the Markov property, meaning the joint probability distribution can be factored into a causally-consistent ordering,}\n\\[ p_\\mathcal{G} (V_1, V_2, \\ldots, V_M)   \\prod_{i1}^M p_\\mathcal{G} (V_i \\mid \\mathrm{Pa}(V_i)), \\]\nwhere $\\mathrm{Pa}(V_i) \\coloneqq \\left\\{ V_j \\in \\mathcal{V} \\mid   E_{j \\to i} \\in \\mathcal{E} \\right\\}$ is the set of parent nodes of $V_i$. We assume that the graph $\\mathcal{G}$ is fully specified and that all variables within $\\mathcal{G}$ are observed.\n\n\\subsection{Sequencification}\nSuppose $V_i$ takes the value ${\\bf v}_i$ from its corresponding distribution. Let $\\mathrm{string}(\\cdot)$ be an injective function that maps ${\\bf v}_i$ to a sequence of tokens: $\\mathrm{string}({\\bf v}_i)   (\\langle\\text{\\texttt{start}}_i\\rangle, w_1, w_2, \\ldots, w_{L_{{\\bf v}_i}})$.\nHere, $\\langle\\text{\\texttt{start}}_i\\rangle$ is a special token indicating the beginning of the string representation for ${\\bf v}_i$, and $L_{{\\bf v}_i}$ is the length of $\\mathrm{string}({\\bf v}_i)$ excluding the $\\langle\\text{\\texttt{start}}_i\\rangle$ token. We define $\\langle\\text{\\texttt{start}}_i\\rangle$ uniquely for each $i$ so that each random variable can be uniquely identified from its string representation by its corresponding initial token.\n\nLet ${\\bf t}   (V_{i_1}, V_{i_2}, \\ldots, V_{i_M})$ be a permutation of the random variables. We say ${\\bf t}$ is a {\\em topological ordering} if $V_i$ precedes $V_j$ in the ordering for all edges $E_{i \\to j}$. Let $\\mathcal{T}$ denote the set of all topological orderings.\n\\footnote{$\\mathcal{T}$ is guaranteed to be non-empty because all DAGs have a topological ordering.} \nConsider $N$ samples drawn from the underlying causal diagram $\\mathcal{G}$: $({\\bf v}_1^{(n)}, {\\bf v}_2^{(n)}, \\cdots, {\\bf v}_M^{(n)}) \\sim p_\\mathcal{G} (V_1,V_2,\\cdots, V_M)$ for $n   1, \\ldots, N$. For each sample, we construct a string ${\\bf s}^{(n)}$ by concatenating the string representations of all random variables according to a topological ordering ${\\bf t}^{(n)}$ selected uniformly at random from $\\mathcal{T}$. \n\nFormally, ${\\bf t}^{(1)}, {\\bf t}^{(2)}, \\ldots, {\\bf t}^{(N)} \\stackrel{i.i.d.}{\\sim} \\mathrm{Uniform}(\\mathcal{T})$ and \n\\[ {\\bf s}^{(n)} \\:\\: \\mathrm{string}({\\bf v}_{i_1}^{(n)}) \\oplus \\mathrm{string}({\\bf v}_{i_2}^{(n)}) \\oplus \\cdots \\oplus \\mathrm{string}({\\bf v}_{i_M}^{(n)}) \\oplus \\text{\\eos}, \\]\nwhere ${\\bf t}^{(n)}   (V_{i_1}, V_{i_2}, \\ldots, V_{i_M})$ and $\\oplus$ denotes string concatenation. We refer to the process of converting observed samples into a sequence of tokens as {\\em sequencification}.\n\nSequencification supports any data that can be transformed into a linear sequence of tokens. For example, tokens may represent specific values for numerical (e.g., binary) data or subwords for text, depending on the problem domain. Our approach can also handle mixed data modalities, as different variables may have separate tokenization strategies.\n\n\\subsection{Randomized topological orderings}\n\nWe randomize over topological orderings consistent with the causal graph to obtain robust estimates of conditional probabilities. For instance, if a node in the graph has multiple independent parents, randomizing the order in which the parents are sequenced helps prevent the model from overfitting to any particular ordering. As a result, different samples may be concatenated using distinct topological orderings. However, because the data is generated by ancestor sampling ${\\bf v}_i^{(n)} \\mid \\mathrm{Pa}({\\bf v}_i^{(n)})$, values that causally influence $\\mathrm{string}({\\bf v}_i^{(n)})$ will always appear earlier in ${\\bf s}^{(n)}$.\n\nA natural question that may arise is how to obtain a good estimate when samples are sequencified according to different topological orderings. This is achieved by using the special $\\langle \\texttt{start}_i \\rangle$ token at the start of each variable $V_i$ during sequencification, which indicates its position in the sequence (regardless of the topological ordering used).\n\n\\subsection{Autoregressive statistical inference engines}\nAfter sequentification, we can train an AR language model, parameterized by $\\theta$, on the sequencified dataset $D   \\{{\\bf s}^{(1)}, {\\bf s}^{(2)}, \\ldots, {\\bf s}^{(N)}\\}$ by the minimizing negative log-likelihood:\n\\begin{align}\n\\label{eq:lm_learn}\n   \\mathcal{L}(\\theta)   -\\frac{1}{N} \\sum_{n1}^N \\log p_\\theta ({\\bf s}^{(n)})   -\\frac{1}{N}\\sum_{n1}^N \\sum^{|{\\bf s}^{(n)}|}_{t1} \\log p_{\\theta}\\left({\\bf s}^{(n)}_t \\Bigm\\vert {\\bf s}^{(n)}_{1:t-1} \\right),\n\\end{align}\nwhere $|\\cdot|$ denotes the length of a string and ${\\bf s}^{(n)}_t$ is the $t^\\text{th}$ token in ${\\bf s}^{(n)}$. The trained model can estimate any conditional probability on $\\mathcal{G}$ by computing $p_\\mathcal{G} (V_i \\mid \\mathrm{Pa}(V_i))\\simeq p_\\theta({\\bf v}_i \\mid \\mathrm{Pa}({\\bf v}_i))$. This is efficiently done by autoregressively traversing the sequence and calculating next-token probabilities using Monte Carlo estimation over the topological orderings.\n\n\\section{Language models as causal inference engines}\n\nIn this section, we illustrate how to infer causal effects by leveraging statistical quantities from a trained AR model, thereby transforming it into a causal inference engine. After learning the conditional distribution over $\\mathcal{G}$ using an AR model, we can estimate causal quantities by deriving the appropriate identification formula from the known causal diagram. Sequencification, combined with knowledge of $\\mathcal{G}$, allows us to estimate various causal effects.\n\n\\subsection{Estimating causal quantities}\n\nWe express the CI problem as a language modeling task. Given our DAG that consists of three variables (the observed confounder $X$, action $A$, and outcome $Y$), we sequencify the data as follows:\n\\[ {\\bf s}^{(n)}   \\text{string}({\\bf x}^{(n)}) \\oplus \\text{string}({\\bf a}^{(n)}) \\oplus \\text{string}(y^{(n)}) \\oplus \\text{\\eos}. \\]\nIn our formulation, ${\\bf x}^{(n)}$ and ${\\bf a}^{(n)}$ can be high-dimensional vector values and the action space can be combinatorially large. Without loss of generality, we treat the outcome variable $Y$ as a scalar, represented using a single token.\n\nWe can use the trained AR model to compute the distribution of $Y$ after intervening on $A$. This is typically intractable when $X$ is high-dimensional because the sample complexity of computing a non-parametric density estimate is exponential in the number of dimensions. However, we can approximate the interventional distribution by sampling from $p_\\theta(X)$ and applying Monte Carlo estimation. \n For ${\\bm x}^{(s)} \\sim p(X)$,\n\\begin{equation}\n   p_\\theta(Yy \\mid \\cido(A{\\bf a}))   \\sum_{{\\bf x}} p_\\theta(y \\mid A{\\bf a},{\\bf x}) p_\\theta({\\bf x}) \\simeq \\frac{1}{S} \\sum_{s1}^{S} p_\\theta(y \\mid A{\\bf a}, {\\bm x}^{(s)}),\n   \\label{eqn:intervention_prob}\n\\end{equation}\nwhere ${\\bm x}^{(s)} \\sim p(X)$.\n\nFurthermore, we can intervene on a prefix subsequence of $A$ even when the action space is large. By expressing $A   A_1 \\oplus A_2$ and marginalizing out $A_2$, we can compute the effect of intervening on only $A_1$.\n\\begin{align}\n   p(Yy \\mid \\cido(A_1{\\bf a}_1)) & \\sum_{{\\bf x}} \\sum_{{\\bf a}_2} p_\\theta(y \\mid A_1{\\bf a}_1, A_2{\\bf a}_2,{\\bf x}) p_\\theta({\\bf a}_2\\mid{\\bf a}_1, {\\bf x}) \\: p_\\theta({\\bf x}) \\label{eqn:exact_partial_action_estimation} \\\\\n   &\\simeq \\frac{1}{S}\\sum^S_{s1} p_\\theta(y \\mid {\\bf a}_1, {\\bm a}_2^{(s)},   {\\bm x}^{(s)}), \\label{eqn:approx_partial_action_estimation} \n\\end{align}\nwhere ${\\bm x}^{(s)} \\sim p(X)$ and ${\\bm a}_2^{(s)} \\sim p(A_2 \\mid {\\bf a}_1,{\\bm x}^{(s)})$. For combinatorially large action spaces, Equation~\\ref{eqn:exact_partial_action_estimation} is generally intractable because the marginalization requires exponentially many operations. \n\nSimilarly, we can condition on a prefix of the confounder by letting $X   X_1 \\oplus X_2$ and marginalizing out $X_2$.\n \\begin{align}\n   p(Yy \\mid \\cido(A{\\bf a}), {\\bf x}_1) & \\sum_{{\\bf x}_2}^{X_2} p_\\theta(y \\mid A{\\bf a}, {\\bf x}_2, {\\bf x}_1) \\, p_\\theta({\\bf x}_2 \\mid {\\bf x}_1) \\label{eqn:exact_conditional_action_estimation}\\\\\n   &\\simeq \\frac{1}{S}\\sum_{s1}^S p_\\theta(y \\mid A{\\bf a}, {\\bm x}_2^{(s)}, {\\bf x}_1)\n   \\label{eqn:approx_conditional_action_estimation}\n \\end{align}\n\\begin{align}\n   p(Yy \\mid \\cido(A{\\bf a}), {\\bf x}_1) & \\sum_{{\\bf x}_2}^{X_2} p_\\theta(y \\mid A{\\bf a}, {\\bf x}_2, {\\bf x}_1) \\, p_\\theta({\\bf x}_2 \\mid {\\bf x}_1) \\simeq \\frac{1}{S}\\sum_{s1}^S p_\\theta(y \\mid A{\\bf a}, {\\bm x}_2^{(s)}, {\\bf x}_1) \\label{eqn:exact_conditional_action_estimation}\n\\end{align}\nwhere ${\\bm x}_2^{(s)} \\sim p(X_2 \\mid X_1   {\\bf x}_1)$. The causal diagrams for these scenarios are shown in Figure~\\ref{fig:causal_diagrams}. Note that we can only intervene on prefixes of the action $A_1$ (or condition on prefixes of the confounder $X_1$) because this ensures the marginalization step samples $A_2$ (or respectively $X_2$) conditioned on preceding variables.\n\nWe emphasize that all causal quantities can be computed by a {\\it single} language model trained on sequencified observations. Our approach enables efficient sampling and computation of conditional $p(Y \\mid A{\\bf a})$, interventional $p(Y \\mid \\cido(A{\\bf a}))$, partial interventional $p(Y \\mid \\cido(A{\\bf a}_1))$, and conditional interventional $p(Y \\mid \\cido(A{\\bf a}), X_1{\\bf x}_1)$   distributions, all using a unified model. This provides an end-to-end framework for computing a variety of causal queries using a single model.\n\nWhile the partial interventional distribution $p(Y \\mid \\cido(A_1   a_1))$ is computable by effectively discarding $A_2$, training an AR model on sequencified data that explicitly includes $A_1$ and $A_2$ is more flexible. Our approach can efficiently estimate not only partial interventional distributions but also interventions on $A_1$ and $A_2$ simultaneously, providing a unified framework for handling multiple causal queries. A similar argument holds for partially conditioning on the confounders.\n\n\\begin{figure*}[t!]\n   \\centering\n   \\begin{minipage}{0.3\\linewidth}\n   \\centering\n   \\includegraphics[width\\linewidth]{figs/maze_path.png}\n   \\subcaption{}\n   \\label{fig:maze_path}\n   \\end{minipage}\n   \\hspace{12pt}\n   \\begin{minipage}{0.3\\linewidth}\n   \\centering\n   \\includegraphics[width\\linewidth]{figs/maze_partial_path.png}\n   \\subcaption{}\n   \\label{fig:maze_partial_path}\n   \\end{minipage}\n   \\hspace{12pt}\n   \\begin{minipage}{0.3\\linewidth}\n   \\centering\n   \\newgame\n   \\notationoff\n   \\fenboard{8/8/5r2/2k5/8/4K3/8/8 w - - 0 1}\n   \\scalebox{0.84}{\\showboard}\n   \\subcaption{}\n   \\label{fig:chess_endgame}\n   \\end{minipage}\n   \\caption{ \\small \\small Illustrations of the maze and chess experimental settings. In the maze experiment, we address two questions: what is the potential outcome given (a) a complete path, and (b) a partial path? The blue path represents the intervention, gray indicates a potential remaining path after a partial intervention, and orange denotes the distance to the end. The chess experiment aims to determine which pieces Black should move to checkmate White the quickest. In the example position shown in (c), the probability of moving the Black king is $0.5$, $0.25$, and $0.8$ with the RCT, non-RCT$_1$, and non-RCT$_2$ policies respectively.}\n   \\label{fig:exp_setup}\n\\end{figure*}\n\n\\section{Experiments} \\label{sec:experiments}\nWe demonstrate the effectiveness of our approach in estimating causal effects for sequential actions and high-dimensional confounders while also assessing robustness to distribution shifts. Our experiments showcase the ability to (1) infer potential outcomes with sequential actions and high-dimensional confounders, (2) efficiently approximate potential outcomes via Monte Carlo sampling, and (3) leverage knowledge from a pre-trained LLM. We evaluate our method across three environments: a maze setting for navigational decision-making, a chess environment analyzing strategic moves in king vs.\\ king-rook endgames, and the \\PeerRead dataset which examines the impact of theorem presence on academic paper acceptance. \n\nThe maze experiments demonstrate that our unified AR model can estimate interventions, partial interventions, and conditional interventions using Equations~\\ref{eqn:intervention_prob}, \\ref{eqn:exact_partial_action_estimation}, and~\\ref{eqn:exact_conditional_action_estimation}, respectively. In contrast, a traditional offline reinforcement learning (RL) model fails to capture all three causal effects. In the chess experiments, we highlight the effectiveness of our method in estimating effects via Monte Carlo approximation using Equation~\\ref{eqn:approx_partial_action_estimation} and its robustness to distribution shifts in the test data. Finally, the \\PeerRead setting demonstrates that the AR model can estimate effects in high-dimensional confounder scenarios and leverage pre-trained language models to improve text-based analysis. These diverse settings enable a comprehensive evaluation of the effectiveness and robustness of our approach for CI \\edit{under variable-length action sequences and high-dimensional confounders such as text. Moreover, our framework is also competitive with existing methods on benchmark treatment effect estimation tasks, as shown in Appendix~\\ref{app:ihdp}.} \n\nAll AR models are trained using a vanilla transformer~\\citep{vaswani2017attention} unless otherwise specified. Additional details on the model architecture and training process can be found in Appendix~\\ref{app:arch_train}.\\footnote{Code is available at \\url{https://github.com/jiwoongim/Deep-Autoregressive-Models-as-Causal-Inference-Engines}.}\n \\footnote{All code and datasets are publicly available at \\href{https://github.com/jiwoongim/causal_inference_timeseries}{https://github.com/jiwoongim/causal\\_inference\\_timeseries}.}\n\n\\subsection{Maze navigation experiments} \\label{sec:maze_exp}\nIn this experiment, we show that a unified AR model can estimate multiple types of causal queries using Equations~\\ref{eqn:intervention_prob}, \\ref{eqn:exact_partial_action_estimation}, and~\\ref{eqn:exact_conditional_action_estimation}. Compared to a baseline offline RL model, our approach offers greater flexibility for causal inference tasks.\n\n\\begin{figure}[t!]\n   \\centering\n   \\includegraphics[width\\linewidth]{figs/maze_violin.png}\n   \\caption{ \\small Error distribution of potential outcome estimates for our AR model (blue) and the offline RL baseline (red). We compute the effect of intervening on the complete path with and without conditioning on the starting row $X_1$. The plot depicts the distribution of errors across all 4096 possible paths of length six, while the black dashed line represents the mean error. The AR and RL models exhibit comparable performance across all settings, with our AR model performing marginally better.}\n   \\label{fig:maze_sequential_action_violin}\n\\end{figure}\n\nWe generate a synthetic maze dataset to analyze the causal effect of traversing different paths. The goal is to determine the distance to the exit after following a given path. The confounding variable $X$ represents the starting position, the action $A$ is a sequence of moves, and the outcome $Y$ denotes the distance from the final position to the exit.\\footnote{$Y$ is the shortest possible distance in the maze while avoiding obstacles, not necessarily the Hamming distance.} We evaluate potential outcomes when intervening on a complete path (cf.\\ Figure~\\ref{fig:maze_path}) or partial path (cf.\\ Figure~\\ref{fig:maze_partial_path}). The obstacle positions are fixed but are not known to the model.\n\nThe end position is fixed at the bottom-right corner, while the starting position is randomly selected from the open spaces with a probability proportional to its distance from the endpoint. Moves in the path are determined by selecting a direction based on the current position. Let the current square be in the $i$th row from the top and the $j$th column from the left. The next move is chosen according to the probabilities:\n\\[ p_\\text{up}   p_\\text{left}   0.1, \\quad p_\\text{right}   \\frac{0.8i}{i+j}, \\quad p_\\text{down}   \\frac{0.8j}{i+j}. \\]\nThis policy encourages paths to move towards the bottom-right corner. All actions are possible at any position, however moves that would collide with obstacles or walls in the maze are treated as no action. Paths are fixed to contain exactly six moves. \n\nWe train an AR model on sequencified data and use a deep Q-learning (DQL) model as an offline reinforcement learning (RL) baseline. The AR model is given only the starting position and must infer the effect of each move along the path. The DQL model follows the standard RL framework, where the current position is known after each move. \\edit{In Appendix~\\ref{app:maze_exp}, we vary the dimensionality of the maze and the length of the path to demonstrate the scalability of our framework in terms of the input and action dimension.}\n\n\\subsubsection{Causal inference using sequential actions}\n\nWe estimate potential outcomes for all paths of length six in a $4 \\times 4$ maze using Equation~\\ref{eqn:intervention_prob}. Additionally, we compute potential outcomes conditioned on the starting row (from the top of the maze) $X_1 \\in \\{1,2,3,4\\}$ using Equation~\\ref{eqn:exact_conditional_action_estimation}. Ground truth values are computed using the corresponding equations with the outcome outcome $\\mathbb{E}_Y[Y \\mid a, x]$ equal to the true number of additional moves required to reach the end of the maze after starting in position $x$ and taking path $a$. For the RL method, we predict the potential outcome as the $q$-value after taking the final action in the intervention.\n\nFigure~\\ref{fig:maze_sequential_action_violin} presents the error distribution for potential outcome estimates across all paths. In all settings, both models produce estimates that closely match the ground truth. Our AR model performs comparably to the offline RL baseline in terms of mean estimation error. The AR error distribution exhibits a large tail, which can be attributed to differences in training setups. \\edit{It is worth emphasizing that the RL model knows the exact position after each move in the path and therefore only needs to predict one step ahead each time, whereas our AR framework only knows the starting position and has to predict six future steps. As expected, the extra knowledge available to the RL model lowers the variance.} However, even without this knowledge, our model achieves slightly \\edit{lower overall error}. These results demonstrate that our approach can accurately predict both intervention and conditional intervention queries.\n\n\\begin{figure}[t!]\n   \\centering\n   \\includegraphics[width0.9\\linewidth]{figs/maze_heatmap.png}\n   \\caption{ \\small Heatmap illustrating the error distribution of potential outcome estimates when intervening on the first four moves in the path. Each row represents all possible actions for the first two moves, while each column represents all possible subsequent two moves. Our AR model accurately estimates potential outcomes for all four-move sequences by learning $p({\\bf a}_2 \\mid \\mathbf{a}_1, \\mathbf{x})$. The offline RL baseline exhibits significant errors across nearly all interventions.}\n   \\label{fig:maze_partial_action_heatmap}\n\\end{figure}\n\n\\subsubsection{Causal inference using partial actions} \n\nIn addition to computing complete interventions, we can also intervene on partial actions. To illustrate this scenario, we estimate potential outcomes when intervening on the initial moves in a path. Specifically, we decompose $A   A_1 \\oplus A_2$, where $A_1$ represents the first four moves and $A_2$ the remaining two moves, and compute $p(Y \\mid \\cido(A_1   \\mathbf{a}_1))$. Ground truth potential outcomes are calculated using Equation~\\ref{eqn:exact_partial_action_estimation} by marginalizing over all possible remaining paths $A_2$. For the RL method, we intervene on the first four moves, while the remaining path is determined according to the learned policy. \n\nFigure~\\ref{fig:maze_partial_action_heatmap} shows the error distribution for potential outcome estimates across all possible four-move interventions. Our AR model accurately computes these estimates using Equation~\\ref{eqn:exact_partial_action_estimation}. In contrast, the offline RL model does not learn the distribution $p({\\bf a}_2 \\mid \\mathbf{a}_1, \\mathbf{x})$ but instead optimizes for the best policy. As a result, it fails to predict partial interventions without modifications, such as discarding information about $A_2$ during training. This highlights the greater flexibility of our AR framework, which allows interventions on any subset of initial moves with a single model. These experiments demonstrate that a {\\em single} AR model can accurately estimate interventional, partial interventional, and conditional interventional distributions.\n\n\\subsection{Chess endgame experiments}\nIn this section, we evaluate the performance of our AR model with Monte Carlo sampling, following Equation~\\ref{eqn:approx_partial_action_estimation}, and assess its robustness to distribution shifts. To explore these aspects in a more complex two-player setting, we use a synthetic chess dataset featuring king vs.\\ king-rook endgames, where White moves first and Black holds the rook. We demonstrate that our AR model can accurately compute causal effects and identify optimal action sequences by comparing potential outcome estimates. Additionally, we leverage Monte Carlo sampling to refine estimates when only partial data is available and introduce a distribution shift between training and testing data to assess generalization.\n\nTo formulate our causal query, we ask: on average, across all starting positions, which pieces should Black move on the first two turns to checkmate White the fastest? Our question aims to uncover a general strategy for king vs.\\ king-rook endgames, much like how controlling the center is a fundamental principle in the opening. More broadly, it parallels CI in scenarios with multiple initial conditions, where the objective is to identify the most effective strategy across a wide range of situations rather than an individual configuration.\n\n Additionally, we leverage Monte Carlo sampling to improve estimates when only a subset of the data is available. To assess the generalization capabilities of our causal effect estimation approach, we also introduce a distribution shift over actions between the training and testing data.\n\nEach endgame comprises a two-move chess game, potentially incomplete. The covariate $X$ is the initial piece positions. The action $A   (a_1, a_2, a_3, a_4)$ represents alternating White and Black moves. Since we consider Black's perspective, we are interested in causal quantities involving $a_2$ and $a_4$. We assume Black plays optimally after selecting which piece to maneuver, so each action only dictates whether to move the king or the rook, but not to which location. The outcome variable $y$ is the number of additional moves required to checkmate with optimal play.\\footnote{In the event of a draw, the outcome is set to 50 due to the 50-move rule.} Formally, we are interested in finding\n\\[ (a_2^*, a_4^*)   \\argmin_{a_2, a_4 \\in \\{\\texttt{king}, \\texttt{rook}\\}} \\mathbb{E}_Y[Y \\mid \\cido(a_2, a_4)]   \\argmin_{a_2, a_4 \\in \\{\\texttt{king}, \\texttt{rook}\\}} \\sum_{x,y} y \\cdot p(Y   y \\mid x, a_2, a_4) p(x). \\]\n\n\\begin{figure*}[t!]\n   \\centering\n   0.99 for arxiv\n   0.98 for tmlr\n   \\includegraphics[width\\textwidth]{figs/chess_rook_king_potential_outcomes.png}\n   \\caption{ \\small \\small Potential outcome estimates for rook-king. The {\\em exact model estimate} uses all 223,660 valid starting positions as test samples, while the {\\em approximate} and {\\em Monte Carlo estimates} use 1,000 randomly selected samples. Each Monte Carlo estimate was repeated 1,000 times, with error bars representing one standard deviation. By sampling from $p_\\theta (X)$, the AR Monte Carlo estimate approaches the AR exact estimate. In contrast, the MLP model cannot perform sampling on $p_\\theta (X)$.}\n   \\label{fig:chess_potential_outcomes}\n\\end{figure*}\n\nWe evaluate the ground truth using the chess engine Stockfish\\footnote{Stockfish is available at \\url{https://github.com/official-stockfish/Stockfish}.}. Figure \\ref{fig:chess_endgame} displays an example endgame.\n\nWe construct three training datasets: one Randomized Control Trial (RCT), which selects each action uniformly at random between moving the king or the rook, and two non-RCT datasets, labeled non-RCT$_1$ and non-RCT$_2$, with distinct action policies. The policy functions for non-RCT$_1$ and non-RCT$_2$ are defined as follows, where $d$ is the Hamming distance between the kings:\n\\[ \\pi_1(a_2, a_4   \\text{king})   \\frac{d}{16}, \\quad \n\\pi_2(a_2, a_4   \\text{king})   \\begin{cases}\n   0.8 & \\text{ if black king is in center $4 \\times 4$ square} \\\\\n   0.2 & \\text{ otherwise}\n\\end{cases}. \\]\n$\\pi_1$ encourages the two kings to be closer while $\\pi_2$ pushes the Black king towards the edge of the board, both of which are required for checkmate. We use different RCT and non-RCT data to demonstrate robustness in the presence or absence of $X \\rightarrow A$ and under varying action assignment mechanisms.\n\nThe testing dataset consists of all 223,660 valid starting positions. To assess out-of-distribution generalization, we use two distinct policies for White in the training and testing phases. White plays uniformly at random over non-optimal moves (unless no such legal moves are available) during training and plays optimally at test time. This introduces a distribution shift, which we use to evaluate generalization and robustness to new settings. Additionally, we show that our proposed AR framework produces more accurate causal estimates by using Monte Carlo sampling from $p_\\theta (X)$ when given only a subset of the testing data.\n\nWe compute the ground truth potential outcomes for all actions and compare them with three different model estimates: the {\\em exact model estimate} using the entire test dataset, the {\\em approximate model estimate} using a subset of the test data, and the {\\em Monte Carlo model estimate}, which also uses the same data subset but additionally generates samples from the model. The approximate and Monte Carlo estimates reflect real-world scenarios, where obtaining a large number of RCT samples is often difficult. We train a non-autoregressive multilayer perceptron (MLP) as a baseline for comparison.\n\nFigure~\\ref{fig:chess_potential_outcomes} presents our three model estimates for the AR and MLP model. As the number of samples in the Monte Carlo approximation increases, the potential outcome estimate converges to the exact estimate. Our results demonstrate that potential outcomes can be efficiently and accurately approximated with an AR model using only a fraction of the test data. Additionally, there is a gap between the AR exact model estimate and the ground truth, caused by the distribution shift in how White plays between the training and test data. The potential outcomes for the remaining actions are provided in Appendix~\\ref{app:chess_exp}. By comparing all potential outcome estimates, we can answer our causal question and conclude that, aggregated across all starting positions, moving the rook twice initially is the best strategy for Black.\n\n\\subsection{\\PeerRead experiments}\nWe use the \\PeerRead dataset~\\citep{kang-etal-2018-dataset} to estimate causal effects in a semi-realistic setting with high-dimensional text confounders. Our results demonstrate that an AR model can leverage pre-trained language models to enhance CI in text-based settings. The dataset consists of paper draft submissions to top computer science conferences, such as NeurIPS, ICML, and ICLR, along with their acceptance or rejection decisions. We investigate the impact of including ``theorems'' on acceptance likelihood and evaluate how well our model captures this causal effect. Building on prior work, we focus on computational linguistics, machine learning, and artificial intelligence papers submitted between 2007 and 2017~\\citep{pmlr-veitch20a}.\n\nThe covariate $X$ represents the paper's abstract text, the action $A$ is a binary variable indicating the presence of the keyword ``theorem'', and the outcome $Y$ is a binary variable indicating acceptance or rejection. Since real-world counterfactual outcomes are inaccessible, we follow prior methods by generating synthetic outcomes based on the action $A$ and the title buzziness $Z$ (i.e., whether the title contains ``deep'', ``neural'', ``embed'', or ``adversarial net''). For example, $z   1$ and $a   1$ likely correspond to a deep learning paper that includes a theorem, while $z   0$ and $a   1$ may represent a theoretical machine learning paper or a deep learning paper with a theorem but without a buzzy title.\n\nDefine $\\pi(z)$ as the proportion of data samples with $a_i   1$ among those satisfying $z_i   z$. Let $\\beta$ be a parameter controlling the level of confounding between title buzziness and the outcome. Following~\\citet{pmlr-veitch20a}, we generate outcomes using the model: ${Y_i \\sim \\text{Bernoulli}(\\sigma(0.25 a_i + \\beta (\\pi(z_i) - 0.2)))}$.\n \\[ Y_i \\sim \\text{Bernoulli}(\\sigma(0.25 a_i + \\beta (\\pi(z_i) - 0.2))). \\]\n\nSince evaluating any causal effect model requires counterfactual outcomes that are inaccessible in real-world data, we use a semi-synthetic setting, where the covariates are real-world data and the labels are generated according to patterns in the data, albeit synthetically. \\edit{We demonstrate the correlation between title buzziness and the text in Appendix~\\ref{app:peerread_exp}}.\n\n\\begin{figure*}[t!]\n   \\captionof{table}{\\small \\PeerRead ATT performance across low, medium, and high confounding levels, with relative error indicated in parentheses. \\edit{For our Deep Autoregressive Causal Inference Engine (DARCIE) BERT and GPT models, we quantify the uncertainty in our estimates by reporting the mean and standard error of the ATT across three trials. Entries in bold and underlined indicate best performing models for each confounding level. Overall, our DARCIE-GPT model achieves the lowest relative error, and both of our models show improvement over other methods.} Training DARCIE-GPT from scratch fails to identify causal effects due to its lack of understanding of the text.}\n   \\label{tab:peerread_performance}\n   {\\small\n   \\begin{tabular}{c||c|c|c} \n   & \\multicolumn{3}{c}{Confounding level}\\\\\\cline{2-4}\n   Confounding level & Low ($\\beta   1$) & Medium ($\\beta   5$) & High ($\\beta   25$) \\\\   \\hline\\hline\n   Ground truth   & $0.062$   & $0.059$ & $0.028$ \\\\   \n   Computed biased & \\underline{\\bfseries \\boldmath $0.065$ ($4.8$\\)} & $0.097$ ($68$\\) & $0.160$ ($470$\\)   \\\\ \n   Reported biased~\\citep{pmlr-veitch20a} & $0.08$ ($30$\\) & $0.15$ ($150$\\) & $0.16$ ($471$\\)   \\\\ \\hline\n   MLP $\\hat \\psi^Q$ & $0.05$ ($20$\\) & $0.10$ ($70$\\) & $0.30$ ($970$\\) \\\\ \n   C-BERT $\\hat \\psi^Q$ & $0.09$ ($45$\\) & \\underline{\\bfseries \\boldmath$0.07$ ($19$\\)} & $0.04$ ($42$\\)   \\\\\n   C-BERT $\\hat \\psi^{\\text{plugin}}$ & $0.10$ ($61$\\) & $0.09$ ($53$\\) & $0.05$ ($78$\\) \\\\ \\hline\n   DARCIE-GPT (No pre-train) & $0.001$ ($98\\$) & $0.002$ ($97\\$) & $0.001$ ($96\\$) \\\\\n   DARCIE-BERT (Ours) & $0.039$ ($37$\\)$\\phantom{}\\pm 0.007$ & $\\:\\, 0.039$ ($34$\\)$\\phantom{}\\pm 0.005$ & $\\:\\, 0.016$ ($43$\\)$\\phantom{}\\pm 0.003$ \\\\\n   DARCIE-GPT (Ours) & $0.041$ ($34$\\)$\\phantom{}\\pm 0.004$ & \\underline{\\bfseries \\boldmath$0.048$ ($19$\\)$\\phantom{}\\pm 0.002$} & \\underline{\\bfseries \\boldmath$0.025$ ($10$\\)$\\phantom{}\\pm 0.002$} \\\\\n   \\end{tabular}}\n\\end{figure*} \n\nFollowing the original experimental design, we report the Average Treatment Effect on the Treated (ATT), \n\\[ \\textup{ATT} \\coloneqq p(Y   1 \\mid \\cido(A   1), \\, A   1) - p(Y   1 \\mid \\cido(A   0), \\, A   1), \\]\nacross three confounding levels: low, medium, and high. A positive ATT indicates that including a theorem increases a paper's chance of acceptance. For larger values of $\\beta$, the outcome becomes more correlated with title buzziness $Z$ rather than the action $A$, so the ground truth ATT is smaller.\n\nWe compare our proposed approach to a non-autoregressive MLP baseline and Causal-BERT (C-BERT) from \\citet{pmlr-veitch20a}.\\footnote{C-BERT learns causally sufficient embeddings: low-dimensional document representations that preserve sufficient information for causal identification and enable efficient causal effect estimation.} Like C-BERT, we fine-tune a pre-trained BERT model on our sequencified representations for a fair comparison. Additionally, we evaluate GPT-2 (referred to as GPT), another pre-trained LLM with a comparable parameter size to BERT~\\citep{radford2019language}.\n\nBERT is trained using masked language modeling (MLM) and next-sentence prediction objectives. MLM randomly masks a fraction of input tokens and trains the model to predict them. GPT is trained with a next-token prediction objective. To adapt BERT for this setting, we randomly sample subsection of the abstract during training and mask the final token to fine-tune it as a next-token prediction model.\n\nAs shown in Table~\\ref{tab:peerread_performance}, our approach outperforms C-BERT and other benchmarks.\n \\footnote{We discuss the strong performance despite differences between synthetic and real-world causal graphs in Appendix~\\ref{app:peerread_exp}.} \nThe primary reason for the improvement is that we jointly learn representations and outcome predictions within a single model, whereas C-BERT uses additional architectural layers for multiple objective functions. \\edit{To quantify the variance in effect estimation for our proposed models, we compute the mean and standard error of the ATT estimates across three trials. Note that most language models tend to have high variance due to the heavy-tailed nature of next-token prediction. \\citet{pmlr-veitch20a} do not report uncertainties in their experiments.}\n\nOur proposed framework leverages the existing knowledge in pre-trained LLMs to accurately estimate causal quantities. While fine-tuning GPT yields successful results, training the model from scratch fails to identify causal effects because the model lacks prior understanding of text. We also demonstrate that our results are robust to the size of the pre-trained language model in Appendix~\\ref{app:peerread_exp}. By using LLMs, our approach outperforms non-autoregressive causal methods and proves more effective for a wider range of CI tasks.\n\n\\section{Conclusion}\nIn this work, we introduce an AR framework for CI that handles high-dimensional confounders and combinatorially large action spaces. Our proposed method, called {\\em sequencification}, transforms data into a linear sequence of tokens based on a known causal diagram. By training a single AR model on sequencified data, we learn conditional distributions between variables in the graph. The framework enables efficient sampling and approximation of several interventional distributions in a unified manner.\n\nWe validate the effectiveness of our method for inferring causal effects across three diverse settings: maze navigation, chess endgames, and academic papers. By handling high-dimensional confounders and combinatorially large action sequences, our work extends the capabilities of CI for a wider range of applications.\n\nOur approach has two main limitations. First, it requires the full causal graph to be known exactly, with all variables observed. A possible solution for handling unobserved values is to impute unconfounded missing data using a mask token. Second, our method supports conditioning or intervening only on variable prefixes. \n While non-prefix interventions is difficult, prior work has explored strategies to address this challenge~\\citep{donahue2020enabling, pmlr-v97-welleck19a, berenberg2022multisegment}. \nWe leave the investigation of solutions to these limitations for future work.\n\n\\section*{Acknowledgements}\n\nThis work was supported by the Institute of Information \\& Communications Technology Planning \\& Evaluation (IITP) with a grant funded by the Ministry of Science and ICT (MSIT) of the Republic of Korea in connection with the Global AI Frontier Lab International Collaborative Research. This work was also supported by the Samsung Advanced Institute of Technology (under the project Next Generation Deep Learning: From Pattern Recognition to AI) and the National Science Foundation (under NSF Award 1922658).\n\n\\bibliography{tmlr/main}\n\\bibliographystyle{tmlr}\n\n\\newpage\n\n\\appendix\n\\section{Training details}\n\\label{app:arch_train}\nThe maze experiments were conducted on a single NVIDIA Tesla T4. All models for the chess and \\PeerRead experiments were trained on a single NVIDIA GeForce RTX 3090 in four and eight hours respectively.\n\n{\\bf Maze experiments.} \nThe maze dataset comprises 10,000 sequencified data points. We use a vanilla transformer with 3 layers, 8 attention heads, and a hidden dimension of 64. For training, we use the Adam optimizer with a batch size of 64. The CI model is trained for 6,250 iterations, while the offline RL model is trained for 5,000 iterations.\n\n{\\bf Chess endgame experiments.} \nWe use a 512-dimensional vanilla transformer with 6 layers and 8 attention heads. The model is trained on a next-token prediction task using the sequencified representation. Training runs for 200 epochs with the Adam optimizer, a batch size of 4096, and a learning rate chosen to be as large as possible without overfitting.   \n\nFor the training dataset, we sample 500,000 two-move chess games per dataset based on Black's policy function. The test dataset includes every game from all 223,660 legal starting positions and all four possible Black actions (king-king, king-rook, rook-king, rook-rook). We sequencify the data by assigning a unique token to each square, legal king and rook move, and outcome.\n\n{\\bf \\PeerRead experiments.}\nWe fine-tuned our models using pre-trained BERT and GPT base model checkpoints.\\footnote{BERT-Base is available at \\url{https://huggingface.co/google/bert_uncased_L-12_H-768_A-12}, and GPT is available at \\url{https://huggingface.co/openai-community/gpt2}.} For BERT, we employed a two-phase training process similar to C-BERT. In the first phase, we trained BERT to generate abstracts, followed by a second phase where it learned to generate full sequences, including both actions and outcomes. GPT, having been pre-trained on next-token prediction, required only a single training phase. This approach ensures a gradual refinement of the generative capabilities specifically tailored to the \\PeerRead corpus. For all training phases, we trained for 100 epochs using the Adam optimizer with a batch size of 16. The learning rate was set as high as possible without overfitting.\n\n\\section{Maze extra experiments} \\label{app:maze_exp}\nWe investigate whether the outcomes produced by our model have some interpretability. To study the predictions of our model, we reuse the synthetic maze setup in Section~\\ref{sec:maze_exp} with slightly modified obstacle positions. The configuration of the maze is shown in Figure~\\ref{fig:maze_ci}.\n\n\\begin{figure*}[h!]\n   \\centering\n   \\centering\n   \\includegraphics[width0.42\\linewidth]{figs/maze_partial_path_app.png}\n   \\caption{ \\small Illustration of the maze setup. We answer a causal question involving intervening on a partial path: is it better to go right or down?}\n   \\label{fig:maze_ci}\n\\end{figure*}\n\nFollowing the original experimental design, the objective is to determine the distance to the exit after following a path. The confounding variable $X$ is the start and end positions, the action $A$ is the sequence of moves in a path, and the outcome $Y$ is the distance from the final position to the exit.\n\nWe consider the question: for a fixed starting state, is it better to go right or down? To answer this, we compute the potential outcome of moving right or down initially. This task is prohibitive for most non-autoregressive CI models as they treat the path as a singular variable. Our model is capable of intervening on any subsequence of actions because the conditional probability given its parents is tractable.\n\n\\begin{figure*}[t]\n   \\centering\n   \\begin{minipage}{0.49\\linewidth}\n   \\includegraphics[width0.98\\linewidth]{figs/error_heatmap_exact_2rows_v2.png}\n   \\subcaption{Marginalization over paths from $A_2 \\cdots A_k$.}\n   \\label{fig:partial_exact}\n   \\end{minipage} \n   \\begin{minipage}{0.49\\linewidth}\n   \\includegraphics[width0.98\\linewidth]{figs/error_heatmap_approx_2rows_v2.png}\n   \\subcaption{Monte Carlo sampling on paths from $A_2 \\cdots A_k$.}\n   \\label{fig:partial_mcapprox}\n   \\end{minipage}\n   \\caption{ \\small Potential outcome MSE between ground truth and model estimates for moving right vs.\\ down first. Both the {\\em exact model estimate} (a) and {\\em approximate model estimate} (b) are shown.}\n   \\label{fig:maze4_partial_potential_outcomes}\n\\end{figure*}\n\nTo compute the potential outcome, we marginalize over subsequent actions as shown in Equation~\\ref{eqn:exact_partial_action_estimation}. Since the paths may be intractable without limiting the maximum length, we cap the longest path to 16 moves as there are a total of 16 positions in the maze. \nTo compare the exact estimates with the Monte Carlo approximations, we report both quantities derived from Equation~\\ref{eqn:exact_partial_action_estimation} and~\\ref{eqn:approx_partial_action_estimation}, and denote them as {\\em exact model estimate} and {\\em approximate model estimate} respectively.\n\nFigure~\\ref{fig:maze4_partial_potential_outcomes} shows the potential outcome estimate of moving right or down initially. The distribution is more concentrated towards smaller distances for moving down and more uniform for moving right. Thus, going down is a better choice for the first move, which matches our intuition based on the maze configuration.\n\n\\begin{figure*}[h!]\n   \\centering\n   \\centering\n   \\includegraphics[width\\linewidth]{figs/maze_errors_rates.png}\n   \\caption{\\small Distribution of effect estimation errors for all possible interventions of the given path length. The inner box indicates the lower and upper quartiles of the distribution, while the point represents the mean. The overall error increases with the input complexity (i.e. higher-dimensional mazes and longer path sequences).}\n   \\label{fig:maze_errors}\n\\end{figure*}\n\nTo demonstrate the scalability of our methodology with respect to the number of variables, we extend the maze experiments using the same setup described in Section~\\ref{sec:maze_exp}. Specifically, we adjust the maze dimensionality and path length for fine-grained control over the covariates $X$ and actions $A$. We increase the maze from 2D (i.e., $4 \\times 4$) up to 5D (i.e., $4 \\times 4 \\times 4 \\times 4 \\times 4$), while keeping the path length fixed at six. Since the starting position in $d$ dimensions is defined by $d$ coordinates, this directly controls the dimensionality of $X$. To vary the number of actions, we modify the path length from four to ten while holding the maze structure fixed at 2D. All model and training details (e.g. number of samples and number of optimization epochs) are identical to the original maze experiments.\n\nWe evaluate our model by computing the absolute error in estimating the effect of intervening on every possible path of a specified length. Figure~\\ref{fig:maze_errors} shows the distribution of these errors as we vary the maze dimensionality and path length. The average estimation error increases with both maze dimensionality and path length due to growing data complexity. The variance also increases significantly with longer paths and more gradually with higher-dimensional mazes. Nonetheless, the overall estimation accuracy remains high, with the majority of errors within $0.5$ of the true effect across all configurations.\n\n\\section{Chess endgame potential outcome estimates}\n\\label{app:chess_exp}\n\nTable~\\ref{tab:chess_performance} compares the potential outcome values for all actions, presenting both exact model estimates and approximate model estimates. Our AR model performs similarly to the baseline across both metrics. Predictions from the model aligns with the ground truth answer to our counterfactual query: on average, moving the rook twice leads to the fastest checkmate.\n\n\\begin{figure*}[h!]\n   \\centering\n   \\captionof{table}{\\small Chess endgame potential outcome estimates for all actions. The outcome represents the number of additional moves required for checkmate. Since Black aims to achieve checkmate as quickly as possible, lower values are desired.}\n   \\label{tab:chess_performance}\n   \\hspace{-0.5cm}\n   {\\small \n   \\begin{tabular}{cc||c|c|c|c} \n   & & \\multicolumn{4}{c}{Potential outcome (Error \\)}\\\\\\hline\n   & & king-king   & king-rook & rook-king & rook-rook \\\\   \\hline\\hline\n   \\multicolumn{2}{c||}{Ground truth} & $22.76$ & $20.18$ & $20.48$ & $17.27$ \\\\ \\hline\\hline\n   & RCT & \\underline{\\bfseries \\boldmath $22.51$ ($1.1\\$)} & \\underline{\\bfseries \\boldmath $19.96$ ($1.1\\$)} & $19.85$ ($3.1\\$) & $17.08$ ($1.1\\$) \\\\\n   MLP exact & non-RCT$_1$ & $22.40$ ($1.6\\$) & $19.85$ ($1.6\\$) & $20.01$ ($2.3\\$) & $17.10$ ($1.0\\$) \\\\ \n   & non-RCT$_2$ & $22.24$ ($2.3\\$) & $19.90$ ($1.4\\$) & $19.69$ ($3.9\\$) & $17.14$ ($0.8\\$) \\\\ \\hline\n   & RCT & $22.17$ ($2.6\\$) & $19.45$ ($3.6\\$) & $17.72$ ($13\\$) & $16.18$ ($6.3\\$) \\\\\n   MLP approx. & non-RCT$_1$ & $22.04$ ($3.2\\$) & $19.32$ ($4.3\\$) & $17.91$ ($13\\$) & $16.12$ ($6.6\\$) \\\\ \n   & non-RCT$_2$ & $21.86$ ($4.0\\$) & $19.31$ ($4.3\\$) & $17.86$ ($13\\$) & $16.18$ ($6.3\\$) \\\\ \\hline\n   & NRCT$_1$ (biased) & $30.90$ ($35\\$) & $19.80$ ($1.9\\$) & $22.87$ ($11\\$) & $14.48$ ($16\\$) \\\\\n   & NRCT$_2$ (biased) & $21.91$ ($3.7\\$) & $21.47$ ($6.4\\$) & $20.33$ ($0.7\\$) & $17.17$ ($0.6\\$) \\\\\n   & RCT & $22.38$ ($1.7\\$) & $19.75$ ($2.1\\$) & $19.96$ ($2.5\\$) & $16.95$ ($1.9\\$) \\\\ \n   AR exact & non-RCT$_1$ & $22.48$ ($1.2\\$) & $19.87$ ($1.5\\$) & \\underline{\\bfseries \\boldmath $20.08$ ($2.0\\$)} & $17.08$ ($1.1\\$) \\\\ \n   & non-RCT$_2$ & $22.05$ ($3.1\\$) & $19.88$ ($1.5\\$) & $19.80$ ($3.3\\$) & \\underline{\\bfseries \\boldmath $17.17$ ($0.6\\$)} \\\\ \\hline\n   & RCT & $21.90$ ($3.8\\$) & $19.19$ ($4.9\\$) & $17.62$ ($14\\$) & $15.97$ ($7.5\\$) \\\\ \n   AR approx. & non-RCT$_1$ & $22.03$ ($3.2\\$) & $19.33$ ($4.2\\$) & $17.95$ ($12\\$) & $16.15$ ($6.5\\$) \\\\ \n   & non-RCT$_2$ & $21.79$ ($4.3\\$) & $19.29$ ($4.4\\$) & $17.54$ ($14\\$) & $16.31$ ($5.6\\$) \\\\ \\hline\n   \\end{tabular}}\n\\end{figure*}\n\nIn Figure~\\ref{fig:chess_potential_outcomes_remaining}, we present the potential outcome graphs for the remaining three actions: king-king, king-rook, and rook-rook. The model behavior on these actions closely aligns with the observations for rook-king in Figure~\\ref{fig:chess_potential_outcomes}. Across multiple interventions, Monte Carlo sampling improves potential outcome estimates when only a subset of the test data is available. Thus, our approach not only effectively models outcomes but also leverages Monte Carlo sampling to enhance predictions in limited data scenarios. \n\n\\begin{figure*}[h]\n   \\begin{subfigure}{\\textwidth}\n   \\centering\n   \\includegraphics[width1.0\\textwidth]{figs/chess_king_king_potential_outcomes.png}\n   \\subcaption{King-king potential outcome estimates.}\n   \\label{fig:king_king_potential_outcome}\n   \\end{subfigure}\n   \\begin{subfigure}{\\textwidth}\n   \\centering\n   \\includegraphics[width1.0\\textwidth]{figs/chess_king_rook_potential_outcomes.png}\n   \\subcaption{King-rook potential outcome estimates.}\n   \\label{fig:king_rook_potential_outcome}\n   \\end{subfigure}\n   \\begin{subfigure}{\\textwidth}\n   \\centering\n   \\includegraphics[width1.0\\textwidth]{figs/chess_rook_king_potential_outcomes.png}\n   \\subcaption{Rook-king potential outcome estimates.}\n   \\end{subfigure}\n   \\begin{subfigure}{\\textwidth}\n   \\centering\n   \\includegraphics[width1.0\\textwidth]{figs/chess_rook_rook_potential_outcomes.png}\n   \\subcaption{Rook-rook potential outcome estimates.}\n   \\label{fig:rook_rook_potential_outcome}\n   \\end{subfigure}\n   \\caption{ \\small Potential outcome estimates for (a) king-king, (b) king-rook, and (c) rook-rook actions. Similar to the rook-king intervention, sampling from $p_\\theta(X)$ enables the AR Monte Carlo estimate to approach the AR exact estimate.}\n   \\label{fig:chess_potential_outcomes_remaining}\n\\end{figure*}\n\n\\section{\\PeerRead extra experiments}\n\\label{app:peerread_exp}\n\n \\begin{table}[t!]\n   \\centering\n   \\caption{ \\small \\edit{Mean and standard error of ATT estimates on \\PeerRead over three training trials. The number in parenthesis indicates the relative error of the mean compared to the ground truth ATT.}}\n   \\begin{tabular}{c||c|c|c} \n   & \\multicolumn{3}{c}{Confounding level}\\\\\\cline{2-4}\n   & Low ($\\beta   1$) & Medium ($\\beta   5$) & High ($\\beta   25$) \\\\   \\hline\\hline\n   Ground truth   & $0.062$   & $0.059$ & $0.028$ \\\\ \n   DARCIE-BERT (Ours) & $0.039$ ($37$\\)$\\phantom{}\\pm 0.007$ & $0.039$ ($34$\\)$\\phantom{}\\pm 0.005$ & $0.016$ ($43$\\)$\\phantom{}\\pm 0.003$ \\\\\n   DARCIE-GPT (Ours)   & $0.041$ ($34$\\)$\\phantom{}\\pm 0.004$ & $0.048$ ($19$\\)$\\phantom{}\\pm 0.002$ & $0.025$ ($10$\\)$\\phantom{}\\pm 0.002$ \\\\\n   \\end{tabular}\n   \\label{tab:peerread_uncertainty}\n \\end{table}\n\n\\edit{We examine the rate at which the ATT error converges to study causal effect prediction in small data cases. Using the \\PeerRead experimental setup, we vary the training set size from a few hundred papers to the full dataset (9305 samples). We repeat each estimation across three trials and report the mean and standard error of the relative ATT error in Figure~\\ref{fig:peerread_error_rate}. For our BERT and GPT models, the error decays gradually with the number of training samples.}\n\n\\begin{figure*}[h!]\n   \\centering\n   \\includegraphics[width0.65\\textwidth]{figs/peerread_error_rate.png}\n   \\caption{ \\small \\edit{Mean and standard deviation of the relative ATT error while varying the training set size. The training data increases exponentially from one hundred points to the entire dataset of 9305 samples.}}\n   \\label{fig:peerread_error_rate}\n\\end{figure*}\n\nWe also study the impact of pre-trained language model size on effect estimation accuracy using three versions of GPT. Table~\\ref{tab:gpt-size-comparison} indicates that larger models generally yield more accurate estimates.\n\nFurthermore, we demonstrate why our model performs well when the outcome is generated from $Z$ and $A$ while the input is derived from $X$ and $A$. To quantify the correlation between title buzziness and the abstract, we train a logistic regression model to predict $Z$ from $X$. For each model, we extract the final hidden layer output as a dense vectorized representation of the abstract. As a baseline, we train a separate logistic regression model using the bag-of-words (BoW) representation of $X$. Table~\\ref{tab:buzzword_abstract_correlation} shows a strong correlation between $X$ and $Z$, explaining the high accuracy of our potential outcome estimations.\n\n\\vspace{6pt}\n\\begin{figure*}[h!]\n   \\begin{minipage}[t]{0.65\\textwidth}\n   \\centering\n   \\captionof{table}{\\small \\PeerRead ATT estimates for GPT small, medium, and large, with relative error shown in parentheses.}\n   \\vspace{6pt}\n   {\\footnotesize \n   \\begin{tabular}{l|c||c|c|c} \n   \\multicolumn{1}{c}{} & & \\multicolumn{3}{c}{Confounding level} \\\\ \\cline{2-5}\n   & \\# params & Low   & Medium & High \\\\   \\hline\\hline\n   Ground truth & & $0.062$   & $0.059$ & $0.028$ \\\\   \n   GPT-small & $117$M & $0.050$ ($20$\\) & $0.044$ ($25$\\) & $0.020$ ($29$\\) \\\\\n   GPT-medium & $345$M & \\underline{\\bfseries \\boldmath $0.052$ ($16$\\)} & $0.053$ ($10$\\) & $0.038$ ($36$\\) \\\\\n   GPT-large & $774$M & $0.051$ ($18$\\) & \\underline{\\bfseries \\boldmath $0.054$ ($8$\\)} & \\underline{\\bfseries \\boldmath $0.021$ ($25$\\)}\n   \\end{tabular}}\n   \\label{tab:gpt-size-comparison}\n   \\end{minipage}\n   \\hspace{12pt}\n   \\begin{minipage}[t]{0.3\\textwidth}\n   \\centering\n   \\captionof{table}{\\small Accuracy and balanced accuracy of title buzziness prediction from the abstract using logistic regression.}\n   {\\footnotesize\n   \\renewcommand{\\arraystretch}{1.06}\n   \\begin{tabular}{c||cc} & Acc. & Balanced acc. \\\\ \n   \\hline\\hline\n   BoW & $87.95\\$ & $73.27\\$ \\\\\n   BERT & $88.57\\$ & $77.91\\$ \\\\\n   GPT2 & $89.18\\$ & $81.00\\$\n   \\end{tabular}}\n   \\label{tab:buzzword_abstract_correlation}\n   \\end{minipage}\n\\end{figure*}\n\\vspace{6pt}\n\n \\edit{To quantify the variance in effect estimation for our proposed model, we compute the mean and standard error of the ATT estimates in Table~\\ref{tab:peerread_performance} across three trials. The results for our GPT and BERT models are included in Table~\\ref{tab:peerread_uncertainty}. Note that most language models tend to have high variance due to the heavy-tailed nature of next-token prediction. \\citet{pmlr-veitch20a} does not report uncertainties in their experiments.}\n\nTo illustrate the positive predicted ATT on the \\PeerRead dataset, we compare $p_\\theta(Y1 \\mid A1, \\, X)$ and $p_\\theta(Y1 \\mid A0, \\, X)$ predicted by GPT in Figure~\\ref{fig:peerread_mode_outcome_dist}. Our model consistently favors papers containing theorems regardless of title buzziness.\n\n\\begin{figure*}[h!]\n   \\centering\n   \\includegraphics[width0.55\\textwidth]{figs/peerread-outcomes.png}\n   \\caption{ \\small Conditional outcome distributions given $A   0$ vs.\\ $A   1$ for medium confounding data ($\\beta   5$).}\n   \\label{fig:peerread_mode_outcome_dist}\n\\end{figure*}\n\n\\section{Ablation studies on IHDP} \\label{app:ihdp}\n\\edit{Although the experiments in Section~\\ref{sec:experiments} are designed to demonstrate the effectiveness of our AR model in scenarios involving variable-length action sequences and high-dimensional covariates, it is also competitive with existing methods on benchmark causal datasets. To illustrate this, we evaluate our model on a semi-synthetic baseline using the Infant Health and Development Program (IHDP) data. The dataset consists of a 25-dimensional covariate $X$ (6 of which are continuous features), a binary treatment $A$, and a continuous outcome $Y$. The evaluation metric is ATE prediction error.} \n\n\\edit{We compare performance with four other baseline models: a neural network (NN), random forest (RF), causal forest (CF), and CRNET~\\citep{Zhu2024ContrastiveBR}. For the AR model, we use a feedforward network embedding layer to handle continuous numerical values. We report the mean and standard error of the ATE error on the test set across 30 different random train-test splits in Table~\\ref{tab:ihdp}. Our method is competitive with common existing methods and outperforms CRNET while having lower variance.}\n\n\\begin{table}[h!]\n   \\centering\n   \\caption{ \\small \\edit{Evaluation of existing causal methods and our proposed AR model on the IHDP benchmark. We report the mean and standard error of the absolute and relative ATE error across 30 training trials. The best performing model is underlined and in bold.}}\n   \\begin{tabular}{c||c|c}\n   Model & ATE Error & ATE Relative Error (\\) \\\\\n   \\hline \\hline\n   NN & $0.1651 \\pm 0.0256$ & $4.11 \\pm 0.64\\$ \\\\\n   RF & $0.1171 \\pm 0.0110$ & $2.92 \\pm 0.27\\$ \\\\\n   CF & \\underline{$\\mathbf{0.1085 \\pm 0.0147}$} & \\underline{$\\mathbf{2.70 \\pm 0.37\\}$} \\\\\n   CRNET & $0.1550 \\pm 0.0694$ & $3.86 \\pm 1.73\\$ \\\\\n   DARCIE (Ours) & $0.1104 \\pm 0.0118$ & $2.75 \\pm 0.29\\$\n   \\end{tabular} \\label{tab:ihdp}\n\\end{table}\n\n \\clearpage\n\n \\input{tmlr/supplement}\n\n\\end{document}\n",
  "title": "Deep Autoregressive Models as Causal Inference Engines"
}
