{
  "authors": [
    "Yibo Jiang",
    "Nakul Verma"
  ],
  "date_published": "2019-10-30",
  "raw_tex": "Clustering is one of the most ubiquitous techniques in data analysis that has important applications in numerous domains that extend far beyond machine learning \\citep{xu2003document,cai2004hierarchical,tung2010enabling,Xu2015}. \n, economics \\TODO{citations}, physics \\TODO{citations}, computer vision \\citealp{tung2010enabling, cai2004hierarchical} and language understanding \\cite{xu2003document} to name a few). \nThe main objective of clustering is to group a set of objects in such a way that objects in the same group (cluster) are more \\emph{similar} to each other (with respect to some domain-specific notion of similarity), than to those in other groups.\n\nThe prevailing approach to clustering is to optimize a specific objective function (usually called the cluster loss) that encodes the desired domain-specific notion of similarity to reveal the underlying group structure in data. Certain cluster losses such as $k$-Means and DBSCAN \\citep{DBLP:conf/kdd/EsterKSX96} have shown promising results in various applications, such as object recognition \\citep{stockman1987object} and recommendation systems \\citep{kim2008recommender}. \nIn modern settings, as the data we collect has become more complex, more expressive cluster losses encoded by deep network architectures have shown some success as well   \\citep{DBLP:conf/icml/HuMTMS17,DBLP:journals/corr/abs-1801-07648}. \\TODO{But deep network needs large amount of data to train, making it difficult to be applied to datasets that might be deemed too small for deep models. This is equivalent to one-shot/few-shot learning in the supervised case}.\n\nDeep neural networks have proven promising for clustering real-world data because of their high representational power \\cite{DBLP:journals/corr/abs-1801-07648}. \n\nA practitioner usually hand-picks a cluster loss in an ad-hoc manner; they basically choose whichever loss (either a preexisting one, or a newly designed one) that seems to give a satisfactory result. And a priori, it is unclear how this choice can be made in a principled way, or better yet completely bypass this step and directly achieve a good clustering.   \n   \nIn this work, we want to take a step towards this direction. Rather than optimizing for a specific custom-designed loss, we develop a model that \\emph{learns} how to cluster. \nSuch ``learning to learn'' approach comes under the framework of meta-learning.\n Instead of training on a large amount of data from a single task, meta-learning systems are trained on a large number of (similar) smaller tasks and are used to make predictions on newly available (similar) tasks.\nDeep meta-learning systems have shown remarkable success on supervised learning \\citep{DBLP:conf/icml/SantoroBBWL16,Mishra2017ASN} and reinforcement learning tasks \\citep{DBLP:journals/corr/MirowskiPVSBBDG16,Wang2018PrefrontalCA}.\n\nWe propose a simple yet highly effective meta-learning model to solve for clustering tasks. Our model finds the cluster structure directly without having to choose a specific cluster loss for each new clustering problem. \nThere are two key challenges in training such a model for clustering:\n\\begin{enumerate}[label(\\roman*)]\n\\item since clustering is fundamentally an unsupervised problem, there is a lack of availability of true cluster identities for each training task, and\n\\item cluster label for each new datapoint depends upon the labels assigned to other datapoints in the same clustering task.\n \\item any permutation of cluster identity (that is, interchanging the cluster labelings across groups) does not change the clustering quality. \\TODO{remove this if nothing to say below}\n\\end{enumerate}\n\nWe systematically address each of these issues: (i) We show that trainining on simple synthetically generated datasets or other real world labelled datasets with similar statistics (such as having the same number of dimensions and number of categories) can generalize to real previously unseen datasets. \nWe use data generated using mixtures of Gaussians, and simple nonlinear transformations on it. \\TODO {Also, training on existing real-world labeled datasets can also generalize to unseen datasets.} \n(ii) We train sequentially and use a recurrent network (using LSTMs, \\cite{hochreiter1997long}) as our clustering model. A recurrent memory model helps assign clustering labels effectively based on data points seen before. \n (iii) \\TODO{say something about point (iii), if this was not addressed, remove}\n\nOur experiments reveal several remarkable observations: even if we train our meta-learning model just on simple synthetically generated data, we can achieve better clustering results on some real-world problems than by using popular preexisting linear and non-linear benchmark cluster losses.\nIn addition, our meta model trained on labelled real datasets of different distributions can also transfer its clustering ability to unseen datasets.\nMoreover, by effectively \\emph{learning} how to cluster, unlike centroid-based clustering approaches like $k$-means, our meta-learning model also has the ability to approximate the right number clusters in simple tasks; thereby obviating the need to pre-specify the number of clusters. \n(NOT EFFECTIVELY ENOUGH) \\TODO{make sure I am not promising more than what we can deliver!!} \nTo best of our knowledge, this is the first clustering model using end-to-end deep meta-learning framework.\n",
  "title": "Meta-Learning to Cluster"
}
