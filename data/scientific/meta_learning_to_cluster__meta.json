{
  "authors": [
    "Yibo Jiang",
    "Nakul Verma"
  ],
  "date_published": "2019-10-30",
  "raw_tex": "We now describe our formulation in detail.\n\n\\subsection{Problem Setup}\n\\label{sec:meta-setup}\n A meta-learning algorithm $\\mathcal{M}$ takes a set of tasks $\\{\\mathcal{T}_i \\}$ as training data and outputs a function $\\mathcal{F}(\\cdot)$ that produces labels based on a new test task $\\mathcal{T}_{\\textup{test}}$. \n\nA meta-clustering model $\\mathcal{M}$ maps data points to cluster labels. During meta-learning, the model\nis trained to be able to adapt to a set of clustering tasks $\\{\\mathcal{T}_i \\}$. At the end of meta-training, $\\mathcal{M}$ would produce clustering labels for new test task $\\mathcal{T}_{\\textup{test}}$.\n\nIn our case, each training task $\\mathcal{T}_i$ consists of a set of data points $\\mathcal{X}^i$ (a dataset) and their associated cluster labels $\\mathcal{L}^i$. Hence, each $\\mathcal{T}_i   (\\mathcal{X}^i, \\mathcal{L}^i)$. It is worth noting that \n$\\mathcal{X}^i$ and $\\mathcal{L}^i$ are themselves partitioned into subsets based on cluster identities. More specifically, $\\mathcal{X}^i   \\{\\mathcal{X}^i_1,\\mathcal{X}^i_2,\\ldots,\\mathcal{X}^i_{K_i} \\}$ and $\\mathcal{L}^i   \\{\\mathcal{L}^i_1,\\mathcal{L}^i_2,\\ldots,\\mathcal{L}^i_{K_i} \\}$, where $K_i$ is the number of clusters in task $i$. Notice that unlike the supervised learning case, we allow the number of clusters for each task to vary. By introducing this cluster-specific generalization, our clustering model has the flexibility to potentially approximate the number of clusters during test time. Since labeling for a test task is yet to be determined, the structure of the test task $\\mathcal{T}_{\\textup{test}}$ is different from training and consists of only a set of datapoints $\\mathcal{X}_\\textup{test}$.\n\nThe cluster labels for training can either come from synthetically generated training tasks or by using labelled data from similar application domains. In Section~\\ref{sec:experiments}, we will present experiments on training with synthetic data, real labeled data and mixture of the two.\n\n To keep our model general, and to make it work on previously unseen application domains, we primarily focus on training using synthetic datasets. \\TODO{also real datasets now}\n\nAnother important distinction between supervised and the unsupervised case is that any permutation of the labeling does not change the cluster quality. This additional degree of freedom can potentially hinder the model optimization and the efficacy of the final prediction. We circumvent this issue by fixing a permutation during training, thereby limiting the parameter search space and accelerating the learning process (see Section \\ref{sec:synth_data} for details).\n\n\\subsection{Proposed Network Architecture and Algorithm}\n\\label{sec:method}\n\\textbf{A Need for Memory-Based Model.} Unlike the supervised case where classification label for a datapoint can be determined by its feature values alone, cluster identity for a datapoint depends solely on the identity assigned to its neighbors (previously seen data points). This necessitates using a memory based clustering model. \nWe choose an LSTM network \\citep{hochreiter1997long} to capture long range dependencies as we do training and testing sequentially.\nApart from storing sequential information over extended time intervals, LSTMs are also flexible enough to learn what kind of information should be passed or blocked for effective prediction (see   \\citealp{hochreiter1997long} for details). LSTMs have also been shown to give good accuracies in supervised meta learning \\citep{DBLP:conf/icml/SantoroBBWL16}. Here we apply them for the unsupervised meta-clustering case.\n\nAt each time step $t$, our LSTM module takes in a datapoint $\\mathbf{x}$ and a score vector $\\mathbf{a}_{t-1}$ from previous time step ${t-1}$\nand outputs a new score $\\mathbf{a}_t$ for the current timestep. The score vector encodes the quality the predicted label assigned to the datapoint $\\mathbf{x}$. \n\n\\begin{figure}[t]\n\\centering\n   \\includegraphics[width0.5\\textwidth]{figures/model_cropped.pdf}\n   \\caption{LSTM architecture of meta-learning for clustering.} \n   \\label{fig:architecture}\n\\end{figure}\n\n\nIn this work, we use four LSTMs stacked on top of each other. The first three layers all have 64 units with residual connections while the last layer can have number of hidden units as either (i) the number of clusters (if the desired number of clusters is known, like in the case of $k$-means), or (ii) the maximum number of possible clusters (if the number of clusters is unknown). \nSee Figure \\ref{fig:architecture} for more details.\n\n\n\\textbf{Loss Function and Optimization.}\nOur network architecture for meta-clustering optimizes for a loss function that is a combination of classification loss $\\Lclassify$ (that ensures output labels match the training labels) and a local loss $\\Llocal$ (that ensures output labels for nearby datapoints match each other). Specifically,\n $\\mathbf{L}_\\textup{meta-cluster}(\\Phi) :\\lambda \\Lclassify (\\Phi) +(1-\\lambda)\\Llocal (\\Phi)$,\n\\begin{align*}\n   \\mathbf{L}_\\textup{meta-cluster}(\\Phi) &:\\lambda \\Lclassify (\\Phi) +(1-\\lambda)\\Llocal (\\Phi),\n\\end{align*}\nwhere $\\Phi$ denotes the parameters of our architecture, $\\lambda$ is a hyper-parameter controlling the trade-off between the two losses.\n\nLearning the model parameters for the task $\\mathcal{T}_i$ proceeds as follows. Let $\\mathbf{x}_j^i \\in \\mathcal{X}^i$ be a datapoint with ground truth cluster label $s_j^i\\in\\{1,2,\\ldots,K_i\\}$, and let $\\mathbf{r}_j^i :[r_j^{i,1}, \\ldots , r_j^{i,K}]$ be the predicted score vector (i.e.\\ the normalized vector $\\mathbf{a}$) returned by our model (here $K$ denotes the size of the output layer), and the individual components $r_j^{i,k}$ represent the predicted probability of $\\mathbf{x}_j^i$ belonging to cluster $k$. One can also view $\\mathbf{r}_j^i$ as a soft assignment of the datapoint to the $K$ clusters. \n\nUsing these definitions, we define $\\Lclassify$ and $\\Llocal$ as \n $\\Lclassify:-\\sum_{j\\textrm{th point in } \\mathcal{X}^i}{\\log(r_j^{i, s_j^i})}$ and $\n   \\Llocal:\\sum_{j\\textrm{th point in } \\mathcal{X}^i} {\\sum_{j' \\in \\mathcal{N}(j)} \\textup{KL}(\\mathbf{r}_j \\|\\mathbf{r}_{j'})}$.\n\\begin{align*}\n   \\Lclassify&:-\\sum_{j\\textrm{th point in } \\mathcal{X}^i}{\\log(r_j^{i, s_j^i})}, \\\\\n   \\Llocal&:\\sum_{j\\textrm{th point in } \\mathcal{X}^i} {\\sum_{j' \\in \\mathcal{N}(j)} \\textup{KL}(\\mathbf{r}_j \\|\\mathbf{r}_{j'})}.\n\\end{align*}\nNote that $r_j^{i,s_j^i}\\in \\R$ is simply the predicted probability of $\\mathbf{x}_j^i$ belonging to its ground truth cluster $s_j^i$, and $\\mathcal{N}(j)$ denotes the nearest neighbors of $j$th datapoint. We chose the number of neighbors as 3 for all our experiments, which can be tuned further by cross-validation.\n\nWe train our model using Adam optimizer \\citep{Kingma2014AdamAM}. Note that typical optimizers used for meta-learning, such as MAML \\citep{Finn2017ModelAgnosticMF} and Reptile \\citep{Nichol2018OnFM}, are specifically geared towards optimizing for supervised meta-learning tasks, and do not perform as well in our case. This is likely due to the sequential nature of our model.\n\nThe overall training and testing (across all training and testing tasks) is performed as follows.\n\n\n\n\\begin{algorithm}\n\\caption{Meta-Clustering (Training)}\n\\label{alg:meta-clustering}\n\\begin{algorithmic}[1]\n\\STATE Initialize model parameters $\\Phi$ \n\\FOR {iteration $i1,2,3,\\ldots$}\n   \\STATE {Clear LSTM cell states}\n   \\STATE {Randomly sample $N$ (batchsize) training datasets $\\{(\\mathcal{X}^{i_1}, \\mathcal{L}^{i_1}),\\ldots, (\\mathcal{X}^{i_N},\\mathcal{L}^{i_N})\\}$}\n\\FOR{epochs $1,2,3,\\ldots$}\n   \\STATE {Shuffle each sampled dataset $(\\mathcal{X}^i, \\mathcal{L}^i)$}\n   \\STATE {Sequentially update $\\Phi$ via Adam Opt. }\n   \\STATE \\COMMENT{LSTM cell states are kept across epochs}\n\\ENDFOR\n\\ENDFOR\n\\STATE \\textbf{return} $\\Phi$\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\\begin{algorithm}\n\\caption{Meta-Clustering (Testing)}\n\\label{alg:meta-clustering-test}\n\\begin{algorithmic}[1]\n\\STATE {Use the learned $\\Phi$ from training}\n\\FOR{each testing dataset $\\mathcal{X}$}\n\\STATE {Clear LSTM cell states}\n\\STATE {Sample a testing dataset $\\mathcal{X}$}\n   \\FOR{epochs $1,2,3,\\ldots$}\n   \\STATE {Shuffle test dataset $\\mathcal{X}$}\n   \\STATE {Sequentially predict the cluster id}\n   \\STATE \\COMMENT {LSTM cell states are kept across epochs}\n   \\ENDFOR\n\\ENDFOR\n\\STATE \\textbf{return} the corresponding clusterings \n\\end{algorithmic}\n\\end{algorithm}\n\n\n\nObserve that during each iteration in training, we randomly sample $N$ (batchsize) training datasets from our given pool of training tasks. The datasets as well as their ground truth labels (for optimization) are fed into our LSTM network architecture sequentially. The LSTM cell states are kept across epochs. This enables the network to keep memory of previously seen data points. \n\n\nDuring the testing phase, each test task is fed into the pre-trained network to obtain the clustering. \nIt is important to note that datapoints in each dataset are shuffled across iterations during both the training and testing phases. This prevents potential prediction errors introduced by specific sequence orders. \n\n\n\n\n\\subsection{The Possibility of Meta-Clustering}\nKleinberg's impossibility theorem \\citep{DBLP:conf/nips/Kleinberg02} states that clustering is impossible because there is no clustering algorithm that can simultaneously satisfy three very intuitive axioms that any clustering algorithm should follow (the so called axioms of \\textit{Scale-Invariance}, \\textit{Richness}, and \\textit{Consistency}). Interestingly, for \\emph{meta clustering} one can formulate a generalized set of axioms that are in fact   consistent showing the \\emph{possibility} of meta-clustering in a specific framework as detailed by   \\citet{DBLP:conf/nips/Garg18}. Since our framework is different from \\citet{DBLP:conf/nips/Garg18} (cf.\\ Section \\ref{sec:related}), their result is not directly applicable.\nLuckily we can, however, formulate an alternate set of axioms that are more akin to our framework and achieve consistency. In this section, we will discuss how we can reframe the three axioms for meta-clustering to circumvent Kleinberg's impossibility result. \n\nConsider a finite set of points $X$ and the class of \nall possible symmetric distance distance functions $D(X)$ on $X$.\nA clustering algorithm $\\mathcal{A}$ can be viewed as taking a distance function $d \\in D(X)$ as an input and returning a partition---i.e., a clustering---of $X$. With this notation, Kleinberg's clustering axioms for any clustering algorithm $\\mathcal{A}$ can be stated as follows. \n\n \\begin{theorem}\n There is a meta clustering algorithm that satisfies Meta-Scale-Invariant and Meta-Richness, Meta-Consistency for any dataset of size greater than 2.\n \\TODO{the proof has been changed}\n \\end{theorem}\n\n\\begin{itemize}\n   \\item \n   \\textbf{Scale-Invariance}. For any distance function $d$ and any $\\alpha > 0$, $\\mathcal{A}(d)   \\mathcal{A}(\\alpha \\cdot d)$.\n   \n   \\item\n   \\textbf{Richness.} For any finite $X$ and clustering $\\mathcal{C}$ of $X$, there exists a distance function $d \\in D(X)$ such that $\\mathcal{A}(d)   \\mathcal{C}$.\n   \n   \\item\n   \\textbf{Consistency.} Let $\\mathcal{C}$ be the clustering produced by some distance function $d \\in D(X)$, that is $\\mathcal{A}(d)   \\mathcal{C}$. Consider any distance function $d'\\in D(X)$, such that for all   $x, \\bar x \\in X$, if $x, \\bar x$ are in the same cluster in $\\mathcal{C}$ then $d'(x,\\bar x) \\leq d(x,\\bar x)$, and if $x,\\bar x$ are in different clusters in $\\mathcal{C}$ then $d'(x,\\bar x) \\geq d(x,\\bar x)$. Then it must be the case that $\\mathcal{A}(d')   \\mathcal{A}(d)$.\n   \n\\end{itemize}\n\\citet{DBLP:conf/nips/Garg18} suggests a re-framing of these axioms for \\emph{meta}-clustering. Specifically, by introducing a variant of Scale-Invariance, \\citet{DBLP:conf/nips/Garg18} shows that there is a meta-clustering algorithm that satisfies the new Scale-Invariance axiom and whose output always satisfies Richness and Consistency. Different from their version, we consider a formulation that is more appropriate and applicable in our setting. \n\nSuppose $\\mathcal{M}$ is a meta-clustering algorithm as described in Section~\\ref{sec:meta-setup}. We can view $\\mathcal{M}$ to consist of two steps. First, $\\mathcal{M}$ takes a distance function $d$ on the input dataset $X$ and outputs a \\emph{clustering algorithm} $\\mathcal{A}$ (instead of a clustering), i.e.\\ $\\mathcal{M}[d]   \\mathcal{A}$. Second, the clustering algorithm $\\mathcal{A}$, in turn, will do the clustering via $d$ and outputs a partition $\\mathcal{C}$, i.e. $\\mathcal{A}(d)   \\mathcal{C}$. Essentially, $\\mathcal{M}[d](d)   \\mathcal{C}$. $\\mathcal{M}$ is trained in the meta-training phase to adapt to clustering tasks, provided with datasets and true clustering labels, unlike \\citet{DBLP:conf/nips/Garg18} which returns one clustering algorithm based on training datasets and labels. In our case, the LSTM architecture does both the steps, performing clustering on input data $X$ but also adapting to $X$. The adaptation happens through the change of activations and gates' values inside the LSTM. But how the activations and gates' values of LSTM are changed for different input data is determined by the LSTM weights which are trained during meta-training and are fixed through meta-testing. \nThe meta version of the axioms in our setting are as follows.\n\\begin{itemize}\n   \\item \n   \\textbf{Meta-Scale-Invariance.} For any $\\alpha > 0$, $\\mathcal{M}[d](d)   \\mathcal{M}[\\alpha d](\\alpha d)$.\n   \\item\n   \\textbf{Meta-Richness.} For any finite $X$ and clustering $C$ of $X$, there exists $d \\in D(X)$ such that $\\mathcal{M}[d](d)   \\mathcal{C}$.\n   \\item\n   \\textbf{Meta-Consistency.} \n   Let $\\mathcal{C}$ be the clustering produced by some distance function $d \\in D(X)$, that is $\\mathcal{M}[d](d)   \\mathcal{C}$. Consider any distance function $d'\\in D(X)$, such that for all   $x, \\bar x \\in X$, if $x, \\bar x$ are in the same cluster in $\\mathcal{C}$ then $d'(x,\\bar x) \\leq d(x,\\bar x)$, and if $x,\\bar x$ are in different clusters in $\\mathcal{C}$ then $d'(x,\\bar x) \\geq d(x,\\bar x)$. Then it must be the case that $\\mathcal{M}[d](d')   \\mathcal{M}[d](d)$.\n   Let $d, d' \\in D(X)$ such that $\\mathcal{M}[d](d)   \\mathcal{C}$ and for all $x, x' \\in X$, if $x, x'$ are in the same cluster in $C$ then $d'(x, x') \\leq d(x, x')$ while if $x, x'$ are in different clusters in $\\mathcal{C}$, then $d'(x,x') \\geq d(x,x')$. Then, $\\mathcal{M}[d](d')   \\mathcal{M}[d](d)$.\n\\end{itemize}\n   The original consistency axiom requires $\\mathcal{A}(d)   \\mathcal{A}(d')$, where $d'$ can shrink intracluster and expand intercluster distances. It makes sense with respect to the clustering $\\mathcal{C}   \\mathcal{A}(d)$. \n   The impossibility arises from the fact that the distance distortion (shrinking and expansion) of $d'$ could have been from any distance function either the original $d$ or some other $d''$ that may produce a different clustering. In other words, we view $d'$ as a change to $d$ but it could also be viewed as a change from a different distance function $d''$. These two paths ($d'' - d'$ and $d - d'$) can demand $\\mathcal{A}(d')$ to have different results because the reference is unclear. \n   By introducing a level of indirection through meta-learning, we can essentially specify which distance function ($d$ or some other $d''$) the clustering algorithm uses, thus resolving the conflict. In fact the following holds true.\n\n\\begin{theorem}\nThere is a meta clustering algorithm that satisfies Meta-Scale-Invariant and Meta-Richness, Meta-Consistency for $|X| > 2$.\n \\TODO{the proof has been changed}\n\\end{theorem}\n \\begin{proof}\n   We will use the class of single-linkage clusterings parameterized by a threshold.   Observe that the meta scale-invariant and meta richness axioms are just the original axioms but with a level of indirection. As shown by \\citet{DBLP:conf/nips/Kleinberg02}, we can use single-linkage clustering with scale-$\\alpha$ stopping condition to achieve this (assume there are at least 3 data points and $\\alpha < 1$). But single-linkage clustering also directly corresponds to meta clustering with a slight modification by taking two distance functions $d, d'$ (when $d$   $d'$, it is the original algorithm). The meta step corresponds to the stopping condition that basically finds the maximum pairwise distance $p$ of $d$, and uses $\\alpha p$ ($\\alpha$ is a parameter) as the threshold of single-linkage clustering, i.e.\\ $\\mathcal{M}[d]$. And the second step uses this threshold to cluster $d'$, i.e.\\ $\\mathcal{A}(d)$. The indirection comes to play for the third axiom. It is easy to see that the threshold returned by $\\mathcal{M}[d]$ works the same on $d'$.\n \\end{proof}\n\n \\TODO{--------------------------}\n\n \\TODO{Detailed proof}\n \\begin{proof}\n   We will create the meta clustering algorithm from the class of single-linkage clusterings with scale-$\\alpha$ stopping condition. Let $\\rho^{*}$ denote the maximum pairwise distance of distance function $d$, single-linkage procedure add edges of weight at most $\\alpha \\rho^{*}$. There are two steps: choosing the stopping condition $\\alpha \\rho^{*}$ and use it as a threshold for clustering. Notice that the first step essentially chooses one single-linkage clustering algorithm from the class because they are parameterized by thresholds. Both steps use the same $d$.\n   \n   But the single-linkage procedure can directly correspond to meta clustering with a slight modification by taking two distance functions $d, d'$ for each step (when $d$   $d'$, it is the original algorithm). \n   \n   The meta step corresponds to choosing the stopping condition, i.e.\\ $\\mathcal{M}[d]$.   And the second step uses this threshold to cluster $d'$, i.e.\\ $\\mathcal{A}(d')$.\n   \n   Observe that the meta scale-invariant and meta richness axioms are just the original axioms but with a level of indirection. As shown by \\citet{DBLP:conf/nips/Kleinberg02}, we can use single-linkage clustering with scale-$\\alpha$ stopping condition to achieve (meta) scale-invariant and (meta) richness (assume there are at least 3 data points and $\\alpha < 1$).\n   \n   The indirection comes to play for the third axiom. It is easy to see that the threshold returned by $\\mathcal{M}[d]$ works the same on $d'$ for the class of single-linkage clusterings. \n   \n \\end{proof}\n\n\n \\TODO{--------------------------}\n\n \\TODO{Detailed proof v2}\n \\begin{proof}\n Consider a family of single linkage clustering functions. Each single linkage clustering function has a threshold $\\lambda$. Single linkage clustering function operates by initializing each point as its own cluster and adding edges between two points if their distance is below $\\lambda$. At the end, the connected components of the output graph are the clusters. \n\n A standard single linkage procedure \\citep{DBLP:conf/nips/Kleinberg02} consists of two steps: \n \\begin{itemize}\n   \\item Choosing one single linkage clustering function by finding the appropriate $\\lambda$ based on distance function $d$ of $X$\n   \\item Applying the chosen single linkage clustering function to cluster $X$ with distance function $d$.\n \\end{itemize}\n In our proof, we use single linkage procedure with scale-$\\tau$ stopping condition. Let $\\rho^{*}$ denote the maximum pairwise distance of distance function $d$, single-linkage procedure choose single linkage clustering function with $\\lambda   \\tau \\rho^{*}$. Theorem 2.2 from \\citet{DBLP:conf/nips/Kleinberg02} suggests that single linkage procedure with scale-$\\tau$ stopping condition condition satisfies \\textit{Scale-Invariance} and \\textit{Richness} when $\\tau \\leq 1$ and $|X| \\geq 2$.\n\n We can modify the single-linkage procedure into a meta clustering algorithm. The meta step corresponds to choosing the stopping condition, i.e.\\ $\\mathcal{M}[d]$.   And the clustering step uses this threshold to cluster $d'$, i.e.\\ $\\mathcal{A}(d')$. Notice the meta version allows the chosen single linkage clustering function to cluster a different distance function $d'$. When $d   d'$, the meta single-linkage procedure is the same as the standard single linkage procedure.\n\n Notice that both \\textbf{Meta-Scale-Invariance} and \\textbf{Meta-Richness} only needs $d'd$. And by Theorem 2.2 of \\citet{DBLP:conf/nips/Kleinberg02}, meta single-linkage procedure   with scale-$\\tau$ stopping condition condition satisfies \\textbf{Meta-Scale-Invariance} and \\textbf{Meta-Richness} when $\\tau \\leq 1$ and $|X| \\geq 2$.\n\n The indirection comes to play for the third axiom. Suppose $\\mathcal{M}[d]$ returns a single-linkage clustering function with threshold $\\lambda$. Now consider applying this clustering function to a different distance function $d'$. Let $\\mathcal{C}$ be the clustering produced by $d$. If $x, \\bar x$ are in the same cluster in $\\mathcal{C}$ then $d'(x,\\bar x) \\leq d(x,\\bar x) \\leq \\lambda$. On the other hand, if $x,\\bar x$ are in different clusters in $\\mathcal{C}$ then $d'(x,\\bar x) \\geq d(x,\\bar x) \\geq \\lambda$. Therefore, the same single-linkage clustering function with threshold $\\lambda$ will create the same graph for $d$ and $d'$ and thus the same clusters. So the meta single-linkage procedure   with scale-$\\tau$ stopping condition condition satisfies \\textbf{Meta-Consistency}. \n\n \\end{proof}\n \\TODO{--------------------------}\n\n\\TODO{--------------------------}\n\n\\TODO{Detailed proof v3}\n\\begin{proof}\nSimilar to \\cite{DBLP:conf/nips/Kleinberg02}, let's consider the family of single-linkage clustering functions. Each single-linkage clustering function has a threshold $\\lambda$. The single-linkage clustering function operates by initializing each point as its own cluster and adding edges between two points if their distance is below $\\lambda$. At the end, the connected components of the output graph are the clusters. \n\nWe can construct a meta-clustering algorithm as a following two step procedure. Let $\\frac{1}{2} < \\tau < 1$ be a fixed constant. Now given any dataset $X$ (such that $|X|>2$).\n\\begin{itemize}\n   \\item (Meta-step, i.e.\\ the choosing the clustering algorithm $\\mathcal{A}$) Given a distance function $d$ on $|X|$, we can pick a single-linkage clustering function choosing $\\lambda: \\tau \\rho^*$, where $\\rho^{*}   \\max_{i,j} d(i, j)$. Observe that this step is akin to $\\mathcal{M}[d]$, basically, $\\mathcal{M}[d]   \\mathcal{A}_\\lambda$ (for the specified $\\lambda$).\n   \\item (Clustering-step, i.e.\\ applying $\\mathcal{A}$ to the dataset for clustering) Once $\\mathcal{A}_\\lambda$ is picked, we can run single-linkage (with threshold $\\lambda$) on $X$ with distance function $d$. This step is akin to $\\mathcal{A}(d)$.\n\\end{itemize}\n\nWe will show that the meta-clustering algorithm as described above satisfy the three meta-clustering axioms.\n\n\\textbf{Meta-Scale-Invariance.} Suppose $\\mathcal{M}[d](d)   \\mathcal{C}$, or equivalently, $\\mathcal{A}_\\lambda(d)   \\mathcal{C}$ ($\\lambda$ as defined above). By the construction of the meta clustering algorithm, $\\mathcal{M}[\\alpha d]$ returns a single linkage function $\\mathcal{A}_{\\alpha\\lambda}$. It is trivial to show that $\\mathcal{A}_\\lambda(d)   \\mathcal{A}_{\\alpha\\lambda}(\\alpha d)$, that is, the scaling gives back exactly the same clustering $\\mathcal{C}$. \n\n\\textbf{Meta-Richness.} Consider an arbitrary partition $\\mathcal{C}$ with at least two clusters. We can construct the following distance function $d$ such that if $x, \\bar x$ are in the same cluster in $\\mathcal{C}$, $d(x, \\bar x)   1$ and $d(x, \\bar x)   2$ if not. It is easy to verify that this is a valid distance metric. Since $\\frac{1}{2} < \\tau < 1$, the threshold $\\lambda$ would be strictly larger than $1$. Therefore, meta-clustering can successfully recover $\\mathcal{C}$. If $\\mathcal{C}$ only has one cluster, we arbitrarily select a pair of data points $x_i$ and $x_j$ and set $d(x_i, x_j)   2$ with the rest of pairwise distances to be $1$. This is still a valid metric. There will be no edge between $x_i$ and $x_j$ while the other nodes are connected. Because $|X| > 2$, we know there exits at least another node $x_z$ such that $x_z$ and $x_i$ is connected and $x_z$ and $x_j$ is connected. Therefore, there is still only one connected component and thus one cluster.\n\n\\textbf{Meta-Scale-Consistency.} Suppose $\\mathcal{M}[d]$ returns a single-linkage clustering function with threshold $\\lambda$, that is, $\\mathcal{A}_\\lambda$. Now consider applying this clustering function to a different distance function $d'$ defined as follows. Let $\\mathcal{C}$ be the clustering produced by $d$, that is $\\mathcal{C}\\mathcal{A}_\\lambda$. If $x, \\bar x$ are in the same cluster in $\\mathcal{C}$ then $d'(x,\\bar x) \\leq d(x,\\bar x) \\leq \\lambda$. And, if $x,\\bar x$ are in different clusters in $\\mathcal{C}$ then $d'(x,\\bar x) \\geq d(x,\\bar x) \\geq \\lambda$. Observe that the single-linkage clustering function with threshold $\\lambda$ will create the same graph for $d$ and $d'$ and thus $\\mathcal{A}_\\lambda(d)   \\mathcal{A}_\\lambda(d')$, producing the same clusters. \n\\end{proof}\n\\TODO{--------------------------}\n",
  "title": "Meta-Learning to Cluster"
}
