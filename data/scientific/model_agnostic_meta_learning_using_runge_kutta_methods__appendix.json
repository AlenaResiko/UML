{
  "authors": [
    "Daniel Jiwoong Im",
    "Yibo Jiang",
    "Nakul Verma"
  ],
  "date_published": "2019-10-16",
  "raw_tex": "\\appendix\n\n\n\n\\section{EXPERIMENTAL DETAILS} \n\\section{Experimental Details} \n\nHere, we include some of the experimental set-up details. \nNote that we worked based on original MAML code base. This makes the\nthe experimental set-up the same as the original paper.\n\n\\subsection{Regression}\nWe used multilayer perceptrons with two ReLU hidden layers of size 40.\nWe trained each model fore 100,000 iterations. Every gradient updates are using 10 batch samples.\nThe learning rate (step size) is set to 0.01.\n\n\\subsection{Classification}\n\\textbf{Omniglot dataset} \nFollowing the same model architectures from previous studies \\citep{Vinyals2016, Finn2017}, \nwe used four convolutonal blocks with 64 $3\\times 3$ convolution filters, followed by \nbatch normalization and   ReLU activations, and $2 \\times 2$ max pooling. The last hidden\nlayer is 64.\n\nWe ran 30,000 epoch for 5-way 1-shot and 5-way 5-shot set-ups for all models.\nWe ran 50,000 epoch for 20-way 1-shot set-up for all models.\nWe ran 90,000 epoch for 20-way 5-shot set-up for all models.\n\nBecause there are total of $NK$ examples, where $N$ is number of tasks and $K$ is number of examples per task,\nwe were able to use 32 batch sizes for each gradient updates for meta-parameters. \nThe network was evaluated using 3 gradient updates with step size 0.4 for 5-way set-up and 5 gradient updates with\nstep size 0.1 for 20-way set-up. \n\n\n\\textbf{MiniImagenet dataset} \nThe same convolutional blocks are used as Omniglot dataset except that the number of filters were reduce to 32.\nThe batch size was set to 4 and 2 for meta-parameter updates. \nAll models were trained for 90,000 epoch and 5 gradient updates with 0.01 learning rate during the training.\nAt test time, 10 gradient updates were used for 15 examples per class. \n\n\n\n\\subsection{Reinforcement learning (2D navigation) }\nFor all meta algorithms presented in this paper, the parameters are set to be the same. In the 2D navigation, like the original MAML paper, the model consists of a 2-layer MLP each with 100 units and it uses ReLU for activation. The fast adaptation learning is rate is 0.16 while the meta learning rate is 0.01. We trained it with 100 iterations and meta batch size of 40.\n\n",
  "title": "Model-Agnostic Meta-Learning using Runge-Kutta Methods"
}
