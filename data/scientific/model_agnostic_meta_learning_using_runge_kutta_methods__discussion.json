{
  "authors": [
    "Daniel Jiwoong Im",
    "Yibo Jiang",
    "Nakul Verma"
  ],
  "date_published": "2019-10-16",
  "raw_tex": "\\section{DISCUSSION AND FUTURE WORK}\n\\section{Discussion and future work}\n\nIn this paper, we extend the fast-adaptation stage (the inner loop) \nto higher-order Runge-Kutta methods in MAML to gain a finer control over the optimization, and show that original\nfast-adaptation update corresponds to the second-order midpoint method.\nThe refined RK optimization helped us control various important aspects of the meta-learning process (fast adaptation and shared representation) \n\nMotivated by the fact that the meta-parameters should have the following \ntwo properties: (i) they should quickly adapt to learning a \nnew task, and (ii) they share a common feature representation \namong tasks, we expanded MAML gradient approximation via   \n\n\nIn order to achieve both we conclude that second-order \nRunge-Kutta methods such as Ralston, Heun's, and ITB are suitable since \ntheir updates consider both properties. We evaluated our methods on \nachieving improved performance on \nregression, classification, and reinforcement learning tasks.\n, and showed its \nsuperior performances. \n\nIt is worth noting that our proposed generalization is not specific to MAML, but can also be applied to other\nmeta-models. We share some potential directions of future work.\n\n{\\bf Exploring other ODE integrators} -\nWe applied explicit Runge-Kutta ODE integrator to generalize \nstochastic gradient optimization of MAML. One can also explore \nother variations on gradient-based updates such as AdaGrad and ADAM \\citep{Duchi2011, Kingma2015} and its effects to the meta-learning models. (Similar types of analysis has been done for image classification via neural networks, \\citealp{Im2017loss}.)\n\nthe formulation and \nto improve the performance of MAML. Similarly to \\citet{Im2017loss}, apply RK to\nstandard stochastic gradient descent and ADAM \\citep{Kingma2015} for training large deep \nneural network, and they used it to analyze the loss surfaces \\citep{Im2017loss}. \nWe can also try to apply RK2 with ADAM, which is one of the most widely used optimizer \nfor training deep learning models. \n\nBeside RK integrators, one can also apply other integrators, such as exponential- and leapfrog integrators. Since different integrators focus on different aspects of the optimization, one expects that they would benefit on different types of tasks.\ntypes of methods have different\nproperties, and one could be more suitable for different set of tasks. \nWe believe that\na thorough analysis of this would be an interesting direction to explore in the future and would be extremely beneficial to the practitioner.\n\n{\\bf Extension to ANIL} - \n\\citet{Raghu2019} recently showed \nthat different parts of the MAML network optimizes for different aspects of meta-learning   \n\n\ninvestigated the source of effectiveness of MAML (whether it is from comes from an initialization providing efficient change in the representation, or \ndue to initialization already providing high quality features \\citep{Raghu2019}. \nthat feature reuse is the dominant factor in MAML optimization and propose to \n\nThey show that feature reuse is the dominant factor and propose \nto only train \nadaption updates on the last layer (i.e.\\ the softmax layer for classification) of the \nmodel. \nIt would be instructive to study how different RK methods effects the various layers of the MAML network.\nWe can simply employ MAML-RK to the last layer as well and apply \n$\\nabla \\mathcal{L}(\\theta)$ to rest of the network.\nFor example, we can use the midpoint method for the last layer and apply Ralston, Heuns, ITB, and the gradient \nfor the lower layers. This essentially has the effect of shifting the balance from $a_2$ to $a_1$ as we move down to the lower layers.\n\n\n{\\bf Extension to Bayesian MAML} - \nThere are several works on Bayesian MAML \\citep{Kim2018, Finn2018} that help in adding robustness and preventing overfitting to few shot learning. It would be interesting to combine MAML-RK optimization with these frameworks.\n), one can incorporate Bayesian meta-learning methods. Note that these \nworks are complementary to our MAML-RK, hence, we can simply adapt the Bayesisan framework to MAML-RK. \n\n\n\n",
  "title": "Model-Agnostic Meta-Learning using Runge-Kutta Methods"
}
