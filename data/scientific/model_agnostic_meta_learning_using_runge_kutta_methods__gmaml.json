{
  "authors": [
    "Daniel Jiwoong Im",
    "Yibo Jiang",
    "Nakul Verma"
  ],
  "date_published": "2019-10-16",
  "raw_tex": "\\begin{align}\n\\min_\\Theta \\mathcal{L}(\\theta')   \\min_\\theta \\sum_{\\mathcal{T_i} ~p(\\mathcal{T}} \\mathcal{L_{T_i}}(\\theta - h\\nabla_\\theta \\mathcal{L}_{\\mathcal{T}_i}(\\theta,\\phi_i),\\phi_i)\n\\end{align}\n\n\n\\section{GENERALIZING MAML USING THE RUNGE-KUTTA METHOD}\n\\section{Generalizing MAML using the Runge-Kutta method}\n\\label{sec:maml-rk}\n\nOne of the central objectives of this paper is understanding the learning dynamics of MAML\nthrough the lens of explicit ODE integrators. \nConsider the vector field $\\mathcal{X}$ that maps the space of parameters to the descent \ndirections induced by the gradient of the MAML objective. So long as $\\mathcal{X}$ is sufficiently smooth, we can look for solutions of the form \n\\[\n   \\frac{d\\theta}{dt}   \\mathcal{X}(\\theta).\n\\]\nThe temporal dynamics of this ODE constitutes the ideal path that the meta-learner takes during training given \nan initial parameter $\\theta_0$. We can therefore use a numerical ODE solver to solve approximate this ideal solution. \nClearly a higher order explicit ODE integrator can be viewed as a black \nbox that transforms $\\mathcal{X}$ into a new vector field $\\mathcal{\\bar X}$ such that\nthe parameters $\\theta$ evolve more closely to the streamlines of $\\mathcal{X}$.\nFor gradient descent, $\\mathcal{X}   \\nabla_{\\theta} \\mathcal{L} $ and the resulting \n$\\mathcal{\\bar X}$ advects $\\theta$ more closely along the path of the steepest descent.\nOne of the central goals of this work is to precisely to discern the efficacy of applying integrators on\n$\\mathcal{\\bar X}$ rather than $\\mathcal{X}$. To do this we will replace the \ngradient $\\mathcal{X}(\\theta_t)   \\nabla_\\theta \\mathcal{L}(\\theta_t)$ with the appropriate \n$\\mathcal{\\bar X}(\\theta_t)$ by calling a chosen explicit ODE integrator. \n\\change{This viewpoint generalizes the MAML optimization framework to optimize with respect to temporal parameters in contrast to considering the spatial parameters as done in the previous literature} \\citep{Park2019, Chen2018, Song2019}. \n\n to enhance by considering or estimating curvature w.r.t parameters\n\n\nwhich exposes to \nbroader set of explicit ODE integrator methods.   \n\n\n\nFor a given timestep $t$, explicit integrator can be seen as a morphism over vector fields \n$\\mathcal{X} \\rightarrow \\mathcal{\\bar{X}}^h$ \\change{(for a fixed stepsize $h$)}. \nHence, for a true gradient $g_t \\nabla_\\theta \\mathcal{L}(\\change{\\theta_t})$ \\change{(at time $t$)}, we solve the modified Runge-Kutta \ngradient $\\bar{g_t}   \\nabla_\\theta \\mathcal{L}(\\change{\\theta^\\prime_t})$ as follows. \nDefine\n\\begin{align}\n   \\texttt{advect}_{g_t}^{\\textup{RK}}(\\theta,h): \\theta_t + \\bar{g_t}h   \\theta_{t+1}   \\texttt{advect}_{g_t}^{\\textup{RK}}(\\theta,h). \n   \\nonumber\n\\end{align}\nThe general form of $\\texttt{advect}_{g_t}^{\\textup{RK}}(\\theta,h)$ is the Runge-Kutta (RK) equation \\change{of order $N$} \\citep{Butcher2008}, given by \n\\allowdisplaybreaks\n\\begin{align}\n   \\theta_{t+\\change{1}} & \\theta_t + h\\sum^{N}_{i1} a_i k_i, \\hspace{0.35in} \\textrm{where}\n\\end{align}\n\\begin{align}\n   k_1 :& \\nabla_{\\theta} \\mathcal{L}(t, \\theta_t), \\nonumber\\\\\n   k_2 :& \\nabla_{\\theta} \\mathcal{L}(t + p_2h, \\theta_t + q_{21} k_1 h),\\nonumber\\\\\n   k_3 :& \\nabla_{\\theta} \\mathcal{L}(t + p_3h, \\theta_t + q_{31} k_1 h + q_{32} k_2 h),\\nonumber\\\\\n   & \\vdots \\nonumber\\\\\n   k_N :& \\nabla_{\\theta} \\mathcal{L}(t + p_nh, \\theta_t + q_{n1} k_1 h + q_{n2} k_2 h \n   + \\cdots + q_{n,n-1} k_n h),\n   \\label{eqn:general_rk}\n\\end{align}\nwhere (i) $a_i$ are combination weights (that should sum to $1$), \n(ii) $p_j$ are the so called \\emph{nodes} that help discretize the temporal dynamics (for numerical approximation), and (iii) $q_{ij}$ are the coefficients that help in fine grain optimization control (with \n (ii) $p_j$ are the so called \\emph{nodes} that scale the timestep (for better numerical approximation), (iii) $q_{ij}$ are the coefficients that scale the step towards the gradient $k_i$ (for a fine grain optimization control), and (iv) $\\sum^{i-1}_{j1} q_{ij}p_j$ for all $j2,\\ldots,N$.\n\\change{The specific choice of these parameters ($a_i$, $p_j$ and $q_{ij}$) gives rise to various popular instantiations of the RK optimization method}.\n\n\n\nFigure~\\ref{fig:rk_slopes} demonstrates the slopes $k_i$ for a quadratic function (depicted in blue color) for $1\\leq i\\leq N4$.\nThe step from $\\theta_0$ to $\\theta_1$ is choosen based on the composition of slopes. \nNotice that it takes a linear combination of the $k_i$'s (red arrows) to calculate the next step vector (green arrow). These $k_i$'s can be thought as \\emph{forward multi-steps} since they compute the gradients at future timesteps (c.f.\\ Figure \\ref{fig:rk_slopes}).\nRearranging the terms with respect to $\\bar{g_t}$, we get\n\\begin{align}\n   \\bar{g_t}^{\\text{MAML-RK}}   :& \\frac{\\texttt{advect}_{g_t}^{\\textup{RK}}(\\theta,h) - \\theta_t}{h} \n   \\frac{\\theta_t+ h\\sum^{N}_{i1} a_i k_i - \\theta_t}{h}\n   \\nonumber\n   \\\\\n   &\n   \n   \\sum^{N}_{i1} a_i k_i. \\nonumber\n\\end{align}\nWe can use this refined Runge-Kutta gradient in the MAML optimization for better convergence and will call it as the {\\textbf{generalized Runge-Kutta MAML}} (MAML-RK) method.\n\nWe obtained   the generalized {\\em Runge-Kutta MAML} (MAML-RK) optimizer by substituting the gradient $g_t$ with $\\bar{g}_t^{\\text{MAML-RK}}$.\nWe denote $\\bar{g_t}$ as a gradient for generalized MAML (MAML-RK). \nObserve that $\\bar{g}_t^{\\text{MAML-RK}}$ is a linear combination of the gradients of the forward multi-steps $k_i$\nover the sum of individual task specific loss functions $\\mathcal{L}_{\\mathcal{T}_i}(t,\\theta_t)$. \nour update considers gradients of multi-step forward, it effectively finds the meta-model parameters that maximizes the \nthe multi-step look ahead gradients   \nUnlike the standard MAML update, which takes the gradient only one step forward (inner-update)\\footnote{The meta \noptimization is performed over the meta-parameter $\\theta$ while the objective is computed using the updated \nmodel parameters $\\theta^\\prime$.}, MAML-RK has the ability to take the gradient multiple steps ahead. This \nencourages the meta-learner to find parameters that adapt to new tasks with very few (sometimes even just one) gradient \nsteps on a new task, just as what we desire. Note that although we advect $g_t$ with RK in this work, any other other explicit \nODE solver can also be used. \n\nFigure~\\ref{fig:gmaml_ill} illustrates of the optimization path of MAML-RK. \nThe bold blue path represents the continuous learning dynamics of the meta-parameter $\\mathcal{X}(\\theta)$.\nThe three parameters $\\theta_1$, $\\theta_2$, and $\\theta_3$ are the optimal parameters\nfor three tasks, and the best meta-parameter $\\theta^*$ is chosen in such a way that it lies close to all of them.\nThe path of standard MAML optimization (the green curve) deviates from $\\mathcal{X}(\\theta)$ due local crude approximations.   to accumulated local truncated error.\nUsing higher-order explicit RK methods (the red curve), follows the ideal path better due to better quality approximation. the cumulative local truncated error gets reduced and the path stays\ntighter to ideal path $\\mathcal{X}$.\n\nNext we show that the original MAML optimization is a special case of MAML-RK \\change{(with a specific setting of $a_i$, $p_j$, $q_{ij}$)} and also explore a wider setting of these parameters giving us other types of\nfirst and second-order optimizations for MAML.\n\n\n\\begin{figure*}[t]\n   \\centering\n   \\begin{minipage}{0.47\\textwidth}\n   \\includegraphics[width\\linewidth]{rk_illustration.pdf} \n   \\vspace{-0.8cm}\n   \\caption{Runge-Kutta method slope $k_i$ illustrations.\\protect\\footnotemark\n   Update direction from $\\theta_0$ to $\\theta_1$ is composed of linearly combining different $k_i$s.\n   }\n   \\label{fig:rk_slopes}\n   \\end{minipage}\n   \\begin{minipage}{0.52\\textwidth}\n   \\includegraphics[width\\linewidth]{maml_rk_illustration.pdf}   \n   \\caption{Diagram of MAML-RK illustrates how parameters $\\theta$ evolve over time.\n   The higher-order MAML-RK takes the path that is closer to continuous path $\\mathcal{X}$ (bold red curve)\n   that is closer to best parameter initialization that can quickly adapts to multiple tasks.}\n   \\label{fig:gmaml_ill}\n   \\end{minipage}\n\\end{figure*}\n\\footnotetext{The original image is from Runge-Kutta Wikipedia: \\texttt{en.wikipedia.org/wiki/File:Runge\\-Kutta\\_slopes.svg}}\n\n\n\n\\subsection{Advect MAML's gradient}\n\\label{sec:advect_maml}\nThe technical key component of MAML is that it computes the gradient with respect to the meta-parameter $\\theta$ \nwhile computing the objective on the updated model parameter $\\theta^\\prime$ (c.f.\\ Equation~\\ref{eqn:maml_objective} and Algorithm \\ref{algo:maml}). \nThis makes the meta-parameter to move towards the direction of $\\nabla_\\theta \\sum_{\\mathcal{T}_i\\sim p(\\mathcal{T})} \\mathcal{L}_{\\mathcal{T}_i} (\\theta_i^\\prime)$.\nObserve that this precisely corresponds to the $k_2$ component of MAML-RK with a specific setting of $p_2$ and $q_{21}$.\nWithout loss of generality, the learning rates $\\alpha$ and $\\beta$ are set to \n$ph$ and $h$\\footnote{$\\alpha$ and $\\beta$ appears in pseudo-algorithm~\\ref{algo:maml}\n$\\alpha$ and $\\beta$ are the learning rates for meta-model and explicit learners.}.\nThe following proposition tells us that MAML is a special case of {\\em second-order} MAML-RK.\n\\begin{proposition}\n   The MAML's gradient corresponds to the second-order explicit Runge-Kutta equation with\n   the parameters $a_10$, $a_21$, $q_{21}\\frac{1}{2}$, $p_2\\frac{1}{2}$. \n   \\label{prop:maml_midpt}\n\\end{proposition}\n\\begin{proof}\nNow, Let us consider the dynamical system of MAML,\n\\begin{align}\n   \\frac{d\\theta}{dt}   -\\nabla \\mathcal{L}(\\theta^\\prime, t).\n\\end{align}\nFor simplicity, let us denote $\\nabla_\\theta \\mathcal{L}(t,\\theta_t)$ as $\\nabla \\mathcal{L}(t,\\theta_t)$.\nConsider the Taylor expansion of MAML's gradient:\n\\allowdisplaybreaks\n\\begin{align}\n   \\frac{d\\theta}{dt}\n   & \\nabla \\mathcal{L} \\left(t+h, \\theta_t - h \\nabla \\mathcal{L} (t,\\theta_t)\\right)\\nonumber\\\\\n   \\begin{split}\n   & \\nabla \\mathcal{L} \\left(\\theta_t, t\\right) + \\frac{d\\mathcal{L}(\\theta_t, t)}{dt}h\\\\\n   &\\quad- \\frac{\\partial\\nabla \\mathcal{L}(\\theta_t, t)}{d\\theta} \\nabla \\mathcal{L}(\\theta_t, t) h + \\mathcal{O}(h^3)\\nonumber\n   \\end{split}\\\\\n   \\begin{split}\n   & \\nabla \\mathcal{L} \\left(t, \\theta_t\\right) + h \\Bigg(\\frac{d\\nabla\\mathcal{L}(t, \\theta_t)}{dt}\n   -\\frac{\\partial\\nabla \\mathcal{L}(t, \\theta_t)}{\\partial \\theta}\\nabla \\mathcal{L}(t, \\theta_t)   \\Bigg) + \\mathcal{O}(h^2)\n   \\label{eqn:maml_taylor}.\n\\end{align}\n\nFollowed by, we show that MAML is one type of explicit Runge-Kutta method.\nWe can compare Equation~\\ref{eqn:maml_taylor} with the second-order explicit Runge-Kutta Equation for MAML, which is \n\\begin{align}\n   \\begin{split}\n   \\theta_{t+h} & \\theta_t + (a_1\\nabla \\mathcal{L}(t, \\theta_t)\n   + a_2\\nabla \\mathcal{L}(t + p_2h, x_t + q_{21} k_1 h)) h\\nonumber \\\\\n   & \\theta_t + (a_1+a_2) \\nabla\\mathcal{L}(t, \\theta_t)   + a_2 h \\Bigg(\\frac{d\\nabla \\mathcal{L}(t, \\theta_t)}{dt}p_2   + \\frac{\\partial\\nabla \\mathcal{L}(t, \\theta_t)}{\\partial \\theta}\\nabla \\mathcal{L}(t, \\theta_t) q_{21}\\Bigg) + \\mathcal{O}(h^2), \\label{eqn:second_order_rk}\n\\end{align}\nsuch that $a_1 + a_2   1, a_2 q_{21}   \\frac{1}{2}, a_2 p_1   \\frac{1}{2}$.\n\nWe can now see that Equation~\\ref{eqn:second_order_rk} equals Equation~\\ref{eqn:maml_taylor} with $a_10$, $a_21$, $p_2\\frac{1}{2}$, and $q_{21}\\frac{1}{2}$.\nTherefore, the gradient of MAML is essentially the midpoint method.\n\nwhere $a_1$, $a_2$, $p_1$ and $q_1$ are parameters that define a Runge-Kutta method.\nThis equation approximates the the analytical solution \nfrom Equation~\\ref{eqn:second_ord_taylor_expand} when $a_1 + a_2   1$, $a_2 q_1   \\frac{1}{2}$, and $a_2 p_1   \\frac{1}{2}$.\nThis is because the Taylor expansion of $\\mathcal{\\bar{X}}$,\n\\begin{align}\n   \\mathcal{\\bar{X}}(\\theta)   \\left(a_1k_1 + a_2k_2\\right)   \\left(a_1 + a_2\\right) \\nabla \\mathcal{L}_{\\theta}(x_t; t) + a_2h\\left(p_1\\frac{d\\nabla f}{dt} + q_1 \\frac{\\partial \\nabla \\mathcal{L}_{\\theta}(x_t; t)}{\\partial \\theta} \\nabla \\mathcal{L}_{\\theta}(x_t; t)\\right)\n\\end{align}\nresembles the Taylor expansion of $\\mathcal{\\bar{X}}$, \n\\begin{align}\n   \\mathcal{\\bar{X}} & \\frac{d\\theta}{dt} +   \\frac{h}{2} \\frac{d^2\\theta}{dt^2}   \\nabla \\mathcal{L}_{\\theta}(\\theta_t; t) + \\frac{h}{2} \\left(\\frac{d \\nabla \\mathcal{L}_{\\theta}(\\theta_t; t)}{d t} + \\frac{\\partial \\nabla \\mathcal{L}_{\\theta}(\\theta_t; t)}{\\partial \\theta} \\nabla \\mathcal{L}_{\\theta}(x_t; t) \\right) + \\mathcal{O}(h^3).\n   \\label{eqn:second_ord_taylor_expand}\n\\end{align}\nEquation~\\ref{eqn:maml_taylor} will be used to show that MAML is a special case of {\\em Generalized MAML} in the next section.\n\\end{proof}\n\nThe optimization done with the specific setting of the RK parameters as shown in Proposition \\ref{prop:maml_midpt} is   usually called the midpoint method \\citep{Butcher2008}. This shows that the original MAML objective is essentially a midpoint optimization method.\n\n\nAs we expected \nA setting of $a_10$ and $a_21$ implies that the classic MAML objective solely relies on $\\nabla \\mathcal{L}_{\\theta}(t + p_2h, x_t + q_{21} k_1 h)$.\nMoreover, a setting of $q_{21}   \\frac{1}{2}$ corresponds to a learning rate of inner-update (i.e.\\ $\\alpha$ in Algorithm \\ref{algo:maml}) to be $\\frac{1}{2}h$ and \nthe learning rate of outer meta-update (i.e.\\ $\\beta$ in Algorithm \\ref{algo:maml}) to be $h$. This results in a   meta-model optimization to stays as close to the ideal trajectory   \npath $\\mathcal{X}(\\theta)$ with an error rate of $\\mathcal{O}(h^3)$ per step (c.f.\\ proof of Proposition \\ref{prop:maml_midpt}).\n\n\n\\subsection{Examples beyond MAML}\n\\label{sec:mamlrk2}\n\nThe Runge-Kutta gradient $\\bar{g}_t^{\\textup{MAML-RK}}$ discussed so far has been generic. is generic Runge-Kutta equation. \nOne can instantiate it (i) at various degrees of order (by choosing $N$), and (ii) by varying the RK parameters in \nalso there are multiple set of parameters that satisfy \nthe parameter constraint $\\sum^{i-1}_{j1} a_{ij}c_j$ for $2\\leq j \\leq N$ in \nEquation~\\ref{eqn:general_rk}. In this section, we examine some example instantiations of MAML-RK. \n\n\n\\begin{example}[First-order MAML]\nChoose $N1$ and $a_11$. Then,\n\\begin{equation}\n   \\bar{g}_t^{\\textup{MAML-RK1}}   h \\nabla_{\\theta} \\sum_{\\mathcal{T}_i \\sim p(\\mathcal{T})} \\mathcal{L}_{\\mathcal{T}_i}(t, \\theta_t).\n\\end{equation}\n\\end{example}\n\n\\begin{table}[t]\n\\begin{wraptable}{r}{0.5\\textwidth}\n   \\centering\n   \\captionof{table}{The coefficients of various $2^{\\textup{nd}}$-order RK methods \\citep{Hairer1987}}\n   \\label{tab:rk2_methods}\n   \\begin{tabular}{| c | c | c | c |}\n   \\hline\n   Methods & $a_1$ & $a_2$ & $q_{21},p_2$ \\\\[1ex]\n   \\hline\\hline\n   \\rule[1.1ex]{0pt}{1ex}\n   Midpoint & $0$ & $1$ & $\\frac{1}{2}$ \\\\[.1ex]\n   \\hline\n   \\rule[1.1ex]{0pt}{1ex}\n   Heun& $\\frac{1}{2}$ & $\\frac{1}{2}$ & $1$ \\\\[.1ex]\n   \\hline\n   \\rule[1.1ex]{0pt}{1ex}\n   Ralston& $\\frac{1}{3}$ & $\\frac{2}{3}$ & $\\frac{3}{4}$\\\\[.1ex]\n   \\hline\n   \\rule[1.1ex]{0pt}{1ex}\n   ITB & $\\frac{2}{3}$ & $\\frac{1}{3}$ & $\\frac{3}{2}$ \\\\[.1ex]\n   \\hline\n   Generic & $\\frac{1}{2x}$ & $1-\\frac{1}{2x}$ & $x$ \\\\[.1ex]\n   \\hline\n   \\end{tabular}\n   \\vspace{-0.65cm}\n\\end{wraptable}\n\\end{table}\nThe first-order MAML is simply the Euler's method. It sums over \nthe gradients of the   loss functions for every task. It is similar in flavor to other first-order approaches, such as FOMAML \\citep{Biswas2018} and \nReptile \\citep{Nichol2018}.   uses first-order approaches.\nThere are existing algorithms along similar flavour which use first-order approaches, \nsuch as . \n\nIn the previous section, we showed that MAML is a special case of second-order MAML-RK by\nadvecting $g_t$ with MAML's gradient. Here, we illustrate several other popular second-order\nRunge-Kutta methods that we can also apply to MAML gradients, thus extending the variety of optimization techniques that are currently in use for meta-learning.\n\n\\begin{example}[Second-order MAMLs]\nChoose $N2$. The methods in Table~\\ref{tab:rk2_methods} satisfy the constraints $a_1+a_21$, $a_2q_{21}\\frac{1}{2}$, \nand $a_2p_2\\frac{1}{2}$. Thus\n\\begin{align}\n   \\begin{split}\n   \\bar{g}_t^{\\textup{MAML-RK2}}&   a_1\\nabla_{\\theta} \\mathcal{L}(t, \\theta_t)\n   + a_2\\nabla_{\\theta} \\mathcal{L}\\left(t + p_2h, (\\theta_t + \\nabla_{\\theta} \\mathcal{L}(t, \\theta_t)) q_{21} h\\right),\n   \\end{split}\n\\end{align}\nwhere the parameters $a_1$, $a_2$, $p_2$, and $q_{21}$ can be substituted accordingly.\n\\end{example}\nWe can thus generalize Algorithm \\ref{algo:maml} by substituting the learning rate $\\alpha$ as $q_{21}h$, and $\\beta$ as $h$.\nNote that $q_{21}   p_2$ for all second-order Runge-Kutta methods. \nAlgorithm~\\ref{algo:gmaml2} presents generalized MAML-RK2 training algorithm.\nIt is worth noting that the existing literature only discusses   the midpoint method for training MAML, and our extension enables the practitioner to explore \nother methods, like Heun's, Ralston, and ITB \\cite{Hairer1987}.\n\n\\subsection{Additional Remarks}\nUsing the gradient ($\\bar{g}^{\\textup{MAML-RK1}}\\nabla \\mathcal{L}(\\theta)$) from a pre-trained model that uses a large dataset and then fine-tuning it on a smaller new dataset is popular in transfer learning and has become a popular techniques in various application domains. For example in computer vision, it is common to use a parameters and gradients of a pre-trained   network on ImageNet \\citep{Deng2009} and use it to perform, say, bird classification \\citep{Zhang2014}.\nThis has became the standard\nframework in solving many computer vision problems. \nHence, there is some evidence that\npre-training with first order gradient, i.e.\\ $\\bar{g}^{\\textup{MAML-RK1}}$ helps the model learn a shared feature \nrepresentation that can be applied across similar tasks. Although it is worth noting that it does not\nencourage the model to learn a meta-parameter that can rapidly adapt to a new task. \nIn contrast, part of the success of the classic MAML optimization \n $\\bar{g}^{\\textup{MAML}}$ (a specific second order method) is because it can directly optimize for this rapid new task adaptation by differentiating \nthrough the fine-tuning process with respect to the meta-parameter \n$\\nabla \\mathcal{L}(\\theta^\\prime)$, but it does not make use of the first order gradient \n$\\nabla \\mathcal{L}(\\theta)$. \nit is important to note our second order Runge-Kutta generalization $\\bar{g}^{\\textup{MAML-RK2}}$ (with specific instantiations as Heun's, Ralston, and ITB), considers\nboth the terms: $\\nabla \\mathcal{L}(\\theta)$ and $\\nabla \\mathcal{L}(\\theta^\\prime)$, and thus have the potential benefit\nof encouraging both rapid adaptation and shared feature representation \\citep{Raghu2019}. See our experiments in   Section~\\ref{sec:experiments}.\n\nFor every meta-learning update, the first-order method performs one evaluation of $\\mathcal{L}$,\n and the second-order method performs two evaluations for RK methods. The number of evaluations grows linearly \nup to the fourth-order, after which it grows faster making it computationally prohibitive.\nbecomes compu method. The number of evaluations required grow faster starting from \nfifth-order and higher, which is computationally inefficient. \nThe fourth-order Runge-Kutta method is often the \npopular method for solving initial value ODE problems. In our case, even the second-order \noptimization requires Hessian-vector products during the MAML updates and more evaluations is impractical. more than\ntwice and is computationally insufficient for training meta model. For this reason, \nWe will therefore limit to second order RK methods in   \nour experiments.   to MAML-RK1 and MAML-R2.\n\n\n\n\n",
  "title": "Model-Agnostic Meta-Learning using Runge-Kutta Methods"
}
