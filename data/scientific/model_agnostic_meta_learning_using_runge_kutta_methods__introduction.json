{
  "authors": [
    "Daniel Jiwoong Im",
    "Yibo Jiang",
    "Nakul Verma"
  ],
  "date_published": "2019-10-16",
  "raw_tex": "\\section{INTRODUCTION}\n\\section{Introduction}\n\nBuilding an intelligent system that can learn quickly on a new task with few examples \nor few experiences is one of the central goals of machine learning. Achieving this goal requires an agent that learns continuously while having the ability to adapt to new tasks with limited data.   \nStudying such a \ngoal is important because we would like to find a learning algorithm that continuously\nlearns and adapts to new tasks. \nMeta-learning \\citep{biggs1985} has emerged as a compelling framework that strives to attain this challenging goal. \nRecent developments have focused on solving this \nchallenge through meta-learning framework .\n\nThere are two \nmain approaches to meta-learning: learning-to-optimize and learning-to-initialize the \nmeta-model (usually encoded as deep network). Learning-to-optimize refers to having a model that encodes the learning algorithm \nand predicts the direction of the parameter updates \\citep{Hochreiter2001}. Learning-to-initialize refers to learning a representation that can quickly adapts \nto solve multiple tasks \\citep{Vinyals2016, Ravi2017, Finn2017}. Here we focus on understanding and improving the \nlatter approach, which is found a wide range of applications \\citep{Li2017, Nichol2018, Antoniou2019}.\nby which the meta-learning algorithm finds the general initialization that shares \ncommon learning properties over multiple tasks from few examples.\n\nLearning a new task from few examples is challenging due to over-fitting during the training.\nAs one expects, over-fitting makes learning any new task from a few examples very difficult.\nMeta-learning overcomes the problem of scarcity by \nIn the meta-learning setup, it overcomes the scarcity of examples by \njointly learning from a collection of related tasks,\na family of \nrelated tasks. \nSome tasks are used for training and others are used for testing.\neach task corresponds to a learning problem on a different (but related) dataset. \n\n\nModel-Agnostic Meta-Learning (MAML) has emerged as a popular state-of-the-art model in this framework \\citep{Finn2017}. It is a gradient based optimization model that learns the meta-parameters (that help to generalize to new tasks) in two update phases:\n fast adaptation (inner-updates) and meta-updates \n(outer-updates). Roughly, the inner-updates optimize the parameters to maximize \nthe performance on new tasks using few examples, and the outer-updates optimize the \nmeta-parameters to find an effective initialization within the few parameter updates.\n\n\nworks by \n\nwe train the initial parameters (the meta-parameters) such \nthat it solves the new task after taking few iterations of gradient updates \\cite{Finn2017}.\nMAML algorithm   consists of two update phases, namely, fast adaptation (inner-updates) and meta-updates \n(outer-updates). Broadly speaking, the inner-updates optimize the parameters to maximize \nthe performance on new tasks using few examples, and the outer-updates optimize the \nmeta-parameters to find an effective initialization within the few parameter updates.\n\nIt is important to understand that {\\em few examples} and {\\em few parameter updates} are the \ntwo primary constraints that we pose on solving this challenge. \nIn order to train a model that can maximize the performance within a few \nupdates, it requires finding parameters that can either quickly change the \ninternal representation (maximize the sensitivity of the loss function of the new task),\nprovide high quality feature representations (maximizing the mutual information between\ndifferent tasks), or a combination of both.\n\n\nFinding a model that can yield good prediction accuracy within a few updates requires (i) fast-adaptation -- that is, finding model parameters that can either quickly change the \ninternal representation (so as to maximize the sensitivity of the loss function of the new task), and/or (ii) shared-representation -- that is, developing a high quality joint latent feature representations (so as to maximize the mutual information between\ndifferent tasks).\nMotivated by this, we propose new learning-dynamics for MAML optimization that gives better flexibility and improves the model on both these fronts.\nto takes \naccount of both fast adaption and shared feature representation. \nSpecifically, we apply the class of Runge-Kutta method to MAML optimization, which can take advantage of computing \nthe gradients of the loss over the parameters \nmultiple-steps ahead when updating the meta-model. \nThis allows us to generalize MAML to using higher-order gradients. Furthermore, \nwe show that the current update rule of MAML corresponds to a specific type of second-order explicit \nRunge-Kutta method called {\\em the midpoint method} \\citep{Hairer1987}.\n\n\n\nThe main contribution of this work is as follows.\n\\begin{enumerate}[(i)]\n   \\item We propose a novel Runge-Kutta method for MAML optimization. This new viewpoint is advantageous as it helps get a more refined control over the optimization. (Section~\\ref{sec:advect_maml})\n   \\item By leveraging this refined control, we demonstrate that there are multiple principled ways to update MAML and show that original MAML update rule corresponds to one of the class of second-order Runge-Kutta methods. (Section~\\ref{sec:mamlrk2})\n   \\item The Runge-Kutta framework helps understand the MAML learning dynamics from the lens of explicit ODE integrators. To the best of our knowledge, this is the first work that successfully applies ODE solvers to \nmeta-learning. (Section~\\ref{sec:maml-rk})\n\n\\item We show that the refinement obtained by the Runge-Kutta method is empirically effective as well. We obtain significant improvements in performance benchmark classification, regression, and reinforcement learning tasks.\n(Section~\\ref{sec:experiments})\n\nthey provide accurate, efficientIn our experiments, we trained MAML on various second-order methods.\n, and robust solutions for sinusoidal \nregression and \nimage classification tasks.\n\\end{enumerate}   \n\n\n",
  "title": "Model-Agnostic Meta-Learning using Runge-Kutta Methods"
}
