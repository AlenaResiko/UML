{
  "authors": [
    "Daniel Jiwoong Im",
    "Yibo Jiang",
    "Nakul Verma"
  ],
  "date_published": "2019-10-16",
  "raw_tex": "\\section{RELATED WORK}\n\\section{Related Work}\n\n\\begin{table}[b]\n\\begin{wraptable}{r}{0.5\\textwidth}\n   \\centering\n   \\caption{The performance of MAML-RK for sinunoid regression tasks on \n   10-shot adaptation problem. MAML-RK1 (the first-order method) corresponds to\n   standard pre-training model on all training tasks. MAML-RK2 (midpoint)\n   corresponds to stanard MAML method. }\n   \\label{tab:regression_results}\n   \\begin{tabular}{|c|c|c|}\\hline\n   \\multicolumn{2}{|c|}{Sinuoid} & 10-shot \\\\\\hline\\hline\n   & MAML-RK1 (pretrained) & 2.72 $\\pm$ 0.174\\\\\n   & MAML-RK2 (midpoint)   & 0.13 $\\pm$ 0.038\\\\\\hline\n   \\parbox[t]{2mm}{\\multirow{3}{*}{\\rotatebox[originc]{90}{ours}}} \n   & MAML-RK2 (Heun's)   & 0.19 $\\pm$ 0.053\\\\\n   & MAML-RK2 (Ralston)   & {\\bf 0.12 $\\pm$ 0.039}\\\\\n   & MAML-RK2 (ITB)   & 0.18 $\\pm$ 0.054\\\\\\hline\n   \\end{tabular}\n   \\vspace{-0.25cm}\n\\end{wraptable}\n\\end{table}\n\n\n\n\n\nThe meta-learning is one of highly popular study topics. \nEarly approaches to meta-learning goes back to the late 1980s and early 1990s \\citep{Schmidhuber1987, Bengio1991}\nwhere it studies evolutionary principles in self-referential learning using\ngenetic programming. There has been a recent surge in interest where meta-models are applied to a wide array of tasks from architecture search \\citep{Zoph2016} and hyperparameter search \\citep{Maclaurin2015},\nto learning optimization \\citep{Chen2017}. It has become a popular approach in , they are most commonly used for\nfew-shot supervised learning \\citep{Hariharan2016} and fast reinforcement learning \\citep{Wang2017}.\n\n\nAmong many approaches to meta-learning, \\citet{Finn2017} proposed the MAML framework that uses a meta-objective\nfunction that performs a two-step gradient-based optimization of the meta-parameters for fast task-specific optimization.   using standard gradient descent.   \\citep{Finn2017}.\nMAML consists of outer loop for learning meta-intialization based on the inner loop that\ncan adapt quickly on a new task. \nVarious studies \\citep{Li2017, Antoniou2019} show that a fast-adaptation \nupdate rule can heavily influence performance, which has initiated several related investigations.\n\\citet{Biswas2018} and \\citet{Nichol2018} use first-order update methods to reduce computational burden, while\n\\citet{Park2019}, \\citet{Chen2018} and \\citet{Song2019}\nleverage second-order curvature information for better generalization.\nSince then, various choices of \nfast-adaptation update rules have been proposed. \nJust as MAML requires computing the Hessian of the loss   with respect to parameters, one \nanother approach is to use first-order methods to update the meta-parameters in order to reduce the \ncomputational burden \\citep{Biswas2018, Nichol2018}. In contrast, another approach is to learning the \ncurvature information for better generalization \\citep{Park2019, Chen2018, Song2019}. \nThus, these approaches \neither approximates the fast-adaptation with first-order derivatives or takes better \nsecond-order curvature of parameter space. Unlike these\nInstead of analyzing the spatial features (such as gradient and curvature of the parameter space), our work focuses on the temporal dynamics of the optimization. \nOur work methods, MAML-RK considers better\nestimating the dynamics of meta-learning through taking high-order gradients with respect \nto time.\n\nRecent work by \\citet{Raghu2019} analyzes the source of effectiveness of MAML -- is it primary due to {\\em rapid learning}, or due to {\\em feature reuse}? Their analysis shows MAML partly achieves both -- large portion of the lower layers of the MAML model helps in feature reuse and \nlarge portion of the upper layers helps in rapid learning. Part of the goal of our work is to think about aspects of a meta-model more effectively. Our RK extension makes \nfast-adaptation and shared-representation more explicit giving practitioners a fine grain control over the optimization.\n\nthe This study motivates us to think about what is the\ncorrect balance between shared-features versus fast-adaptation and how to encourage them. \n\n\\begin{figure*}[t]\n   \\centering\n   \\begin{minipage}{0.24\\textwidth}\n   \\includegraphics[width\\linewidth]{omniglot_5way_1shot.pdf} \n   \\subcaption{5way-1shot}\n   \\end{minipage}\n   \\begin{minipage}{0.24\\textwidth}\n   \\includegraphics[width\\linewidth]{omniglot_5way_5shot.pdf} \n   \\subcaption{5way-5shot}   \n   \\end{minipage}\n   \\begin{minipage}{0.24\\textwidth}\n   \\includegraphics[width\\linewidth]{omniglot_20way_1shot.pdf} \n   \\subcaption{20way-1shot}\n   \\end{minipage}\n   \\begin{minipage}{0.24\\textwidth}\n   \\includegraphics[width\\linewidth]{omniglot_20way_5shot.pdf} \n   \\subcaption{20way-5shot}   \n   \\end{minipage}\n   \\caption{Fast-adaptation of MAML-RK on Ominiglot test datsets.}\n   \\label{fig:perm_iter}\n\\end{figure*}\n\\begin{table*}[t]\n   \\vspace{-0.25cm}\n   \\centering\n   \\caption{The performance of MAML-RK for Omniglot and MiniImagenet image classification tasks on \n   1- and 5-shot adaptation problems. The midpoint method corresponds to MAML.}\n   \\label{tab:omniglot_results}\n   \\begin{tabular}{|c|c|c|c|c|c|}\\hline\n   \\multicolumn{2}{|c|}{\\multirow{2}{*}{Omniglot}}   & \\multicolumn{2}{c|}{5-way}   & \\multicolumn{2}{c|}{20-way}   \\\\\\cline{3-6}\n   \\multicolumn{2}{|c|}{}   & 1-shot & 5-shot   & 1-shot   & 5-shot \\\\\\hline\n   \\hline & Midpoint & 98.26 $\\pm$ 1.09   & 99.24 $\\pm$ 0.33   & 90.87 $\\pm$ 1.51   & 97.20 $\\pm$ 0.55 \\\\\\hline\n   \\parbox[t]{2mm}{\\multirow{3}{*}{\\rotatebox[originc]{90}{ours}}} \n   & Heun's   & 99.24 $\\pm$ 0.73   & 99.09 $\\pm$ 0.66 & 94.27 $\\pm$ 1.30   & {\\bf 97.48 $\\pm$ 0.53} \\\\\n   & Ralston   & \\bf{99.39 $\\pm$ 0.65} & {\\bf 99.61 $\\pm$ 0.24}   & {\\bf 94.91 $\\pm$ 1.19} & 97.45 $\\pm$ 0.54\\\\\n   & ITB   & 98.93 $\\pm$ 0.89   & 99.12 $\\pm$ 0.67   & 93.81 $\\pm$ 1.33   & 97.49 $\\pm$ 0.50 \\\\\\hline\n   \\end{tabular}\n   \\centering\n   \\caption{}\n   \\label{tab:miniimage_results}\n   \\begin{tabular}{|c|c|c|c|}\\hline\n   \\multicolumn{2}{|c|}{\\multirow{2}{*}{MiniImagenet}} & \\multicolumn{2}{c|}{5-way} \\\\\\cline{3-4}\n   \\multicolumn{2}{|c|}{}   & 1-shot & 5-shot   \\\\\\hline\\hline\n   & Midpoint   & 45.21 $\\pm$ 5.20 & 59.92 $\\pm$ 5.17 \\\\\\hline\n   \\parbox[t]{2mm}{\\multirow{3}{*}{\\rotatebox[originc]{90}{ours}}} \n   & Heun's   & {\\bf 46.65 $\\pm$ 5.10} & {\\bf 60.40 $\\pm$ 5.15} \\\\\n   & Ralston   & 44.66 $\\pm$ 5.10 & 59.95 $\\pm$ 5.16 \\\\\n   & ITB   & 45.29 $\\pm$ 5.38 & 60.19 $\\pm$ 4.85 \\\\\\hline\n   \\end{tabular}\n   \\vspace{-0.25cm}\n\\end{table*}\n\n",
  "title": "Model-Agnostic Meta-Learning using Runge-Kutta Methods"
}
