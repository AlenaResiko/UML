{
  "authors": [
    "Alexandre Louis Lamy",
    "Ziyuan Zhong",
    "Aditya Krishna Menon",
    "Nakul Verma"
  ],
  "date_published": "2019-01-30",
  "raw_tex": "auto-ignore\nWe present experimental results validating the preceding theory.\nSpecifically,\nWe demonstrate that it is viable to learn fair classifiers given noisy sensitive features.\\footnote{\\FINALCHANGE{}{Source code is available at \\url{https://github.com/AIasd/noise\\_fairlearn}.}}\nWe present two case studies in which we show that using our method, we can preserve fairness even when data is corrupted.\nAs our underlying fairness-aware classifier,\nwe use a modified version of the classifier implemented in \\citet{reduction} with the DDP and DEO constraints which, as discussed in \\S\\ref{section:fairnessdef}, are special cases \nof our more general constraints \\eqref{eqn:ddp} and \\eqref{eqn:deo}. The classifier's original constraints can also be shown to be noise-invariant but in a slightly different way (see Appendix \\ref{appendix:agarwal_constraint} for a discussion). An advantage of this classifier is that it is shown to reach levels of fairness violation that are very close to the desired level ($\\tau$), i.e., for small enough values of $\\tau$ it will reach the constraint boundary.\n\nWhile we had to choose a particular classifier, our method can be used before using any downstream fair classifier as long as it can take in a parameter $\\tau$ that controls the strictness of the fairness constraint and that its constraints are special cases of our very general constraints \\eqref{eqn:ddp} and \\eqref{eqn:deo}.\n\n\\subsection{Noise setting}\n\\label{ssec:CCN_and_MC}\n While our results apply for any instantiation of the MC learning noise framework, in practice this general kind of noise is rarely observed.\n Instead, \nOur case studies focus on two common special cases of MC learning: CCN and PU learning.\nUnder CCN noise the sensitive feature's value is randomly flipped with probability $\\rho^+$ if its value was 1, or with probability $\\rho^-$ if its value was 0. As shown in \\citet[Appendix C]{corruption}, CCN noise is a special case of MC learning. \n with:\n \\begin{align*}\n   \\pi_{a,\\mathrm{corr}} & (1-\\rho^+) \\cdot \\pi_{a}+\\rho^- \\cdot (1-\\pi_{a})\\\\\n   \\alpha & \\pi_{a,\\text{corr}}^{-1} \\cdot (1-\\pi_{a}) \\cdot \\rho^-\\\\\n   \\beta & (1-\\pi_{a,\\text{corr}})^{-1} \\cdot \\pi_{a} \\cdot \\rho^+\n \\end{align*}\nFor PU learning we consider the censoring setting~\\citep{Elkan} which is a special case of CCN learning where one of $\\rho^+$ and $\\rho^-$ is 0.\nWhile our results also apply to the case-controlled setting of PU learning~\\citep{Ward:2009}, the former setting is more natural in our context. Note that from $\\rho^+$ and $\\rho^-$ one can obtain $\\alpha$ and $\\beta$ as described in~\\citet{corruption}.\nConsequently, in both case studies we only need to estimate $\\rho^+$ and $\\rho^-$ (as well as $\\pi_{a, \\text{corr}}$, which is trivial) to be able to calculate the scaling coefficient $1-\\alpha-\\beta$. Since noise parameter estimation is not the aim of our paper, we assume that\nthe values of $\\rho^+$ and $\\rho^-$ are known.\n\n\\subsection{Benchmarks}\n \\input{experiment_sanity_check.text}\n\nFor each case study, we evaluate our method\n(termed {\\sf cor scale});\nrecall this scales the input parameter $\\tau$ using Theorem \\ref{thm: sensitive noise reduction} and the values of $\\rho^+$ and $\\rho^-$, and then uses the fair classifier to perform classification.\nWe compare our method with three   different baselines.\nThe first two trivial baselines are applying the fair classifier directly on the non-corrupted data (termed {\\sf nocor}) and on the corrupted data (termed {\\sf cor}). While the first baseline is clearly the ideal, it won't be possible when only the corrupted data is available. The second baseline should show that there is indeed an empirical need to deal with the noise in some way and that it cannot simply be ignored.\n\n AKM: we do better than denoise\nThe third, non-trivial, baseline (termed {\\sf denoise}) is to first denoise $A$ and then apply the fair classifier on the denoised distribution. This denoising is done by applying the {\\sf RankPrune} method in \\cite{northcutt2017rankpruning}. Note that we provide {\\sf RankPrune} with the same known values of $\\rho^+$ and $\\rho^-$ that we use to apply our scaling so this is a fair comparison to our method.\n\\aditya{Reword ``privacy''.}\n{Compared to {\\sf denoise}, we do \\emph{not} explicitly infer individual sensitive feature values;\nthus, our method does not compromise privacy.}\n\nFor both case studies, we study the relationship between the input parameter $\\tau$ and the testing error and fairness violation. for all three benchmarks and our method.\nFor simplicity, we only consider the DP constraint.\n\n\n\\subsection{Case study: privacy preservation}\\label{casestudy: privacy}\n\\input{case_study_privacy.tex}\n\n\n\n\\subsection{Case study: PU learning}\\label{casestudy: pu}\n\\input{case_study_pu.tex}\n",
  "title": "Noise-tolerant fair classification"
}
