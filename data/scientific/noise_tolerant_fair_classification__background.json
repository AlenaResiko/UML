{
  "authors": [
    "Alexandre Louis Lamy",
    "Ziyuan Zhong",
    "Aditya Krishna Menon",
    "Nakul Verma"
  ],
  "date_published": "2019-01-30",
  "raw_tex": "We recall the settings of standard and fairness-aware binary classification\n\\footnote{For simplicity, we consider the setting of binary target and sensitive features.\nHowever, our derivation and method can be easily extended to the multi-class setting.},\nand establish notation.\nused in the sequel.\n\nOur notation is summarized in Table \\ref{table:notation}.\n\n\n\\subsection{Standard binary classification}\n\nBinary classification concerns predicting the label or \\emph{target feature} $Y \\in \\set{0,1}$ that best corresponds to a given instance $X \\in \\cX$.\nFormally, suppose $D$ is a distribution over (instance, target feature) pairs from $\\cX \\times \\set{0,1}$.\nLet $f \\colon \\cX \\to \\R$ be a score function,\nand $\\cF \\subset \\R^{\\cX}$ be a user-defined class of such score functions.\nFinally, let $\\ellc: \\R \\times \\set{0,1} \\to \\R_+$ be a loss function measuring the disagreement between a given score and binary label.\nWithout loss of generality we suppose that the classification is done via taking the sign of a score function $f: \\R^d \\to \\R$.\nAnd:\n$$\\Lc_D(f) : \\Ex_{(x, a, y) \\sim D}[\\ellc(f(x), y)]$$\nThe goal of binary classification is to minimise $f^*$, where\n\\begin{align}\n   \\label{eqn:standard-risk}\n   f^*   &: \\argmin_{f\\in \\mathcal{F}} \\Lc_D(f) \\\\\n   \\Lc_D(f) &: \\Ex_{(X, Y) \\sim D}[\\ellc(f(X), Y)].\n\\end{align}\n In practice, one only observes samples from $D$, and minimizes the empirical counterpart to $\\Lc_D$.\n\n\n\\subsection{Fairness-aware classification}\n\\label{section:fairnessdef}\n\nIn fairness-aware classification, the goal of accurately predicting the target feature $Y$ remains.\nHowever, there is an additional \\emph{sensitive feature} $A \\in \\set{0,1}$ upon which we do not wish to discriminate.\nIntuitively, some user-defined fairness loss should be roughly the same regardless of $A$.\n\nFormally, suppose $D$ is a distribution over \n(instance, sensitive feature, target feature) triplets from\n$\\cX \\times \\{0,1\\} \\times \\{0,1\\}$.\nwhere when $(X,A,Y) \\sim D$ we have that $X$ is the feature vector, $A$ is the sensitive attribute, and $Y$ is the label.\nThe goal in fair machine learning is to learn a good classifier while keeping unfairness low.\nIn fair binary classification, the problem can be formulated as finding $f^*$ where\nThe goal of \\emph{fairness-aware} binary classification is to find\n\\footnote{Here, $f$ is assumed to not be allowed to use $A$ at test time, which is a common legal restriction~\\citep{lipton2018does}.\nOf course, $A$ can be used at training time to find an $f$ which satisfies the constraint.}\n$f^*$, where\n\\begin{equation}\n   \\label{eqn:fairness-objective}\n   \\begin{aligned}\n   f^* : \\argmin_{f\\in \\mathcal{F}} \\Lc_D(f) \\textrm{ s.t. some fairness constraint(s) }\n   f^*   &: \\argmin_{f\\in \\mathcal{F}} \\Lc_D(f), \\textrm{ such that } \\Lambda_D( f ) \\leq \\tau \\\\\n   \\Lc_D(f) &: \\Ex_{(X, A, Y) \\sim D}[\\ellc(f(X), Y)],\n   \\end{aligned}\n\\end{equation}\nfor user-specified \n\\emph{fairness tolerance} $\\tau \\geq 0$, and\n\\emph{fairness constraint} $\\Lambda_D \\colon \\cF \\to \\R_+$. is a user-specified the fairness constraint(s) are as described in section \\ref{section:fairnessdef}.\nSuch constrained {optimisation} problems can   be solved in various ways,\ne.g., convex relaxations~\\citep{Donini:2018},\nalternating {minimisation}~\\citep{ZafarWWW,Cotter:2018},\nor {linearisation}~\\citep{Hardt2016}.\n or by \n considering randomized classifiers to linearize the constraints~\\citep{Hardt2016,cost}.\n\nthis definition does not allow $A$ to be used for classification.\n While it would be interesting to extend the approach to when $A$ is available as input to $f$ as well, this proves challenging when subsequently considering the effect of noise on $A$.\nas the noise would then also come into the classifier.\n\n\n\\subsection{Common Group Fairness Definitions}\\label{section:fairnessdef}\n\nA number of fairness constraints $\\Lambda_D( \\cdot )$ have been proposed in the literature.\nWe focus on two {important and} specific choices in this paper, inspired by~\\citet{Donini:2018}:\n\\begin{align}\n   \\label{eqn:ddp}\n   &\\textbf{DDP}_{\\ellf}: \n   \\Lambda^{\\mathrm{DP}}_D( f ) &: \\abs{ \\Lf_{D_{0, \\cdot}}(f) - \\Lf_{D_{1, \\cdot}}(f) } \\\\ \n   \\leq \\tau \\\\\n   &\\textbf{DEO}_{\\ellf}: \n   \\label{eqn:deo}\n   \\Lambda^{\\mathrm{EO}}_D( f ) &: \\abs{ \\Lf_{D_{0, 1}}(f) - \\Lf_{D_{1, 1}}(f) }, \n   \\leq \\tau,\n\\end{align}\nwhere we denote by $D_{a,\\cdot},D_{\\cdot,y},$ and $D_{a,y}$ the distributions over $\\cX \\times \\{0,1\\} \\times \\{0,1\\}$ given by $D_{\\mid Aa}, D_{\\mid Yy}, $ and $D_{\\mid Aa, Yy}$ and $\\ellf: \\R \\times \\set{0,1} \\to \\R_+$ is the user-defined fairness loss with corresponding $\\Lf_D(f) : \\Ex_{(X, A, Y) \\sim D}[\\ellf(f(X), Y)]$.\nIntuitively, these measure the difference in the average of the fairness loss incurred among the instances with and without the sensitive feature.\n\nConcretely, if $\\ellf$ is taken to be $\\ellf(s, y)   \\1[\\sign(s) \\neq 1]$ and the 0-1 loss $\\ellf(s, y)   \\1[\\sign(s) \\neq y]$ respectively, then \nfor $\\tau   0$,\n\\eqref{eqn:ddp} and \\eqref{eqn:deo}\n simplify to\n \\begin{align*}\n   &\\textbf{DDP}_{01}: \n   \\Lambda^{\\mathrm{DP}}_D( f ) & \\abs{ \\mathbb{P}_{D_{0, \\cdot}}[ f(X) \\geq 0 ] - \\mathbb{P}_{D_{1, \\cdot}}[ f(X) \\geq 0 ] } \\\\\n   \\leq \\tau \\\\\n   &\\textbf{DEO}_{01}: \n   \\Lambda^{\\mathrm{EO}}_D( f ) & \\abs{ \\mathbb{P}_{D_{0, 1}}[ f(X) \\geq 0 ] - \\mathbb{P}_{D_{1, 1}}[ f(X) \\geq 0 ] } \n   \\leq \\tau.\n \\end{align*}\n For $\\tau   0$,\n these constraints\ncorrespond to the \\textit{demographic parity}~\\citep{Dwork:2011} and \\textit{equality of opportunity}~\\citep{Hardt2016} constraints. \nThus, we denote these two relaxed fairness measures \\textit{disparity of demographic parity} (DDP) and \\textit{disparity of equality of opportunity} (DEO).\nThese quantities are also known as the \\emph{mean difference score} in~\\citet{Calders2010}. \n\\footnote{Keeping $\\ellf$ generic allows us to capture a range of group fairness definitions, not just demographic parity and equality of opportunity; e.g., disparate mistreatment~\\citep{ZafarWWW} corresponds to using the 0-1 loss and $\\Lambda^{\\mathrm{DP}}_D$, and equalized odds can be captured by simply adding another constraint for $Y0$ along with $\\Lambda^{\\mathrm{EO}}_D$. }\n\n \\footnote{\\FINALCHANGE{}{Keeping $\\ellf$ generic allows us to capture a range of group fairness definitions, not just demographic parity and equality of opportunity; e.g., disparate mistreatment~\\citep{ZafarWWW} corresponds to using the 0-1 loss and $\\Lambda^{\\mathrm{DP}}_D$, and equalized odds can be captured by simply adding another constraint for $Y0$ along with $\\Lambda^{\\mathrm{EO}}_D$. }}\n to be\n \\begin{align*}\n   &\\textbf{DDP}: \n   \\Lambda^{\\mathrm{DP}}_D( f ) & |P_{D_{0,\\cdot}}[\\text{sign}(f(x)1)]-P_{D_{1,\\cdot}}[\\text{sign}(f(x) 1)]| \\\\\n   &\\textbf{DEO}: \n   \\Lambda^{\\mathrm{EO}}_D( f ) & |P_{D_{0,1}}[\\text{sign}(f(x)1)]-P_{D_{1,1}}[\\text{sign}(f(x) 1)]|.\n \\end{align*}\n\n Sometimes \\textit{equalized odds} is required.\n This can be viewed as a special case under the same conditions as the second constraint above and, simultaneously, of the following additional \n constraint:\n $$\\abs{ \\Lf_{D_{0, 0}}(f) - \\Lf_{D_{1, 0}}(f) } \\leq \\tau$$\n Although we do not explicitly consider this fairness scenario, all of the results for the generalized equality of \n opportunity constraint also apply for this third constraint\n .and having two constraints simultaneously poses no issue.\n\n\\subsection{Balanced error}\nWhen we regard $Y$ as our target label as usual, we have the following notations:\n\\begin{align*}\n   \\text{FNR}_y^D(f) &: \\mathbb{P}^-_{D_{\\cdot,1}}(f)\\\\\n   \\text{FPR}_y^D(f) &: \\mathbb{P}^+_{D_{\\cdot,0}}(f)\\\\\n   \\text{BER}_y^D(f) &: \\frac{\\text{FNR}_y^D(f)+\\text{FPR}_y^D(f)}{2}\n\\end{align*}\nIf instead, we regard $A$ as our target label, we have the following notations:\n\\begin{align*}\n   \\text{FNR}_a^D(f) &: \\mathbb{P}^-_{D_{1,\\cdot}}(f)\\\\\n   \\text{FPR}_a^D(f) &: \\mathbb{P}^+_{D_{0,\\cdot}}(f)\\\\\n   \\text{BER}_a^D(f) &: \\frac{\\text{FNR}_a^D(f)+\\text{FPR}_a^D(f)}{2}\n\\end{align*}\n\n\n\n\\begin{table}[h]\n   \\centering\n   \\begin{tabular}{ll}\n   \\toprule\n   \\toprule\n   Symbol & Meaning \\\\\n   \\midrule\n   $D_{\\cdot, a}$&   \\\\\n   \\bottomrule\n   \\end{tabular}\n   \\caption{Caption}\n   \\label{tab:my_label}\n\\end{table}\n\n\n\\begin{table}[t]\n\\caption {Glossary of commonly used symbols} \\label{table:notation}\n\\centering\n\n\\resizebox{0.99\\linewidth}{!}{\n\\begin{tabular}{llll}\n\\toprule\n\\toprule\n\\textbf{Symbol} &   \\textbf{Meaning} & \\textbf{Symbol} &   \\textbf{Meaning} \\\\ \n\\toprule\n$X$ &   instance & $D_{\\textrm{corr}}$ &   corrupted distribution $D$\\\\\n$A$ &   sensitive feature & $f$ &   score function $f: \\cX \\to \\R$\\\\\n$Y$ &   target feature & $\\ellc$ & accuracy loss $\\ellc: \\R \\times \\set{0, 1} \\to \\R_+$ \\\\\n$D$ &   distribution $\\mathbb{P}(X,A,Y)$ &   $\\Lc_D$ &   expected accuracy loss on $D$\\\\\n$D_{a,\\cdot}$ &   distribution $\\mathbb{P}(X,A,Y|Aa)$ & $\\ellf$ & fairness loss $\\ellf: \\R \\times \\set{0, 1} \\to \\R_+$\\\\\n$D_{\\cdot,y}$ &   distribution $\\mathbb{P}(X,A,Y|Yy)$ & $\\Lf_D$ & expected fairness loss on $D$ \\\\\n$D_{a,y}$ &   distribution $\\mathbb{P}(X,A,Y|Aa,Yy)$   & $\\Lambda_D$ &   fairness constraint   \\\\ \n\\bottomrule\n\\end{tabular}\n}\n\\end{table}\n\n\n\n\\subsection{Mutually contaminated learning}\n\\label{sec:mc-learning}\n\nIn the framework of learning from mutually contaminated distributions (MC learning)~\\citep{pmlr-v30-Scott13},\ninstead of observing samples from the ``true'' (or ``clean'') joint distribution $D$,\none observes samples from a corrupted distribution $D_{\\mathrm{corr}}$.\nThe corruption is such that the observed \\emph{class-conditional} distributions are mixtures of their true counterparts.\nMore precisely, let $D_{y}$ denote the conditional distribution for label $y$.\nThen, one assumes that\n\\FINALCHANGE{\n\\begin{equation}\n   \\label{equation:mc-model}\n   \\begin{aligned}\n   D_{0, \\mathrm{corr}} & (1-\\alpha) \\cdot D_{1} + \\alpha \\cdot   D_{0}\\\\\n   D_{1, \\mathrm{corr}} & \\beta \\cdot D_{1} + (1-\\beta) \\cdot   D_{0},\n   \\end{aligned}\n\\end{equation}\n}{\n\\begin{equation}\n   \\label{equation:mc-model}\n   \\begin{aligned}\n   D_{1, \\mathrm{corr}} & (1-\\alpha) \\cdot D_{1} + \\alpha \\cdot   D_{0}\\\\\n   D_{0, \\mathrm{corr}} & \\beta \\cdot D_{1} + (1-\\beta) \\cdot   D_{0},\n   \\end{aligned}\n\\end{equation}\n}\nwhere $\\alpha, \\beta \\in (0,1)$ are (typically unknown) noise parameters with $\\alpha + \\beta < 1$.\n\\footnote{\\AKMEDIT{The constraint imposes no loss of generality:\nwhen $\\alpha+\\beta>1$, we can simply flip the two labels and apply our theorem.\nWhen $\\alpha+\\beta1$, all information about the sensitive attribute is lost.\nThis pathological case \nis equivalent to not measuring the sensitive attribute at all.}}\nFurther, the corrupted base rate $\\pi_{\\mathrm{corr}} : \\mathbb{P}[ Y_{\\mathrm{corr}}   1 ]$ may be arbitrary.\nand in general is distinct from the clean base rate $\\pi : \\mathbb{P}[ Y   1 ]$.\nThe MC learning framework subsumes CCN and PU learning~\\citep{pmlr-v30-Scott13,corruption}, \\AKMEDIT{which are prominent noise models that have seen sustained study in recent years~\\citep{Jain:2017,Kiryo:2017,vanRooyen:2018,Katz:2019,Charoenphakdee:2019}.}\n Thus, it is {a flexible and} appealing noise model.\n",
  "title": "Noise-tolerant fair classification"
}
