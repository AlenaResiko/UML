{
  "authors": [
    "Alexandre Louis Lamy",
    "Ziyuan Zhong",
    "Aditya Krishna Menon",
    "Nakul Verma"
  ],
  "date_published": "2019-01-30",
  "raw_tex": "auto-ignore\n\n\n\\begin{figure*}[t]\n   \\centering\n   \n   \\subfloat[{\\tt COMPAS} dataset (privacy case study).]{\n   \\includegraphics[width0.4\\textwidth]{img_privacy/{disp_test_compas,0.15,0.15,1.0,DP,Agarwal,3,False}.pdf}\n   \\includegraphics[width0.4\\textwidth]{img_privacy/{error_test_compas,0.15,0.15,1.0,DP,Agarwal,3,False}.pdf}\n   \\label{fig:DP_compas}\n   }\n   \n   \\subfloat[{\\tt law school} dataset (PU learning case study).]{\n   \\includegraphics[width0.4\\textwidth]{img_pu_law/{disp_test_law,0.0,0.2,1.0,DP,Agarwal,3,False}.pdf} \n   \\includegraphics[width0.4\\textwidth]{img_pu_law/{error_test_law,0.0,0.2,1.0,DP,Agarwal,3,False}.pdf}\n   \\label{fig:law}\n   }\n   \n   \\caption{Relationship between input fairness tolerance $\\tau$ versus DP fairness violation (left panels), and versus error (right panels).\n   The black dotted line \n   {gray dashed line} represents the ideal fairness violation.\n   {Our method ({\\sf cor scale}) achieves approximately the ideal fairness violation (indicated by the gray dashed line in the left panels),\n   with only a mild degradation in accuracy compared to training on the uncorrupted data (indicated by the {\\sf nocor} method).\n   Baselines that perform no noise-correction ({\\sf cor})\n   and explicitly denoise the data ({\\sf denoise})\n   offer suboptimal tradeoffs by comparison;\n   for example, the former achieves slightly lower error rates, but does so at the expense of greater fairness violation.}}\n\\end{figure*}\n\nIn this case study, we look at {\\tt COMPAS}, a dataset from Propublica~\\citep{COMPAS} that is widely used in the study of fair algorithms.\nGiven various features about convicted individuals, the task is to predict recidivism and the sensitive attribute is race.\nThe data comprises 7918 examples and 10 features.\nIn our experiment, we assume that to preserve differential privacy, \nCCN noise with \n$\\rho^+\\rho^- \\in \\set{0.15, 0.3}$ \n{$\\rho^+\\rho^-   {0.15}$}\nis added to the sensitive attribute. \nAs per {Lemma} \\ref{thm: randomized response for differential privacy}, this guarantees $(\\epsilon, \\delta0)$ differential privacy with \n$\\epsilon \\in \\set{1.73, 0.85}$ respectively. \n{$\\epsilon   {1.73}$.}\nWe assume that the noise level $\\rho$ is released with the dataset (and is thus known). \nWe performed fair classification on this noisy data using our method and compare the results to the three benchmarks described above.\n\nFigure \\ref{fig:DP_compas} shows the average result over three runs each with a random 80-20 training-testing split.\n(Note that fairness violations and errors are calculated with respect to the true uncorrupted features.)\n{We draw two key insights from this graph:}\n\\begin{enumerate}[label(\\roman*),itemsep-2pt,topsep0pt,leftmargin24pt]\n   \\item {in terms of fairness violation,\n   our method ({\\sf cor scale}) approximately achieves the desired fairness tolerance (shown by the {gray dashed} line).}\n   This is both expected and ideal, and it matches what happens when there is no noise ({\\sf nocor}).\n   By contrast, the na\\\"{i}ve method {\\sf cor} strongly violates the fairness constraint.\n\n   \\item {in terms of accuracy, \n   our method only suffers mildly\n   compared with the ideal noiseless method ({\\sf nocor});\nsome degradation} is expected as noise will lead to some loss of information.\nBy contrast, {\\sf denoise} sacrifices much more {predictive} accuracy than our method.\n\n   \\item our method ({\\sf cor scale}) achieves the best accuracy while respecting the desired fairness tolerance.\n   This is evidenced by\n   the red line being below the gray dashed line (Figure~\\ref{fig:DP_compas}, left panel).\n\n   \\item \\AKMEDIT{the other baselines fare worse than our method:}\n indeed, as expected, the na\\\"{i}ve method {\\sf cor} violates the fairness constraint.\n While {\\sf denoise} achieves the desired fairness level, it has large fluctuations and sacrifices much more {predictive} accuracy than our method.\n \\AKMEDIT{Finally, while the {\\sf cor} method achieves slightly lower error rates, it does so at the expense of greater fairness violation.}\n\\end{enumerate}\nIn light of both the above, our method is seen to achieve the best overall tradeoff between fairness and accuracy.\n{Experimental results with EO constraints,\nand other commonly studied datasets in the fairness literature ({\\tt adult}, {\\tt german}), \nshow similar trends as}\nWe also tried using the EO constraint as well as other datasets. The results obtained showed the same trends that are observable \nin Figure \\ref{fig:DP_compas}, and are included in Appendix \\ref{appendix:privacy} for completeness.\n\n The sensitive attribute race is hard to be learned when it is regarded as the target label, even when no noise is present, \\cite{northcutt2017rankpruning} with SVM with rbf kernel as the base classifier can only get $65\\$ accuracy when no noise is present, which is even worse than using the corrupted labels. It can also get $65$ when $\\rho_a^+\\rho_a^-0.2$. When $\\rho_a^+\\rho_a^-0.4$, its accuracy decreases to $60\\$.\n\n\n\n\n\n",
  "title": "Noise-tolerant fair classification"
}
