{
  "authors": [
    "Alexandre Louis Lamy",
    "Ziyuan Zhong",
    "Aditya Krishna Menon",
    "Nakul Verma"
  ],
  "date_published": "2019-01-30",
  "raw_tex": "auto-ignore\n\\iffalse\n\n\n\n\n \\subsubsection{Assuming we know noise rates}\n \\label{ssec:num1:known_nr}\n\nFollowing a large number of previous studies,\nwe use four datasets for our experiments:\n\\begin{itemize}[itemsep0pt,topsep0pt]\n   \\item {\\tt adult}, a dataset from the UCI repository~\\citep{UCI}.\n   The task is to predict if one's annual income is more than 50k and the sensitive attribute is gender or race. \n   The data comprises 48842 examples and 14 features.\n\n   \\item {\\tt german}, another dataset from the UCI repository~\\citep{UCI}. The task is to predict if one has good credit and the sensitive attribute is if a person is foreign.\n   The data comprises 1000 examples and 20 features.\n   \n   \\item {\\tt Law School}, a dataset from LSAC~\\citep{Wightman}.\n   The task is to predict the passage of the bar exam and the sensitive attribute is race. \n   The data comprises 16638 examples and 11 features.\n   \n   \\item {\\tt COMPAS}, a dataset from Propublica~\\citep{COMPAS}.\n   The task is to predict recidivism and the sensitive attribute is race.\n   The data comprises 7918 examples and 10 features.\n   \n\n\\end{itemize}\n\nWe now present experiments in several progressively challenging settings:\nwhere there is noise in $A$,\nand where the noise aims to preserve privacy.\n\n\n\n\\subsection{Noise in the sensitive attribute}\n\nFor simplicity, we add noise according to CCN model, i.e. we randomly flip the sensitive attribute of samples with $A   1$ with probability $\\rho^+$ and randomly flip the sensitive attribute of samples with $A   0$ with probability $\\rho^-$.\nFor further simplicity, we let $\\rho:\\rho^+\\rho^-$.\n\nWe present results first assuming prior knowledge of the noise rates,\nand then where the noise rates are estimated from data.\n\n\n\n \\subsubsection{Assuming we know noise rates}\n \\label{ssec:num1:known_nr}\n\nWe assume that we know the values of $\\rho$. In this case, we can estimate $\\alpha$ and $\\beta$ using relationship specified in 3.3.2.\n\nAs a baseline (termed {\\sf denoise}) and then apply \\cite{reduction}. We apply the {\\sf RankPrune} method in \\cite{northcutt2017rankpruning} to denoise $A$. {\\sf denoise} is a two-steps method, it first estimate noise rates and then estimate A based on the estimated noise rates. For the purpose of fair comparison, we pass the known noise rates to {\\sf denoise} and only use its second step in this section.\n\nOur method (termed {\\sf cor\\_scale}) scales the input parameter $\\tau$ and then ~\\citet{reduction} to perform fair classification.\n\nWe run experiments at noise level $\\rho0.2$ on both {\\tt adult} and {\\tt COMPAS}.\n\nWe compare the relationship between the input $\\epsilon$ and training/testing error/fairness violation under both DP and EO of four situations.\n\n\\begin{itemize}\n   \\item Apply \\cite{reduction} on no-corrupted data(termed {\\sf nocor}).\n   \\item Apply \\cite{reduction} on corrupted data(termed {\\sf cor}).\n   \\item {\\sf denoise}\n   \\item {\\sf cor scale}\n\\end{itemize}\n\nEach plot is the average of three runs. At each run, we make a random $80$--$20$ train-test split.\n\nThe $x$-axis is the given input $\\tau$ that controls the strictness of fairness constraint and $y$-axis is DEO or DDP as defined in \\ref{section:fairnessdef}.\n\nIt can be seen from Figures~\\ref{fig:DP_compas_known} and \\ref{fig:EO_compas_known} that the fairness violation and error rates of {\\sf cor\\_scale} are much closer to {\\sf no\\_cor} when compared with both {\\sf cor} and {\\sf denoise} under both DP and EO.\n\nThis result is consistent with our derivation and implies that our scaling indeed helps to achieve the originally wanted degree of fairness before corruption.\nBesides, without surprise, due to down-scaling the $\\epsilon$, the corruption with scaling causes slightly higher error than the {\\sf no\\_cor} for both datasets on both training set and testing set.\n\nSee Appendix \\ref{appendix:knowing-nr-training} for the corresponding results on the training set.\n\nSee Appendix \\ref{appendix:noise-level} for the same experiment performed at different noise levels.\n\\fi\n\n\n\n\n\\iffalse\n\\subsubsection{Assuming we do not know noise rates}\n\nWe next turn to the more realistic (but challenging) setting where the noise rates are unknown.\nHere, as a baseline (termed {\\sf denoise}), we apply the {\\sf RankPrune} method in \\cite{northcutt2017rankpruning} to denoise $A$, and then apply \\cite{reduction}.\nOur method (termed {\\sf scale\\_est}) follows Algorithm~\\ref{alg:reduction},\nemploying {\\sf RankPrune} to estimate $\\rho^+$ and $\\rho^-$ (and thus $\\hat{\\alpha}_a$ and $\\hat{\\beta}_a$),\nand then ~\\citet{reduction} to perform fair classification.\n\nWe use the same setup as before (except choosing $\\rho0.3$ to see clearer pattern). We compare the training/testing fairness violation/error rate of the two methods at different input $\\epsilon$. For the purpose of comparison, we also include the three curves specified in the previous section.\n\nIt can be seen from Figure~\\ref{fig:DP_compas_five} that our method produces better preservation of fairness requirement compared with the baseline method.\n\n\\begin{center}\n   \\includegraphics[width0.4\\textwidth]{imgs2/DP_Agarwal_compas_03_3_ref_five}\n   \\captionof{figure}{(DP) Relationship between input epsilon and fairness violation/error on the UCI {\\tt adult} dataset.}\n   \\label{fig:DP_compas_five}\n\\end{center}\n\\fi",
  "title": "Noise-tolerant fair classification"
}
