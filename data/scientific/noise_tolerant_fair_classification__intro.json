{
  "authors": [
    "Alexandre Louis Lamy",
    "Ziyuan Zhong",
    "Aditya Krishna Menon",
    "Nakul Verma"
  ],
  "date_published": "2019-01-30",
  "raw_tex": "auto-ignore\nClassification is concerned with maximally discriminating between a number of pre-defined groups.\n(e.g., determining if an individual is likely to repay a loan, or not).\n\\emph{Fairness-aware} classification concerns the analysis and design of classifiers that\ndo not discriminate with respect to some sensitive feature (e.g., race, gender, age, income).\nRecently, much progress has been made\non devising {appropriate} measures of fairness~\\citep{Calders,Dwork:2011,Feldman,Hardt2016,ZafarWWW,Zafar:2017b,Kusner,Kim,Speicher:2018,Heidari:2019},\nand\nmeans of achieving them~\\citep{ZemelICML,ZafarWWW,Calmon:2017,Dwork:2018,reduction,Donini:2018,Cotter:2018,Williamson:2019,Mohri:2019}.\n\nTypically, fairness is achieved by adding constraints which depend on the sensitive feature, and then correcting one's learning procedure to achieve these fairness constraints.\nFor example, suppose the data comprises of pairs of individuals and their loan repay status,\nand the sensitive feature is gender.\nThen, we may add a constraint that we should predict equal loan repayment for both men and women (see \\S\\ref{section:fairnessdef} for a more precise statement).\nHowever, this and similar approaches assume that we are able to correctly measure or obtain the sensitive feature. \nIn many real-world cases, one may only observe noisy versions of the sensitive feature.\nFor example, survey respondents may choose to conceal or obfuscate their group identity out of privacy concerns.\n{concerns of potential mistreatment or outright discrimination}. \n\nOne is then brought to ask whether fair classification in the presence of such \\emph{noisy} sensitive features is still possible.\nIndeed, if the noise is high enough and all original information about the sensitive features is lost, then it is as if the sensitive feature was not provided.\nStandard learners can then be \n\\aditya{I don't know if ``necessarily'' is true.}\n\\EDIT{necessarily} \nunfair on such data~\\citep{Dwork:2011, pedreshi2008discrimination}.\nRecently,~\\citet{hashimoto2018fairness} showed that progress is   possible, albeit for specific fairness measures.\n(and potentially impossible).\nThe question of what can be done under a smaller amount of noise is thus both interesting and non-trivial.\n\nIn this paper, we consider two {practical} scenarios where we may only observe noisy sensitive features:\n\\begin{enumerate}[itemsep0pt,topsep0pt,leftmargin16pt,label{(\\arabic*)}]\n   \\item suppose we are releasing data involving human participants.\n   Even if noise-free sensitive features are available,\n   for legal or ethical reasons,\n   we may wish to \\emph{add} noise so as to obfuscate sensitive attributes,\n   and thus protect the participants' privacy.\n   \\EDIT{Even if noise-free sensitive features are available,\n   we may wish to \\emph{add} noise so as to obfuscate sensitive attributes, for legal or ethical reasons\n   to protect participant data from potential misuse.}\n   Even if noise-free sensitive features are available,\n   we may wish to \\emph{add} noise so as to obfuscate sensitive attributes,\n   and thus protect participant data from potential misuse.\n   in some cases, while the noise-free sensitive feature might be available, it may be desirable to add noise to it before being able to publish the data.\n   Indeed, one might legally or ethically have to obfuscate sensitive attributes so as to protect the privacy of a study's participants.\n   Thus, being able to learn fair classifiers under sensitive feature noise is a way to achieve both privacy \\emph{and} fairness.\n   \n   \\item suppose we wish to analyse data where \n   the presence of the sensitive feature is only known for a subset of individuals,\n   while for others the feature value is unknown.\n   For example,\n   consider a historical dataset where the presence of the sensitive feature (e.g., disability) was occasionally recorded.\n   Alternatively,\n   patients filling out a form may feel comfortable disclosing that they do not have a pre-existing medical condition; however, some who do have this condition may not provide truthful responses.   \n   {wish to refrain from responding}.\n   This can be seen as a variant of the \\emph{positive and unlabelled} (PU) setting~\\citep{Denis:1998a},\n   where the sensitive feature is present (positive) for some individuals,\n   but absent (unlabelled) for others.\n   The sensitive feature can be considered   (positive points) but where most entries could have been either positive or negative (thus unlabelled points).\n   we consider cases where \n   That is, rather than being able to tell whether the sensitive feature is positive or negative, we are simply given positive and unlabeled entries.\n\\end{enumerate}\n\nBy considering a general measure of fairness and model of noise, \nBy considering {popular measures of fairness and a general model of noise},\nwe show that fair classification is possible under \nmany (including the above) settings.\nmany settings, including the above.\nOur precise contributions are:\n\\begin{enumerate}[itemsep0pt,topsep0pt,leftmargin24pt,label(\\textbf{C\\arabic*})]\n   \\item we show that if the sensitive features are subject to noise as per the \\emph{mutually contaminated learning model}~\\citep{Scott13},\nand one measures fairness using the \\emph{mean-difference score}~\\citep{Calders2010},\nthen a simple identity (Theorem~\\ref{thm: sensitive noise reduction}) yields that we only need to change the desired fairness-tolerance.\nThe requisite tolerance can be estimated by leveraging existing noise-rate estimators\nfrom the label noise literature,\n \\AKMEDIT{from the label noise literature},\nyielding a reduction (Algorithm~\\ref{alg:reduction}) to regular noiseless fair classification.\n\n   \\iffalse \\item We additionally show that the framework can handle noise in both the sensitive \\emph{and} target feature.\\fi\n\n   \\item we show that our procedure is empirically effective on both case-studies mentioned above.\n   in which the sensitive feature is purposefully obfuscated for privacy reasons, or only available in the PU setting.\n\\end{enumerate}\n\nIn what follows,\nwe review the existing literature on learning fair and noise-tolerant classifiers in~\\S\\ref{sec:related},\nand introduce the (to our knowledge novel)\nnovel\nproblem formulation of noise-tolerant fair learning in~\\S\\ref{sec:problem-formulation}.\nin~\\S\\ref{sec:background}.\nWe then detail how to address this problem in~\\S\\ref{section:sensnoise},\nand empirically confirm the efficacy of our approach in~\\S\\ref{sec:experiments}.\n",
  "title": "Noise-tolerant fair classification"
}
