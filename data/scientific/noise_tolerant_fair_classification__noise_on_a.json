{
  "authors": [
    "Alexandre Louis Lamy",
    "Ziyuan Zhong",
    "Aditya Krishna Menon",
    "Nakul Verma"
  ],
  "date_published": "2019-01-30",
  "raw_tex": "auto-ignore\nThe standard fairness-aware learning problem assumes we have access to the true sensitive attribute,\nso that we can both measure and control our classifier's unfairness as measured by, e.g., Equation~\\ref{eqn:ddp}.\nNow suppose that rather than being given the sensitive attribute, we get a noisy version of it. \nWe will show that the fairness constraint on the clean distribution is \\emph{equivalent} to a \\emph{scaled} constraint on the \nnoisy distribution.\n That is,\n we show that fairness constraints are naturally robust to noise in the sensitive attribute.\nThis gives a simple reduction from fair machine learning in the presence of noise to\nthe regular fair machine learning, which can be done in a variety of ways as discussed in~\\S\\ref{sec:bg-fairness}.\n\n\n\\subsection{Sensitive attribute noise model}\n\\label{section:sensnoisesetup}\n\n\\iffalse\nTo design a suitable noise model, let us consider two plausible settings wherein the observed sensitive feature $A$ may not be reflective of the ground-truth:\n\\begin{itemize}[itemsep0pt,topsep0pt]\n   \\item the \\emph{obfuscation} setting, wherein some individuals decide to purposely obfuscate their sensitive feature,\n   perhaps for fear of being the subject of algorithmic discrimination.\n   For example, applicants to a job may be way of disclosing their race.\n\n   \\item the \\emph{opt-out} setting, wherein some individuals decide to provide a sensitive feature, but others decide to abstain from providing this information, perhaps over concerns of privacy.\n   For example, patients filling out a medical form may feel comfortable disclosing that they do not have a pre-existing medical condition;\n   however, those who do have this condition may not provide truthful responses.\n\\end{itemize}\nThe former can be modelled as a form of class-conditional noise (CCN) on $A$,\nwhere each individual makes an independent decision whether or not to provide their true value of $A$ or not.\n\\footnote{This assumption of instance-independent noise is unlikely to hold exactly in practice. Nonetheless, we believe it to be a useful first step in studying the effects of noise on fairness-aware learning methods.}\nThe latter can be modelled as a form of positive unlabelled (PU) learning,\nwhere only a subset of individuals with a sensitive feature decide to reliably provide that information.\n\nTo capture both types of noise in the sensitive attribute,\nwe adopt the framework of learning from mutually contaminated distributions (MC learning)~\\cite{pmlr-v30-Scott13},\nas introduced in \\S\\ref{sec:mc-learning}.\nOur choice is motivated by the fact that MC learning captures both PU and CCN learning as special cases;\nhence, our results imply results for both of these interesting special cases. \n\\fi\n\nAs previously discussed, we use MC learning as our noise model,\nas this captures both CCN and PU learning as special cases;\nhence, we automatically obtain results for both these interesting settings.\n\nOur specific formulation of MC learning noise on the sensitive feature is as follows.\nRecall from \\S\\ref{section:fairnessdef} that $D$ is a distribution over $\\mathscr{X} \\times \\set{0,1} \\times \\set{0,1}$.\nFollowing (\\ref{equation:mc-model}),\nfor unknown noise parameters\n$\\alpha,\\beta \\in (0,1)$ with $\\alpha + \\beta < 1$,\nwe assume that the corrupted class-conditional distributions are:\n\\begin{equation}\\label{equation:mcnoise1}\n   \\begin{aligned}\n   &D_{1,\\cdot,\\mathrm{corr}}   (1-\\alpha) \\cdot D_{1,\\cdot} + \\alpha \\cdot   D_{0,\\cdot}\\\\\n   &D_{0,\\cdot,\\mathrm{corr}}   \\beta \\cdot D_{1,\\cdot} + (1-\\beta) \\cdot   D_{0,\\cdot},\n   \\end{aligned}\n\\end{equation}\nand that the corrupted base rate is $\\pi_{a,\\mathrm{corr}}$ (we write the original base rate, $\\mathbb{P}_{(X, A, Y) \\sim D}[A1]$ as $\\pi_a$).\nIn words,\n{That is,} the distribution over (instance, label) pairs for the group with $A   1$, i.e. $\\mathbb{P}( X, Y \\mid A   1 )$,\nis assumed to be mixed with the distribution for the group with $A   0$, and vice-versa.\n\nNow, when interested in the EO constraint, it can be simpler to assume that the noise instead satisfies\n\\begin{equation}\\label{equation:mcnoise2}\n   \\begin{aligned}\n   &D_{1,1,\\mathrm{corr}}   (1-\\alpha') \\cdot D_{1,1} + \\alpha' \\cdot D_{0,1}\\\\\n   &D_{0,1,\\mathrm{corr}}   \\beta' \\cdot D_{1,1} + (1-\\beta') \\cdot D_{0,1},\n   \\end{aligned}\n\\end{equation}\nfor noise parameters $\\alpha', \\beta' \\in (0, 1)$.\nAs shown by the following, this is not a different assumption.\nbut simply a direct implication of \\eqref{equation:mcnoise1}. \n\n\\begin{lemma}\\label{lemma:mcrelation}\n   Suppose there is noise in the sensitive attribute only, as given in Equation \\eqref{equation:mcnoise1}.\n   Then,\n   there exists constants $\\alpha', \\beta'$ such that\n   Equation \\eqref{equation:mcnoise2} holds.\n   \\begin{align*}\n   \\alpha' & \\frac{\\alpha\\mathbb{P}[Y1 \\mid A   0] }{(1-\\alpha)\\mathbb{P}[Y1 \\mid A   1] + \\alpha \\mathbb{P}[Y1 \\mid A0]} \\\\\n   \\beta' & \\frac{\\beta\\mathbb{P}[Y1 \\mid A   1] }{(1-\\beta)\\mathbb{P}[Y1 \\mid A   0] + \\beta \\mathbb{P}[Y1 \\mid A1]}\n   \\end{align*}\n\\end{lemma}\n\n In the proof of Lemma~\\ref{lemma:mcrelation}, we see\n that $\\alpha\\alpha'$ when $Y \\independent A$, i.e., $\\mathbb{P}[Y1|A0]\\mathbb{P}[Y1|A1]$.\n However, in practice this will rarely be the case as it implies there is no intrinsic unfairness.\n Furthermore,\nAlthough the lemma shows that \\eqref{equation:mcnoise1} implies \\eqref{equation:mcnoise2} and\ngives a way to calculate $\\alpha', \\beta'$ from $\\alpha, \\beta$, in practice it may be useful to consider \\eqref{equation:mcnoise2} independently.\nIndeed, when one is interested in the EO constraints we will show below that only knowledge of $\\alpha', \\beta'$ is required.\nRather than estimating all of $\\alpha, \\beta, \\mathbb{P}[Y1 | A1],$ and $\\mathbb{P}[Y1 | A0]$\nIt is often much easier to estimate $\\alpha', \\beta'$ directly\n(which can be done in the same way as estimating $\\alpha, \\beta$ simply by considering $D_{\\cdot, 1, \\mathrm{corr}}$ rather than $D_{\\mathrm{corr}}$).\n\n\n\n\\subsection{Fairness constraints under MC learning}\n\nWe now show that the previously introduced fairness constraints for demographic parity and equality of opportunity are automatically robust to MC learning noise in $A$.\n\n\\begin{theorem}\\label{thm: sensitive noise reduction}\n   Assume that we have noise as per Equation \\eqref{equation:mcnoise1}.\n   Then, for any $\\tau > 0$ and $f \\colon X \\to \\mathbb{R}$,\n   \\begin{align*}\n   &\\abs{\\Lf_{D_{0,\\cdot}}(f)-\\Lf_{D_{1,\\cdot}}(f)} \\leq \\tau \\iff \\\\ &\\abs{\\Lf_{D_{0,\\cdot, \\text{corr}}}(f)-\\Lf_{D_{1,\\cdot, \\text{corr}}}(f)} \\leq \\tau(1-\\alpha - \\beta).\n   \\Lambda^{\\mathrm{DP}}_{D}( f ) \\leq \\tau &\\iff \\Lambda^{\\mathrm{DP}}_{D_{\\mathrm{corr}}}(f) \\leq \\tau \\cdot (1-\\alpha - \\beta) \\\\\n   \\end{align*}\n   Additionally, for the EO-like constraints,\n   \\begin{align*}\n   &\\abs{\\Lf_{D_{0,1}}(f)-\\Lf_{D_{1,1}}(f)} \\leq \\tau \\iff \\\\\n   &\\abs{\\Lf_{D_{0,1, \\text{corr}}}(f)-\\Lf_{D_{1,1, \\text{corr}}}(f)} \\leq \\tau(1-\\alpha' - \\beta'),   \n   \\Lambda^{\\mathrm{EO}}_{D_{\\cdot, 1}}( f ) \\leq \\tau &\\iff \\Lambda^{\\mathrm{EO}}_{D_{\\mathrm{corr}, \\cdot, 1}}(f) \\leq \\tau \\cdot (1-\\alpha' - \\beta'),\n   \\end{align*}\n   where $\\alpha'$ and $\\beta'$ are as per Equation~\\eqref{equation:mcnoise2} and Lemma~\\ref{lemma:mcrelation}.\n\\end{theorem}\n\nThe above can be seen as a consequence of the immunity of the \\emph{balanced error}~\\citep{Chan:1998,Brodersen:2010,consistency} to corruption under the MC model.\nSpecifically, consider a distribution $D$ over an input space $\\mathscr{Z}$ and label space $\\mathscr{W}   \\set{0, 1}$.\nDefine\n$$ B_{D} : \\mathbb{E}_{Z \\mid W   0}[ h_0( Z ) ] + \\mathbb{E}_{Z \\mid W   1}[ h_1( Z ) ] $$\nfor functions $h_0, h_1 \\colon \\mathscr{Z} \\to \\R$.\n This is a generalised form of balanced error, being the average of false positive and negative rates of a classifier.\nThen, if for every $z \\in \\mathbb{R}$\n$h_0( z ) + h_1( z )   0$, is a constant,\nwe have~\\citep[Theorem 4.16]{vanRooyen:2015}, \\citep{Blum:1998,Zhang:2008,corruption}\n\\begin{equation}\n   \\label{eqn:ber-identity}\n   B_{D_{\\mathrm{corr}}}   (1 - \\alpha - \\beta) \\cdot B_{D},+ \\frac{\\alpha + \\beta}{2},   \n\\end{equation}\nwhere $D_{\\mathrm{corr}}$ refers to a corrupted version of $D$ under MC learning with noise parameters $\\alpha, \\beta$.\nIn words, \n{That is,} the effect of MC noise on $B_D$ is simply to perform a scaling.\nObserve that\n$B_D   \\Lf_D( f )$\nif\nwe set\n$\\mathscr{Z}   \\mathscr{X} \\times \\set{0, 1}$,\n$Z$ to $X \\times Y$,\n$W$ to the sensitive feature $A$,\nand\n \\begin{align*}\n   h_0( (x, y) ) & +\\ellf( y, f( x ) ) \\\\\n   h_1( (x, y) ) & -\\ellf( y, f( x ) ).\n \\end{align*}\n$h_0( (x, y) )   +\\ellf( y, f( x ) )$,\n$h_1( (x, y) )   -\\ellf( y, f( x ) )$.\nThus, (\\ref{eqn:ber-identity}) implies $\\Lf_D( f )   (1 - \\alpha - \\beta) \\cdot \\Lf_{D_{\\mathrm{corr}}}( f )$,\nand thus Theorem~\\ref{thm: sensitive noise reduction}.\n\n\n\n\n\\subsection{Algorithmic implications}\n\nTheorem~\\ref{thm: sensitive noise reduction} has an important algorithmic implication.\nSuppose we pick a fairness constraint $\\Lambda_D$ and seek to solve\nEquation~\\ref{eqn:fairness-objective} for a given tolerance $\\tau \\geq 0$.\nThen,\ngiven samples from $D_{\\mathrm{corr}}$, it suffices to simply change the tolerance to $\\tau'   \\tau \\cdot (1 - \\alpha - \\beta)$.\n\nUnsurprisingly, $\\tau'$ depends on the noise parameters $\\alpha, \\beta$.\nIn practice, these will be unknown;\nhowever, there have been several algorithms proposed to estimate these from noisy data alone~\\citep{pmlr-v30-Scott13,corruption,Liu2016ClassificationWN,Ramaswamy:2016,northcutt2017rankpruning}.\nThus, we may use these to construct estimates of $\\alpha, \\beta$, and plug these in to construct an estimate of $\\tau'$.\n\nIn sum, we may tackle fair classification in the presence of noisy $A$ by suitably combining \\emph{any} existing fair classification method (that takes in a parameter $\\tau$ that is proportional to mean-difference score of some fairness measures), and \\emph{any} existing noise estimation procedure.\nThis is summarised in Algorithm~\\ref{alg:reduction}.\nHere, {\\sf FairAlg} is any existing fairness-aware classification method that solves Equation~\\ref{eqn:fairness-objective}, and {\\sf NoiseEst} is any noise estimation method that estimates $\\alpha, \\beta$.\n\n\\renewcommand{\\algorithmicrequire}{\\textbf{Input:}}\n\\renewcommand{\\algorithmicensure}{\\textbf{Output:}}\n\n\\begin{algorithm}\n\t\\caption{Reduction-based algorithm for fair classification given noisy $A$.}\n\t\\label{alg:reduction}\n\t\\begin{algorithmic}[1]\n\t   \\REQUIRE Training set $S   \\{ (x_i, y_i, a_i) \\}_{i1}^n$,\n\t   scorer class $\\cF$,\n\t   fairness tolerance $\\tau \\geq 0$,\n\t   fairness constraint $\\Lambda( \\cdot )$,\n\t   fair classification algorithm {\\sf FairAlg},\n\t   noise estimation algorithm {\\sf NoiseEst}\n\t   \\ENSURE   Fair classifier $f^* \\in \\cF$\n\t   \\STATE   $\\hat{\\alpha}, \\hat{\\beta} \\leftarrow {\\sf NoiseEst}( S )$\n\t   \\STATE   $\\tau' \\leftarrow ( 1 - \\hat{\\alpha} - \\hat{\\beta} ) \\cdot \\tau$\n\t   \\STATE   \\textbf{return} ${\\sf FairAlg}( S, \\cF, \\Lambda, \\tau' )$\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\\subsection{Practical implications}\n\nWhen data gets released it is often necessary to protect the identities of those that the data pertains to. \nThis is especially true for medical data or data from the social sciences.\nWhile this concern pervades all areas of machine learning, it is particularly important in the context of fair learning since those same datasets are often the ones where fairness is a major concern.\nWhile anonymization is sometimes enough, there are cases where stronger steps have to be taken in order to protect the participant's privacy.\n\nSurprisingly, our result immediately gives an extraordinarily simple and relatively effective method to protect privacy while still allowing for fair classification. Indeed, consider simply adding CCN noise to the sensitive attribute (and perhaps other features) of the of dataset instances. While this does not protect against multiple stronger privacy attacks, it does give some assurance that an instance's true features (and thus hopefully its identity) cannot be recovered with 100\\ certainty. Meanwhile our result implies that fair classification would still be possible. Furthermore, it gives a way to achieve fair classification by simply scaling $\\tau$ as per Theorem \\ref{thm: sensitive noise reduction} before calling any of a large group of downstream fair classifiers with several options as to which group fairness constraint to use.\n\nThere are many real world cases in which a PU setting for the sensitive attribute is very realistic. Indeed, one may be able to clearly identify some instances of the minority group while being unable to tell whether the rest of the instances were in the majority or minority groups.\n\n\n\n\\FINALCHANGE{}{\\subsection{Noise rate and sample complexity}\n\nSo far, we have shown that at a distribution level, fairness with respect to the noisy sensitive attribute is equivalent to fairness with respect to the real sensitive attribute. \nHowever, from a sample complexity perspective, a higher noise rate will require a larger number of samples for the empirical fairness to generalize well, i.e., guarantee fairness at a distribution level. \nA concurrent work by~\\citet{fairlearning_withprivatedata} derives precise sample complexity bounds and makes this relationship explicit.\n}\n\n\n\n\n\n\n\n\\subsection{Connection to \\FINALCHANGE{differential privacy}{privacy and fairness}}\n\nWhile Algorithm \\ref{alg:reduction} gives a way of achieving fair classification on an already noisy dataset such as the {use case} described in example (2) of \\S\\ref{sec:intro}, it can also be used to simultaneously achieve fairness and privacy.\nAs described in example (1) of \\S\\ref{sec:intro}, the very nature of the sensitive attribute makes it likely that even if noiseless sensitive attributes are available, one might want to add noise to guarantee some form of privacy.\nNote that {simply} removing the feature does not suffice, because it would make difficult the task of\n \\AKMEDIT{make difficult the task of}\ndeveloping fairness-aware classifiers for the dataset\n \\AKMEDIT{~\\citep{Gupta:2018}}.\n~\\citep{Gupta:2018}.\nFormally, we can give the following privacy guarantee by adding CCN noise to the sensitive attribute. \n\n\\begin{lemma}\\label{thm: randomized response for differential privacy}\n   To achieve $(\\epsilon, \\delta0)$ differential privacy\n   on the sensitive attribute we can add CCN noise with $\\rho^+   \\rho^-   \\rho \\geq \\frac{1}{\\exp{(\\epsilon)}+1}$ to the sensitive attribute.\n\\end{lemma}\n\nThus, if a desired level of differential privacy is required before releasing a dataset, one could simply add the required amount of CCN noise to the sensitive attributes, publish this modified dataset as well as the noise level, and researchers could use Algorithm \\ref{alg:reduction} (without even needing to estimate the noise rate) to do fair classification as usual. \n\n\\FINALCHANGE{Recently,~\\citet{Kearns18} explored preserving differential privacy~\\citep{Dwork06} while at the same time of preserving \n{maintaining} fairness constraints.}{There is previous work that tries to preserve privacy of individuals' sensitive attributes while learning a fair classifier.\n\\cite{pmlr-v80-kilbertus18a} employs the cryptographic tool of secure multiparty computation (MPC) to try to achieve this goal. \nHowever, as noted by~\\citet{Kearns18}, the individual information that MPC tries to protect can still be inferred from the learned model.\nFurther, the method of \\citet{pmlr-v80-kilbertus18a} is limited to using demographic parity as the fairness criteria. \n}\n\nand achieving good accuracy.\nA more recent work of~\\citet{Kearns18} explored preserving differential privacy~\\citep{Dwork06} while \nat the same time of preserving \n{maintaining} \nfairness constraints. \nThe authors proposed two methods:\none adds Laplace noise to training data and apply the post-processing method in~\\citet{Hardt2016},\nwhile \\FINALCHANGE{another}{the other} modifies the method in~\\citet{reduction} using the exponential mechanism as well as Laplace noise.\nOur work differs from them in three major ways:\n\\begin{enumerate}[itemsep0pt,topsep0pt,label(\\emph{\\arabic*})]\n   \\item \\AKMEDIT{our work can leverage \\emph{any} algorithm for achieving fairness, while~\\citet{Kearns18} is tied to specific algorithms};\n   rather than modifying specific algorithms, our work can be used before applying a large group of fair algorithms;\n   \n   \\item \\FINALCHANGE{our work allows for fair classification to be done using a vast\n{\\emph{any}} fairness-aware classifier, whereas the method of \\citet{Kearns18} forces\n{requires the use of} a particular classifier.}{our work allows for fair classification to be done using vast\n{\\emph{any}} \\emph{in-process} fairness-aware classifier that allows user to specify desired fairness level. On the other hand, the first method of \\citet{Kearns18} forces\nrequires the use of a post-processing algorithm \n(in-processing algorithms generally result in better trade-offs than post-processing as shown in \\citet{reduction}), \n(which generally have worse trade-offs than in-processing algorithms~\\citet{reduction}), \nwhile the second method requires the use of a single \\emph{particular} classifier.}\n   \n   \\item although our framework can also be used for preserving differential privacy, we mainly focus on the case of data corruption   under the framework of MC learning rather than privacy;\n   \n   \\item our focus is on designing fair-classifiers with noise-corrupted sensitive attributes; {by contrast, the main concern in~\\citet{Kearns18} is achieving differential privacy}\\FINALCHANGE{}{and thus they do not discuss how to deal with noise that is already present in the dataset}.\n   \n   \\item \\FINALCHANGE{we deal with not only equalized odds, but also demographic parity.}{our method is shown to work for a large class of different fairness definitions.}\n\\end{enumerate}\n\n\\FINALCHANGE{}{Finally, a concurrent work of~\\citet{fairlearning_withprivatedata} \nbuilds upon our method\nfor the problem of preserving privacy of the sensitive attribute.\nThe authors use a randomized response procedure on the sensitive attribute values, followed by a two-step procedure to train a fair classifier using the processed data. \nTheoretically, their method improves upon the sample complexity of our method and extends our privacy result to the case of non-binary groups. \nHowever, their method solely focuses on preserving privacy rather than the general problem of sensitive attribute noise.}\nFurthermore, no empirical result has been shown for the effectiveness of their proposed method.}\n\n Note that while other methods, such as those described in \\citet{Kearns18}, \n may be better suited if differential privacy is the main concern, our method differs in two important ways. First, it allows for fair classification to be done using a vast\n {broader} class of fairness-aware classifiers, whereas the method of \\citet{Kearns18} forces\n {restricts} the user to use a particular classifier.\n Second, {unlike~\\citet{Kearns18}, our focus is on designing fair-classifiers with noise corrupted sensitive attributes.}\n\nit is important to keep in mind that the method of \\citet{Kearns18} is specifically geared towards this problem, while our method can also be used to achieve fair classification using naturally noisy data.\n",
  "title": "Noise-tolerant fair classification"
}
