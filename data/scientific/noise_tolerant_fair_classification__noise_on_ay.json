{
  "authors": [
    "Alexandre Louis Lamy",
    "Ziyuan Zhong",
    "Aditya Krishna Menon",
    "Nakul Verma"
  ],
  "date_published": "2019-01-30",
  "raw_tex": "auto-ignore\n\\subsection{Noise model}\n\nWe consider class-conditional label noise (CCN learning)\\cite{Angluin88}.\nSpecifically, let $\\rho_a^+, \\rho_a^-, \\rho_y^+, \\rho_y^-\\in [0,1]$.\nWe assume samples with $A1$ flipped with probability $\\rho_a^+$ and samples with $A0$ flipped with probability $\\rho_a^-$.\nWe also assume samples with $Y1$ flipped with probability $\\rho_y^+$ and samples with $Y0$ flipped with probability $\\rho_y^-$.\nThus, we have:\n\\begin{equation}\n   \\begin{aligned}\n   \\eta_a   & (1-\\rho^+_a) \\cdot \\eta_a + \\rho^-_a \\cdot (1-\\eta_a)\\\\\n   (1-\\eta_a) & \\rho^+_a \\cdot \\eta_a + (1-\\rho^-_a) \\cdot (1-\\eta_a)\\\\\n   \\eta_y   & (1-\\rho^+_y) \\cdot \\eta_y + \\rho^-_y \\cdot (1-\\eta_y)\\\\\n   (1-\\eta_y) & \\rho^+_y \\cdot \\eta_y + (1-\\rho^-_y) \\cdot (1-\\eta_y)\\\\\n   \\end{aligned}\n\\end{equation}\n\nSince CCN setting is a special case of MC learning \\cite{corruption}, the above setting is equivalent to the conditional probability caused by the corruption on the sensitive attribute as in 2.1 (we use the notation $\\pi_{a,\\cdot,\\text{corr}_{ay}}$ for the corrupted base rate in this setting), and the following conditional distribution caused by the corruption on the label:\n\\begin{equation}\n   \\begin{aligned}\n   &D_{\\cdot,1,\\text{corr}_{ay}}   (1-\\alpha_y) \\cdot D_{\\cdot,1} + \\alpha_y \\cdot D_{\\cdot,0}\\\\\n   &D_{\\cdot,0,\\text{corr}_{ay}}   \\beta_y \\cdot D_{\\cdot,1} + (1-\\beta_y) \\cdot D_{\\cdot,0},\n   \\end{aligned}\n\\end{equation}\nwhich has the corrupted base rate $\\pi_{\\cdot,y,\\text{corr}_{ay}}$.\n\nAs shown in \\citet[Appendix C]{corruption}, the parameters are as the follows:\n\\begin{align*}\n   \\pi_{a,\\cdot,\\text{corr}_{ay}} & (1-\\rho_a^+)\\pi_{a,\\cdot}+\\rho_a^-(1-\\pi_{a,\\cdot})\\\\\n   \\alpha_a & \\pi_{a,\\cdot,\\text{corr}_{ay}}^{-1}(1-\\pi_{a,\\cdot})\\rho_a^-\\\\\n   \\beta_a & (1-\\pi_{a,\\cdot,\\text{corr}_{ay}})^{-1}\\pi_{a,\\cdot}\\rho_a^+\\\\\n   \\pi_{\\cdot,y,\\text{corr}_{ay}} & (1-\\rho_y^+)\\pi_{\\cdot,y}+\\rho_y^-(1-\\pi_{\\cdot,y})\\\\\n   \\alpha_y & \\pi_{\\cdot,y,\\text{corr}_{ay}}^{-1}(1-\\pi_{\\cdot,y})\\rho_y^-\\\\\n   \\beta_y & (1-\\pi_{\\cdot,y,\\text{corr}_{ay}})^{-1}\\pi_{\\cdot,y}\\rho_y^+\n\\end{align*}\n\n\n\n\n\n\\subsection{Demographic Parity(To be fixed)}\n\n\\begin{theorem}\\label{thm: sensitive and label noise reduction}\n   Under the noise setting of section \\ref{section:bothnoise}, in the DP case, the constrained optimization problem\n   \\begin{align*}\n   \\min_{f\\in \\mathcal{F}}& \\ \\ \\mathrm{BER}_y^D(f)\\\\\n   \\text{s.t.}& \\ \\ \\left| \\mathbb{P}^+_{D_{0,\\cdot}}(f)-\\mathbb{P}^+_{D_{1,\\cdot}}(f) \\right| \\leq \\tau\n   \\end{align*}\n   is equivalent to\n   \\begin{align*}\n   \\min_{f\\in \\mathcal{F}}& \\ \\ (1-\\alpha_y-\\beta_y)^{-1} \\mathrm{BER}_y^{D_{\\text{corr}_{ay}}}(f)\\\\\n   \\text{s.t.}& \\ \\ \\left| \\mathbb{P}^+_{D_{0,\\cdot,{\\text{corr}_{ay}}}}(f)-\\mathbb{P}^+_{D_{1,\\cdot,{\\text{corr}_{ay}}}}(f) \\right| \\leq \\tau(1-\\alpha_a-\\beta_a)\n   \\end{align*}\n   \n\\end{theorem}\n\\begin{proof}[Proof]\n We know from \\cite{corruption} that $\\frac{1}{(1-\\alpha_a - \\beta_a)} \\paren{\\mathrm{BER}_y^{D_{\\text{corr}_{ay}}}(f) - \\frac{\\alpha_a+\\beta_a}{2}}   \\mathrm{BER}_y^D(f)$. So\n \\begin{align*}\n   &\\min_{f\\in \\mathcal{F}} \\ \\ \\mathrm{BER}_y^D(f)\\\\\n   &\\iff \\min_{f\\in \\mathcal{F}} \\ \\ \\frac{1}{(1-\\alpha_a - \\beta_a)} \\paren{\\BER_y^{D_{\\text{corr}_{ay}}}(f) - \\frac{\\alpha_a+\\beta_a}{2}}\\\\\n   &\\iff \\min_{f\\in \\mathcal{F}} \\frac{1}{(1-\\alpha_a - \\beta_a)} \\BER_y^{D_{\\text{corr}_{ay}}}(f)\n \\end{align*}\nBecause the two noises are independent with each other, the constraint comes directly from \\ref{thm: sensitive noise reduction}.\n\\end{proof}\n\n\n\n\\subsection{Equality of Opportunity}\n",
  "title": "Noise-tolerant fair classification"
}
