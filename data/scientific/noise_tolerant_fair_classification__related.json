{
  "authors": [
    "Alexandre Louis Lamy",
    "Ziyuan Zhong",
    "Aditya Krishna Menon",
    "Nakul Verma"
  ],
  "date_published": "2019-01-30",
  "raw_tex": "auto-ignore\nWe review relevant literature on fair and noise-tolerant , and privacy-preserving\nmachine learning.\n\n\n\\subsection{Fair machine learning}\n\\label{sec:bg-fairness}\n\nAlgorithmic fairness has gained significant attention recently because of the undesirable social impact caused by bias in machine learning algorithms~\\citep{COMPAS,gendershades,Lahoti}.\nThere are two central problems:\n{objectives}:\nwhat is an appropriate fairness definition, and\nhow to optimize the original objective while satisfying the chosen fairness definition.\n \\EDIT{designing appropriate application-specific fairness criterion, and\n how to develop predictors that respect the chosen fairness conditions}.\ndesigning appropriate application-specific fairness criterion, and\ndeveloping predictors that respect the chosen fairness conditions.\n\nTo deal with the first problem, multiple fairness definitions have been proposed.\n\\aditya{US English: categorized}\n{Broadly, fairness objectives can be categorised} into\nThese fall into two main categories: \nindividual- and group-level fairness.\nIndividual-level fairness~\\citep{Dwork:2011,Kusner,Kim} requires the treatment of ``similar'' individuals to be similar.\nGroup-level fairness \nasks the treatment of the groups divided based on some sensitive attributes (e.g., gender, race) to be similar.\nPopular notions of group-level fairness include\ndemographic parity~\\citep{Calders}\nand\nequality of opportunity~\\citep{Hardt2016}\n, and disparate mistreatment~\\citep{ZafarWWW}\n;\nsee \\S\\ref{section:fairnessdef} for formal definitions. \n\n \\EDIT{Group-level fairness criteria tend to be more conducive to algorithmic design and analysis, and is achieved by three possible ways:}\n\\aditya{Don't want to say ``more conducive''. Someone working on individual fairness could get irate.}\n{Group-level fairness criteria have been the subject of significant algorithmic design and analysis,}\nand are achieved in three possible ways:\n\nThere are three main types of method\nto achieve fairness while optimizing the original objective:\n\\begin{itemize}[itemsep0pt,topsep0pt,leftmargin16pt]\n   \\item[--] pre-processing methods~\\citep{ZemelICML,ZemelICLR,Lum,Johndrow:2017,Calmon:2017,delBarrio:2018,Adler:2018},\n   which\n   usually embed the representation of data into a new space such that the bias is removed.\n   {usually find a new representation of the data where the bias with respect to the sensitive feature is explicitly removed.}\n   \n   \\item[--] methods enforcing fairness during training~\\citep{Calders,Woodworth,ZafarWWW,reduction},\n   which\n   usually add a constraint that is a proxy of the fairness criteria or add a regularization term to penalise fairness violation.\n\n   \\item[--] post-processing methods~\\citep{Feldman,Hardt2016},\n   which usually apply a thresholding function to make the prediction satisfying the chosen fairness notion across groups.\n\\end{itemize}\n\n\n\n\\subsection{Noise-tolerant classification}\n\nDesigning noise-tolerant classifiers is a classic topic of study, concerned with the setting where one's training labels are corrupted in some manner.\nTypically, works in this area postulate a particular model of label noise, and study the viability of learning under this model.\n Prominent noise models include\n symmetric and class-conditional noise~\\citep{Angluin88,Bylander:1994,Blum:1996,Blum:1998},\n constant-partition noise~\\citep{Decatur:1997,Ralaivola:2006},\n and instance-dependent noise~\\citep{Awasthi:2015}.\n It has also been shown that learning performance degrades as label noise increases~\\citep{Nettleton2010}.\nClass-conditional noise (CCN)~\\citep{Angluin88} is one such well-studied \n{effective}\nnoise model.\nHere, samples from each class have their labels flipped with some constant (but class-specific) probability.\nAlgorithms that deal with CCN corruption have been well studied~\\citep{natarajan2013learningw,Liu2016ClassificationWN,northcutt2017rankpruning}.\nThese methods typically first estimate the noise rates, which are then used for prediction.\nA special case of CCN learning is learning from positive and unlabelled data (PU learning)~\\citep{Elkan},\nwhere in lieu of explicit negative samples, one has a pool of unlabelled data.\n\nMultiple noise settings have been explored, e.g., mutually contaminated (MC) learning~\\citep{Scott13} and class-conditional noise (CCN) learning~\\citep{Angluin88}.\n\\iffalse\nOur particular interest in this paper will be in the class-conditional noise (CCN) setting.\nHere, samples from each class have their labels flipped with some constant (but class-specific) probability.\nAlgorithms that deal with CCN corruption have been well studied~\\citep{natarajan2013learningw,Liu2016ClassificationWN,northcutt2017rankpruning}.\nThese methods typically first estimate the noise rates and then use these rates for prediction.\nThe CCN model is a special case of the more general mutually contaminated (MC) learning model~\\citep{Scott13}.\n\nAn important related problem is that of learning from positive and unlabelled data (PU learning)~\\citep{Denis:1998a},\nwhere in lieu of explicit negative samples, one has a pool of unlabelled data.\nPU learning in turn has two prominent settings:\nthe censoring setting, which is a special case of class-conditional noise~\\citep{Elkan}, and\nthe case-controlled setting~\\citep{Ward:2009}, which is a special case of class-conditional noise \\emph{plus} an additional rebalancing of labels.\nBoth of these can be modelled as a special case of a more general corrupted label framework~\\citep{pmlr-v30-Scott13,corruption}.\n\\fi\n\n PU learning in turn has two prominent settings.\n In the censoring setting of the problem~\\citep{Elkan}, one imagines first drawing labelled samples, and then placing some fraction of positives in the unlabelled pool.\n In the case-controlled setting~\\citep{Ward:2009}, one imagines independently drawing samples from the positive and marginal distribution over instances.\n Both of these can be modelled as a special case of MC learning.\n\nOur interest in this paper will be the \\emph{mutually contaminated} (MC) \\emph{learning} noise model~\\citep{Scott13}.\nThis model (described in detail in \\S\\ref{sec:mc-learning}) captures both CCN and (both settings of)\nPU learning as special cases~\\citep{pmlr-v30-Scott13,corruption}, {as well as other interesting noise models}.\n\n\n\n\\subsection{fair classification preserving privacy of sensitive attribute}\n\\ZIYUANEDIT{There exist previous work \\cite{pmlr-v80-kilbertus18a} that try to preserve privacy of every individual's sensitive attribute's value while learning a fair classifier by employing the cryptographic tool of secure multiparty computation (MPC). However, as shown in \\cite{Kearns18}, the individual information that MPC tries to protect can be inferred from the learned model. Besides, their method is limited to using Demographic Parity as fairness creteria.}\n\\ZIYUANEDIT{Concurrent work \\cite{fairlearning_withprivatedata} on preserving privacy of sensitive attribute uses a randomized response procedure on the sensitive attribute values and a two-step procedure to train a fair classifier using the processed data. Theoretically, their method improves the sample complexity compared with our method and extends our privacy result to the case of non-binary groups. However, their method solely focuses on preserving privacy rather than a general noise setting. Besides, no empirical result has been shown for the effectiveness of their proposed method.\n}\n\n \n \\subsection{Fairness and differential privacy}\n\n Recently,~\\citet{Kearns18} explored preserving differential privacy~\\citep{Dwork06} while at the same time of preserving \n {maintaining} fairness constraints. \n and achieving good accuracy.\n The authors proposed two methods:\n one adds Laplace noise to training data and apply the post-processing method in~\\citet{Hardt2016},\n while another modifies the method in~\\citet{reduction} using the exponential mechanism as well as Laplace noise.\n Our work differs from them in three major ways:\n \\begin{enumerate*}[itemsep0pt,topsep0pt,label(\\emph{\\arabic*})]\n   \\item \\AKMEDIT{our work can leverage \\emph{any} algorithm for achieving fairness, while~\\citet{Kearns18} is tied to specific algorithms};\n   rather than modifying specific algorithms, our work can be used before applying a large group of fair algorithms;\n   \\item although our framework can also be used for preserving differential privacy, we mainly focus on the case of data corruption   under the framework of MC learning rather than privacy;\n   \\item we deal with not only equalized odds, but also demographic parity.\n \\end{enumerate*}\n",
  "title": "Noise-tolerant fair classification"
}
