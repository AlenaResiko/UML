{
  "authors": [
    "Nakul Verma",
    "Kristin Branson"
  ],
  "date_published": "2015-05-11",
  "raw_tex": "\\documentclass{article}\n\\pdfoutput1\n\n\n\n ICML 2014 EXAMPLE LATEX SUBMISSION FILE \n\n\n Use the following line _only_ if you're still using LaTeX 2.09.\n\\documentstyle[icml2014,epsf,natbib]{article}\n If you rely on Latex2e packages, like most moden people use this:\n\n use Times\n\\usepackage{times}\n For figures\n\\usepackage{graphicx}   more modern\n\\usepackage{epsfig}   less modern\n\\usepackage{subfigure} \n\n For citations\n\\usepackage{natbib}\n\n For algorithms\n\\usepackage{algorithm}\n\\usepackage{algorithmic}\n\n As of 2011, we use the hyperref package to produce hyperlinks in the\n resulting PDF.   If this breaks your system, please commend out the\n following usepackage line and replace \\usepackage{icml2014} with\n \\usepackage[nohyperref]{icml2014} above.\n\\usepackage{hyperref}\n\n\n\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsmath}\n\\usepackage{epsfig}\n\\usepackage{multirow}\n\\usepackage{color}\n\\usepackage{enumerate}\n\\usepackage{units}\n\\usepackage{natbib}\n\\usepackage{ifthen}\n\\usepackage{authblk}\n\\usepackage[margin1.5in]{geometry}\n\n Packages hyperref and algorithmic misbehave sometimes.   We can fix\n this with the following command.\n\\newcommand{\\theHalgorithm}{\\arabic{algorithm}}\n\n Employ the following version of the ``usepackage'' statement for\n submitting the draft version of the paper for review.   This will set\n the note in the first column to ``Under review.   Do not distribute.''\n\\usepackage{icml2015} \n\n Employ this version of the ``usepackage'' statement after the paper has\n been accepted, when creating the final version.   This will set the\n note in the first column to ``Proceedings of the...''\n\\usepackage[accepted]{icml2015}\n\n\n\n\n The \\icmltitle you define below is probably too long as a header.\n Therefore, a short form for the running title is supplied here:\n\\icmltitlerunning{Sample Complexity of Learning Mahalanobis Distance Metrics}\n\n\n\n\\def\\Pr{{\\bf P}}\n\\def\\E{{\\mathbb E}}\n\n\\newcommand{\\alg}[1]{\\textsc{#1}}\n\n\\DeclareMathOperator{\\indicate}{\\mathbf{1}} 1 \\kern -4pt 1}\n\\DeclareMathOperator{\\classD}{\\mathfrak{D}}\n\\DeclareMathOperator{\\R}{\\mathbb{R}}\n\\DeclareMathOperator{\\M}{\\mathcal{M}}\n\\DeclareMathOperator{\\D}{\\mathcal{D}}\n\\DeclareMathOperator{\\Xb}{\\mathbf{X}}\n\\DeclareMathOperator{\\pdf}{\\mathrm{pdf}}\n\\DeclareMathOperator{\\diam}{\\mathrm{diam}}\n\\DeclareMathOperator{\\distM}{\\rho_{_M}}\n\\DeclareMathOperator{\\Dim}{\\mathrm{Dim}}\n\\DeclareMathOperator{\\T}{^\\mathsf{T}}   transpose\n\\DeclareMathOperator{\\Hnet}{\\mathcal{H}^{\\textrm{2-net}}_{\\sigma^\\gamma}} \n\n\\DeclareMathOperator{\\ALG}{\\mathcal{A}}\n\\DeclareMathOperator{\\fat}{\\mathsf{Fat}}\n\\DeclareMathOperator{\\err}{\\textup{err}}\n\\DeclareMathOperator{\\tr}{\\textup{tr}}\n\\DeclareMathOperator{\\mean}{\\textup{mean}}\n\\DeclareMathOperator{\\Mgap}{\\mathcal{M}_\\textup{0-1}}\n\\DeclareMathOperator{\\dist}{\\textup{dist}}\n\\DeclareMathOperator{\\reg}{\\textup{reg}}\n\\DeclareMathOperator{\\hypoth}{\\textup{hypoth}}\n\\DeclareMathOperator{\\cov}{\\textrm{cov}}\n\\DeclareMathOperator{\\pak}{\\textrm{pak}}\n\\DeclareMathOperator{\\argmin}{\\textup{argmin}}\n\\DeclareMathOperator{\\resol}{mD}\n\\DeclareMathOperator{\\resolSZ}{(2mD+3)^{D^2}}\n\\DeclareMathOperator{\\resolSZlb}{(2mD)^{D^2}}\n\n\\newcommand{\\TODO}[1]{$\\ll$ #1 $\\gg$}\n\n\\newcommand{\\keyword}[1]{\\texttt{#1}}\n\\newcommand{\\ie}{{\\emph{i.e.,}}}\n\\newcommand{\\eg}{{\\emph{e.g.,}}}\n\n\\newcommand*\\xbar[1]{\n   \\hbox{\n   \\vbox{\n   \\hrule height 0.5pt   The actual bar\n   \\kern0.17ex   Distance between bar and symbol\n   \\hbox{\n   \\kern-0.25em   Shortening on the left side\n   \\ensuremath{#1}\n   \\kern-0.12em   Shortening on the right side\n   }\n   }\n   }\n} \n\n\n\n\\def\\qed{\\vrule height8pt width3pt depth0pt}\n\\def\\jump{\\vskip0.05in}\n\n\\newtheorem{theorem}{Theorem}   \n\\newtheorem{prop}[theorem]{Proposition}   \n\\newtheorem{definition}{Definition}   \n\\newtheorem{assumption}[theorem]{Assumption}   \n\\newtheorem{corollary}[theorem]{Corollary}   \n\\newtheorem{lemma}[theorem]{Lemma}   \n\\newtheorem{fact}[theorem]{Fact}   \n\\newtheorem{claim}[theorem]{Claim}   \n\\newenvironment{proof}{\\noindent{\\it Proof.} }{\\qed\\jump}   \n\n\n\n\n\n\n\n\n\n\\begin{document} \n\n\\title{Sample Complexity of Learning Mahalanobis \\\\Distance Metrics}\n\n\n\\author{Nakul Verma\\thanks{email: \\texttt{verman@janelia.hhmi.org}; corresponding author.} }\n\\author{Kristin Branson\\thanks{email: \\texttt{bransonk@janelia.hhmi.org}}}\n\\affil{Janelia Research Campus\\\\Howard Hughes Medical Institute, Virginia, USA}\n\\date{}\n\\affil[2]{Department of Mechanical Engineering, \\LaTeX\\ University}\n\n\n\\address{Janelia Research}\n\\address{Janelia Research}\n\n\\maketitle\n\\twocolumn[\n\\icmltitle{Sample Complexity of Learning Mahalanobis Distance Metrics\n   }\n\n It is OKAY to include author information, even for blind\n submissions: the style file will automatically remove it for you\n unless you've provided the [accepted] option to the icml2014\n package.\n\\icmlauthor{Nakul Verma}{verman@janelia.hhmi.org}\n\\icmladdress{Your Fantastic Institute,\n   314159 Pi St., Palo Alto, CA 94306 USA}\n\\icmlauthor{Kristin Branson}{bransonk@janelia.hhmi.org}\n\\icmladdress{Janelia Research Campus, HHMI,\n   Ashburn, Virginia, USA}\n\n You may provide any keywords that you \n find helpful for describing your paper; these are used to populate \n the \"keywords\" metadata in the PDF but will not be shown in the document\n\\icmlkeywords{Metric Learning, Learning Theory, Sample Complexity, Mahalanobis Distance Metrics}\n\n\\vskip 0.3in   <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< THIS IS THE ORIGINAL \n\\vskip 0.4in\n]\n\n\\begin{abstract} \nOUTLINE: \n\n- Want to study metric learning more formally, understand both theoretically and practically how the error rates scale with key attributes of a given datasets.\n\n- Problem types considered: pairwise distance metric learning, triplet distance metric learning, classifier based metric learning.\n\n- Previous theoretical studies have focused on understanding sample complexity rates by 'Algorithmic stability' with regularization on the metric, and convex losses. These are studies only focus on pairwise distance metric learning. Recent work on 'Algorithmic robustness' has incorporates pairwise and triplet learning, but the bound is effectively exponential.   We focus on a smoothed version on the 0-1 loss and analyze all three canonical types of metric learning problems.\n\n- Main theory result 1: without any specific assumptions on the underlying distribution the sample complexity scales as $D^2 / eps^2$.\n\n- Main theory result 2: Lower bounds indicate that at least $D /eps^2$ necessary.   (have it for pairwise, and classifier based)   (MISSING triplet)\n\nBUT, in practice we typically see much better error rates, indicating some sort of adaptivity to intrinsic discrimination complexity of data...(ref expts)   (MISSING)\nBriefly describe what do we mean and distinguish between representation dimension, underlying data dimension (manifold dim/doubling dim/etc in literature), and 'discrimination dimension' (new for supervised data) with cartoon examples.\n\n\nA 'better' (more relating to reality) data model to capture above notions more formally ??   (MISSING)\n\n- Main theory result 3: sample complexity of metric learning scales with 'discrimination dimension' of this better data model (MISSING).\n\n\n- Experiments: (using different at least two standard metric learning algorithms: LMNN,ITML)\n- Synthetic: Gaussians controlling the representation dimension, underlying data dimension, 'discrimination dimension' (IN PROGRESS)\n- ImageNet: Need more details (dimensions are varied by adding deep learning features with standard sift bag of words?) and how it and supports the paper message.\n\nMetric learning seeks a transformation of the feature space that enhances prediction\nquality for the given task at hand. In this work we provide PAC-style \nsample complexity rates for supervised metric learning. We give matching lower-\nand upper-bounds showing that the sample complexity scales with the representation\ndimension when no assumptions are made about the underlying data distribution. \nHowever, by leveraging the structure of the data distribution, we show that one can achieve rates that are \\emph{fine-tuned} to \na specific notion of intrinsic complexity for a given dataset. Our analysis \nreveals that augmenting the metric learning optimization criterion with a simple norm-based regularization \ncan help adapt to a dataset's intrinsic complexity, yielding better\ngeneralization.\nExperiments on benchmark datasets validate our\nanalysis and show that regularizing the metric can help discern the signal even when the data contains high amounts of noise.\n\nof weighting metrics in can\n\ndataset-dependent sample \ncomplexity result suggests that norm-regularized training error minimizing algorithms can\nyield better generalization error for datasets with low intrinsic complexity.\nExperiments on benchmark datasets validate our\nanalysis and show that regularizing the metric can help evade noise that may be present in data.\n\\end{abstract} \n\n\\allowdisplaybreaks\n\n\\section{Introduction}\n\nIn many machine learning tasks, data is represented in a\nhigh-dimensional Euclidean space where each dimension corresponds to some\ninteresting measurement of the observation. Often, \npractitioners include a variety of measurements in hopes that some\ncombination of these features will capture the relevant information. in data.\nWhile it is natural to represent such data in a Real space of measurements,\nthere is no reason to expect that using Euclidean ($L_2$) distances to compare the\nobservations will be necessarily useful for the task at hand. Indeed, the presence of\nuninformative or mutually correlated measurements simply inflates the $L_2$-distances \nbetween pairs of observations, rendering distance-based comparisons ineffective. \n\n\\emph{Metric learning} has emerged as a powerful technique to learn a good\nnotion of distance or a \\emph{metric} in the representation space that can emphasize the \nfeature combinations that help in the predication task while suppressing the contribution of\nspurious measurements. The past decade has seen a variety of successful metric learning algorithms \nthat leverage various attributes of the problem domain. A few notable examples include exploiting class labels \nto find a Mahalanobis distance metric that maximizes the distance between\ndissimilar observations while minimizing distances between similar ones to improve classification quality\n\\citep{met_learn:alg_LMNN, met_learn:alg_ITML}, and explicitly optimizing for\na downstream prediction task such as information retrieval \n\\citep{met_learn:alg_mlr_mcfee}.   or network connectivity \\citep{met_learn:alg_spml_shaw}. \n\nclassification task such as good nearest neighbor performance\n\\citep{met_learn:alg_LMNN} or ranking accuracy \n\nor (iii) explicitly finding metrics that use the most relevant set of input features \\citep{met_learn:alg_rob_stru_mcfee}.\n\n\n\nDespite the popularity of metric learning methods, few studies have focused on\nstudying how the problem complexity scales with key attributes of a given\ndataset. For instance, how do we expect the generalization error to scale---both\ntheoretically and practically---as one varies the number of informative and\nuninformative measurements, or changes the noise levels? \nIt would be instructive to \nunderstand the strengths and failure modes of popular metric learning\ntechniques, thus providing us with a set of guiding principles for metric learning\nin different application domains.\n\nHere we study supervised metric learning more\nformally and gain a better understanding of how different modalities in data affect the metric learning problem. \nWe develop two general frameworks for PAC-style analysis of supervised metric learning. \nWe can categorize the popular metric learning algorithms into an empirical\nerror minimization problem in one of the two frameworks.   The first generic\nframework, the distance-based metric learning framework, uses class label\ninformation to derive distance constraints.\nThe key objective is to learn a metric that on average yields smaller distances between\npairs (or triples) of \nexamples from the same class than those from\ndifferent classes. \n\\footnote{We will explicitly show results on pairwise constraints, the with slight more work, results are generalizable \nto triplet constraints.}\nSome popular examples of \nalgorithms that optimize for such distance-based objectives include Mahalanobis Metric for Clustering (MMC) by \\citet{met_learn:alg_xing} and Information Theoretic Metric Learning \n(ITML) by \\citet{met_learn:alg_ITML}.\nInstead of using\ndistance comparisons as a proxy, however, one can also optimize for a specific prediction task directly.\nThe second generic framework, the classifier-based metric learning framework,\n explicitly incorporates the hypothesis associated with the prediction\ntask of interest to learn effective distance metrics. \nA few interesting examples in this regime include the work by \\citet{met_learn:alg_mlr_mcfee} that finds metrics that improve ranking quality in information retrieval tasks, and\nthe work by \\citet{met_learn:alg_spml_shaw} that learns metrics that help predict connectivity structure in networked data.\n\nOur analysis shows that in both frameworks, the sample complexity\nscales with the representation dimension for a given dataset (Lemmas \\ref{lm:unif_conv_all} and \\ref{lm:hypoth_ub}), and\nthis dependence is necessary in the absence of any specific assumptions on the underlying\ndata distribution (Lemmas \\ref{lm:lb_dist} and \\ref{lm:hypoth_lb}). By\nconsidering any Lipschitz loss, our results generalize previous sample\ncomplexity results (see our discussion in Section\n\\ref{sec:related_work}) and, for the first time in the literature, provide\nmatching lower bounds.\n \n\nIn light of the observation made earlier that data measurements often include uninformative or weakly informative features, \nwe expect a metric that yields good generalization performance to de-emphasize such features and accentuate the relevant ones.\nWe can thus formalize the \\emph{metric learning complexity} of a given dataset in terms of\nthe intrinsic complexity $d$ of the metric that reweights the features in a way that yields the best generalization\nperformance. (For Mahalanobis distance metrics, we can characterize the intrinsic complexity by the \\emph{norm} of the\nmatrix representation of the metric.) We refine our\nsample complexity result and show a \\emph{dataset-dependent} bound for both frameworks that scales with \ndataset's intrinsic metric learning complexity $d$ (Corollary \\ref{cor:unif_conv_refined}). \n\nTaking guidance from our dataset-dependent result, we propose a simple variation on\nthe empirical risk minimizing (ERM) algorithm that, when given an i.i.d.\\ sample, \nreturns a metric (of complexity $\\hat{d}$) that jointly minimizes the observed\nsample bias and the expected intra-class variance for metrics of fixed\ncomplexity $\\hat{d}$. This bias-variance balancing algorithm can be viewed as a\nstructural risk minimizing algorithm that provides better generalization\nperformance than an ERM algorithm and justifies norm-regularization of\nweighting metrics in the optimization criteria for metric learning. \n\nFinally, we evaluate the practical efficacy of our proposed norm-regularization criteria with some popular metric\nlearning algorithms on benchmark datasets (Section \\ref{sec:experiments}). Our experiments highlight that the norm-regularization indeed helps\nin learning weighting metrics that better adapt to the signal in data in high-noise regimes. \n\n\\TODO{hopefully our experimental results show something interesting to talk about.}\n\n\nfare in practice \n\n\n\nbest weithting $M$ \n\n\n\nperformacthe concept of \\emph{intrinsic discriminative complexity} of a\ngiven dataset. Similar in spirit to the notions of ``intrinsic dimensionality\"\nthat are often used to analyze unsupervised learning problems (for instance, the concept of \\emph{doubling dimension} used\nby \n\\citet{manifold_cover_tree_langford} \nfor nearest neighbor retrieval, or the concept of \n\\emph{local covariance dimension}\nformalized by \\citet{manifold:rptree_dasgupta_freund} for vector quantization), \\emph{intrinsic discriminative complexity}\nprovides an intuitive way to capture the classification complexity in terms of the features used to represent the data. \n\\TODO{We \nshow that the metric learning problem scales with the data's\ndiscriminative complexity and representation complexity.}\n\nHaving established how we expect the prediction accuracy to scale with key characteristics of data, we study\nhow well the theoretical rates match with the rates obtained by the popular instantiations of metric learning\nin both the regimes in representative application domains. \\TODO{discuss the practical results and insights a bit more}.\n\n\\TODO{concluding para goes here}.\n\nDatasets are often over-represented with a number of features in hopes that some combination of these features \nwould capture the relevant information in data. It is often empirically observed that data is often represented in far more features than what is\nnecessary to get reasonable performance. \n\nHere we will formalize this concept of intrinsic discriminatative complexity of a given dataset, and evaluate \nhow the metric learning problem scales with the data's discrimintative complexity and representation complexity.\n\n- Problem types considered: pairwise distance metric learning, triplet distance metric learning, classifier based metric learning.\n\n- Previous theoretical studies have focused on understanding sample complexity rates by 'Algorithmic stability' with regularization on the metric, and convex losses. These are studies only focus on pairwise distance metric learning. Recent work on 'Algorithmic robustness' has incorporates pairwise and triplet learning, but the bound is effectively exponential.   We focus on a smoothed version on the 0-1 loss and analyze all three canonical types of metric learning problems.\n\n- Main theory result 1: without any specific assumptions on the underlying distribution the sample complexity scales as $D^2 / eps^2$.\n\n- Main theory result 2: Lower bounds indicate that at least $D /eps^2$ necessary.   (have it for pairwise, and classifier based)   (MISSING triplet)\n\n\n\n\n\n\n\n\n\n\n\n\\section{Preliminaries}\n\n\nGiven a representation space $X   \\R^D$ of $D$ real-valued measurements of \nobservations of interest, the goal of metric learning is to learn a\n\\emph{metric} $M$ (that is, a $D\\times D$ real-valued weighting matrix on\n$X$; to remove arbitrary scaling we shall assume that the\nmaximum singular value of $M$, that is, $\\sigma_{\\max}(M)   1$)\\footnote{Note\n; we shall use $\\mathcal{M}$ to denote the set of such matrices)\\footnote{Note\n that we are \nlooking at the linear form of the metric $M$; usually the corresponding quadratic form $M^\\mathsf{T}M$ is\ndiscussed in the literature, which is necessarily positive semi-definite.}\n; to remove arbitrary scaling we\nshall also assume that the maximum singular value of M, that is,\n$\\sigma_{\\max}(M)   1$) \nthat minimizes some notion of \\emph{error} on data drawn from an unknown underlying\ndistribution $\\D$ on $X \\times \\{0,1\\}$. Specifically, we want to find the metric $M^*$ s.t.\\\n\\begin{eqnarray*}\nM^* : \\argmin_{M \\in \\mathcal{M}} \\err(M,\\D),\n\\end{eqnarray*}\nfrom the class of metrics $\\mathcal{M}$ under consideration, that is, $\\mathcal{M}: \\{ M \\;|\\; M\\in \\R^{D\\times D}, \\sigma_{\\max}(M)1   \\}$.\nFor supervised metric learning, this \\emph{error} is typically label-based and can be defined in multiple\nreasonable ways. As discussed earlier, we explore two intuitive regimes for defining error.\n\\\\\n\n\\textbf{Distance-based error.} \nA popular criterion for quantifying error in metric learning is by comparing \\emph{distances} amongst (pairs or triples of) \npoints\ndrawn from the underlying data distribution.   \nIdeally, we want a weighting metric $M$ that brings data from the same class closer\ntogether than those from opposite classes. In a distance-based framework, a natural way to accomplish this is to\nfind a weighting $M$ that yields shorter distances between pairs of observations from the same class\nthan those from different classes.\nBy penalizing\nhow often and by how much the distances violate these constraints gives rise\nto the particular form of the error. \n\n\nLet the variable $z(x,y)$ denote a random draw from $\\D$ with $x \\in X$ as \nthe observation and $y\\in\\{0,1\\}$ its associated label,\nand let $\\lambda$ denote \nhow severely one wants to penalize the distance violations, \nthen a natural definition of \\emph{distance-based} error becomes: \n\\begin{align*}\n\\err_{\\dist}^\\lambda(M,\\D) : \\E_{z_1, z_2 \\sim \\D} \\Big[ \\phi^\\lambda\\big( \\distM(x_1,x_2), Y \\big) \\Big],   \n\\end{align*}\nfor a generic distance-based loss function $\\phi^\\lambda(\\distM, Y)$, that computes\nthe degree of violation between weighted distance $\\distM(x_1,x_2) : \\|M(x_1-x_2)\\|^2$ and the label agreement $Y : \\indicate[y_1   y_2]$\namong a pair $z_1   (x_1,y_1)$ and $z_2   (x_2,y_2)$ drawn from the underlying data distribution $\\D$.\n$\\D$.\n\ndenote the (squared) Euclidean distance between arbitrary points\n$x$ and $x'$ with respect to $M$, and let $\\phi^\\lambda(\\hat Y, Y)$ be some $\\lambda$-Lipschitz loss function,\nthen we can define\n\n\n\n\nAn example instantiation of $\\phi$ popular in literature encourages metrics\nthat yield distances that are no more than some upper limit $U$ between\nobservations from the same class, and distances that are no less than some\nlower limit $L$ between those from\ndifferent classes (for some $U<L$). Thus\n\\begin{align}\n\\phi_{L,U}^\\lambda(\\distM,Y) : \\Bigg\\{ \n\\begin{array}{ll} \n   \\min\\{1, \\lambda[\\distM - U]_{_+} \\} & \\textrm{if $Y   1$} \\\\ \\vspace{0.05in}\n   \\min\\{1, \\lambda[L - \\distM]_{_+} \\}   & \\textrm{otherwise} \\end{array},\n\\label{eq:pair_LUloss}\n\\end{align}\n\\vspace{-0.25in}\nwhere $[A]_{_+} : \\max\\{0,A\\}$.\n\n \\citet{met_learn:alg_xing} optimize an efficiently computable variant of\nthis criterion, in which they look for a metric that keeps the total pairwise distance\namongst the observations from the same class less than a constant while\nmaximizing the total pairwise distance amongst the observations from \nopposite classes. The variant proposed by \\citet{met_learn:alg_ITML} explicitly includes the\nupper and lower limits with an added regularization on the learned $M$\nto be close to a pre-specified metric of interest $M_0$.   as possible.\n\nWhile we discuss loss-functions $\\phi$ that handle distances between a \\emph{pair} of observations, it is\neasy to extend to distances among \\emph{triplets}. Rather than \nhaving hard upper and lower limits which every pair of the same and the opposite\nclasses must obey, a triplet-based comparison typically focuses on relative distances between three\nobservations at a time. A natural instantiation in this case becomes:\n\\begin{align*}\n&\\phi_{\\textrm{triple}}^\\lambda(\\distM(x_1,x_2), \\distM(x_1,x_3), (y_1,y_2,y_3)) : \n\\Bigg\\{ \\!\\! \\begin{array}{ll} \n   \\min\\{1,\\lambda [\\distM(x_1,x_2) - \\distM(x_1,x_3)]_{_+}\\}   & \\!\\!\\! \\textrm{if $y_1   y_2 \\neq y_3$} \\\\ \n0 & \\!\\!\\! \\textrm{otherwise} \n\\end{array} \\!\\!, \n\\end{align*}\nfor a triplet $(x_1,y_1)$, $(x_2,y_2)$, $(x_3,y_3)$\ndrawn from $\\D$.\n\n \\citet{met_learn:alg_LMNN} discuss an interesting variant of this,\nin which instead of looking at all triplets in a given training sample, they focus on\ntriplets of observations in local neighborhoods and learn a metric that maintains\na gap or a margin among distances between observations from the same class and those from the\nopposite class. Improving the quality of distance comparisons in local\nneighborhoods directly affects the nearest neighbor performance, making this a\npopular technique. \n\\\\\n\n\n\\textbf{Classifier-based Error.} Distance comparisons\ntypically act as a surrogate for a specific downstream prediction task.\nIf we want a metric that directly optimizes for a task,\nwe need to explicitly incorporate the hypothesis class being used for that\ntask while finding a good weighting metric. \n\nThis simple but effective insight has been used recently by\n\\citet{met_learn:alg_mlr_mcfee} for improving ranking results in information retrieval problems by explicitly\nincorporating ranking losses while learning an effective weighting metric.\n\\citet{met_learn:alg_spml_shaw} also follow this principle and explicitly include network topology constraints\nto learn a weighting metric that can better predict the connectivity structure in\nsocial and web networks.\n\nWe can formalize the classifier-based metric learning framework by considering a fixed hypothesis class $\\mathcal{H}$ of interest on the measurement\ndomain. To keep the discussion general, we shall assume that the hypotheses are real-valued and can be regarded as a measure of confidence \nin classification, that is, each $h\\in \\mathcal{H}$ is of the form $h:\nX\\rightarrow [0,1]$. (One can obtain the binary predictions from $h$ by a simple thesholding at $1/2$.) Then, the error induced by a particular weighting metric $M$ on the measurement space $X$ can be defined as the best possible error that can be obtained by hypotheses in $\\mathcal{H}$, that is\n\\begin{eqnarray*}\n\\err_{\\hypoth}(M,\\D) : \\inf_{h\\in\\mathcal{H}} \\E_{(x,y) \\sim \\D} \\Big[ \\indicate \\big[ | h(Mx) - y| \\geq 1/2 \\big] \\Big].\n\\end{eqnarray*}\n\nWe shall study how this error scales with various key parameters of the metric learning problem.\n\n\\section{Learning a Metric from Samples}\n\nIn any practical setting, we estimate the ideal weighting metric $M^*$ by minimizing the\nempirical version of the error criterion from a finite size sample from $\\D$.   \nPAC-analysis provides a powerful tool to study the estimation errors rates in terms of number of samples from the\nunderlying data distribution.\n\nLet $S_m$ denote a sample of size $m$, and $\\err(M,S_m)$ denote the empirical error on the sample $S_m$ (the exact definitions of $S_m$ and the form\nof $\\err(M,S_m)$ are discussed later). We can then define the empirical risk minimizing metric based on $m$ samples as\n$M^*_m : \\argmin_M\n\\err(M,S_m).$ Most practical algorithms, of course, return some approximation of $M^*_m$, and thus it is important to compare the generalization ability of\n$M^*_m$ to that of theoretically optimal $M^*$. That is, how\n\\begin{equation}\n \\err(M^*_m, \\D) - \\err(M^*,\\D)   \n\\label{eq:erm_conv}\n\\end{equation}\nconverges as the sample size $m$ grows.\n\n\n\\subsection{Distance-Based Error Analysis} \nGiven an i.i.d.\\ sequence of observations $z_1,z_2,\\ldots$ from $\\D$, we can pair the observations\ntogether to form a \\emph{paired} sample $S_m   \\{(z_1,z_2),(z_3,z_4),\\ldots,(z_{2m-1},z_{2m}) \\}   \\{(z_{1,i},z_{2,i})\\}_{i1}^m$ of size $m$, and define\nthe sample based distance error $\\err_{\\dist}^\\lambda(M, S_m)$ induced by a metric $M$ as\n\\begin{eqnarray*}\n\\err_{\\dist}^\\lambda(M, S_m) : \\frac{1}{m} \\sum_{i1}^m \\phi^\\lambda\\big( \\distM(x_{1,i}, x_{2,i}), \\indicate[ y_{1,i}   y_{2,i} ] \\big).\n\\end{eqnarray*}\n\nThen for any bounded support distribution $\\D$ (that is, each $(x,y)\\sim \\D$, $\\|x\\|\\leq B <\\infty$), we have the following convergence result.\\footnote{We only \npresent the results for paired distance comparisons; the results are easily extended to triplet-based comparisons.}\n\n\n\\begin{lemma}\n\\label{lm:unif_conv_all}\nFix any sample size $m$, and let $S_m$ be an i.i.d.\\ \\emph{paired} sample of\nsize $m$ from an unknown bounded distribution $\\D$ (with bound $B$). For any distance-based loss function $\\phi^\\lambda$ that is $\\lambda$-Lipschitz in the first argument, with probability at least $1-\\delta$ over the draw of $S_m$,\n\\begin{align*}\n\\lefteqn{\n \\sup_{M\\in \\mathcal{M}}   \\big[ \\err_{\\dist}^\\lambda   (M,\\D) - &\n\\err_{\\dist}^\\lambda (M,S_m) \\big]\n\\\\\n\\leq O\\Bigg( \\lambda B^2 \\sqrt{\\frac{D \\ln(1/\\delta)}{m}}\\Bigg).\n\\end{align*}\n\\end{lemma}\n\n\n\n\nubsection{Upper bound}\n** We only analyze the triplet based distance error, the pairwise distance error is analogous **\n \nIn order to get a reasonable bound on \\eqref{eq:erm_conv}, we shall analyze the\nuniform convergence rates of sample errors to expected errors for weighting metrics, that is, $|\\err_{\\dist}(M,S_m) - \\err_{\\dist}(M,\\D)|$ (for arbitrary $M$).\n\nIt turns out that our current definition of distance based error is not\nstable in the following sense: a tiny perturbation in $M$ can yield arbitrary\nlarge generalization error. $\\ll$ Example goes here $\\gg$.   So instead, we shall use a `$\\lambda$-smoothed' version of $f$ in our definition of the distance based error. \n\n \\begin{eqnarray*}\n\\lefteqn{\nf_\\lambda((x_1,y_1),(x_2,y_2),(x_3,y_3); M) \n: }\\\\ &&\n\\Bigg\\{ \\begin{array}{ll}\n\\min\\{1,\\lambda[ \\|Mx_1 - Mx_2\\|- \\|Mx_1 - Mx_3\\| \\big]_+\\}   & \\textrm{if $y_1   y_2 \\neq\ny_3$} \\\\ 0 & \\textrm{o.w.} \\end{array} .\n\\end{eqnarray*}\n\n\\noindent \\textbf{Uniform convergence rates of $\\lambda$-smoothed distance\nerrors.} Let $\\mathcal{M}$ be the class of all $D\\times D$ weighting matrices (with each entry in $[-1,1]$, and unit spectral norm) and define an $\\epsilon$-covering of $\\mathcal{M}$\n as $$\\mathcal{M}_\\epsilon : \\big\\{ M: M_{ij} \\in\n\\{-1,\\ldots,-3\\epsilon,-2\\epsilon,-\\epsilon,0,\\epsilon,2\\epsilon,3\\epsilon,\\ldots,1\\}   \n\\textrm{ for all $i,j$} \\big\\}.$$ \n\n\n\\subsection{Lower bound}\n** We analyze the pairwise distance error **\n\nWe want to establish that our analysis for the upper bound on the sample\ncomplexity convergence rate is optimal (upto logarithmic factors) in the\nfollowing sense: \nit would necessarily take the number of \\emph{pairwise} samples $m$ to be at least $\\frac{D^2}{\\epsilon^2}$ in order to find a weighting metric $ M^*_m$, which with confidence\nat least $1-\\delta$ has ($\\lambda$-smoothed distance based) excess error of at most $\\epsilon$ over a class of bounded support distributions. \nRecall that the $\\lambda$-smoothed version of error (based on pairwise consistency) for a weighting metric $M$ induced by an underlying distribution $\\D$ (on $X \\times \\{0,1\\}$) is defined as\n\\begin{eqnarray*}\n\\err_{\\dist}(M,\\D) : \\E_{(x_1,y_1), (x_2,y_2) \\sim \\D} \\big[f_\\lambda((x_1,y_1),(x_2,y_2); M)\\big], \\textrm{   where} \\\\\nf_\\lambda((x_1,y_1),(x_2,y_2); M) : \\Bigg\\{ \\begin{array}{ll} \\min\\{1,\\lambda [ \\|Mx_1 - Mx_2\\| - U]_+\\}   & \\textrm{if $y_1   y_2$} \\\\ \\vspace{0.01in}\n \\min\\{1,\\lambda [L - \\|Mx_1 - Mx_2\\|]_+\\} & \\textrm{o.w.} \\end{array} .\n\\end{eqnarray*}\nfor some $U < L$. A typical choice of $\\lambda$ is $\\frac{1}{(L-U)}$.\nThen we have the following.\n\n\\noindent \\textbf{Convergence rate of empirical risk minimizing metric.} Now we can compare the generalization ability of the empirical risk minimizing metric $M^*_m$ with that of optimal metric $M^*$ (i.e., Eq. \\eqref{eq:erm_conv}).\nUsing this lemma we can get the desired convergence rate (Eq.\\ \\ref{eq:erm_conv}). Fix $M^*\\in \\mathcal{M}$, then for any $0<\\delta<1$ and $m\\geq1$, \nfix any $M' \\in \\mathcal{M}$ such\nthat $\\err_{\\dist}^\\lambda( M', \\D) \\leq \\err_{\\dist}^\\lambda(M^*,\\D) +\n\\sqrt{\\frac{\\ln(2/\\delta)}{2m}}$. Then, \nwith probability at least $1-\\delta$,\nwe have\n\\begin{align*}\n \\err_{\\dist}^\\lambda(M^*_m, \\D)   - \\err_{\\dist}^\\lambda &(M^*,\\D)   \n \\\\\n &\\;\\;\n   \\err_{\\dist}^\\lambda(M^*_m, \\D) - \\err_{\\dist}^\\lambda(M^*_m,S_m)   \n +   \\err_{\\dist}^\\lambda(M^*_m, S_m) - \\err_{\\dist}^\\lambda(M^*,S_m) \\\\\n&\\;\\; +   \\err_{\\dist}^\\lambda(M^*, S_m) - \\err_{\\dist}^\\lambda(M^*,\\D)   \\\\\n&\\;\\; +   \\err_{\\dist}^\\lambda(M', \\D) - \\err_{\\dist}^\\lambda(M^*,\\D)   \\\\\n\n\n   \\err_{\\dist}^\\lambda(M^*_m, \\D) - \\err_{\\dist}^\\lambda(M^*_m,S_m)   \\\\\n&\\;\\; +   \\err_{\\dist}^\\lambda(M^*_m, S_m) - \\err_{\\dist}^\\lambda(M^*,S_m) \\\\\n&\\;\\; +   \\err_{\\dist}^\\lambda(M^*, S_m) - \\err_{\\dist}^\\lambda(M^*,\\D)   \\\\\n\\leq&\\;\\; 2   \\max_{M\\in \\mathcal{M}} | \\err_{\\dist}^\\lambda(M,S_m) -\n\\err_{\\dist}^\\lambda(M,\\D)| \\\\\n\\leq&\\;\\; O\\Bigg( \\lambda B^2 \\sqrt{\\frac{D \\ln(1/\\delta)}{m}}\\Bigg) + \\sqrt{\\frac{\\ln(2/\\delta)}{2m}}   \\\\\n&\\;\\; O\\Bigg( \\lambda B^2 \\sqrt{\\frac{D \\ln(1/\\delta)}{m}}\\Bigg),\n\\end{align*}\nby noting (i) $\\err_{\\dist}^\\lambda(M^*_m, S_m)   \\leq \\err_{\\dist}^\\lambda(M^*,S_m)$, since $M^*_m$ is empirical error minimizing on $S_m$, \nand (ii) by using Hoeffding's inequality on the fixed $M^*$ to conclude\nthat with probability at least $1-\\delta/2$, \n$\\err_{\\dist}^\\lambda(M^*, S_m) - \\err_{\\dist}^\\lambda(M^*,\\D) \\leq \\sqrt{\\frac{\\ln(2/\\delta)}{2m}}$.\n. , and (ii) by applying Lemma \\ref{lm:unif_conv_all}.\n\nThus to achieve a specific estimation error rate $\\epsilon$, the number of\nsamples $m   \\Omega\\Big(\\big(\\frac{\\lambda B^2}{\\epsilon}\\big)^2 D \\ln(\\frac{1}{\\delta})\\Big)$ are sufficient to\nconclude, with confidence at least $1-\\delta$, the empirical risk minimizing metric\n$M^*_m$ will have estimation error of at most $\\epsilon$.\nThis shows that one never needs more than a number proportional to the representation dimension $D$ examples to achieve the desired level of accuracy. \n\nSince\ntypical applications have a large representation dimension, it is instructive to study if such a strong dependency on $D$ necessary. \nIt turns out that even for simple distance-based loss functions like $\\phi^\\lambda_{L,U}$ (cf.\\ Eq.\\ \\ref{eq:pair_LUloss}), \nthere are data distributions for which one cannot get away with fewer than linear in $D$ samples and ensure good estimation errors.\nIn particular we have the following.\nThe following lemma confirms \nthat there are data distributions for which one cannot get away with fewer than linear in $D$ samples and ensure good estimation errors.\n\nsee if such scaling is n\n scales with $D$ measurements.   a it \nThe following lemma that a linear dependence on $D$ is necessary in general. \n\n\\TODO{Show Lemma 1 implies Eq.\\ (1)}.\n\n\\begin{lemma}\n\\label{lm:lb_dist}\nLet $\\mathcal{A}$ be any algorithm that, given an i.i.d.\\ sample $S_m$ (of size $m$) from a\nfixed unknown bounded support distribution $\\D$, returns a weighting metric \nfrom $\\mathcal{M}$ that minimizes the empirical error with respect to distance-based loss function $\\phi^\\lambda_{L,U}$. \nThere exist $\\lambda\\geq 0$, $0\\leq U<L$, such that for\nall $0<\\epsilon,\\delta<1/64$, there   \nexists a bounded\nsupport distribution $\\D$, such that if $m \\leq \\frac{D+1}{512\\epsilon^2}$ then\ni.i.d.\\ samples are\ndrawn from $\\D$\nIf $m < D^2/\\epsilon^2$ then \n$$\n\\Pr_{S_m} \\Big[ \\err_{\\dist}^\\lambda(\\mathcal{A}(S_m),\\D) - \\err_{\\dist}^\\lambda(M^*,\\D) > \\epsilon \\Big] > \\delta.\n$$\nwhere $M^* : \\argmin_M \\err_{\\dist}(M,\\D)$.\n\\end{lemma}\n\nWhile this may seem discouraging for large-scale applications of metric learning, note that here we made no assumptions about the underlying structure of\nthe data distribution $\\mathcal{D}$, making this a worst-case analysis. As the individual features in real-world datasets contain\nvarying amounts of information for good classification performance, one hopes for a more relaxed dependence on $D$ for metric learning in these settings. This is explored in Section \n\\ref{sec:discrim_complex}.\n\n\n\n\\subsection{Classifier-Based Error Analysis}\n\n\\subsection{Upper bound}\nAs before, we shall use a real valued relaxation of our base hypothesis class.\nFix a base real valued hypothesis class $\\mathcal{H}$ on the underlying space\n$X\\R^D$ as $\\mathcal{H} \\subset [0,1]^X$. We shall additionally assume\nLipschitz smoothness on each $h \\in \\mathcal{H}$, that is $\\forall\nh\\in\\mathcal{H}$, $x,x' \\in X$, $|h(x)-h(x')| \\leq \\lambda \\|x-x'\\|$ for some\nfixed $\\lambda$. Note that the underlying labellings are still binary. We can get\nthe binary labellings from our real valued hypotheses by a simple thresholding at $1/2$.\nThus, we can redefine classification based error for a given weighting metric $M$ as:\n\\begin{eqnarray*}\n\\err_{\\hypoth}(M,\\D) : \\inf_{h \\in\\mathcal{H}}\\E_{(x,y)\\sim \\D} \\big[ |h(Mx)-y| \\geq 1/2 \\big],\n\\end{eqnarray*}\n\nIn this setting, we can use an i.i.d.\\ sequence of observations\n$z_1,z_2,\\ldots$ from $\\D$ to obtain the sample $S_m   \\{z_i\\}_{i1}^m$ of size\n$m$ directly. To analyze the generalization ability of the weighting\nmetrics optimized with respect to an underlying hypothesis class $\\mathcal{H}$, we need to \neffectively analyze the classification complexity of $\\mathcal{H}$. The\nscale sensitive version of VC-dimension, also known as the ``fat-shattering dimension\", of a real-valued hypothesis class (denoted by\n$\\fat_\\gamma(\\mathcal{H})$) encodes the right notion of classification\ncomplexity and provides an intuitive way to relate the generalization error to the\nempirical error at a \\emph{margin} $\\gamma$ (see for instance the work of\n\\citet{lt:anthony_bartlett} for an excellent discussion).\n\nIn the context of metric learning with respect to a fixed hypothesis class, define the empirical error at a margin $\\gamma$ as\nLet \nThe corresponding sample based error (for a given an i.i.d.\\ sample $S_m   \\{(x_i,y_i)\\}_{i1}^m$ of size $m$), at a \ngiven \\emph{margin} $\\gamma$ is defined as\n\\begin{align*}\n\\err_{\\hypoth}^\\gamma&(M,S_m) : \n \\inf_{h \\in\\mathcal{H}}\\frac{1}{m} \\sum_{(x_i,y_i)\\in S_m} \\indicate[\\mathsf{Margin}(h(Mx_i),y_i)<\\gamma] ,\n\\end{align*}\nwhere $\\mathsf{Margin}(\\hat{y},y) : \\Big\\{ \\begin{array}{ll} \\hat{y}-1/2 & \\textrm{if $y1$} \\\\ 1/2-\\hat y & \\textrm{otherwise} \\end{array}.$\n\\\\\n\nAs before, let $\\mathcal{M}$ be the class of all $D\\times D$ weighting matrices (with each entry in $[-1,1]$, and unit spectral norm) and define an $\\epsilon$-covering of $\\mathcal{M}$\n as $$\\mathcal{M}_\\epsilon : \\big\\{ M: M_{ij} \\in\n\\{-1,\\ldots,-3\\epsilon,-2\\epsilon,-\\epsilon,0,\\epsilon,2\\epsilon,3\\epsilon,\\ldots,1\\}   \n\\textrm{ for all $i,j$} \\big\\}.$$ \n\nThen for any bounded support distribution $\\D$ (that is, each $(x,y)\\sim \\D$,\n$\\|x\\|\\leq B <\\infty$), we have the following convergence result that relates\nthe estimation error rate of the weighting metrics with that of the\nfat-shattering dimension of the underlying base hypothesis class.\n\n\\begin{lemma}\n\\label{lm:hypoth_ub}\nLet $\\mathcal{H}$ be a $\\lambda$-Lipschitz base hypothesis class.\nPick any $0<\\gamma<1/2$, and let $m\\geq \\fat_{\\gamma/16}(\\mathcal{H})\\geq 1$.\nThen with probability at least $1-\\delta$ over an i.i.d.\\ draw of sample $S_m$\n(of size $m$) from a bounded unknown distribution $\\D$ (with bound $B$) on\n$X\\times \\{0,1\\}$, \nwe have that for every $M \\in \\mathcal{M}$\n\\begin{align*}\n\\sup_{M\\in \\mathcal{M}} \\Big[   \\err_{\\hypoth}(M,\\D) - \\err^\\gamma_{\\hypoth}(M,S_m) \\Big]   \n\\leq O\\Bigg( \n\\sqrt{\\frac{1}{m}\\ln\\frac{1}{\\delta} + \\frac{D^2}{m}\\ln\\frac{D}{\\epsilon_0} + \\frac{\\fat_{\\gamma/16}(\\mathcal{H})}{m}\\ln\\Big(\\frac{m}{\\gamma}\\Big) }\n \\Bigg).\n\\end{align*}\nwhere $\\epsilon_0 : \\min\\{\\frac{\\gamma}{2},\\frac{1}{2\\lambda B} \\}$, and $\\fat_{\\gamma/16}(\\mathcal{H})$ is the \\emph{fat-shattering dimension} of the\nbase hypothesis class $\\mathcal{H}$ at margin $\\gamma/16$.\n\\end{lemma}\n\nUsing a similar line of argument as before, we can bound the key quantity of interest (Eq.\\ \\ref{eq:erm_conv}) and conclude\n\n. For any\nfor any $0<\\gamma<1/2$ and any $m\\geq 1$, with probability $\\geq 1-\\delta$\n, fix any $\\xbar M$ such\nthat $\\err_{\\hypoth}^\\gamma(\\xbar M, \\D) \\leq \\err_{\\hypoth}^\\gamma(M^*,\\D) +\n\\sqrt{\\frac{\\log(2/\\delta)}{2m}}$. Then, with probability at least $1-\\delta$,\nwe have\n\\begin{align*}\n \\err_{\\hypoth}&(M^*_m, \\D) - \\err_{\\hypoth}^\\gamma(M^*,\\D)   \n O\\Bigg( \n\\sqrt{\\frac{D^2\\ln(D/\\epsilon_0)}{m} + \\frac{\\fat_{\\gamma/16}(\\mathcal{H}) \\ln(m/ \\delta \\gamma)}{m} }\n \\Bigg).\n\\end{align*}\nby noting (i) $\\err_{\\hypoth}^\\gamma(M^*_m, S_m)   \\leq\n\\err_{\\hypoth}^\\gamma(\\xbar M, S_m)$ since $M^*_m$ is empirical error\nminimizing on $S_m$, and (ii) by using Hoeffding's inequality on fixed $\\xbar M$ to conclude\nthat with probability at least $1-\\delta/2$, \n$\\err_{\\hypoth}^\\gamma(\\xbar M, S_m) - \\err_{\\hypoth}^\\gamma(\\xbar M,\\D) \\leq \\sqrt{\\frac{\\log(2/\\delta)}{2m}}$.\n , and (ii) by applying Lemma \\ref{lm:unif_conv_all}.\nHere $\\epsilon_0   \\min\\{\\frac{\\gamma}{2},\\frac{1}{2\\lambda B} \\}$ for a $\\lambda$-Lipschitz hypothesis class $\\mathcal{H}$.\nThus to achieve a specific estimation error rate $\\epsilon$, the number of\nsamples $m   \\Omega\\Big(\\frac{D^2\\ln(\\lambda DB/\\gamma)+\\fat_{\\gamma/16}(\\mathcal{H})\\ln(1/\\delta\\gamma)}{\\epsilon^2}\\Big) $ suffices to\nsay, with confidence at least $1-\\delta$, the empirical risk minimizing metric\n$M^*_m$ will have estimation error at most $\\epsilon$.\n\n\nIt is interesting to note that the task of finding an optimal metric\nonly additively increases the sample complexity over the complexity of finding\nthe optimal hypothesis from the underlying hypothesis class. \n\n\\subsection{Lower bound}\n\nAgain, we want to establish that the upper bound on the convergence rate of classifier based metric learning is optimal (upto logarithmic factors) in the following sense:   \nthere is a class of smooth ($\\lambda$-Lipschitz) hypothesis class\n$\\mathcal{H}$, such that it would necessarily take about $\\geq\n\\frac{D^2}{\\epsilon^2}$ samples in order to find a weighting metric which with\nconfidence at least $1-\\delta $ has excess error of at most $\\epsilon$ on an unknown arbitrary bounded support distribution. In\nparticular we have the following.\n\n\nIn contrast to the sample complexity of distance-based framework (c.f.\\ Lemma \\ref{lm:unif_conv_all}), here we get a quadratic dependence \non the representation dimension. \nThe following lemma shows that a strong dependence on the representation dimension is necessary in absence of any specific assumptions on \nthe underlying data distribution and the base hypothesis class.\n\n\\begin{lemma}\n\\label{lm:hypoth_lb}\nPick any $0<\\gamma<1/8$.\nLet $\\mathcal{H}$ be a base hypothesis class of $\\lambda$-Lipschitz\nfunctions mapping from $X\\R^D$ into the\ninterval $[1/2-4\\gamma,1/2+4\\gamma]$ that is closed under addition of\nconstants. That is\n$$h \\in \\mathcal{H} \\implies h' \\in \\mathcal{H}, \\textrm{ where }   h':x\\rightarrow h(x)+c \\;\\;\\; \\textrm{ for all $c$.} $$\nThen for any \nclassification algorithm $\\mathcal{A}$, and for any $B\\geq 1$, there exists $\\lambda\\geq 0$, for all $0<\\epsilon,\\delta<1/64$, there exists a bounded support distribution $\\D$ (with bound $B$) such that if $m \\ln^2 m < O\\big(\\frac{D^2 + d}{\\epsilon^2 \\ln(1/\\gamma^2) }\\big)$ \n$$\n\\Pr_{S_m\\sim\\D}[\\err_{\\hypoth}(h^*,\\D) > \\err^\\gamma_{\\hypoth}(\\mathcal{A}(S_m),\\D) + \\epsilon] > \\delta,\n$$\nwhere $d : \\fat_{768\\gamma}(\\pi_{4\\gamma}(\\mathcal{H})) $ is the \\emph{fat-shattering dimension} of \n$\\pi_{4\\gamma}(\\mathcal{H})$---the $(4\\gamma)$-\\emph{squashed} function class of $\\mathcal{H}$, see Definition \\ref{def:squash_fxn} below---at margin $768\\gamma$.\nwhere $d : \\fat_{768\\gamma}(\\mathcal{H}) $ is the \\emph{fat-shattering dimension} of $\\mathcal{H}$ at margin $768\\gamma$.\n\\end{lemma}\n\n\\section{Data with Uninformative and Weakly Informative Features}\n\\label{sec:discrim_complex}\nDifferent measurements have varying degrees of ``information content\" for the\nparticular supervised classification task of interest.   Any algorithm or \nanalysis that studies the design of effective comparisons between observations\nmust account for this variability. \n\n\nTo get a solid footing for our study, we introduce the concept of\n\\emph{metric learning complexity} of a given dataset.\nOur key observation is that a metric that yields good generalization performance\nshould emphasize relevant features while suppressing the contribution of\nspurious features. Thus, a good metric reflects the quality of individual feature measurements of data and their relative value for the learning task.\nWe can leverage this and define the metric learning complexity of\na given dataset as the \\emph{intrinsic complexity} $d$ of the weighting metric that\nyields the best generalization performance for that dataset (if multiple\nmetrics yield best performance, we select the one with minimum $d$).\nA natural way to characterize the intrinsic complexity of a weighting metric $M$\nis via the norm of the matrix representation of $M$. \nUsing metric learning complexity as our gauge for the richness of the feature set in a \ngiven dataset, we can refine our analysis in both our canonical metric learning frameworks.\n\nWe shall use this in our analysis of both the frameworks.\n\n\\subsection{Distance-Based Refinement}\nWe start with the following refinement of the distance-based metric learning\nsample complexity for a class of Frobenius norm-bounded weighting metrics.\n\n\n\\emph{intrinsic discriminative complexity} of a given dataset that\nprovides an intuitive way to capture the classification complexity in terms of the features used to represent the data. \n\\begin{definition} Given a data distribution $\\D$ with $D$ measurements, let $\\xi$ be the best possible accuracy achievable by using all $D$ measurements. We \ncall the intrinsic discriminative complexity of $\\D$ as $(d,\\epsilon)$, if $d$ is the smallest number such that any subset of $d$ measurements can achieve an accuracy of at least $(1-\\epsilon)\\xi$.\n\\end{definition}\nThe `best possible accuracy' can be quantified more theoretically as the one achievable by the Bayes classifier, or more practically as the one achieved by some \nclassifier of interest (such as $k$NNs or SVMs). Notice that a small value of $d$ indicates that a few features contain the most discriminative power. One \nhopes that the theoretical metric learning rates and some of the popular practical instantiations do adapt to the intrinsic complexity of data; this is explored \nin Sections \\ref{sec:theory_discrim_complex} and \\ref{sec:experiments} respectively.\n\n\\begin{lemma}\n\\label{lm:unif_conv_dist_refi}\nLet $\\mathcal{M}$ be any class of weighting metrics on the feature space $X   \\R^D$.\nFix any sample size $m$, and let $S_m$ be an i.i.d.\\ \\emph{paired} sample of\nsize $m$ from an unknown bounded distribution $\\D$ on $X\\times \\{0,1\\}$ (with bound $B$). For any distance-based loss function $\\phi^\\lambda$ that is $\\lambda$-Lipschitz in the first argument, with probability at least $1-\\delta$ over the draw of $S_m$,\n\\begin{align*}\n \\sup_{M\\in \\mathcal{M}}   \\big[ \\err_{\\dist}^\\lambda   (M,\\D) - &\n\\err_{\\dist}^\\lambda (M,S_m) \\big]\n\\\\\n&\n\\leq O\\Bigg( \\lambda B^2   \\sqrt{\\frac{d\\ln(1/\\delta)}{m}}\\Bigg),\n\\end{align*}\nwhere $d$ is a uniform upperbound on the Frobenius norm of the quadratic form of weighting metrics in $\\mathcal{M}$, that is, $\\sup_{M\\in\\mathcal{M}} \\|M^\\mathsf{T} M\\|^2_{_F }\\leq d$.\n\\end{lemma}\nObserve that if our dataset has a low metric learning complexity (say, $d \\ll D$), then\nconsidering an appropriate class of norm-bounded weighting metrics can help sharpen the sample complexity\nresult, yielding a \\emph{dataset-dependent} bound. We discuss how to automatically adapt\nto the right complexity class in Section \\ref{sec:auto_refine} below.\n\n\n\\subsection{Classifier-Based Refinement}\n\nEffective data-dependent analysis of classifier-based metric learning requires accounting for \npotentially complex interactions between an arbitrary base hypothesis class and the\ndistortion induced by a weighting metric to the unknown underlying data\ndistribution. , makes the analysis for a data-dependent metric learning bound prohibitive.\n\nIt is unclear how to derive a data-dependent metric learning bound for an\narbitrary base hypothesis class $\\mathcal{H}$ since it is difficult to\ncharacterize interactions between an arbitrary hypothesis function and the\ndistortion induced by a weighting metric to the unknown underlying data\ndistribution. \nTo make the analysis tractable while still keeping\nour base hypothesis class $\\mathcal{H}$ general, we shall assume that\n$\\mathcal{H}$ is a class of two layer feed-forward neural networks. Recall that for \nany smooth target function $f^*$, a two layer feed-forward neural network (with appropriate \nnumber of hidden units and connection weights) can approximate $f^*$ arbitrarily well \\citep{nnet:univ_approx_bddfxn}, so\nthis class is flexible enough to incorporate most reasonable target hypotheses.\n\nMore formally, define the base hypothesis class of two layer feed-forward neural network with $K$ hidden units as\n\\begin{equation*}\n\\Hnet : \\Big\\{ x \\mapsto \\sum_{i1}^K w_i \\; \\sigma^\\gamma(v_i \\; \\cdot \\; x) \\; \\Big| \\; \\|w\\|_1 \\leq 1, \\|v_i\\|_1 \\leq 1   \\Big\\},\n\\end{equation*}\nwhere $\\sigma^\\gamma: \\R \\rightarrow [-1,1]$ is a smooth, strictly monotonic, $\\gamma$-Lipschitz activation function with $\\sigma^\\gamma(0)0$. \nThen for the generalization error of a weighting metric $M$ defined with respect to any classifier-based $\\lambda$-Lipschitz loss function $\\phi^\\lambda$\n\\begin{equation*}\n\\err^\\lambda_{\\hypoth}(M,D): \\inf_{h\\in\\mathcal{\\Hnet}} \\E_{(x,y)\\sim \\D} \\big[ \\phi^\\lambda \\big(h(Mx),y\\big) \\big],\n\\end{equation*}\nwe have the following.\\footnote{Since we know the functional form of the base hypothesis class $\\mathcal{H}$ (\\ie~a two layer feed-forward neural net), we can provide a more precise bound than leaving it as $\\fat(\\mathcal{H})$.}\n\n\\begin{lemma}\n\\label{lm:unif_conv_clf_refi}\nLet $\\mathcal{M}$ be any class of weighting metrics on the feature space $X   \\R^D$. For any $\\gamma>0$, \nlet $\\Hnet$ be a two layer feed-forward neural network base hypothesis class (as defined above) and $\\phi^\\lambda$ be a classifier-based loss function that $\\lambda$-Lipschitz in its first argument. Fix any sample size $m$, and let $S_m$ be an i.i.d.\\ sample of\nsize $m$ from an unknown bounded distribution $\\D$ on $X\\times \\{0,1\\}$ (with bound $B$). Then with probability at least $1-\\delta$,\n\\begin{align*}\n\\lefteqn{\n \\sup_{M\\in \\mathcal{M}}   \\big[ \\err_{\\hypoth}^\\lambda (M,\\D) - \n&\\err_{\\hypoth}^\\lambda(M,S_m)   \\big]   \n\\\\\n&\n\\leq O\\Bigg(B\\lambda \\gamma \\sqrt{\\frac{d\\ln(D/\\delta)}{m} } \\Bigg),\n\\end{align*}\nwhere $d$ is a uniform upperbound on the Frobenius norm of the quadratic form of weighting metrics in $\\mathcal{M}$, that is, $\\sup_{M\\in\\mathcal{M}} \\|M^\\mathsf{T} M\\|^2_{_F }\\leq d$.\n\\end{lemma}\n\n\n\\subsection{Automatically Adapting to Intrinsic Complexity}\n\\label{sec:auto_refine}\n\nNote that while Lemmas \\ref{lm:unif_conv_dist_refi} and \\ref{lm:unif_conv_clf_refi} provide a\nsample complexity bound that is tuned to the metric learning complexity of a given dataset, these results are \\emph{not} useful directly since one cannot select the correct\nnorm bounded class $\\mathcal{M}$ a priori (as the underlying\ndistribution $\\D$ is unknown). \n \nFortunately, by considering an appropriate sequence of norm-bounded \nclasses of weighting metrics, we can provide a uniform bound that \\emph{automatically adapts} to the intrinsic complexity of the unknown underlying data distribution $\\D$.\nIn particular, we have the following.\n\\begin{corollary} \n\\label{cor:unif_conv_refined}\nFix any $m$, and let $S_m$ be an i.i.d.\\ sample of size $m$ from an unknown bounded distribution $\\D$ (with bound $B$). \nDefine $\\mathcal{M}^d : \\{M \\;|\\; \\|M^\\mathsf{T}M\\|^2_{_F} \\leq d \\}$, and consider\nthe nested sequence of weighting metric class $\\mathcal{M}^1 \\subset\n\\mathcal{M}^2 \\subset \\cdots$. Let $\\mu_d$ be any non-negative measure across\nthe sequence $\\mathcal{M}^d$ such that $\\sum_d \\mu_d   1$ (for $d1,2,\\cdots$).\nThen for any $\\lambda\\geq 0$, with probability at least $1-\\delta$, for all $d1,2,\\cdots$, and all $M^d \\in \\mathcal{M}^d$,\n\\begin{align}\n\\nonumber\n \\big[ \\err^\\lambda   (M^d,\\D) - \n \\err^\\lambda (M^d,S_m)   \\big]   \n\\\\\n&\n\\leq O\\Bigg(C \\cdot B\\lambda \\sqrt{\\frac{d\\ln(1/\\delta \\mu_d)}{m}} \\Bigg),\n\\label{eq:main_unif_refind_bound}\n\\end{align}\nwhere $C:B$ for distance-based error, or $C:\\gamma\\sqrt{\\ln{D}}$ for classifier-based error (with base hypothesis class $\\Hnet$).\n\n\n\n\\begin{figure*}[ht]\n\\vskip 0.2in\n\\begin{center}\n\\includegraphics[width2.22in]{synth_MMC_errVSsample4}\n\\includegraphics[width2.22in]{synth_MMC_errVSDIM2}\n\\includegraphics[width2.22in]{synth_MMC_errVSiDIM2}\n\\\\\n\\includegraphics[width2.22in]{synth_MMC_errVSsample2}\n\\includegraphics[width2.22in]{synth_MMC_errVSDIM2}\n\\includegraphics[width2.22in]{synth_MMC_errVSiDIM2}\n\\caption{$k$-means (with $k$ set to the number of classes) clustering performance when key parameters are varied in the synthetic \\alg{Simplex} dataset with MMC metric learning algorithm. \n\\textbf{Left:} Scaling behavior as a function of training sample size for different ambient dimensions and fixed intrinsic dimension $d1$; dashed red lines show average error rates \nfor MMC, and solid blue lines show the error rates for regularized MMC.\n\\textbf{Center:} Scaling behavior as a function of ambient dimension for different sample sizes (and fixed intrinsic dimension $d1$); dashed lines are represent scaling behavior at small sample sizes with no regularization (red) and with regularization (blue). \n\\textbf{Right:} Scaling behavior as a function of intrinsic dimension for different sample sizes (for fixed ambient dimension $D100$).\n}\n\\label{fig:synth_MMC}\n\\end{center}\n\\vskip -0.2in\n\\end{figure*}\n\n\n\n\n\n\n\nIn particular, for a data distribution $\\D$ that has metric learning complexity\nat most $d \\in \\mathbb{N}$, if there are $m \\geq\n\\Omega\\Big(\\frac{d(CB\\lambda)^2   \\ln(1/\\delta\\mu_d)}{\\epsilon^2} \\Big)$ samples, then with probability at least\n$1-\\delta$\n\\begin{align*}\n\\nonumber \\big[ \\err^\\lambda   (M_m^{\\reg},\\D) - \\err^\\lambda (M^*,\\D)   \\big] \\; \\leq \\; O( \\epsilon), \n& \\leq O\\Bigg(C \\cdot dB\\lambda \\sqrt{\\frac{\\ln(1/\\delta \\mu_d)}{m}} \\Bigg).\n\\end{align*}\nfor ${ M_m^{\\reg}\\!:\\! \\argmin_{M\\in \\mathcal{M}}   \\Big[ \\err^\\lambda(M,S_m) + \\Lambda_{_M}   \\|M^\\mathsf{T}M\\|_{_F} \\Big] }$, where $\\Lambda_{_M} : C B \\lambda \\sqrt{{\\ln(\\frac{1}{\\delta\\mu_{d'}})}/{m}},\\;$ for ${d'\\!\\!\\big\\lceil\\|M^\\mathsf{T}M\\|_{_F}\\big\\rceil}$. \nfor ${ M_m^{\\reg}\\!:\\! \\argmin_{M\\in \\mathcal{M}}   \\Big[ \\err^\\lambda(M,S_m) + \\Lambda_{_M} d_{_M} \\Big]}$, \nwhere \n$\\Lambda_{_M} : C B \\lambda \\sqrt{{\\ln\\Big(\\frac{1}{\\delta\\mu_{_{d_{M}}}}\\Big)}/{m}}\\;$\nand \n$d_{_M}: \\big\\lceil\\; \\|M^\\mathsf{T}M\\|^2_{_F} \\big\\rceil $ \n. \n\\end{corollary}\n\nObserve that the measure $(\\mu_d)$ above encodes our prior belief \non the complexity class $\\mathcal{M}^d$ from which a target metric is selected by a metric learning algorithm given the training sample $S_m$. In absence of any prior beliefs, $\\mu_d$ can be simply set to $1/D$ (for $d1,\\ldots,D$) for unit spectral-norm weighting metrics.\n\nThus, for an unknown underlying data distribution $\\D$ with metric learning\ncomplexity $d$, with number of samples just proportional to $d$, we can find a good weighting metric. \n\nThis result also highlights that the generalization error of \\emph{any} weighting metric returned by an\nalgorithm is proportional to the (smallest) norm-bounded class to which it\nbelongs (cf.\\ Eq.\\ \\ref{eq:main_unif_refind_bound}). If two metrics $M_1$ and\n$M_2$ have similar empirical errors on a given sample, but have different \nintrinsic complexities, then the expected risk of the two metrics \ncan be considerably different. We expect the metric with lower intrinsic complexity to yield better\ngeneralization error.\nThis partly explains the observed empirical success of various types of norm-regularized optimization criteria for\nfinding the optimal weighting metric \\citep{met_learn:alg_rob_stru_mcfee, met_learn:alg_fantope_regularization}. \n\n\nUsing this as a guiding principle, we can design an improved optimization\ncriteria for metric learning problems that jointly minimizes the sample error and\na Frobenius norm regularization penalty. In particular, \n\\begin{align}\n\\min_{M\\in\\mathcal{M}} \\hspace{0.2in} \\err(M,S_m) \\hspace{0.1in} + \\hspace{0.1in} \\Lambda   \\; \\|M^\\mathsf{T}M\\|^2_{_F}\n\\label{eq:opt_regularize}\n\\end{align}\nfor any error criteria `$\\err$' used in a downstream prediction task of interest and a regularization hyper-parameter $\\Lambda$ proportional to $m^{-1/2}$. \nWe explore the practical efficacy of this augmented optimization on some representative applications below.\n\n a metric learning   \n\\TODO{Discuss the motivation for a bias-variance   algorithm. ERM algorithm would overfit on limited on samples. So we should incorporate expected variance}\n\n\nTaking cue from \n\nThus if a algorithm\n\nIt is also beneficial to validate our intuition that datasets from different application domains do in fact have a low discriminative complexity. Section \\ref{sec:expt_discrim_complex} below explores the intrinsic discriminative complexity of some representative datasets.\n\n\\subsection{Experiments with Dimension}\n\\label{sec:expt_discrim_complex}\n\nOCR MNIST dataset, 20newsgroups text dataset, speech MFCC dataset\n\n\\subsection{Adapted Learning Rates}\n\\label{sec:theory_discrim_complex}\n\n\n\n\n\n\\begin{figure*}[t]\n\\vskip 0.2in\n\\begin{center}\n\\includegraphics[width1.81in]{iris_errVSDIM2}\n\\includegraphics[width1.81in]{wine_errVSDIM2}\n\\includegraphics[width1.81in]{ion_errVSDIM2}\n\\caption{\\small Nearest-neighbor classification performance of LMNN and ITML metric learning algorithms without regularization (dashed red lines) and with regularization (solid blue lines) on benchmark UCI datasets.\nThe horizontal dotted line is the classification error of random label assignment drawn according to the class proportions, and solid gray line shows classification error of $k$-NN performance with respect to identity metric (no metric learning) for baseline reference.\n}\n\\label{fig:UCI_NN}\n\\end{center}\n\\vskip -0.2in\n\\end{figure*}\n\n\n\n\\section{Empirical Evaluation}\n\\label{sec:experiments}\n\n\nOur analysis shows that the generalization error of metric learning \ncan scale with the representation dimension, and regularization can help mitigate this \nby adapting to the intrinsic \\emph{metric learning complexity} of the given\ndataset. We want to explore to what degree these effects manifest in practice. \n\nWe select two popular metric learning algorithms, LMNN by \\citet{met_learn:alg_LMNN} and ITML by \\citet{met_learn:alg_ITML}, that are designed to find\nmetrics that improve nearest-neighbor classification quality. \nThese algorithms have varying degrees of regularization built into their optimization criteria:\nLMNN implicitly regularizes the metric\nvia its ``large margin\" criterion, while ITML allows for explicit regularization by letting the practitioners specify a ``prior\" weighting metric.   \nWe modified the LMNN optimization criteria as per Eq.\\ \\eqref{eq:opt_regularize} to also allow for an explicit norm-regularization controlled by the trade-off parameter $\\Lambda$.\n\nWe can evaluate how the unregularized criteria (\\ie~unmodified LMNN, or ITML with the prior set to the identity matrix) compares to the regularized criteria (\\ie~modified LMNN with best $\\Lambda$, or ITML with the prior set to a low-rank matrix).\n\\\\\n\nWe select two popular learning tasks, clustering and classification, which metric learning\nhas been shown to benefit. The MMC algorithm is designed to learn metrics that\nimprove $k$-means clustering quality \\citep{met_learn:alg_xing}, and the LMNN and\nITML algorithms are designed to find metrics that improve nearest-neighbor classification quality\n\\citep{met_learn:alg_LMNN,met_learn:alg_ITML}. Our goal is to evaluate how these algorithms fare\nwhen key parameters (such as representation dimension and intrinsic complexity)\nof a given dataset are changed. \nThese algorithms have varying degrees of regularization built into their optimization criteria. \nITML allows for explicit regularization by letting the practitioners specify a ``prior\" weighting metric. LMNN implicitly regularizes the metric\nvia its ``large margin\" criteria. MMC, on the other hand, doesn't include any regularization. \nWe can thus study how much does norm-regularization help these algorithms. \n\n\\noindent \\textbf{Datasets.} \nFor the $k$-means clustering task, we use a carefully\ndesigned synthetic dataset that serves a two-fold goal: understanding how changing\nthe ambient and intrinsic dimension affect performance, and studying the\ninterplay between sample size and regularization. We create the synthetic\n\\alg{Simplex} dataset as follows. For any ambient dimension $D$, we sample\nuniformly from the vertices of a regular unit side-length $D$-simplex that is\ncentered at the origin. We add mild independent Gaussian noise to each\nsample, giving us our $D$ dimensional measurements. To set the intrinsic\ndimension $d$ we assign one of the $d+1$ labels to each of the vertex clusters. \nFor the nearest-neighbor classification task, we create a grid dataset as follows.\nSee Appendix \\ref{appendix:synth_simplex_data_gen} for more details.\nWe use the UCI benchmark datasets for our experiments:\n\\alg{Iris} (4 dim., 150 samples), \\alg{Wine} (13 dim., 178 samples) and \\alg{Ionosphere} (34\ndim., 351 samples) datasets \\citep{dataset:UCI_ML_repo}. Each dataset has a\nfixed (unknown) intrinsic dimension; we can vary the representation dimension by \naugmenting each dataset with synthetic correlated noise of varying dimensions,\nsimulating regimes where datasets contain\nlarge numbers of uninformative features. \n\nEach UCI dataset is augmented with synthetic $D$-dimensional correlated noise as follows. We first sample a covariance matrix $\\Sigma_D$ from\nunit-scale Wishart distribution (that is, let $A$ be a $D\\times D$ Gaussian random matrix with\nentry $A_{ij} \\sim N(0,1)$ drawn i.i.d., and set $\\Sigma_D : A^\\mathsf{T}A$). Then each sample $x_i$ from the dataset is appended independently by drawing\nnoise vector $x_\\sigma \\sim N(0,\\Sigma_D)$.\n\\\\\n\n\n\nby appending the observations with synthetic $D$-dimensional correlated noise that is sampled according to \ncovariance matrix $\\Sigma_D$ drawn from a unit-scale Wishart distribution. See Appendix \\ref{appendix:synth_noise_gen} for details.\n\n\n\n\n\\noindent \\textbf{Experimental setup.}\nWe varied the ambient noise dimension $D$ between 0 and 500 dimensions and added it to the UCI datasets, creating the noise-augmented datasets.\nEach noise-augmented dataset was randomly split between 70\\ training, 10\\\nvalidation, and 20\\ test samples. \n\nWe used the default settings for each algorithm. \nFor regularized LMNN, we picked the best performing trade-off parameter $\\Lambda$ from $\\{0,0.1,0.2,...,1\\}$ on the validation set.\nFor regularized ITML, we seeded with the rank-one\ndiscriminating metric, \\ie~we set the prior as the matrix with all zeros,\nexcept the diagonal entry corresponding to the most\ndiscriminating coordinate set to one.\n\nAll the reported results were averaged over 20 runs. \n\\\\\n\n\n\nWe modified the MMC and LMNN optimization criteria as per Eq.\\\n\\eqref{eq:opt_regularize} to include an explicit norm-regularization with\ntrade-off parameter $\\lambda$. To regularize ITML, we seeded with a rank one\ndiscriminating metric\\footnote{The rank one metric is chosen as a matrix with all zeros,\nexcept the diagonal entry corresponding to the most\ndiscriminating coordinate set to one.} as the\n``prior\". \n\nFor the $k$-means experiments, we sampled from the \\alg{Simplex} dataset with varying sample sizes $m$, ambient dimensions $D$, and intrinsic dimensions $d$.   (see Figure \\ref{fig:synth_MMC}). \nFor the nearest-neighbor experiments, we varied the ambient dimension $D$\nfor a fixed sample size $m$ that was randomly split between 70\\ training, 10\\\nvalidation and 20\\ test.   (see Figure \\ref{fig:UCI_NN}).\nTo pick the trade-off parameter $\\lambda$ for regularization, we selected the best performing $\\lambda \\in \\{0,0.1,0.2,...,1\\}$ on the validation set.\nAll the reported results were averaged over 20 runs. \n\n\nfor MMC and LMNN the best regularization parameter was ($\\lambda \\in \\{0, 0.1, 0.2,\\ldots, 1\\}$) was \nITML seeded with identity metric (no regularization) vs. rank 1 metric (regularization) \nSplit training 70\\, validation 10\\, test 20\\.\n\n\\noindent \\textbf{Results.}\nFigure \\ref{fig:synth_MMC} shows the $k$-means clustering performance of MMC on the \\alg{Simplex} dataset.\nThe left plot shows the error rates with respect to the sample size. Notice that even when the unregularized MMC (dashed red curves) doesn't seem\nto achieve good accuracy, the regularized version (solid blue curves) can quickly converge with small sample sizes. The center plot shows the same\nexperiment data as a function of ambient dimension. Here we uncover that regularization \\emph{does not} seem to adapt to intrinsic dimension for very small sample sizes (blue dashed lines) but it helps significantly once the sample size grows (solid blue lines). The right plot shows the expected scaling behavior as a function of intrinsic dimension.\n\nFigure \\ref{fig:UCI_NN} shows the nearest-neighbor performance (with $k3$) of LMNN and ITML on noise-augmented UCI datasets.\nNotice that the unregularized versions of both algorithms (dashed red lines) scale poorly when noisy features are introduced. \nAs the number of uninformative features grows, the performance of both\nalgorithms quickly degrades to that of classification performance in the original unweighted space with no metric learning (solid gray line),\nshowing poor adaptability to the signal in the data.\n\nInterestingly, neither of the unregularized algorithms performs consistently \nbetter than the other on datasets with high noise:\nITML yields better results on \\alg{Wine}, whereas LMNN seems better for\n\\alg{Ionosphere}, and both algorithms yield similar performance on \\alg{Iris}.\n\nThe regularized versions of both algorithms (solid blue lines) significantly\nimprove the classification performance. Remarkably, regularized ITML shows almost no\ndegradation in classification performance, even in very high noise regimes, demonstrating a strong robustness to noise. \n\n\nThe relatively flat curves of regularized ITML shows that that   , the classification results improve significantly.\nIn addition, regularized ITML typically performs better that regularized LMNN in high noise regimes.\n\n once\nregularization is introduced. In addition, regularized ITML typically performs better that regularized LMNN in high noise regimes.\n\n These results underscore the value of regularization in metric learning, showing that regularization encourages adaptability to the\nintrinsic complexity and improved robustness to noise.\n\n\n improve   clustering We shall systematically study \\alg{Mmc} \\alg{Lmnn} \\alg{Itml}.\n\n\n\nWe can validate . Our two-fold goal \nWe experimentally validate the degree to which \n\nAlgorithms to use: \\alg{Erm} vs.\\ $\\alg{Erm-Reg}$, \\alg{Mmc} vs.\\ \\alg{Mmc-Reg}, \\alg{Lmnn} vs.\\ \\alg{Lmnn-Reg}.\n\nSynthetic Gaussian dataset controlling the intrinsic and representation dimension.\nFaces, ImageNet dataset\n\n\n\n\\section{Discussion and Conclusion}\n\\label{sec:related_work}\n\nPrevious theoretical work on metric learning has focused almost exclusively on\nanalyzing the generalization error of variants of the optimization criteria for\nthe distance-based metric learning framework. \n\n\\citet{met_learn:analysis_regularized}, for instance, analyzed the generalization ability of\nregularized, convex-loss optimization criteria for pairwise distances via an algorithmic \\emph{stability} analysis. They derive an\ninteresting sample complexity result that is sublinear in $\\sqrt{D}$ \nfor datasets of representation dimension $D$. They discuss that the sample complexity can potentially be \nindependent of $D$, but do not characterize specific instances or classes of problems where this may be possible.\n\n\\citet{met_learn:analysis_conv_loss} analyze lipschitz convex losses.\n\nLikewise, recent work by \\citet{met_learn:analysis_robust_bellet} uses algorithmic\n\\emph{robustness} to analyze the generalization ability for pairwise- and triplet-based\ndistance metric learning. Their analysis relies on the existence of a partition of the input space, such that \nin each cell of the partition, the training loss and test loss does not deviate\nmuch (robustness criteria). Note that their sample\ncomplexity bound scales with the partition size, which in general can be\nexponential in the representation dimension. \n\nPerhaps the works most similar to our approach are the sample complexity\nanalyses by \\citet{met_learn:analysis_conv_loss} and\n\\citet{met_learn:analysis_cao_guo_ying}.\n\\citet{met_learn:analysis_conv_loss} analyze the consistency of the ERM\ncriterion for metric learning. They show a $O(m^{-1/2})$ rate of convergence for the ERM with $m$ samples to the expected risk \nfor thresholds on bounded convex losses for distance-based metric\nlearning. Our upper-bound in Lemma \\ref{lm:unif_conv_all} generalizes this\nresult by considering arbitrary (possibly non-convex) distance-based Lipschitz losses and explicitly shows the \ndependence on the representation dimension $D$.\n\\citet{met_learn:analysis_cao_guo_ying} provide an alternate analysis based on norm regularization of the weighting metric for distance-based metric learning.\non the other hand analyze the sample complexity of\nnorm-regularized metric learning. \nTheir result parallels our\nnorm-regularized criterion in Lemma \\ref{lm:unif_conv_dist_refi}. While they focus on\nanalyzing a specific optimization criterion -- thresholds on the hinge loss with\nnorm-regularization, our result holds for general Lipschitz losses. \nA key\nconceptual difference between their and our work is the motivation for\nanalyzing norm-regularized metric learning: they approach the problem to understand   They approach the problem\n\nA key difference is\nthat they analyze the thresholds on the hinge loss whereas our result is for general\nLipschitz losses. \n\nIt is worth emphasizing that none of these related works discuss the importance of or leverage the intrinsic structure in data for the metric learning problem. \nexploit are in some sense \\emph{algorithm-centric}: the sample complexity rates either only hold for \\emph{stable}, \\emph{robust} metric learning algorithms. Our results, on\nthe other hand, \nare algorithm agnostic and characterize a \\emph{data-centric} complexity of metric learning.\n \nOur results in Section \\ref{sec:discrim_complex} formalize an intuitive notion of dataset's intrinsic complexity for metric learning and show\ngive an intuitive characterization of data complexity in terms of the \\emph{metric learning complexity}, \nand show \nsample complexity rates that are\nfinely tuned to this \\emph{metric learning complexity}. \nand\nonly mildly depend on its representation dimension.\n\nThe classifier-based framework we discuss has parallels with the kernel\nlearning literature. The typical focus in kernel learning is to analyze the generalization\nability of the hypothesis class of linear separators in general Hilbert spaces\n\\citep{kern_learn:analysis_ying_campbell, kern_learn:analysis_cortes_mohri}.\nOur work provides a complementary analysis for learning explicit linear transformations of the given representation space for arbitrary hypotheses classes.\n\nOur theoretical analysis partly justifies the empirical success of norm-based regularization as well. Our empirical results show that such regularization not only helps \nin designing new metric learning algorithms \\citep{met_learn:alg_rob_stru_mcfee, met_learn:alg_fantope_regularization},\nbut can even benefit existing metric learning algorithms in high-noise regimes.\n\n\\section{Discussion and Conclusion}\n\n Acknowledgements should only appear in the accepted version. \n\\section*{Acknowledgments} \n\nThe authors would like to thank Aditya Menon for helpful discussions. \n\\textbf{Do not} include acknowledgements in the initial version of\nthe paper submitted for blind review.\n\nIf a paper is accepted, the final camera-ready version can (and\nprobably should) include acknowledgements. In this case, please\nplace such acknowledgements in an unnumbered section at the\nend of the paper. Typically, this will include thanks to reviewers\nwho gave useful comments, to colleagues who contributed to the ideas, \nand to funding agencies and corporate sponsors that provided financial \nsupport.   \n\n\n In the unusual situation where you want a paper to appear in the\n references without citing it in the main text, use \\nocite\n\n{\n\\bibliographystyle{icml2015}\n\\bibliography{mlearn}\n\n\\nocite{lt:anthony_bartlett}\n\\nocite{met_learn:analysis_robust_bellet}\n\\nocite{met_learn:analysis_conv_loss}\n\\nocite{met_learn:analysis_regularized}\n\\nocite{met_learn:analysis_cao_guo_ying}\n\\nocite{met_learn:analysis_guo_ying}\n\\nocite{met_learn:survey_bellet}\n\\nocite{met_learn:alg_ITML}\n\\nocite{met_learn:alg_LMNN}\n\\nocite{met_learn:alg_rob_stru_mcfee}\n\\nocite{kern_learn:analysis_cortes_mohri}\n\\nocite{kern_learn:analysis_ying_campbell}\n\\nocite{dataset:UCI_ML_repo}\n}\n\n\\newpage\n\\clearpage\n\n\\appendix\n\\section{Appendix: Various Proofs}\nThroughout the text we shall use the following notation. Define $\\mathcal{M}$ to be the \nset of all $D \\times D$ weighting matrices $M$ such that each entry $M_{ij} \\in [-1,1]$.\n\nDefine $\\mathcal{M}_\\epsilon$ as element-wise $\\epsilon$-cover of $\\mathcal{M}$ as\n\\begin{align*}\n\\mathcal{M}_\\epsilon : \\big\\{ M : M_{ij} \\in \\{ -1,\\ldots, -2\\epsilon, -\\epsilon, 0, \\epsilon, 2\\epsilon,& \\ldots, 1\\} \\\\ &\\textrm{ for all $i,j$}   \\big\\}\n\\end{align*}\n\nThe following fact about the size of $\\mathcal{M}_\\epsilon$ will he helpful in our proofs. \n\\begin{lemma}\n\\label{lm:eps_net_sz}\nFor all $\\epsilon>0$, $ \\epsilon^{-D^2} \\leq |\\mathcal{M}_\\epsilon| \\leq (\\epsilon^{-1} +3)^{D^2}.$\n\\end{lemma}\n\n\n\\subsection{Proof of Lemma \\ref{lm:unif_conv_all}}\n\n\nLet $\\mathcal{P}$ be the probability measure induced by the random variable\n$(\\mathbf{X}, Y)$, where $\\mathbf{X}: (x,x')$, $Y:\\indicate[yy']$, st. $((x,y),(x',y')) \\sim (\\D \\times \\D)$.\n\nDefine function class \n\\begin{align*}\n& \\mathcal{F} : \n\\\\\n&\n \\Bigg\\{ f_M \\!: \\mathbf{X} \\mapsto \\|M(x-x')\\|^2 \\Bigg|\\!   \\begin{array}{c} M \\in \\mathcal{M} \\\\ \\mathbf{X}   (x,x') \\in (X \\times X) \\end{array} \\!\\! \\Bigg\\},\n\\end{align*}\nand consider any loss function $\\phi^\\lambda(\\rho, Y)$ that is $\\lambda$-Lipschitz in the first argument.\nThen, we are interested in bounding the quantity \n\\begin{align*}\n\\sup_{f_M \\in \\mathcal{F}} \\E_{(\\mathbf{X},Y)\\sim \\mathcal{P}} [\\phi^\\lambda(f_M(\\mathbf{X}), Y)] - \\frac{1}{m} \\sum_{i1}^m \\phi^\\lambda(f_M(\\mathbf{X}_i),Y_i),\n\\end{align*}\nwhere $\\mathbf{X}_i:(x_{1,i},x_{2,i}) $, $Y_i:\\indicate[y_{1,i}y_{2,i}]$\nfrom the paired sample $S_m   \\{((x_{1,i},y_{1,i}),(x_{2,i},y_{2,i}))\n\\}_{i1}^m$. \n\n\nDefine $\\bar{x}_i : x_{1,i} - x_{2,i}$ for each $ \\mathbf{X}_i(x_{1,i},x_{2,i})$. \nThen, the Rademacher complexity\\footnote{See the definition of Rademacher complexity in the statement of Lemma \\ref{lm:rad_complexities_unif_bound}.} of our function class $\\mathcal{F}$ (with respect to the distribution $\\mathcal{P}$) is bounded, since (let $\\sigma_1,\\ldots, \\sigma_m$ denote\nindependent uniform $\\{\\pm 1\\}$-valued random variables)\n\\begin{align*}\n\\mathcal{R}_m(\\mathcal{F}, \\mathcal{P}) \n& : \\E_{\\substack{\\mathbf{X}_i,\\sigma_i \\\\ i\\in[m]}}   \\Bigg[ \\sup_{f_M \\in \\mathcal{F}} \\frac{1}{m} \\sum_{i1}^m \\sigma_i f_M(\\mathbf{X}_i) \\Bigg]   \\\\\n& \\frac{1}{m} \\E_{\\substack{\\mathbf{X}_i,\\sigma_i \\\\ i\\in[m]}} \\sup_{M \\in \\mathcal{M}} \\Big[ \\sum_{i1}^m \\sigma_i \\bar{x}_i^\\mathsf{T} M^\\mathsf{T}M \\bar{x}_i   \\Big] \n\\\\\n&\n \\frac{1}{m} \\E_{\\substack{\\mathbf{X}_i,\\sigma_i \\\\ i\\in[m]}} \\sup_{\\substack{M \\in \\mathcal{M}, \\textrm{ s.t.} \\\\ [a^{jk}]_{jk} : M^\\mathsf{T}M }} \\Bigg[ \\sum_{j,k} a^{jk} \\sum_{i1}^m \\sigma_i \\bar{x}_i^j \\bar{x}_i^k \\Bigg] \\\\\n&\\leq \\frac{1}{m} \\E_{\\substack{\\mathbf{X}_i,\\sigma_i \\\\ i\\in[m]}} \\sup_{M \\in \\mathcal{M}} \\Bigg[ \\|M^\\mathsf{T}M\\|_{_\\textrm{F}} \\Bigg( \\sum_{j,k} \\Big(\\sum_{i1}^m \\sigma_i \\bar{x}_i^j \\bar{x}_i^k \\Big)^2 \\Bigg)^{1/2} \\Bigg] \\\\\n&\\leq \\frac{\\sqrt{D}}{m} \\E_{\\substack{\\mathbf{X}_i,   i\\in[m]}} \\Bigg( \\E_{\\substack{\\sigma_i,i\\in[m]}}   \\sum_{j,k} \\Big(\\sum_{i1}^m \\sigma_i \\bar{x}_i^j \\bar{x}_i^k \\Big)^2 \\Bigg)^{1/2} \\\\\n& \\frac{\\sqrt{D}}{m} \\E_{\\substack{\\mathbf{X}_i,   i\\in[m]}} \\Bigg( \\sum_{j,k} \\sum_{i1}^m \\Big(\\bar{x}_i^j \\Big)^2 \\Big(\\bar{x}_i^k \\Big)^2 \\Bigg)^{1/2} \\\\\n& \\frac{\\sqrt{D}}{m} \\E_{\\substack{\\mathbf{X}_i,   i\\in[m]}} \\Bigg( \\sum_{i1}^m \\|\\bar{x}_i\\|^4 \\Bigg)^{1/2} \\\\\n& \\frac{\\sqrt{D}}{m} \\E_{\\substack{ (x_i, x'_i) \\sim (\\D|_X \\times \\D|_X) ,\\\\   i\\in[m]}} \\Bigg( \\sum_{i1}^m \\|x_i - x'_i\\|^4 \\Bigg)^{1/2} \\\\\n&\\leq \\sqrt{\\frac{D}{m}} \\Bigg (\\E_{\\substack{ (x, x') \\sim (\\D|_X \\times \\D|_X)}} \\|x - x'\\|^4 \\Bigg)^{1/2} \\\\\n&\\leq 4B^2 \\sqrt{\\frac{D}{m}}, \n&   \\\\\n&\\leq \\frac{1}{m} \\Bigg( \\E_{\\substack{\\mathbf{X}_i,\\sigma_i \\\\ i\\in[m]}} \\Bigg[ \\sup_{f_M \\in \\mathcal{F}} \\Big[ \\sum_{i1}^m \\sigma_i f_M(\\mathbf{X}_i)\\Big]^2 \\Bigg] \\Bigg)^{1/2}   \\\\\n&   \\frac{1}{m} \\Bigg( \\E_{\\mathbf{X}_i, i\\in[m]} \\sup_{f_M \\in \\mathcal{F}} \\sum_{i1}^m f^2_M(\\mathbf{X}_i) \\Bigg)^{1/2} \\\\\n&\\leq \\frac{1}{\\sqrt{m}} \\Bigg( \\E_{\\substack{x_{1,i},x_{2,i},\\\\i\\in[m]}} \\Big\\|\\bar{M}(x_{1,i} - x_{2,i} ) \\Big\\|^2 \\Bigg)^{1/2}\\\\\n& \\frac{1}{\\sqrt{m}} \\Bigg(2 \\tr \\big( (\\bar{M}^{\\mathsf{T}}\\bar{M}) \\; \\E_x(x x\\T) \\big)   \\Bigg)^{1/2}   \\sqrt{\\frac{2d}{m}}.\n& \\Big( \\E_{\\sigma_1,\\ldots, \\sigma_m} \\Big[ \\Big| \\frac{1}{m} \\sum_{i1}^m \\sigma_i f_\\lambda(z_i,z_i; M) \\Big| \\Big]   \\Big)^2 \\\\\n& \\leq \\lambda^2 \\sigma^2_{\\max}(M) \\Big(   \\E_{\\sigma_1,\\ldots, \\sigma_m} \\Big[ \\frac{1}{m} \\sum_{i1}^m \\sigma_i \\|[x_i x'_i] \\| \\Big] \\Big)^2 \\\\\n& \\leq \\lambda^2 \\sigma^2_{\\max}(M) \\cdot   \\E_{\\sigma_1,\\ldots, \\sigma_m} \\Big[ \\frac{1}{m} \\sum_{i1}^m \\sigma_i \\|[x_i x'_i] \\| \\Big]^2 \\\\\n& \\leq \\frac{ \\lambda^2 \\sigma^2_{\\max}(M) }{m^2}   \\cdot   \\E_{\\sigma_1,\\ldots, \\sigma_m} \\Big[ \\sum_{i1}^m \\sigma_i \\|[x_i x'_i] \\|^2 \\Big] \n\\end{align*}\nwhere the second inequality is by noting that $\\sup_{M\\in\\mathcal{M}} \\|M^\\mathsf{T}M\\|_{_F} \\leq \\sqrt{D} $   for the class of weighting metrics $\\mathcal{M} : \\big\\{M \\;|\\; M\\in\\R^{D\\times D}, \\sigma_{\\max}(M)1 \\big\\}$. \n\nRecall that $\\D$ has bounded support (with bound $B$). Thus, by noting that $\\phi^\\lambda$ is $8B^2$ bounded function that is $\\lambda$-Lipschitz in the first argument, we can apply Lemma \\ref{lm:rad_complexities_unif_bound} and get the desired \nuniform deviation bound. \\qed\n\n\\begin{lemma} \\textbf{\\emph{[Rademacher complexity of bounded Lipschitz loss functions \\cite{lt:bartlett_mendelson_radgauss_complexities}] } }\n\\label{lm:rad_complexities_unif_bound}\nLet $\\D$ be a fixed unknown distribution over $X \\times \\{-1,1\\}$, and\nlet $S_m$ be an i.i.d.\\ sample of size $m$ from $\\D$.\nGiven\na hypothesis class $\\mathcal{H}\\subset\\R^{X}$ and a loss function \n$\\ell : \\R \\times \\{-1,1\\} \\rightarrow \\R$, such that \n$\\ell$ is $c$-bounded, and is $\\lambda$-Lipschitz in the first argument, that\nis, $\\sup_{(y',y)\\in\\R\\times\\{-1,1\\}} | \\ell(y',y) | \\leq c$, and $|\\ell(y',y)\n- \\ell(y'',y)|\\leq \\lambda |y' - y''|$, we have the following:\n\ndefine\n$\\mathcal{H}_\\ell : \\{ \\ell(y,h(x)) | h\\in\\mathcal{H}, x\\in\\mathcal{X}, y\\in\\{0,1\\} \\}$.\nfor any $0<\\delta<1$, with probability at least $1-\\delta$, every $h\\in \\mathcal{H}$ satisfies\n\\begin{equation*}\n\\err(\\ell \\circ h,\\D) \\leq \\err(\\ell \\circ h,S_m) + 2\\lambda \\mathcal{R}_m(\\mathcal{H}, \\D) + c\\sqrt{\\frac{2 \\ln(1/\\delta)}{m}},\n\\end{equation*}\nwhere \n\\begin{itemize}\n\\item $\\err(\\ell \\circ h,\\D) : \\E_{{x,y}\\sim \\D} [\\ell(h(x),y)]$,\n\\item $\\err(h,S_m) : \\frac{1}{m} \\sum_{(x_i,y_i)\\in S_m} \\ell(h(x_i),y_i)   $,\n\\item \n$\\mathcal{R}_m(\\mathcal{H}, \\D)$ is the Rademacher complexity of the function class $\\mathcal{H}$ with respect to the distribution\n$\\D$ given $m$ i.i.d.\\ samples, and is defined as:\n\\begin{equation*}\n\\mathcal{R}_m(\\mathcal{H},\\D) : \\E_{\\substack{ x_i\\sim \\D|_X, \\\\ \\sigma_i \\sim \\mathrm{unif}\\{\\pm 1\\}, \\\\ i\\in[m] }} \\Bigg[\\sup_{{h}\\in\\mathcal{H}} \\frac{1}{m} \\sum_{i1}^m \\sigma_i h(x_i)   \\Bigg],\n\\end{equation*}\nwhere $\\sigma_i$ are independent uniform $\\{\\pm 1\\}$-valued random variables.\n\\end{itemize}\n\\end{lemma}\n\n\n\n\n\n\n\n\n\n\n\nConsider a $(1/\\resol)$-cover $\\mathcal{M}_{(1/\\resol)}$ of\n$\\mathcal{M}$. Then for any $M \\in \\mathcal{M}$, let $\\widehat M$ be the\n\\emph{closest} matrix in $\\mathcal{M}_{(1/\\resol)}$ such that for each $i,j$ entry, $|M_{ij} - \\widehat{M}_{ij}| \\leq 1/\\resol$. \nThus, we have\n\\begin{align*}\n|\\err_{\\dist}^\\lambda(M,S_m)   - & \\err_{\\dist}^\\lambda(M,\\D) | \\\\\n\\leq& \\;\\;\n|\\err_{\\dist}^\\lambda(M,S_m) - \\err_{\\dist}^\\lambda(\\widehat M,S_m) |   \\\\\n&+|\\err_{\\dist}^\\lambda(\\widehat M,S_m) - \\err_{\\dist}(\\widehat M,\\D) |   \\\\\n&+|\\err_{\\dist}^\\lambda(\\widehat M,\\D) - \\err_{\\dist}^\\lambda(M,\\D) |.\n\\end{align*}\n\nThe lemma follows by bounding the first and third terms by $4B\\lambda/m$\n(by noting that it is a difference of averages, where each term can be paired and bounded by applying Lemma \\ref{lm:perturb_bound} with $\\epsilon$ set to $1/\\resol$) and with probability at least $1-\\delta$ (over the\ndraw of the \\emph{paired} sample $S_m$) uniformly bounding the second term by\n$\\sqrt{\\frac{\\log(2(\\resolSZ)/\\delta)}{2m}}$.\n(by noting that $|\\mathcal{M}_{(1/\\resol)}|\\leq \\resolSZ$ and by applying Lemma \\ref{lm:unif_cover_bound}).\n\n\\begin{lemma}\n\\label{lm:perturb_bound}\nPick any pair of vectors $a,b \\in \\R^D$ from a bounded domain. That is, $\\|a\\|, \\|b\\| \\leq B$. \nFor any $D \\times D$ matrix $M$ and any perturbation $\\widehat M$ of $M$ such   that $|M_{ij} - \\widehat M_{ij}|\\leq \\epsilon$, we have (for all $\\lambda\\geq 0$)\n$$\n\\lambda \\Big| \\|Ma-Mb\\|   - \\|\\widehat Ma-\\widehat Mb\\|   \\Big| \\leq 2BD\\lambda\\epsilon.\n$$ \n\\end{lemma}\n\\begin{proof}\nLet $a' : a-b$. Then, we need to bound\n\\begin{eqnarray*}\n\\Big| \\|Ma'\\| - \\|\\widehat M a'\\|   \\Big| &\\leq&\n\\|(M-\\widehat M) a' \\| \n\\\\\n&\\leq& \\sigma_{\\max}(M-\\widehat M) \\|a'\\| \n\\\\\n&\\leq& 2B D\\epsilon,\n\\end{eqnarray*}\nwhere $\\sigma_{\\max}(\\cdot)$ is the largest singular value of $\\cdot$, and\nnoting that (i) $\\|a'\\|$ is bounded by $2B$, and (ii) the\nlargest singular value of $E:(M-\\widehat M)$ is at most $D\\epsilon$ (because\nthe largest eigenvalue of $E\\T E$ is at most $D^2\\epsilon^2$ by Gershgorin\ncircle theorem).\n\\end{proof}\n\n\n\\begin{lemma}\n\\label{lm:unif_cover_bound}\nFix any $m$, and let $S_m$ be an i.i.d.\\ \\emph{paired} sample of\nsize $m$. For any $\\epsilon$, let $\\mathcal{M}_{\\epsilon}$ be an\n$\\epsilon$-covering of weighting metrics (as above). Then, with probability at\nleast $1-\\delta$,\n$$\n \\max_{M\\in \\mathcal{M}_\\epsilon} | \\err_{\\dist}^\\lambda(M,S_m) - \\err_{\\dist}^\\lambda(M,\\D)| \\leq \\sqrt{\\frac{\\log(2|\\mathcal{M}_\\epsilon|/\\delta)}{2m}}.\n$$\n\\end{lemma}\n\\begin{proof} Fix any $M \\in \\mathcal{M}_\\epsilon$.\nFor any $\\lambda\\geq 0$, define random variable $X_i : \nf_\\lambda((x_{1,i},y_{1,i}),(x_{2,i},y_{2,i}); M) $, where\n$(x_{j,i},y_{j,i})$ ($j\\in\\{1,2\\}$, and $i\\in[m]$) is from the sample $S_m$.\nNote that $X_i$ is an i.i.d.\\ bounded random variable in $[0,1]$. \n\nRecall Hoeffding's inequality, for \n$X_1,\\ldots,X_m$ i.i.d.\\ bounded random variables with each $0\\leq X_i\\leq 1$,\n$\n\\Pr\\big[\\big|\\frac{1}{m}\\sum_i X_i - \\E[X_1]\\big| \\geq \\epsilon\\big] \\leq 2 e^{-2m\\epsilon^2}.\n$\n\nThus by applying Hoeffding's Inequality and union bounding over the choices of $M$ in $\\mathcal{M}_\\epsilon$, the result follows.\n\\end{proof}\n\\vspace{0.2in}\n\\begin{lemma} [Hoeffding's Inequality]\n\\label{lm:hoeffding}\nLet $X_1,\\ldots,X_m$ be i.i.d.\\ bounded random variables with each $0\\leq X_i\\leq 1$. Then,\n$$\n\\Pr\\Big[\\big|\\frac{1}{m}\\sum_i X_i - E[X_1]\\big| \\geq \\epsilon\\Big] \\leq 2 e^{-2m\\epsilon^2}.\n$\n\\end{lemma}\n\n\n\\subsection{Proof of Lemma \\ref{lm:lb_dist}}\n\nWe shall exhibit a finite class of bounded support distributions $\\classD$, such that if $\\D$ is chosen uniformly at random from $\\classD$, the expectation (over the random choice of $\\D$) of the probability of failure (that is, generalization error of the metric returned by $\\mathcal{A}$ compared to that of the optimal metric exceeds the specified tolerance level $\\epsilon$) is at least $\\delta$. This implies that for some distribution in $\\classD$ the probability of failure is at least $\\delta$ as well.\n\nLet $\\Delta_D : \\{x_0,\\ldots, x_{D}\\}$ be a set of $D+1$ points that from the vertices of a regular unit-simplex from the underlying space $X\\R^D$ as per Definition\n\\ref{def:reg_simplex} (see below). For a fixed parameter $0<\\alpha<1$ (exact value determined later), define $\\classD$ as the class of all distributions $\\D$\non $X \\times \\{0,1\\}$ such that:\n\\begin{itemize}\n\\item $\\D$ assigns zero probability to all sets not intersecting $\\Delta_D \\times \\{0,1 \\}$.\n\\item for each $i   0,\\ldots,D$, either\n   \\begin{itemize}\n   \\item $\\Pr[(x_i,1)]   (1+\\sqrt{\\alpha})/2 $ and $\\Pr[(x_i,0)]   (1-\\sqrt{\\alpha})/2$, or\n   \\item $\\Pr[(x_i,1)]   (1-\\sqrt{\\alpha})/2 $ and $\\Pr[(x_i,0)]   (1+\\sqrt{\\alpha})/2$.\n   \\end{itemize}\n\\end{itemize}\n\nFor concreteness, we shall use a specific instantiation of $\\phi^\\lambda_{L,U}$ in $\\err_{\\dist}^\\lambda$ with $U0$, $L4/D$ and $\\lambdaD/4$.\n\\\\\n\n\\noindent \\textbf{Proof overview.} We first show, by the construction of the\ndistributions under consideration in $\\classD$, the sample error and the\ngeneralization error minimizing metrics over any $\\D \\in \\classD$ belong\nto a restricted class of weighting matrices (Eq.\\ \\ref{eq:Mall_M01}). We then make a second simplification by noting that finding these (sample- and generalization-) error minimizing metrics (in the restricted class) is equivalent to solving a binary classification problem (Eq.\\ \\ref{eq:M01_binf}). This reduction to binary classification enables us to use VC-style lower bounding techniques to give a lower bound on the sample complexity. We now fill in the details.\n\\\\\n\nConsider a subset of weighting metrics $\\Mgap$ that map points in $\\Delta_D$ to exactly one of two possible points that are \n(squared) distance at least $4/D$ apart, that is,\n\\begin{align*}\n\\Mgap :   \\{ & M \\;|\\; M\\in \\mathcal{M}, \\exists z_0, z_1 \\in \\R^D, \\forall x\\in\\Delta_D, \\\\\n& Mx \\in \\{z_0,z_1\\} \\textrm{ and }   \\|z_0 - z_1\\|^2\\geq 4/D   \\}.\n\\end{align*}\n\n\nNow pick any $\\D \\in \\classD$, let $S_m$ be an i.i.d.\\ paired sample from $\\D$. Observe that \nboth the sample-based and the distribution-based error minimizing weighting metric from $\\mathcal{M}$ on $\\D$ also belongs to $\\Mgap$. That is,\n(c.f.\\ Lemma \\ref{lm:dist_lb_errall_eq_err01})\n\\begin{align}\n\\nonumber \\argmin_{M\\in\\M}\\err_{\\dist}(M,\\D) & \\argmin_{M\\in\\Mgap}\\err_{\\dist}(M,\\D) \\\\\n\\argmin_{M\\in\\M}\\err_{\\dist}(M,S_m) & \\argmin_{M\\in\\Mgap}\\err_{\\dist}(M,S_m).\n\\label{eq:Mall_M01}\n\\end{align}\n\n\\noindent \\textbf{A reduction to binary classification on product space.}\nFor each $M \\in \\Mgap$, we associate a classifier $f_M : (\\Delta_D \\times \\Delta_D) \\rightarrow \\{0,1\\}$ defined as\n$(x_i,x_j) \\mapsto \\indicate[Mx_i   Mx_j]$. Now, consider the probability\nmeasure $\\mathcal{P}$ induced by the random variable $(\\Xb, Y)$, where $\\Xb : (x,x')$, $Y:\\indicate[y   y']$, s.t.\\ $((x,y),(x',y')) \\sim \\big(\\D|_{(\\Delta_D \\times \\{0,1\\})} \\times \\D|_{(\\Delta_D \\times \\{0,1\\})}\\big)$. It is easy to check that for all $M \\in \\Mgap$\n\\begin{align}\n\\nonumber \\err_{\\dist}^\\lambda(M,\\D)   &   \\E_{(\\Xb,Y) \\sim \\mathcal{P}}\\big[ \\indicate[f_M(\\Xb)\\neq Y] \\big]\\\\\n\\err_{\\dist}^\\lambda(M,S_m) &   \\frac{1}{m} \\!\\! \\sum_{((x,y),(x',y'))\\in S_m} \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \\indicate\\big[f_M((x,x')) \\neq \\indicate[yy']\\big]. \n\\label{eq:M01_binf}\n\\end{align}\nDefine \n\\begin{eqnarray}\n\\nonumber\n\\eta(\\Xb) &:& \\Pr_{Y\\sim\\mathcal{P}|_{Y|\\Xb}}[Y1|\\Xb]   \\\\\n\\nonumber\n&& \\Pr_{ (y,y') \\sim (\\D\\times \\D)|_{(y,y')|(x,x')}}[yy'|x,x']   \\\\\n&&\n\\Bigg\\{\\begin{array}{ll} \n\\vspace{1mm}\n   \\frac{1}{2}+\\frac{\\alpha}{2} & \\textrm{if $\\Pr(y|x)\\Pr(y'|x')$} \\\\\n   \\frac{1}{2}-\\frac{\\alpha}{2} & \\textrm{if $\\Pr(y|x)\\neq \\Pr(y'|x')$} \n   \\end{array} . \n\\label{eq:eta_val}\n\\end{eqnarray}\nObserve that $\\eta(\\Xb)$ is the Bayes error rate at $\\Xb$ for distribution\n$\\mathcal{P}$. Since, by construction of $\\Mgap$, the class\n$\\{f_M\\}_{M\\in\\Mgap}$ contains a classifier that achieves the Bayes error rate,\nthe optimal classifier $f^* : \\argmin_{f_M}\\E_{(\\Xb,Y) \\sim \\mathcal{P}}\\indicate[f_M(\\Xb)\\neq\nY]$ necessarily has $f^*(\\Xb)   \\indicate[\\eta(\\Xb)>\\frac{1}{2}]$ (for all $\\Xb$).\nThen, for any $f_M$,\n\\begin{align}\n\\nonumber\n\\lefteqn{\n\\E_{(\\Xb,Y) \\sim \\mathcal{P}}\\big[ & \\indicate[f_M(\\Xb)\\neq Y] \\big]   -\n\\E_{(\\Xb,Y) \\sim \\mathcal{P}}\\big[ \\indicate[f^*(\\Xb)\\neq Y] \\big]\n}\n\\\\\n\\nonumber\n& \\E_{\\Xb \\sim \\mathcal{P}|_{\\Xb}} \\big[ \\eta(\\Xb) \\big( \\indicate[f^*(\\Xb)1] -\\indicate[f_M(\\Xb)1] \\big) \n\\\\\n\\nonumber\n&\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\n+ (1-\\eta(\\Xb)) \\big( \\indicate[f^*(\\Xb)0] -\\indicate[f_M(\\Xb)0] \\big) \\big] \\\\\n\\nonumber\n& \\E_{\\Xb \\sim \\mathcal{P}|_{\\Xb}} \\big[ (2\\eta(\\Xb)-1) \\big( \\indicate[f^*(\\Xb)1] -\\indicate[f_M(\\Xb)1] \\big) \\big] \\\\\n\\nonumber\n&   \\E_{\\Xb \\sim \\mathcal{P}|_{\\Xb} }\\big[ 2|\\eta(\\Xb)-{1}/{2}| \\cdot \\indicate[f_M(\\Xb) \\neq f^*(\\Xb)]\\big] \\\\\n& \\frac{2\\alpha}{(D+1)^2} \\sum_{i>j} \\big[\\indicate[f_M((x_i,x_j)) \\neq f^*((x_i,x_j))]\\big], \n\\label{eq:binf_valf}\n\\end{align}\nwhere (i) the second to last equality is by noting that $f^*(\\Xb) \\neq 1 \\iff \\eta(\\Xb)\\leq 1/2$, and (ii) the last equality is by noting Eq.\\ \\eqref{eq:eta_val}, $f_M((x_i,x_i))   f^*((x_i,x_i))1$ for all $i$ and $f((x_i,x_j))   f((x_j,x_i))$ for all $f$. \nFor notational simplicity, we shall define $\\Xb_{i,j} : (x_i,x_j)$. \n\nNow, for a given paired sample $S_m$, let $N(S_m) : (N_{i})_{i}$ (for all $0\\leq i\\leq D$),\nwhere $N_{i}$ is the number of occurrences of the point\n$x_i$ in $S_m$. , that is, total number of occurrences of\n$((x_i,y),(x_j,y'))$ or   $((x_j,y),(x_i,y'))$ (for any $y,y'\\in\\{0,1\\}$), or equivalently\ntotal number of $(\\Xb_{ij},Z)$ with $\\Xb_{ij}   $ $(x_i,x_j)$ or $(x_j,x_i)$ and $Z\\in \\{0,1\\}$.\nThen for any $f_M$,\n\\begin{align*}\n\\lefteqn{\n\\E_{S_m}& \\Bigg[   \\frac{1}{(D+1)^2}\\sum_{i>j}   \\indicate[f_M(\\Xb_{i,j}) \\neq f^*(\\Xb_{i,j})] \\Bigg]\n}\n\\\\\n& \n\\frac{1}{(D+1)^2}\\sum_{i>j} \\Pr_{S_m}[f_M(\\Xb_{i,j}) \\neq f^*(\\Xb_{i,j})] \\\\\n& \n\\frac{1}{(D+1)^2}\\sum_{i>j} \\sum_{N\\in\\mathbb{N}^{D+1}} \\Pr_{S_m}[f_M(\\Xb_{i,j}) \\neq f^*(\\Xb_{i,j}) | N(S_m)   N] \\cdot \n\\\\ \n& \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\;\\;\\;\\;\\; \n\\Pr[N(S_m)   N]   \\\\\n& \n\\frac{1}{(D+1)^2}\\sum_{N\\in\\mathbb{N}^{D+1}} \\Pr[N(S_m)   N] \\cdot \n\\\\\n& \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;   \n\\sum_{i>j} \\Pr_{S_m}[f_M(\\Xb_{i,j}) \\neq f^*(\\Xb_{i,j}) | N_{i}, N_{j}]   \\\\\n& \\geq\n\\frac{1}{(D+1)^2}\\sum_{N\\in\\mathbb{N}^{D+1}} \\Pr[N(S_m)   N] \\cdot \n\\\\\n& \\;\\;\\; \\;\\;\\; \n\\sum_{i>j} \\frac{1}{4} \\Bigg(1-\\sqrt{1-\\exp{\\Bigg( \\frac{-(\\max\\{N_{i},N_{j}\\} +1) \\alpha^2}{1-\\alpha^2}\\Bigg)}} \\Bigg) \\\\\n&\\geq \n\\frac{1}{4}\\frac{D}{D+1} \\Bigg(1-\\sqrt{1-\\exp{\\Bigg( \\frac{-((2m/(D+1)) +1) \\alpha^2}{1-\\alpha^2}\\Bigg)}} \\Bigg)\\\\\n&\\geq \n\\frac{1}{8} \\Bigg(1-\\sqrt{1-\\exp{\\Bigg( \\frac{-((2m/(D+1)) +1) \\alpha^2}{1-\\alpha^2}\\Bigg)}} \\Bigg),\n\\end{align*}\nwhere (i) the first inequality is by applying Lemma \\ref{lm:two_coin_lb}, (ii) the second inequality is by assuming WLOG $N_i \\geq N_j$, and   \nnoting that the expression above is convex in $N_{i}$ so one can apply Jensen's inequality and by observing that $\\E[N_{i}]2m/(D+1)$ and that there are total $D(D+1)$ summands for $i>j$, and (iii) the last inequality is by noting that $D\\geq1$. Now, let $B$ denote the r.h.s.\\ quantity above.\nThen by recalling that for any $[0,1]$-valued random variable $Z$, $\\Pr(Z>\\gamma) > \\E Z - \\gamma$ (for all $0<\\gamma<1$), we have\n\\begin{align*}\n\\Pr_{S_m}\\Big[ \\frac{1}{(D+1)^2}\\sum_{i>j} \\indicate[f_M((x_i,x_j))   \\neq f^*&((x_i,x_j))]   > \\gamma B \\Big] \n\\\\ \n&\n> (1-\\gamma) B.\n\\end{align*}\nOr equivalently, by combining Eqs.\\ \\eqref{eq:Mall_M01}, \\eqref{eq:M01_binf} and \\eqref{eq:binf_valf}, we have\n\\begin{align*}\n\\Pr_{S_m}\\big[   \\err_{\\dist}(\\mathcal{A}(S_m),\\D) - \\err_{\\dist}(M^*_{\\D},\\D) > \\gamma B \\big] > (1-\\gamma) B \\\\\n\\E_{\\D \\sim \\textrm{unif($\\classD$)}} \\Pr_{S_m \\sim \\D} & \\Big[   \\err_{\\dist}(\\mathcal{A}(S_m),\\D) \n\\\\ \n& \n- \\err_{\\dist}(M^*_{\\D},\\D)   > 2\\alpha \\gamma B \\Big] > (1-\\gamma) B,\n\\end{align*}\nwhere $M^*_{\\D} : \\argmin_{M\\in\\mathcal{M}} \\err_{\\dist}(M,\\D)$ and $\\mathcal{A}(S_m)$ is any metric returned by empirical error minimizing\nalgorithm. \nNow, if (cond.\\ 1) $B\\geq \\delta/1-\\gamma$ and (cond.\\ 2) $\\epsilon \\leq 2\n\\gamma \\alpha B$, it follows that for some $\\D\\in\\classD$\n\\begin{eqnarray}\n\\Pr_{S_m \\sim \\D} \\Big[ \\err_{\\dist}(\\mathcal{A}(S_m),\\D) - \\err_{\\dist}(M^*_{\\D},\\D) > \\epsilon \\Big] > \\delta.\n\\label{eq:lb_main}\n\\end{eqnarray}\nNow, to satisfy cond.\\ 1 \\& 2, we shall select $\\gamma   1-16\\delta$. Then cond.\\ 1 follows\nif\n$$\nm\\leq \\frac{(D+1)}{2} \\Bigg( \\frac{1-\\alpha^2}{\\alpha^2} \\ln(4/3) -1 \\Bigg).\n$$\nChoosing parameter $\\alpha   8\\epsilon/\\gamma$ (and by noting $B\\geq 1/16$ by cond.\\ 1 for choice of $\\gamma$ and $m$), cond.\\ 2 is satisfied as well. Hence, \n$$\nm\\leq \\frac{(D+1)}{2} \\Bigg( \\frac{(1-16\\delta)^2-(8\\epsilon)^2}{64\\epsilon^2} \\ln(4/3) -1 \\Bigg)\n$$\nimplies Eq.\\ \\eqref{eq:lb_main}. Moreover, if $0<\\epsilon,\\delta<1/64$ then $m\\leq \\frac{(D+1)}{512\\epsilon^2}$ would suffice. \\qed\n\\\\\n\n\\begin{definition} \\emph{\\textbf{[Vertices of a regular simplex in $\\R^n$]}}\n\\label{def:reg_simplex}\nDefine $n+1$ vectors $\\Delta_n   \\{v_0,\\ldots,v_n \\}$, with each $v_i \\in \\R^n$   as \n\\begin{align*}\nv_{0,j} & \\frac{-1}{\\sqrt{n}}   &\\textrm{for $1\\leq j \\leq n$} \\\\\nv_{i,j} & \\Bigg\\{ \\begin{array}{ll} \\frac{(n-1)\\sqrt{n+1}+1}{n\\sqrt{n}} & \\textrm{if $ij$} \\\\ \\frac{-(\\sqrt{n+1} -1)}{n\\sqrt{n}} & \\textrm{otherwise} \\end{array} &\\textrm{for $1\\leq i,j \\leq n$}\n\\end{align*}\n\\end{definition}\n\n\\begin{fact} \n\\label{fact:simplex_properties}\n\\emph{\\textbf{[properties of vertices of a regular $n$-simplex]}}\nLet $\\Delta_n   \\{v_0,\\ldots, v_n\\}$ be a set of $n+1$ vectors in $\\R^n$ as per Definition \\ref{def:reg_simplex}. Then, $\\Delta_n$ defines vertices of a regular $n$-simplex circumscribed in a unit $(n-1)$-sphere, with\n\\begin{enumerate}[(i)]\n\\item $\\|v_i\\|^21$ (for all $i$), and \n\\item $\\|v_i-v_j\\|^22(n+1)/n$ (for $i\\neq j$). \n\\end{enumerate}\nMoreover, for any non-empty bi-partition of $\\Delta_n$ into $\\Delta^{(1)}_n$ and\n$\\Delta^{(2)}_n$ with $|\\Delta^{(1)}_n|k$ and $|\\Delta^{(2)}_n|   n+1-k$, define $a^{(1)}$ and $a^{(2)}$ the means (centroids) of the\npoints in $\\Delta^{(1)}_n$ and $\\Delta^{(2)}_n$ respectively. Then, we also have\n\\begin{enumerate}[(i)]\n\\item $(a^{(1)} - a^{(2)}) \\cdot (a^{(i)} - v_j )   0$ (for $i\\in\\{1,2\\}$, and $v_j \\in \\Delta^{(i)}_n$).\n\\item $\\|a^{(1)} - a^{(2)} \\|^2   \\frac{(n+1)^2}{kn(n+1-k)} \\geq \\frac{4}{n}$, for $1\\leq k\\leq n$.\n\\end{enumerate}\n\\end{fact}\n\n\n\n\n\\begin{lemma} \n\\label{lm:dist_lb_errall_eq_err01}\nLet $\\Delta_D$ be a set of $D+1$ points $\\{X_0,\\ldots,X_D\\}$ in $\\R^D$ as per\nDefinition \\ref{def:reg_simplex}, and let $\\D$ be an arbitrary distribution\nover $\\Delta_D \\times \\{0,1\\}$. Define $P_i : \\indicate[\\Pr_{\\D}[(X_i,1)]>1/2]$.\nDefine $\\Pi : \\{\\pi: \\Delta_D \\rightarrow\n\\R^D\\}$ be the collection of all functions that maps points in $\\Delta_D$ to\narbitrary points in $\\R^D$. \nDefine \n\\begin{align*}\nf((x,y),&(x',y');\\pi) : \n\\\\ & \n\\Bigg\\{ \n   \\begin{array}{ll} \n   \\min\\{1,\\frac{D}{4} \\|\\pi(x) - \\pi(x')\\|^2\\} & \\textrm{if $yy'$} \\\\\n   \\min\\{1,[1-\\frac{D}{4} \\|\\pi(x) - \\pi(x')\\|^2]_{_+}\\} & \\textrm{if $y\\neq y'$} \n   \\end{array}.\n\\end{align*}\nLet $\\mathcal{E}(\\pi) : \\E_{(x,y),(x',y')\\sim \\D\\times \\D}[f((x,y),(x',y');\\pi)] $ and $\\mathcal{E}^* : \\inf_\\pi \\mathcal{E}(\\pi)$. Then,\nfor any $\\bar \\pi \\in \\Pi$ such that\n\\begin{enumerate}[(i)]\n\\item $\\bar \\pi(X_i)   \\bar \\pi(X_j)$, if $P_i   P_j$\n\\item $\\|\\bar \\pi(X_i) - \\bar \\pi(X_j)\\|^2 \\geq \\frac{4}{D}$, if $P_i \\neq P_j$,\n\\end{enumerate}\nwe have that $\\mathcal{E}(\\bar \\pi)   \\mathcal{E}^*$.   \nMoreover, define $\\bar A$ as\n\\begin{itemize}\n\\item $\\bar A : \\frac{A_1-A_0}{\\|A_1-A_0 \\|}$, where $A_0 : \\mean(X_i)$ such that $P_i0$, and $A_1 : \\mean(X_i)$ such that $P_i   1$ (if exists at least one $P_i0$ and at least one $P_i   1$). \n\\item $\\bar A : 0$, i.e.\\ the zero vector in $\\R^D$ (otherwise).\n\\end{itemize}\nAnd let $M$ be a $D\\times D$ matrix (with $\\sigma_{\\max}(M)1$) defined as\n$$\nM : {\\bar A} {\\bar A}^{\\T}.\n$$\nThen the map $\\pi_M: x \\mapsto M x$ constitutes a map that satisfies conditions (i) and (ii) and thus $\\mathcal{E}(\\pi_M)   \\mathcal{E}^*$.\n\\end{lemma}\n\\begin{proof}\nThe proof follows from the geometric properties of $\\Delta_D$ and Fact \\ref{fact:simplex_properties}.\n\\end{proof}\n\n\\begin{lemma} \n\\label{lm:two_coin_lb}\nGiven two random variables $\\alpha_1$ and $\\alpha_2$, each uniformly distributed on\n$\\{\\alpha_-,\\alpha_+\\}$ independently, where $\\alpha_-   1/2-\\epsilon/2$ and $\\alpha_+ \n1/2+\\epsilon/2$ with $0<\\epsilon<1$. Suppose that $\\xi_1^1,\\ldots,\\xi_m^1$ and $\\xi_1^2,\\ldots,\\xi_m^2$ are\ntwo i.i.d.\\ sequences of $\\{0,1\\}$-valued random variables with $\\Pr(\\xi^1_i1)\\alpha_1$ and $\\Pr(\\xi^2_i1)\\alpha_2$ for all\n$i$. Then, for any likelihood maximizing function $f$ from $\\{0,1\\}^m$ to $\\{\\alpha_-,\\alpha_+\\}$ that estimates the bias $\\alpha_1$ and $\\alpha_2$ from the samples, \n\\begin{align*}\n&\\Pr\\Big[ \\big( f(\\xi^1_1,\\ldots,\\xi^1_m)\\neq \\alpha_1 \\textup{ and } f(\\xi^2_1,\\ldots,\\xi^2_m)   \\alpha_2 \\big), \n\\\\\n&\\;\\;\\;\\;\\; \n\\textup{or} \n\\big( f(\\xi^1_1,\\ldots,\\xi^1_m)   \\alpha_1 \\textup{ and } f(\\xi^2_1,\\ldots,\\xi^2_m) \\neq \\alpha_2 \\big)\n \\Big] \n\\\\\n& \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \n> \\frac{1}{4}\\Bigg(1-\\sqrt{1-\\exp{\\Big( \\frac{-2 \\lceil m/2\\rceil \\epsilon^2}{1-\\epsilon^2}\\Big)}} \\Bigg).\n\\end{align*}\n\\end{lemma}\n\\begin{proof} Note that\n\\begin{align*}\n&\\Pr\\Big[ \\big( f(\\xi^1_1,\\ldots,\\xi^1_m)\\neq \\alpha_1 \\textup{ and } f(\\xi^2_1,\\ldots,\\xi^2_m)   \\alpha_2 \\big), \n\\\\\n&\\;\\;\\;\\;\\; \n\\textup{or} \n\\big( f(\\xi^1_1,\\ldots,\\xi^1_m)   \\alpha_1 \\textup{ and } f(\\xi^2_1,\\ldots,\\xi^2_m) \\neq \\alpha_2 \\big)\n\\Big] \n\\\\\n&\\;\\;\\;\n \\;\\;\n\\Pr[f(\\xi^1_1,\\ldots,\\xi^1_m)\\neq \\alpha_1]\\cdot \\Pr[f(\\xi^2_1,\\ldots,\\xi^2_m)   \\alpha_2 ]\n\\\\\n&\\;\\;\\;\\;\\;\\;\n+ \\Pr[ f(\\xi^1_1,\\ldots,\\xi^1_m)   \\alpha_1] \\cdot \\Pr[f(\\xi^2_1,\\ldots,\\xi^2_m) \\neq \\alpha_2 ] \\\\\n&\\;\\;\\;\\geq \\;\n\\frac{1}{2}\\Pr\\big[f(\\xi^1_1,\\ldots,\\xi^1_m)\\neq \\alpha_1\\big] + \\frac{1}{2} \\Pr\\big[f(\\xi^2_1,\\ldots,\\xi^2_m) \\neq \\alpha_2 \\big] \\\\\n&\\;\\;\\;> \\;\\;\n\\frac{1}{4}\\Bigg(1-\\sqrt{1-\\exp{\\Big( \\frac{-2 \\lceil m/2\\rceil \\epsilon^2}{1-\\epsilon^2}\\Big)}} \\Bigg),\n\\end{align*}\nwhere the first inequality is by noting that a likelihood maximizing $f$ will select the correct bias better than random (which has probability $1/2$), and\nthe second inequality is by applying Lemma \\ref{lm:coin_lb}.\n\\end{proof}\n\n\\begin{lemma} \\emph{\\textbf{[Lemma 5.1 of \\citet{lt:anthony_bartlett}]}}\n\\label{lm:coin_lb}\nSuppose that $\\alpha$ is a random variable uniformly distributed on\n$\\{\\alpha_-,\\alpha_+\\}$, where $\\alpha_-   1/2-\\epsilon/2$ and $\\alpha_+ \n1/2+\\epsilon/2$, with $0<\\epsilon<1$. Suppose that $\\xi_1,\\ldots,\\xi_m$ are\ni.i.d.\\ $\\{0,1\\}$-valued random variables with $\\Pr(\\xi_i1)\\alpha$ for all\n$i$. Let $f$ be a function from $\\{0,1\\}^m$ to $\\{\\alpha_-,\\alpha_+\\}$. Then\n$$\n\\Pr\\big[ f(\\xi_1,\\ldots,\\xi_m)\\neq \\alpha \\big] \\! > \\! \\frac{1}{4}\\Bigg(\\!1-\\sqrt{1-\\exp{\\Big( \\frac{-2 \\lceil m/2\\rceil \\epsilon^2}{1-\\epsilon^2}\\Big)}} \\Bigg).\n$$\n\\end{lemma}\n\\begin{lemma}\n\\label{lm:dist_lb_opt_err}\nLet $\\Delta_D : \\{x_0,\\ldots, x_{D}\\}$ be a set of $D+1$ points from the underlying space $\\R^D$ as per Definition\n\\ref{def:reg_simplex}. Pick any $0<\\epsilon<1$, and define $\\classD$ be the class of all distributions $\\D$ on $\\R^D \\times \\{0,1\\}$\nsuch that:\n\\begin{itemize}\n\\item $\\D$ assigns zero probability to all sets not intersecting $\\Delta_D \\times \\{0,1 \\}$.\n\\item for each $i   1,\\ldots,D+1$, either\n   \\begin{itemize}\n   \\item $\\Pr[(x_i,1)]   (1+\\sqrt{\\epsilon}) $ and $\\Pr[(x_i,0)]   (1-\\sqrt{\\epsilon})$, or\n   \\item $\\Pr[(x_i,1)]   (1-\\sqrt{\\epsilon}) $ and $\\Pr[(x_i,0)]   (1+\\sqrt{\\epsilon})$.\n   \\end{itemize}\n\\end{itemize}\n\n\n\\end{lemma}\n\n\n\\begin{corollary}\nLet $\\classD$ be as per Lemma \\ref{lm:lb_dist}. There exists a distribution $\\D \\in \\classD$ such that \n$$\n\\Pr_{S_m \\sim \\D} \\Big[ \\err_\\textrm{dist}(\\mathcal{A}(S_m),\\D) - \\err_\\textrm{dist}(M^*_{\\D},\\D) > \\epsilon \\Big] > \\delta.\n$$\n\\end{corollary}\n\n\n\n\n\n\\subsection{Proof of Lemma \\ref{lm:hypoth_ub}}\nFor any $M \\in \\mathcal{M}$ define real-valued hypothesis class on domain $X$ as $\\mathcal{H}_M : \\{x\\mapsto h(Mx) : h\\in \\mathcal{H}\\}$ and define\n\\begin{eqnarray*}\n\\mathcal{F} : \\{ x \\mapsto h(Mx) : M\\in \\mathcal{M}, h \\in \\mathcal{H} \\}   \\bigcup_M \\mathcal{H}_M.\n\\end{eqnarray*}\n\nObserve that a uniform convergence of errors induced by the functions in\n$\\mathcal{F}$ implies convergence of the class of weighted matrices as well.\n\nNow for any domain $X$, real-valued hypothesis class $\\mathcal{G} \\subset [0,1]^X$, margin $\\gamma>0$, and a sample $S\\subset X$, define\n\\begin{equation*}\n\\cov_\\gamma(\\mathcal{G},S) : \\Bigg\\{C\\subset \\mathcal{G} \\Big| \\begin{array}{cc} \\forall\ng\\in\\mathcal{G}, \\exists g'\\in C,   \\\\ \\max_{s\\in S}|g(s)-g'(s)| \\leq \\gamma \\end{array} \\Bigg\\}\n\\end{equation*}\nas the set of $\\gamma$-covers of $S$ by $\\mathcal{G}$. Let $\\gamma$-covering number of $\\mathcal{G}$ for any integer $m>0$ be defined as\n\\begin{equation*}\n\\mathcal{N}_\\infty(\\gamma,\\mathcal{G},m) : \\max_{S\\subset X: |S|m} \\min_{C\\in \\cov_\\gamma(\\mathcal{G},S)}|C|,\n\\end{equation*}\nwith the minimizing cover $C$ called as the minimizing $(\\gamma,m)$-cover of $\\mathcal{G}$\n\\\\\n\nNow, for the given $\\gamma$, we will first estimate the $\\gamma$-covering number of $\\mathcal{F}$, that is, $\\mathcal{N}_\\infty(\\gamma,\\mathcal{F},m)$. \n\nFor any $M\\in \\mathcal{M}$, let $H_M$ be the minimizing $(\\gamma/2,m)$-cover of\n$\\mathcal{H}_M$. \nNote that $|H_M|   \\mathcal{N}_\\infty(\\gamma/2,\\mathcal{H}_M,m) \\leq \\mathcal{N}_\\infty(\\gamma/2,\\mathcal{H},m)$ (because $MX \\subset X$). \n\nNow\nlet $\\mathcal{M}_\\epsilon$ be an $\\epsilon$-spectral cover of $\\mathcal{M}$ (that is, for every $M\\in \\mathcal{M}$, exists $M' \\in \\mathcal{M}_\\epsilon$ such that $\\sigma_{\\max}(M-M')\\leq \\epsilon$), and define \n\\begin{eqnarray*}\n\\bar{F}_\\epsilon : \\{x\\mapsto h(Mx): M \\in \\mathcal{M}_\\epsilon, h\\in \\mathcal{H}_M   \\}.\n\\end{eqnarray*}\nNote that $|\\bar F_\\epsilon| \\leq |\\mathcal{M}_\\epsilon| |H_I| \\leq \\mathcal{N}_\\infty(\\gamma/2,\\mathcal{H},m) (1+2D/\\epsilon)^{D^2}$ (c.f.\\ Lemma \\ref{lm:matrix_cover}). Observe that $\\bar F_\\epsilon$ is a \n$(\\gamma/2 + B \\lambda \\epsilon)$-cover of $\\mathcal{F}$, since (i) for any $f\\in\nF$ (formed by combining, say, $M_0 \\in \\mathcal{M}$ and $h_0\\in \\mathcal{H}$),\nexists $\\bar{f} \\in \\bar F_\\epsilon$, namely the $\\bar{f}$ formed by $\\bar{M}_0$ such\nthat $\\sigma_{\\max}(M_0 - \\bar{M}_0) \\leq \\epsilon$, and (ii) $\\bar h_0 \\in H_{\\bar M_0}$ such that $|h_0(\\bar M_0 x) - \\bar\nh_0(\\bar M_0 x)|\\leq \\gamma/2$ (for all $x \\in X$). So, (for any $x\\in X$)\n\\begin{eqnarray*}\n|f(x) - \\bar f(x)| \n&& |h_0(M_0 x) - \\bar h_0(\\bar M_0 x)|   \\\\\n&\\leq& |h_0(M_0 x) - h_0(\\bar M_0 x)|\\\\&& + |h_0(\\bar M_0 x)   - \\bar h_0(\\bar M_0 x)|   \\\\\n&\\leq& \\lambda \\|M_0 x - \\bar M_0 x\\| + \\gamma/2   \\\\\n&\\leq& \\lambda \\sigma_{\\max}(M_0 - \\bar M_0) \\|x\\| + \\gamma/2   \\\\\n&\\leq& \\lambda \\epsilon B + \\gamma/2.   \n\\end{eqnarray*} \nSo, if we pick $\\epsilon   \\min\\{\\frac{1}{2\\lambda B}, \\frac{\\gamma}{2}\\}$, it follows that\n\\begin{eqnarray*}\n\\mathcal{N}_\\infty(\\gamma,\\mathcal{F},m) \\leq |\\bar F_\\epsilon| \\leq \\mathcal{N}_\\infty(\\gamma/2,\\mathcal{H},m) (1+2D/\\epsilon)^{D^2}.\n\\end{eqnarray*}\nBy noting Lemmas \\ref{lm:ab_cover_to_fat} and \\ref{lm:ab_unif_conv_cover}, it follows that\n\\begin{align*}\n\\lefteqn{\n\\Pr_{S_m\\sim\\D} \\Big[ \\exists f \\in \\mathcal{F} : \\err(f) & \\geq \\err_\\gamma(f,S_m) + \\alpha \\Big]   \n\\\\\n&\n\\leq   4 \\Big(1+\\frac{2D}{\\epsilon}\\Big)^{\\! D^2}\\!\\! \\Big(\\frac{128m}{\\gamma^2}\\Big)^{\\fat_{\\gamma/16}(\\mathcal{H})\\ln\\big(\\frac{32 e m}{\\fat_{\\gamma/16}(\\mathcal{H})\\gamma}\\big)} e^{-\\alpha^2m/8}.\n\\end{align*}\nThe lemma follows by bounding this failure probability with at most $\\delta$.\n\\qed\n\n\\begin{lemma} \\textbf{\\emph{[$\\epsilon$-spectral coverings of $D\\times D$ matrices]}}\n\\label{lm:matrix_cover}\nLet $\\mathcal{M} : \\{M \\;|\\; M\\in \\R^{D\\times D}, \\sigma_{\\max}(M)1\\}$ be the set of matrices with unit spectral norm. \nDefine $\\mathcal{M}_\\epsilon$ as the $\\epsilon$-cover of $\\mathcal{M}$, that is, for every $M \\in \\mathcal{M}$, there exists $M' \\in \\mathcal{M}_\\epsilon$ such that $\\sigma_{\\max}(M-M') \\leq \\epsilon$.\nThen for all $\\epsilon>0$, there exists $\\mathcal{M}_\\epsilon$ such that\n$|\\mathcal{M}_\\epsilon| \\leq   \\big(1 + \\frac{2D}{\\epsilon} \\big)^{D^2}$.\n\\end{lemma}\n\\begin{proof}\nFix any $\\epsilon > 0$ and let $\\mathcal{N}_{\\epsilon/D}$ be a minimal size $(\\epsilon/D)$-cover of Euclidean unit ball $\\mathbf{B}_D$ in $\\R^D$. That is,\nfor any $v\\in \\mathbf{B}_D$, there exists $v' \\in \\mathcal{N}_{\\epsilon/D}$ such that $\\|v-v'\\|\\leq \\epsilon/D$.\nUsing standard volume arguments (see e.g.\\ proof of Lemma 5.2 of \\citet{matrix:vershynin}), we know that\n$|\\mathcal{N}_{\\epsilon/D}| \\leq \\big(1+\\frac{2D}{\\epsilon}\\big)^D$. \nDefine \n\\begin{align*}\n\\mathcal{M}_\\epsilon : \\Big\\{ M' \\; \\big| \\; M'   [v'_1 \\; \\cdots \\; v'_D] \\in \\R^{D\\times D}, v'_i \\in \\mathcal{N}_{\\epsilon/D} \\Big\\}.\n\\end{align*}\nThen $\\mathcal{M}_\\epsilon$ constitutes as an $\\epsilon$-cover of\n$\\mathcal{M}$, since for any $M   [v_1 \\cdots v_D] \\in \\mathcal{M}$ there\nexists $M'   [v'_1 \\cdots v'_D] \\in \\mathcal{M}_\\epsilon$, in particular $M'$ such that $\\|v_i - v'_i\\| \\leq \\epsilon/D$ (for all $i$). Then\n\\begin{align*}\n\\sigma_{\\max}(M-M') \\leq \\|M-M'\\|_{_F}   \\sum_i \\|v_i - v'_i\\| \\leq \\epsilon.\n\\end{align*}\nWithout loss of generality we can assume that each $M'\\in \\mathcal{M}_\\epsilon$, $\\sigma_{\\max}(M')1$.\nMoreover, by construction, $| \\mathcal{M}_\\epsilon| \\leq \\big(1+\\frac{2D}{\\epsilon}\\big)^{D^2}$.\n\\end{proof}\n\n\n\\begin{lemma} \\textbf{\\emph{[extension of Theorem 12.8 of \\citet{lt:anthony_bartlett}] }}\n\\label{lm:ab_cover_to_fat}\nLet $\\mathcal{H}$ be a set of real functions from a domain $X$ to the interval\n$[0,1]$. Let $\\gamma>0$. Then for all $m\\geq 1$,\n\\begin{eqnarray*}\n\\mathcal{N}_\\infty(\\gamma,\\mathcal{H},m) < c_0 (4m/\\gamma^2)^{\\fat_{\\gamma/4}(\\mathcal{H}) \\ln \\frac{4em}{\\fat_{\\gamma/4}(\\mathcal{H})\\gamma}}.\n\\end{eqnarray*}\nfor some universal constant $c_0$.\n\\end{lemma}\n\\begin{proof}\nTheorem 12.8 of \\citet{lt:anthony_bartlett} asserts this for $m\\geq \\fat_{\\gamma/4}(\\mathcal{H})\\geq 1 $ with $c_0   2$. Now, if $1\\leq m< \\fat_{\\gamma/4}(\\mathcal{H})$, \nfor some universal constant $c'$, we have $\\mathcal{N}_\\infty(\\gamma,\\mathcal{H},m) \\leq (c'/\\gamma)^m \\leq (c'/\\gamma)^{\\fat_{\\gamma/4}(\\mathcal{H})}$. \n\\end{proof}\n\n\\begin{lemma} \\textbf{\\emph{ [Theorem 10.1 of \\citet{lt:anthony_bartlett}] }}\n\\label{lm:ab_unif_conv_cover}\nSuppose that $\\mathcal{H}$ is a set of real-valued functions defined on domain $X$. Let $\\D$ be any probability distribution on\n$Z   X \\times \\{0,1\\}$, $0\\leq \\epsilon \\leq 1$, real $\\gamma>0$ and integer $m\\geq 1$. Then,\n\\begin{align*}\n\\Pr_{S_m\\sim\\D}   \\Big[ \\exists h \\in \\mathcal{H} : \\err(h) \\geq & \\err_\\gamma(h,S_m) + \\epsilon \\Big]   \n\\\\\n&\n\\leq 2 \\mathcal{N}_\\infty\\Big(\\frac{\\gamma}{2},\\mathcal{H},2m\\Big) e^{-\\epsilon^2m/8},\n\\end{align*}\nwhere $S_m$ is an i.i.d.\\ sample of size $m$ from $\\D$.\n\\end{lemma}\n\n\n\n\n\\subsection{Proof of Lemma \\ref{lm:hypoth_lb}}\nFor any fixed $0<\\gamma<1/8$ and the given bounded class of distributions with bound $B\\geq 1$, consider a $(1/B)$-bi-Lipschitz base hypothesis class $\\mathcal{H}$ that maps hypothesis from the domain $X$ to $[1/2-4\\gamma, 1/2+4\\gamma]$, and define \n$$\n   \\mathcal{F} : \\{x \\mapsto h(Mx): M\\in \\mathcal{M}, h\\in \\mathcal{H}\\}.\n$$\nNote that finding $M$ that minimizes $\\err_{\\hypoth}$ is equivalent to finding $f$ that minimizes error on $\\mathcal{F}$.\nUsing Lemma \\ref{lb:ab_fat_lb}, we have for any $0<\\gamma<1/2$, the sample complexity of \n$\\mathcal{F}$ is (for all $0<\\epsilon,\\delta<1/64$)\n\\begin{eqnarray}\nm \\geq \\frac{\\fat_{2\\gamma}(\\pi_{4\\gamma}(\\mathcal{F}))}{320\\epsilon^2},\n\\label{eq:lb_hypoth_lbF}\n\\end{eqnarray}\nwhere $\\pi_{4\\gamma}(\\mathcal{F})$ is the $(4\\gamma)$-\\emph{squashed} function class of $\\mathcal{F}$ (see Definition \\ref{def:squash_fxn} below).\nWe lower bound $\\fat_{2\\gamma}(\\pi_{4\\gamma}(\\mathcal{F}))$ in terms of fat-shattering dimension of $\\mathcal{H}$ to yield the lemma. \n\nTo this end\nwe shall first define the $(\\gamma,m)$-covering and packing number of a generic real-valued hypothesis class $\\mathcal{G}$.\nFor any domain $X$, real-valued hypothesis class $\\mathcal{G} \\subset [0,1]^X$, margin $\\gamma>0$, and a sample $S\\subset X$, define\n\\begin{align*}\n\\cov_\\gamma(\\mathcal{G},S) &: \\Bigg\\{C\\subset \\mathcal{G} \\Big| \\begin{array}{cc} \\forall\ng\\in\\mathcal{G}, \\exists g'\\in C,   \\\\ \\max_{s\\in S}|g(s)-g'(s)| \\leq \\gamma \\end{array} \\Bigg\\}, \\\\\n\\pak_\\gamma(\\mathcal{G},S) &: \\Bigg\\{P\\subset \\mathcal{G} \\Big| \\begin{array}{cc} \\forall\ng \\neq g' \\in P,   \\\\ \\max_{s\\in S}|g(s)-g'(s)| \\geq \\gamma \\end{array} \\Bigg\\} \n\\end{align*}\nas the set of $\\gamma$-covers (resp.\\ $\\gamma$-packings) of $S$ by $\\mathcal{G}$. Let $\\gamma$-covering number (resp.\\ $\\gamma$-packing number) of $\\mathcal{G}$ for any integer $m>0$ be defined as\n\\begin{align*}\n\\mathcal{N}_\\infty(\\gamma,\\mathcal{G},m) & : \\max_{S\\subset X: |S|m} \\min_{C\\in \\cov_\\gamma(\\mathcal{G},S)}|C|, \\\\\n\\mathcal{P}_\\infty(\\gamma,\\mathcal{G},m) & : \\max_{S\\subset X: |S|m} \\max_{P\\in \\pak_\\gamma(\\mathcal{G},S)}|P|\n\\end{align*}\nwith the minimizing cover $C$ (resp.\\ maximizing packing $P$) called as the minimizing $(\\gamma,m)$-cover (resp.\\ maximizing $(\\gamma,m)$-packing) of $\\mathcal{G}$.\n\\\\\n\n\n\nWith these definitions, we have the following (for some universal constant $c_0$).\n{\\allowdisplaybreaks\n\\begin{align}\n \\nonumber c_0   \\Big(\\frac{m}{16\\gamma^2}\\Big)^{\\fat_{2\\gamma}(\\pi_{4\\gamma}(\\mathcal{F})) \\ln(em/2\\gamma)} & \\\\\n& \n\\nonumber \\geq \\mathcal{N}_\\infty(8\\gamma, \\pi_{4\\gamma}(\\mathcal{F}),m) & \\textrm{[Lemma \\ref{lm:ab_cover_to_fat}]}\\\\\n& \\nonumber \\geq \\mathcal{P}_\\infty(16\\gamma, \\pi_{4\\gamma}(\\mathcal{F}),m) & \\textrm{[Lemma \\ref{lm:ab_covering_packing}]}\\\\\n& \\nonumber \\geq \\Big(\\frac{1}{32\\gamma}\\Big)^{D^2} \\mathcal{P}_\\infty(48\\gamma, \\pi_{4\\gamma}(\\mathcal{H}),m)   & \\textrm{[see (*) below]}\\\\\n& \\nonumber   \\Big(\\frac{1}{32\\gamma}\\Big)^{D^2} \\mathcal{P}_\\infty(48\\gamma, \\mathcal{H},m)   & \\textrm{[by the choice of $\\mathcal{H}$]}\\\\\n& \\nonumber \\geq \\Big(\\frac{1}{32\\gamma}\\Big)^{D^2} \\mathcal{N}_\\infty(48\\gamma, \\mathcal{H},m) & \\textrm{[Lemma \\ref{lm:ab_covering_packing}]}\\\\\n& \\geq \\Big(\\frac{1}{32\\gamma}\\Big)^{D^2} e^{\\fat_{768\\gamma}(\\mathcal{H})/8}. & \\textrm{[Lemma \\ref{lm:ab_cover_to_fat_lb}]}\n\\end{array}\n\\label{eq:hypoth_fat_lb}\n\\end{align}\n}\nwhere $\\mathcal{N}_\\infty(\\gamma,\\mathcal{G},m)$ and $\\mathcal{P}_\\infty(\\gamma,\\mathcal{G},m)$ are the\n$\\gamma$-covering and -packing numbers of $\\mathcal{G}$ for $m$ samples.\n\\\\\n\n\\noindent (*) We show that $\\mathcal{P}_\\infty(16\\gamma, \\pi_{4\\gamma}(\\mathcal{F}),m) \\geq (1/32\\gamma)^{D^2}\n\\mathcal{P}_\\infty(48\\gamma, \\pi_{4\\gamma}(\\mathcal{H}),m) $, by exhibiting a\nset $\\mathcal{S}\\subset\\pi_{4\\gamma}(\\mathcal{F})$ of size $ (1/32\\gamma)^{D^2}\n\\mathcal{P}_\\infty(48\\gamma, \\pi_{4\\gamma}(\\mathcal{H}),m)$ that is a\n$(16\\gamma)$-packing of $\\pi_{4\\gamma}(\\mathcal{F})$.\n\nLet $\\pi_{4\\gamma}(\\mathcal{H}_{48\\gamma}) \\subset \\pi_{4\\gamma}(\\mathcal{H})$ be a maximal $(32\\gamma)$-packing of\n$\\pi_{4\\gamma}(\\mathcal{H})$ (that is, a maximal set such that for all distinct $(\\pi_{4\\gamma}\\circ h),(\\pi_{4\\gamma} \\circ h')\\in\n\\pi_{4\\gamma}(\\mathcal{H}_{48\\gamma})$, exists $x\\in X$ such that $|\\pi_{4\\gamma}(h(x)) - \\pi_{4\\gamma}(h'(x))| \\geq\n48\\gamma$). Fix $\\epsilon$ (exact value determined later), and define \n$$\\mathcal{S}_\\epsilon : \\Bigg\\{x\\mapsto (\\pi_{4\\gamma}\\circ h)(Mx) \\; \\Big| \\;\n\\begin{array}{cc} (\\pi_{4\\gamma}\\circ h)\\in\\pi_{4\\gamma}(\\mathcal{H}_{48\\gamma}), \\\\ M\\in \\mathcal{M}_\\epsilon \\end{array} \\Bigg\\},$$\nwhere $\\mathcal{M}_\\epsilon$ is a $\\epsilon$-spectral net of $\\mathcal{M}$, that is, for all $M \\in \\mathcal{M}$, exists $M' \\in \\mathcal{M}_\\epsilon$ such\nthat $\\sigma_{\\max}(M-M') \\leq \\epsilon$, and for all distinct $M', M'' \\in \\mathcal{M}_\\epsilon$, $\\sigma_{\\max}(M'-M'') \\geq \\epsilon/2$.\n\n Then for any two distinct\n$f,f' \\in \\mathcal{S}_\\epsilon$, such that $f(x)   (\\pi_{4\\gamma}\\circ h)(Mx)$ and $f'(x)   (\\pi_{4\\gamma}\\circ h')(M'x)$, we have\n\\begin{itemize}\n\\item (case 1) $h$ and $h'$ are distinct. In this case, there exists $x\\in X$, s.t.\\\n\\begin{align*}\n|f(x) - f'(x)|   & |\\pi_{4\\gamma}(h(Mx)) - \\pi_{4\\gamma}(h'(M'x))|\\\\\n\\geq & \\; |\\pi_{4\\gamma}(h(Mx)) - \\pi_{4\\gamma}(h'(Mx))| \\\\ &-   |\\pi_{4\\gamma}(h'(Mx)) - \\pi_{4\\gamma}(h'(M'x))|\\\\\n\\geq &\\; 48\\gamma - (1/B) \\sigma_{\\max}(M-M') \\|x\\| \\\\\n\\geq &\\; 48\\gamma - (1/B) \\epsilon B   48\\gamma - \\epsilon .\n\\end{align*}\n\n\\item (case 2) $h$, $h'$ same but $M$ and $M'$ distinct. In this case, there exists $x$ (with $\\|x\\|1$) s.t.\\\n\\begin{eqnarray*}\n|f(x) - f'(x)| &&   |\\pi_{4\\gamma}(h(Mx)) - \\pi_{4\\gamma}(h(M'x))|\\\\\n&&   |h(Mx) - h(M'x)| \\\\\n&\\geq& B \\|(M - M')x\\| \\\\\n&\\geq& B \\cdot \\min_{M\\neq M' \\in \\mathcal{M}_\\epsilon} \\sigma_{\\max} (M - M') \\\\\n&\\geq& B (\\epsilon/2).   \\epsilon. \n\\end{eqnarray*}\n\\end{itemize}\nThus, by setting $\\epsilon   32\\gamma$, distinct classifiers $f,f' \\in\n\\mathcal{S}_{32\\gamma}$ are at least $16\\gamma$ apart (since $B\\geq 1$). Hence $\\mathcal{S}_{32\\gamma}$ forms a $(16\\gamma)$-packing of\n$\\pi_{4\\gamma}(\\mathcal{F})$. Therefore, the packing number\n\\begin{align*}\n\\mathcal{P}_\\infty(16\\gamma,\\pi_{4\\gamma}(\\mathcal{F}),m) &\\geq\n|\\mathcal{S}_{32\\gamma}|   |\\mathcal{M}_{32\\gamma}| |\\mathcal{H}_{48\\gamma}| \n\\\\\n&\n\\geq\n(1/32\\gamma)^{D^2}\\mathcal{P}_\\infty(48\\gamma,\\pi_{4\\gamma}(\\mathcal{H}),m). \n\\end{align*}\n\n\\noindent Thus, from Eq.\\ \\eqref{eq:hypoth_fat_lb}, it follows that \n$$\\fat_{2\\gamma}(\\pi_{4\\gamma}(\\mathcal{F})) \\geq \\Omega\\Big( \\frac{D^2 \\ln(1/\\gamma) + \\fat_{768\\gamma}(\\mathcal{H})}{\\ln(m/\\gamma^2)\\ln(m/\\gamma)}\\Big).$$\nCombining this with Eq.\\ \\eqref{eq:lb_hypoth_lbF}, the lemma follows.\n\\qed\n\n\\begin{lemma} \\textbf{\\emph{[$\\epsilon$-spectral packings of $D\\times D$ matrices]}}\n\\label{lm:matrix_packing}\nLet $\\mathcal{M} : \\{M \\;|\\; M\\in \\R^{D\\times D}, \\sigma_{\\max}(M)1\\}$ be the set of matrices with unit spectral norm. \nDefine $\\mathcal{M}_\\epsilon \\subset \\mathcal{M}$ as the $\\epsilon$-packing of\n$\\mathcal{M}$, that is, for every distinct $M,M' \\in \\mathcal{M}_\\epsilon$,\n$\\sigma_{\\max}(M-M') \\geq \\epsilon$.\nThen for all $\\epsilon>0$, there exists $\\mathcal{M}_\\epsilon$ such that\n$|\\mathcal{M}_\\epsilon| \\geq   \\big(\\frac{1}{2\\epsilon} \\big)^{D^2}$.\n\\end{lemma}\n\\begin{proof}\nFix any $\\epsilon > 0$ and let $\\mathcal{P}_{\\epsilon}$ be a maximal size $\\epsilon$-packing of Euclidean unit ball $\\mathbf{B}_D$ in $\\R^D$. That is,\nfor all distinct $v, v'\\in \\mathbf{B}_D$, $\\|v-v'\\|\\geq \\epsilon$.\nUsing standard volume arguments (see e.g.\\ proof of Lemma 5.2 of \\citet{matrix:vershynin}), we know that\n$|\\mathcal{P}_{\\epsilon}| \\geq \\big(\\frac{1}{2\\epsilon}\\big)^D$. \nDefine \n\\begin{align*}\n\\mathcal{M}_\\epsilon : \\Big\\{ M' \\; \\big| \\; M'   [v'_1 \\; \\cdots \\; v'_D] \\in \\R^{D\\times D}, v'_i \\in \\mathcal{P}_{\\epsilon} \\Big\\}.\n\\end{align*}\nThen $\\mathcal{M}_\\epsilon$ constitutes as an $\\epsilon$-packing of\n$\\mathcal{M}$, since for any distinct $M,M'\\in \\mathcal{M}_\\epsilon$ such that $M   [v_1 \\cdots v_D]$ and $ M'   [v'_1 \\cdots v'_D]$, we have\n\\begin{align*}\n\\sigma_{\\max}(M-M') \\geq \\max_{i} \\|v_i -v'_i \\| \\geq \\epsilon.\n\\end{align*}\nWithout loss of generality we can assume that each $M\\in \\mathcal{M}_\\epsilon$, $\\sigma_{\\max}(M)1$.\nMoreover, by construction, $| \\mathcal{M}_\\epsilon| \\geq \\big(\\frac{1}{2\\epsilon}\\big)^{D^2}$.\n\\end{proof}\n\n\n\n\n\\begin{lemma} \\textbf{\\emph{[follows from Theorem 12.1 of \\citet{lt:anthony_bartlett}] }}\n\\label{lm:ab_covering_packing}\nFor any real valued hypothesis class $\\mathcal{H}$ into $[0,1]$, all $m\\geq 1$, and $0<\\gamma<1/2$, \n$$\n\\mathcal{P}_\\infty(2\\gamma,\\mathcal{H},m) \\leq \\mathcal{N}_\\infty(\\gamma,\\mathcal{H},m)\\leq \\mathcal{P}_\\infty(\\gamma,\\mathcal{H},m).\n$$\n\\end{lemma}\n\n\\begin{lemma} \\textbf{\\emph{[Theorem 12.10 of \\citet{lt:anthony_bartlett}]}}\n\\label{lm:ab_cover_to_fat_lb}\nLet $\\mathcal{H}$ be a set of real functions from a domain $X$ to the interval\n$[0,1]$. Let $\\gamma>0$. Then for $m\\geq \\fat_{16\\gamma}(\\mathcal{H})$,\n\\begin{eqnarray*}\n\\mathcal{N}_\\infty(\\gamma,\\mathcal{H},m) \\geq e^{\\fat_{16\\gamma}(\\mathcal{H})/8}.\n\\end{eqnarray*}\n\\end{lemma}\n\n\n\\begin{lemma}\\textbf{\\emph{[Theorem 13.5 of \\citet{lt:anthony_bartlett}]}}\n\\label{lb:ab_fat_lb}\nSuppose that $\\mathcal{H}$ is a set of real-valued functions mapping into the interval $[0,1]$ that is closed under addition of\nconstants, that is, \n$$h \\in \\mathcal{H} \\implies h' \\in \\mathcal{H}, \\textrm{ where }   h':x\\rightarrow h(x)+c \\;\\;\\; \\textrm{ for all $c$.} $$\nPick any $0<\\gamma<1/2$.\n Then for any metric learning algorithm $\\mathcal{A}$ for\nall $0<\\epsilon,\\delta<1/64$, there exists a distribution $\\D$ such\nthat if\n$m\\leq\\frac{d}{320\\epsilon^2}$, then\n$$\n\\Pr_{S_m\\sim\\D} [ \\err(h^*,\\D) > \\err_\\gamma(\\mathcal{A}(S_m),\\D) + \\epsilon] >\\delta\n$$\nwhere $d : \\fat_{2\\gamma}(\\pi_{4\\gamma}(\\mathcal{H})) \\geq 1 $ is the fat-shattering\ndimension of $\\pi_{4\\gamma}(\\mathcal{H})$---the $(4\\gamma)$-\\emph{squashed} function class of $\\mathcal{H}$, see Definition \\ref{def:squash_fxn} below---at margin $2\\gamma$.\n\\end{lemma}\n\n\\begin{definition}   \\textbf{\\emph{[squashing function]}}\n\\label{def:squash_fxn}\nFor any $0<\\gamma<1/2$, define the \\emph{squashing function} $\\pi_\\gamma:\\R \\rightarrow [1/2-\\gamma,1/2+\\gamma]$ as\n$$\n\\pi_\\gamma(\\alpha)   \\Bigg\\{ \n   \\begin{array}{ll} \n   1/2+\\gamma & \\textrm{if $\\alpha\\geq 1/2+\\gamma$} \\\\ \n   1/2-\\gamma & \\textrm{if $\\alpha\\leq 1/2-\\gamma$} \\\\ \n   \\alpha & \\textrm{otherwise} \\\\ \n   \\end{array} .\n$$\nMoreover, for a collection $F$ of functions into $\\R$, define $\\pi_\\gamma(F) : \\{ \\pi_\\gamma \\circ f \\; | \\; f\\in F\\}$. \n\\end{definition}\n\n\n\n\\subsection{Proof of Lemma \\ref{lm:unif_conv_dist_refi}}\n\nLet $\\mathcal{P}$ be the probability measure induced by the random variable\n$(\\mathbf{X}, Y)$, where $\\mathbf{X}: (x,x')$, $Y:\\indicate[yy']$, st. $((x,y),(x',y')) \\sim (\\D \\times \\D)$.\n\nDefine function class \n\\begin{align*}\n& \\mathcal{F} : \n\\\\\n& \n\\Bigg\\{ f_M \\!: \\mathbf{X} \\mapsto \\|M(x-x')\\|^2 \\Bigg|\\!   \\begin{array}{c} M \\in \\mathcal{M} \\\\ \\mathbf{X}   (x,x') \\in (X \\times X) \\end{array} \\!\\! \\Bigg\\},\n\\end{align*}\n\nFollowing the steps of proof of Lemma \\ref{lm:unif_conv_all}, we can conclude that the Rademacher complexity of $\\mathcal{F}$ is bounded. In particular,\n\\begin{align*}\n\\mathcal{R}_m(\\mathcal{F}) \\leq 4B^2 \\sqrt{ \\frac{ \\sup_{M \\in \\mathcal{M}} \\|M^\\mathsf{T}M\\|^2_{_F}   } {m}}.\n\\end{align*}\nThe result follows by noting that $\\phi$ is $\\lambda$-Lipschitz in the first argument and by applying Lemma \\ref{lm:rad_complexities_unif_bound}.\n\\qed\n\n   Let $\\mathcal{P}$ be the probability measure induced by the random variable\n   $(\\mathbf{X}, Y)$, where $\\mathbf{X}: (x,x')$, $Y:\\indicate[yy']$, st. $((x,y),((x',y')) \\sim (\\D \\times \\D)$.\n   \n   Define function class \n   \\begin{align*}\n   \\mathcal{F} : \\Bigg\\{ f_M : \\mathbf{X} \\mapsto \\|M(x-x')\\|^2 \\Bigg| \\begin{array}{c} M \\in \\mathcal{M} \\\\ \\mathbf{X}   (x,x') \\in (\\D|_X \\times \\D|_X) \\end{array} \\Bigg\\},\n   \\end{align*}\n   and consider the loss function\n   \\begin{align*}\n   \\phi_{L,U}^\\lambda (\\hat{Y},Y) : \n   \\Bigg\\{   \\begin{array}{ll} \\min\\{1,\\lambda[\\hat{Y}-U]_+\\} & \\textrm{if $Y1$} \\\\\n   \\min\\{1,\\lambda[L-\\hat{Y}]_+\\} & \\textrm{otherwise} \n   \\end{array}.\n   \\end{align*}\n   Then, we are interested in bounding the quantity \n   \\begin{align*}\n   \\sup_{f_M \\in \\mathcal{F}} \\E_{(\\mathbf{X},Y)\\sim \\mathcal{P}} [\\phi^\\lambda_{L,U}(f_M(\\mathbf{X}), Y)] - \\frac{1}{m} \\sum_{i1}^m \\phi^\\lambda_{L,U}(f_M(\\mathbf{X}_i),Y_i),\n   \\end{align*}\n   where $\\mathbf{X}_i:(x_{1,i},x_{2,i}) $, $Y_i:\\indicate[y_{1,i}y_{2,i}]$\n   from the paired sample $S_m   \\{((x_{1,i},y_{1,i}),(x_{2,i},y_{2,i}))\n   \\}_{i1}^m$. Define $\\bar{x}_i : x_{1,i} - x_{2,i}$ for each $ \\mathbf{X}_i(x_{1,i},x_{2,i})$.\n   \n   Observe that our loss function $\\phi^\\lambda_{L,U}$ is $\\lambda$-Lipschitz in the first argument, and\n   the Rademacher complexity of our function class $\\mathcal{F}$ is bounded, since (let $\\sigma_1,\\ldots, \\sigma_m$ denote\n   independent uniform $\\{\\pm 1\\}$-valued random variables)\n   \\begin{align*}\n   &\\mathcal{R}_m(\\mathcal{F}) \n   : \\E_{\\substack{\\mathbf{X}_i,\\sigma_i \\\\ i\\in[m]}} \\Bigg[ \\sup_{f_M \\in \\mathcal{F}} \\frac{1}{m} \\sum_{i1}^m \\sigma_i f_M(\\mathbf{X}_i) \\Bigg]   \\\\\n   & \\frac{1}{m} \\E_{\\substack{\\mathbf{X}_i,\\sigma_i \\\\ i\\in[m]}} \\sup_{M \\in \\mathcal{M}} \\Big[ \\sum_{i1}^m \\sigma_i \\bar{x}_i^\\mathsf{T} M^\\mathsf{T}M \\bar{x}_i   \\Big] \\\\\n   & \\frac{1}{m} \\E_{\\substack{\\mathbf{X}_i,\\sigma_i \\\\ i\\in[m]}} \\sup_{\\substack{M \\in \\mathcal{M}, \\textrm{ s.t.} \\\\ [a^{jk}]_{jk} : M^\\mathsf{T}M }} \\Bigg[ \\sum_{j,k} a^{jk} \\sum_{i1}^m \\sigma_i \\bar{x}_i^j \\bar{x}_i^k \\Bigg] \\\\\n   &\\leq \\frac{1}{m} \\E_{\\substack{\\mathbf{X}_i,\\sigma_i \\\\ i\\in[m]}} \\sup_{M \\in \\mathcal{M}} \\Bigg[ \\|M^\\mathsf{T}M\\|_F \\Bigg( \\sum_{j,k} \\Big(\\sum_{i1}^m \\sigma_i \\bar{x}_i^j \\bar{x}_i^k \\Big)^2 \\Bigg)^{1/2} \\Bigg] \\\\\n   &\\leq \\frac{d}{m} \\E_{\\substack{\\mathbf{X}_i,   i\\in[m]}} \\Bigg( \\E_{\\substack{\\sigma_i,i\\in[m]}}   \\sum_{j,k} \\Big(\\sum_{i1}^m \\sigma_i \\bar{x}_i^j \\bar{x}_i^k \\Big)^2 \\Bigg)^{1/2} \\\\\n   & \\frac{d}{m} \\E_{\\substack{\\mathbf{X}_i,   i\\in[m]}} \\Bigg( \\sum_{j,k} \\sum_{i1}^m \\Big(\\bar{x}_i^j \\Big)^2 \\Big(\\bar{x}_i^k \\Big)^2 \\Bigg)^{1/2} \\\\\n   & \\frac{d}{m} \\E_{\\substack{\\mathbf{X}_i,   i\\in[m]}} \\Bigg( \\sum_{i1}^m \\|\\bar{x}_i\\|^4 \\Bigg)^{1/2} \\\\\n   & \\frac{d}{m} \\E_{\\substack{ (x_i, x'_i) \\sim (\\D|_X \\times \\D|_X) ,\\\\   i\\in[m]}} \\Bigg( \\sum_{i1}^m \\|x_i - x'_i\\|^4 \\Bigg)^{1/2} \\\\\n   &\\leq \\frac{d}{\\sqrt{m}} \\Bigg (\\E_{\\substack{ (x, x') \\sim (\\D|_X \\times \\D|_X)}} \\|x - x'\\|^4 \\Bigg)^{1/2} \\\\\n   &\\leq \\frac{4dB^2}{\\sqrt{m}}. \n   &   \\\\\n   &\\leq \\frac{1}{m} \\Bigg( \\E_{\\substack{\\mathbf{X}_i,\\sigma_i \\\\ i\\in[m]}} \\Bigg[ \\sup_{f_M \\in \\mathcal{F}} \\Big[ \\sum_{i1}^m \\sigma_i f_M(\\mathbf{X}_i)\\Big]^2 \\Bigg] \\Bigg)^{1/2}   \\\\\n   &   \\frac{1}{m} \\Bigg( \\E_{\\mathbf{X}_i, i\\in[m]} \\sup_{f_M \\in \\mathcal{F}} \\sum_{i1}^m f^2_M(\\mathbf{X}_i) \\Bigg)^{1/2} \\\\\n   &\\leq \\frac{1}{\\sqrt{m}} \\Bigg( \\E_{\\substack{x_{1,i},x_{2,i},\\\\i\\in[m]}} \\Big\\|\\bar{M}(x_{1,i} - x_{2,i} ) \\Big\\|^2 \\Bigg)^{1/2}\\\\\n   & \\frac{1}{\\sqrt{m}} \\Bigg(2 \\tr \\big( (\\bar{M}^{\\mathsf{T}}\\bar{M}) \\; \\E_x(x x\\T) \\big)   \\Bigg)^{1/2}   \\sqrt{\\frac{2d}{m}}.\n   & \\Big( \\E_{\\sigma_1,\\ldots, \\sigma_m} \\Big[ \\Big| \\frac{1}{m} \\sum_{i1}^m \\sigma_i f_\\lambda(z_i,z_i; M) \\Big| \\Big]   \\Big)^2 \\\\\n   & \\leq \\lambda^2 \\sigma^2_{\\max}(M) \\Big(   \\E_{\\sigma_1,\\ldots, \\sigma_m} \\Big[ \\frac{1}{m} \\sum_{i1}^m \\sigma_i \\|[x_i x'_i] \\| \\Big] \\Big)^2 \\\\\n   & \\leq \\lambda^2 \\sigma^2_{\\max}(M) \\cdot   \\E_{\\sigma_1,\\ldots, \\sigma_m} \\Big[ \\frac{1}{m} \\sum_{i1}^m \\sigma_i \\|[x_i x'_i] \\| \\Big]^2 \\\\\n   & \\leq \\frac{ \\lambda^2 \\sigma^2_{\\max}(M) }{m^2}   \\cdot   \\E_{\\sigma_1,\\ldots, \\sigma_m} \\Big[ \\sum_{i1}^m \\sigma_i \\|[x_i x'_i] \\|^2 \\Big] \n   \\end{align*}\n   \n   We can thus apply Lemma \\ref{lm:rad_complexities_unif_bound} and get the desired \n   uniform deviation bound.\n   \n   \n   \n   \\begin{lemma}\n   $f_\\lambda(z_1,z_2;M)$ is $(\\lambda\\sigma_{\\max}(M))$-Lipschitz with respect to $L_2$-norm. \n   \\end{lemma}\n   \\begin{proof}\n   hello\n   \\end{proof}\n\n\n\\subsection{Proof of Lemma \\ref{lm:unif_conv_clf_refi}}\n\nConsider the function class \n\\begin{align*}\n\\mathcal{F} : \\Big\\{ f_{v,M} : x \\mapsto v \\cdot Mx \\; \\big| \\; \\|v\\|_1 \\leq 1, M \\in \\mathcal{M} \\Big\\},\n\\end{align*}\nand define the composition class\n\\begin{align*}\n\\mathcal{F}_\\sigma : \\Bigg\\{ x \\mapsto \\sum_{i1}^K w_i \\sigma^\\gamma(f_i(x)) \\; \\Big| \\;\\begin{array}{c} \\|w_i\\|_1 \\leq 1, \\\\ f_1,\\ldots, f_K \\in \\mathcal{F} \\end{array} \\Bigg\\}.\n\\end{align*}\n\n\nThen, first note that the Gaussian complexity of $\\mathcal{F}$ (with respect to the distribution $\\mathcal{D}$) is bounded, since (let $g_1,\\ldots,g_m$ denote independent standard Gaussian random variables)\n\\begin{align*}\n\\mathcal{G}_m(\\mathcal{F}, \\mathcal{D}) & :   \\E_{\\substack{ x_i \\sim \\D|_X \\\\g_i, i\\in[m]}} \\Bigg[ \\sup_{f_{v,M}\\in \\mathcal{F}} \\frac{1}{m} \\sum_{i1}^m g_i f_{v,M}(x_i) \\Bigg] \\\\\n&\\frac{1}{m} \\E_{\\substack{x_i \\sim \\D|_X \\\\ g_i, i\\in[m]}}   \\Bigg[ \\sup_{\\substack{M\\in\\mathcal{M} \\\\ \\|v\\|_1\\leq 1}} v \\cdot \\sum_{i1}^m g_i (Mx_i) \\Bigg] \\\\\n&\\frac{1}{m} \\E_{\\substack{x_i \\sim \\D|_X \\\\ g_i, i\\in[m]}} \\Bigg[ \\max_j \\sup_{M\\in\\mathcal{M}} \\sum_{i1}^m g_i (Mx_i)_j \\Bigg] \\\\\n&\\leq \\frac{1}{m} \\E_{\\substack{x_i \\sim \\D|_X \\\\ g_i, i\\in[m]}} \\max_{j\\in[D]} \\Bigg[ \\sum_{i1}^m g_i \\sup_{M\\in\\mathcal{M}} \\big|(Mx_i)_j\\big| \\Bigg] \\\\\n&\\leq \\frac{c \\ln^{\\frac{1}{2}}(D)}{m} \\E_{x_i \\sim \\D_X} \\max_{j,j'\\in[D]} \\Bigg( \\E_{g_i } \\Bigg[ \\sum_{i1}^m g_i \\Big( \\sup_{M\\in\\mathcal{M}} \\big|(Mx_i)_j\\big| \n\\\\\n& \\hspace{1.8in} \n- \\sup_{M'\\in\\mathcal{M}} \\big|(M'x_i)_{j'}\\big| \\Big) \\Bigg]^2 \\Bigg)^{\\frac{1}{2}} \n\\\\\n& \\frac{c \\ln^{\\frac{1}{2}}(D)}{m} {\\E_{x_i \\sim \\D_X}} \\max_{j,j'\\in[D]} \\Bigg( \\sum_{i1}^m \\Big[   \\sup_{M\\in\\mathcal{M}} \\big|(Mx_i)_j\\big| \n\\\\\n& \\hspace{1.8in} \n- \\sup_{M'\\in\\mathcal{M}} \\big|(M'x_i)_{j'}\\big| \\Big]^2 \\Bigg)^{\\frac{1}{2}} \\\\\n&\\leq   c'B \\sqrt{\\frac{d\\ln{D}}{m}},\n\\end{align*}\nwhere (i) second to last inequality is by applying Lemma \\ref{lm:bartlett_mendelson_slepian_lemma}, (ii) $c,c'$ are absolute constants, (iii) $d : \\sup_{M\\in \\mathcal{M}} \\| M^\\mathsf{T}M \\|^2_{_F}$. Note that bounding the Gaussian complexity also bounds the Rademacher\ncomplexity by Lemma \\ref{lm:bartlett_mendelson_relate_rag_gauss}.\n\nFinally by noting that $\\mathcal{F}_\\sigma$ is a $\\gamma$-Lipschitz composition class of $\\mathcal{F}$ and $\\phi^\\lambda$ is a classification based loss function that is $\\lambda$-Lipschitz in the first argument, we can apply Lemma \\ref{lm:rad_complexities_unif_bound} yielding the desired result.\n\\qed\n\n\\begin{lemma} \\emph{\\textbf{[Lemma 20 of \\citet{lt:bartlett_mendelson_radgauss_complexities}]}}\n\\label{lm:bartlett_mendelson_slepian_lemma}\nLet $Z_1,\\ldots,Z_D$ be random variables such that each $Z_j   \\sum_{i1}^m a_{ij} g_i$, where each $g_i$ is independent $N(0,1)$\nrandom variables. Then there is an absolute constant $c$ such that \n\\begin{align*}\n\\E_{g_i} \\max_j Z_j \\leq c\\ln^{\\frac{1}{2}}(D) \\max_{j,j'} \\sqrt{\\E_{g_i}(Z_j - Z_{j'})^2}.\n\\end{align*}\n\\end{lemma}\n\n\\begin{lemma} \\emph{\\textbf{[Lemma 4 of \\citet{lt:bartlett_mendelson_radgauss_complexities}]}}\n\\label{lm:bartlett_mendelson_relate_rag_gauss}\nThere are absolute constants $c$ and $C$ such that for every class $\\mathcal{F}$ and every integer $m$\n\\begin{align*}\nc \\mathcal{R}_m(\\mathcal{F},\\mathcal{D}) \\;\\leq\\; \\mathcal{G}_m(\\mathcal{F},\\mathcal{D}) \\;\\leq\\; C \\ln(m)   \\mathcal{R}_m(\\mathcal{F},\\mathcal{D}),\n\\end{align*}\nwhere $\\mathcal{R}$ and $\\mathcal{G}$ are Rademacher and Gaussian complexities of a function class $\\mathcal{F}$ with respect to the distribution $\\mathcal{D}$ respectively. \n\\end{lemma}\n\n\\subsection{Proof of Corollary \\ref{cor:unif_conv_refined}}\nThe conclusion of Eq.\\ \\eqref{eq:main_unif_refind_bound} is immediate by\ndividing the given failure probability $\\delta$ across the sequence\n$\\mathcal{M}^1,\\mathcal{M}^2,\\cdots$ such that $\\delta \\mu_d$ failure\nprobability is associated with class $\\mathcal{M}^d$, then apply Lemma\n\\ref{lm:unif_conv_dist_refi} (for distance based metric learning) or Lemma\n\\ref{lm:unif_conv_clf_refi} (for classifier based metric learning) to each\nclass $\\mathcal{M}^d$ individually, and finally combining the individual deviations together with a union\nbound. \n\\\\\n\nFor the second part, consider \nthe (regularized) minimizing weighting metric\n\\begin{align}\nM_m : \\argmin_{M\\in \\mathcal{M}}   \\Big[ \\err^\\lambda(M,S_m) + \\Lambda \\|M^\\mathsf{T}M\\|_{_F} \\Big],\n\\label{eq:regualized_argminimizer}\n\\end{align}\nwhere $\\Lambda : C B \\lambda \\sqrt{{\\ln(1/\\delta\\mu_d)}/{m}}$. \n\nNow, if $\\err^\\lambda(M_m,\\D) - \\err^\\lambda(M^*,\\D) \\leq \\epsilon$, then we\nare done. So assume (towards contradiction) that\n $\\err^\\lambda(M_m,\\D) - \\err^\\lambda(M^*,\\D) > \\epsilon$. Then since there exists $M'\\in \\mathcal{M}$ (viz.\\ $M'M^*$) such that with probability at least $1-\\delta$, $\\err^\\lambda(M',\\D) - \\err^\\lambda(M',S_m) \\leq d \\Lambda$ (cf.\\ Eq.\\ \\ref{eq:main_unif_refind_bound}). Therefore,   $\\err^\\lambda(M',\\D)\\leq \\err^\\lambda(M',S_m) + d \\Lambda $\n\nFor the second part we have the following.\nFor the given $0<\\delta<1$ and $m\\geq 1$, \nfix any $M' \\in \\mathcal{M}$ such\nthat $\\err^\\lambda( M', \\D) \\leq \\err^\\lambda(M^*,\\D) +\n\\sqrt{\\frac{\\ln(2/\\delta)}{2m}}$. Then, with probability at least $1-\\delta$,\nwe have\n\\begin{align*}\n\\err^\\lambda&(M_m,\\D) - \\err^\\lambda(M^*,\\D) \\\\\n&\\leq   \\err^\\lambda(M_m, \\D)   - \\err^\\lambda(M_m,S_m)   \\\\\n&\\;\\;\\; +   \\err^\\lambda(M_m, S_m) + \\Lambda_{_{M_m}} \\| M_m^\\mathsf{T} M_m\\|_{_F} \\\\\n&\\;\\;\\; - \\err^\\lambda(M',S_m) - \\Lambda_{_{M'}}   \\| {M'}^\\mathsf{T} M'\\|_{_F}   \\\\\n&\\;\\;\\; +   \\err^\\lambda(M', S_m)   + \\Lambda_{_{M'}}   \\| {M'}^\\mathsf{T} M'\\|_{_F}   - \\err^\\lambda(M',\\D)   \\\\\n&\\;\\;\\; +   \\err^\\lambda(M', \\D)   - \\err^\\lambda(M^*,\\D)   \\\\\n&\\leq\n\\end{align*}\n\nFor the second part, for any $M\\in \\mathcal{M}$ define $d_M$ and $\\Lambda_M$ as per the lemma statement. Then with probability at least $1-\\delta$\n\\begin{align*}\n\\err^\\lambda( M_m^{\\reg},\\D)   - \\err^\\lambda(M^*,\\D) \n\\\\\n& \\leq \\;\\; \\err^\\lambda(M_m^{\\reg},S_m) + d_{_{M_m^{\\reg}}}\\Lambda_{_{M_m^{\\reg}}} - \\err^\\lambda(M^*,\\D) \\\\\n& \\leq \\;\\; \\err^\\lambda(M^*,S_m) + d_{_{M^*}}\\Lambda_{_{M^*}} - \\err^\\lambda(M^*,\\D) \\\\\n& \\leq \\;\\; O(d_{_{M^*}}\\Lambda_{_{M^*}}) \\;\\;   \\;\\; O(\\epsilon), \n\\end{align*}\nwhere (i) the first inequality is by applying Eq.\\\n\\eqref{eq:main_unif_refind_bound} on weighting metric $M_m^{\\reg}$ (with\nfailure probability set to $\\delta/2$), (ii) the second inequality is by noting\nthat $M_m^{\\reg}$ is the (regularized) sample error minimizer as per the lemma\nstatement, (iii) the third inequality is by applying Eq.\\\n\\eqref{eq:main_unif_refind_bound} on weighting metric $M^*$ (with failure\nprobability set to $\\delta/2$), and (iv) the last equality by noting the\ndefinitions of $\\Lambda_{M^*}$ and our choice of $m$.\n\\qed\n\n\\section{Appendix: Data Generation Process}\n\n\\subsection{Creating Synthetic \\alg{Simplex} Dataset}\n\\label{appendix:synth_simplex_data_gen}\nFor any ambient dimension $D$ and intrinsic dimension $d$ ($d<D$),\nLet $\\{v_0,\\ldots,v_D\\}\\subset\\R^D$ denote the vertices of a regular $D$-simplex centered at origin and circumscribed \nin a unit sphere. That is:\n\\begin{align*}\nv_{0,j} & \\frac{-1}{\\sqrt{D}}   &\\textrm{for $1\\leq j \\leq D$} \\\\\nv_{i,j} & \\Bigg\\{ \\begin{array}{ll} \\frac{(D-1)\\sqrt{D+1}+1}{D\\sqrt{D}} & \\textrm{if $ij$} \\\\ \\frac{-(\\sqrt{D+1} -1)}{D\\sqrt{D}} & \\textrm{otherwise} \\end{array} &\\textrm{for $1\\leq i,j \\leq D$}\n\\end{align*}\nDefine the \\emph{normalized} vertices $\\{\\bar v_0,\\ldots,\\bar v_D\\}$ as $\\bar v_i : \\frac{v_i}{\\|v_0 - v_1\\|}$, that constitutes vertices of a regular $D$-simplex with unit-side length.\n\n\n\n\n\\subsection{Adding Correlated Noise to a Dataset}\n\\label{appendix:synth_noise_gen}\n\nGiven a dataset with fixed dimension, we can embed it in higher dimensions by appending each sample $x_i$ with $D$-dimensional \ncorrelated noise as follows. \n\\begin{enumerate}\n\\item Sample a covariance matrix $\\Sigma_D$ from unit scale Wishart distribution. That is, let $A$ be a $D\\times D$ matrix with\nentry $A_{ij} \\sim N(0,1)$ drawn i.i.d. Define $\\Sigma_D : A^\\mathsf{T}A$.\n\\item For each sample $x_i$, \\\\\n$$\nx_i^{\\textup{new}} : \\binom{x_i}{x_\\sigma}, \\hspace{0.5in} x_\\sigma \\sim N(0,\\Sigma_D),\n$$\nwhere $x_\\sigma$ is drawn independently for each sample.\n\\end{enumerate}\n\n\n\\end{document} \n\n\n",
  "title": "Sample Complexity of Learning Mahalanobis Distance Metrics"
}
