{
  "authors": [
    "Iddo Drori",
    "Nakul Verma"
  ],
  "date_published": "2021-11-16",
  "raw_tex": " This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.\n\\pdfoutput1\n In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.\n\n\\documentclass[11pt]{article}\n\n Remove the \"review\" option to generate the final version.\n\\usepackage{acl}\n\n Standard package includes\n\\usepackage{times}\n\\usepackage{latexsym}\n\\usepackage{graphicx}\n\n For proper rendering and hyphenation of words containing Latin characters (including in bib files)\n\\usepackage[T1]{fontenc}\n For Vietnamese characters\n \\usepackage[T5]{fontenc}\n See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets\n\n\\usepackage{listings}\n\n This assumes your files are encoded as UTF8\n\\usepackage[utf8]{inputenc}\n\n This is not strictly necessary, and may be commented out,\n but it will improve the layout of the manuscript,\n and will typically save some space.\n\\usepackage{microtype}\n\n\n\\definecolor{dkgreen}{rgb}{0,0.6,0}\n\\definecolor{gray}{rgb}{0.5,0.5,0.5}\n\\definecolor{mauve}{rgb}{0.58,0,0.82}\n\n\\lstset{languagePython,\n   aboveskip1mm,\n   belowskip1mm,\n   showstringspacesfalse,\n   columnsflexible,\n   basicstyle{\\small\\ttfamily},\n   numbersnone,\n   numberstyle\\tiny\\color{gray},\n   keywordstyle\\color{blue},\n   commentstyle\\color{dkgreen},\n   stringstyle\\color{mauve},\n   breaklinestrue,\n   breakatwhitespacetrue,\n   tabsize3,\n   framenone\n}\n\n If the title and author information does not fit in the area allocated, uncomment the following\n\n\\setlength\\titlebox{<dim>}\n\n and set <dim> to something 5cm or larger.\n\n\\title{Solving Linear Algebra by Program Synthesis}\n\n Author information can be set in various styles:\n For several authors from the same institution:\n \\author{Author 1 \\and ... \\and Author n \\\\\n   Address line \\\\ ... \\\\ Address line}\n if the names do not fit well on one line use\n   Author 1 \\\\ {\\bf Author 2} \\\\ ... \\\\ {\\bf Author n} \\\\\n For authors from different institutions:\n\\author{Iddo Drori \\\\ MIT \\\\ EECS\n   \\And\n   Nakul Verma \\\\ Columbia University \\\\ Department of Computer Science}\n To start a seperate ``row'' of authors use \\AND, as in\n \\author{Author 1 \\\\ Address line \\\\   ... \\\\ Address line\n   \\AND\n   Author 2 \\\\ Address line \\\\ ... \\\\ Address line \\And\n   Author 3 \\\\ Address line \\\\ ... \\\\ Address line}\n\n\\author{Iddo Drori \\\\\n   MIT \\\\\n   EECS\\\\\n   \\texttt{idrori@mit.edu} \\\\\\And\n   Nakul Verma \\\\\n   Columbia University\\\\\n   Department of Computer Science\\\\\n   \\texttt{verma@cs.columbia.edu} \\\\}\n\n\\begin{document}\n\\maketitle\n\\begin{abstract}\nWe solve MIT's Linear Algebra 18.06 course and Columbia University's Computational Linear Algebra COMS3251 courses with perfect accuracy by interactive program synthesis. This surprisingly strong result is achieved by turning the course questions into programming tasks and then running the programs to produce the correct answers. We use OpenAI Codex with zero-shot learning, without providing any examples in the prompts, to synthesize code from questions. We quantify the difference between the original question text and the transformed question text that yields a correct answer. Since all COMS3251 questions are not available online the model is not overfitting. We go beyond just generating code for questions with numerical answers by interactively generating code that also results visually pleasing plots as output. Finally, we automatically generate new questions given a few sample questions which may be used as new course content. This work is a significant step forward in solving quantitative math problems and opens the door for solving many university level STEM courses by machine.\n\\end{abstract}\n\n\n\\section{Introduction}\nLanguage models have vastly improved in recent years, with the advent of large-scale Transformer models such as GPT-3   \\cite{brown2020language} that perform well on question answering tasks. However, when it comes to answering quantitative problems such as word problems in mathematics or deduction from formal logic, these models show poor performance achieving accuracies close to random baselines \\cite{hendrycks2020measuring}, failing on even the most simple questions such as computing the length of a vector.   \n\nPart of the challenge in finding a solution to quantitative problems is to have access to a working tree-like recursive memory. Quantitative problems often require building arithmetic expression trees that help in mathematical deduction. These kinds of trees are also common in program representation and program synthesis. With this insight, we study the efficacy of solving math problems, specifically problems from introductory level undergraduate Linear Algebra courses, by turning each problem into the task of writing a function or program to solve that question. This is done using OpenAI's Codex \\cite{chen2021evaluating}, a foundation model trained on both text and code.\n\nWe demonstrate the surprisingly simple yet strong result that foundation models for program synthesis such as OpenAI Codex succeed in synthesizing correct code for solving such quantitative math problems. Surprisingly, we find that Codex not only synthesizes correct code for problems that expect numerical answers, but also generates code for questions that ask to plot solutions. We achieve perfect accuracy in solving undergraduate level Linear Algebra course problems, and validate that our results are not merely overfitting the training data by solving a new course which is not available online, and is therefore unseen during Codex training.\n\nAs an example, consider a moderately involved question from MIT's Linear Algebra course 18.06, Question 1 in Chapter 7.3 of Gilbert Strang's textbook (\\citeyear{strang_lin_alg}), as shown in Figure \\ref{fig:workflow}. To the best of our knowledge none of the state-of-the-art quantitative reasoning models correctly answers such questions. As shown in Figure \\ref{fig:workflow}, given the question as text, we run the question through Codex as is without any modification to generate a program and execute the synthesized program to generate the correct solution. \n\n\\begin{figure*}[ht!]\n   \\centering\n   \\includegraphics[width\\textwidth]{workflow.png}\n   \\caption{Workflow for solving Linear Algebra questions: (i) Given a question in text, the example shown is Q1 in Ch.\\ 7.3 of \\citet{strang_lin_alg}, (ii) we run the question through Codex to generate a program, (iii) we execute the program to generate the solution. We transform the question repeating steps (ii) and (iii) until we get it correct.}\n   \\label{fig:workflow}\n\\end{figure*}\n\n\n\\section{Related Work}\n\nThere have been several recent works that attempt to improve quantitative reasoning in math problems. MathBERT \\cite{peng2021mathbert}, for instance, is a Transformer based pre-trained language model that uses symbol and operator trees as intermediary representations of formulas. \n \nAnother line of work has focused on solving math questions from a large database of questions collected from Chinese elementary school math classes. Techniques have included sequence-to-sequence and graph-to-tree Transformers which achieve around $80\\$ on Math23k and MAWPS datasets \\cite{koncel2016mawps,li2019modeling,wang2019template,zhang2020graph2tree,li2020graph2tree,wu2020knowledge,qin2020semantically,lan2021mwptoolkit}. Other work \\cite{tsai2021sequence} includes knowledge graphs of geometry formulas into sequence-to-tree transformers to improve performance on the geometry section of Math23k dataset, and MWP-BERT, which adds masked fine-tuning to the BERT model using a large corpus of over 100,000 math word problems achieves an impressive $96.2\\$ accuracy on the Math23k   dataset \\cite{liang2021mwp} of elementary school math problems.\n\nFor solving university level machine learning problems specifically, a recent approach \\citep{tran2021solving} uses graph neural networks and Transformers to predict an expression tree from the input question to calculate the answer. This achieves over $95\\$ accuracy on numerical machine learning exercises, which is above human performance; however only works on the specific course it is trained on.\n\nRather than building a custom-designed solution, our work explores the use of a foundation model such as Codex which is trained on both text and code. Any program may be represented as an abstract syntax tree and many questions may also be represented as expression trees. Bringing the question and answer into a common representation makes it easier to find a correct solution. The advantage of studying this pre-trained model is that it may be applied at scale to many different topics or subjects without additional training. Our work is the first to demonstrate perfect performance of interactively solving linear algebra problems at a university-level difficulty.\n\n\\section{Methods}\n\n\\begin{figure*}[ht!]\n   \\centering\n   \\includegraphics[width\\textwidth]{workflow_interactive.png}\n   \\caption{Interactive workflow: (A) We begin with the original question. Codex generates a program which is executed. The result is missing the projection. (B) We transform the question and Codex generates a program again to get the correct answer, though the zero projection vector does not appear on the plot, (C) An additional task to plot the projection vector with a marker so that it is visible results in Codex generating modified code which is executed to yield a correct answer and visually pleasing result.}\n   \\label{fig:workflow_interactive}\n\\end{figure*}\n\n\nHere we describe our dataset, solution generation pipeline, and evaluation methodology. The key components leading to our success are:\n\\begin{itemize}\n   \\item Program synthesis: Insight to use a program synthesis to generate a program, that has a built-in tree representation, that produces the solution to the given problem.\n   \\item Interactive workflow: we interactively work with Codex to produce both the correct result and visually pleasing plots as shown in Figure \\ref{fig:workflow_interactive}. We place the question in context by augmenting the question with definitions and information required for the solving the question, rephrase and simplify. See the Appendix for all the original and transformed questions.\n\\end{itemize}\n\n\\subsection{Datasets}\nWe use (i) problem exercises from Gilbert Strang's \\emph{Introduction to Linear Algebra} textbook (\\citeyear{strang_lin_alg}), which is used for MIT's Linear Algebra 18.06 course, and (ii) exercises given as homework problems in Columbia's Computational Linear Algebra COMS3251 course, as two challenging real-world university-level datasets. Both courses have multivariable calculus as their prerequisites and are usually taken by second-year EE/CS undergraduate students.\nTo keep things tractable, we select 3 to 4 random problems from each chapter of the textbook (for MIT 18.06) and from each topic (for COMS3251), resulting in two datasets of 30 questions each for our evaluation. These questions range in difficulty level, and output type (such as a numerical output or drawing a figure with multiple equations). See Figures \\ref{fig:workflow}, \\ref{fig:workflow_interactive} for examples and the Appendix for a full list of questions.\n\n\\subsection{Interactive Workflow}\nOur interactive workflow is illustrated in Figure \\ref{fig:workflow_interactive}. We begin with the original question from Strang's book (Question 2b, Chapter 4.2), which we feed into Codex that generates a Python program that is then executed. In this example, the result is missing the projection solution. We therefore transform the question to explicitly ask for the projection and have Codex re-generate a program to get the correct answer. The answer also consists of a plot, however the zero projection vector is not visible in the plot since it is a point. We therefore add an additional task which is to plot the projection vector with a marker so that it becomes visible. Codex re-generates the code which is executed to yield both a correct answer and a visually pleasing plot. In all of our experiments we set Codex parameters to be the same fixed default values (using davinci-codex with temperature 0 and response length 200).\n\n\n\\begin{figure}[b!]\n   \\centering\n   \\includegraphics[width0.7\\columnwidth]{course_similarityla.png}\n   \\caption{High similarity between the original questions and programming prompts for both COMS3251 and MIT 18.06. Baseline mean pairwise similarity among original questions for both courses shown as a solid horizontal line.}\n   \\label{fig:course_similarityla}\n\\end{figure}\n\n\\begin{table*}[ht!]\n\\small\n\\centering\n\\begin{tabular}{|l|l|p{6.5cm}|p{6.5cm}|} \n\\hline\n\\textbf{ID} & \\textbf{Course} & \\textbf{Auto-Generated question} & \\textbf{Closest question in the dataset}\\\\\n\\hline\n1 & MIT 18.06 & Find the eigenvalues and eigenvectors of the matrix $A[1,1,1;1,2,3;1,3,6]$. & \nFind the eigenvalues of A and B (easy for triangular matrices) and A + B: A   [3,0;1,1], B   [1,1;0,3], A+B   [4,1;1,4].\n(Ch 6.1 Q5)\n\\\\\n\\hline\n2 & MIT 18.06 & Find a matrix $A$ such that $A^2$ is not invertible but $A^3$ is invertible. &\nFind a matrix that has $A^2$ does not equal 0 but $A^3$   0.\n(Ch 2.4 Q23b)\n\\\\\n\\hline\n3 & MIT 18.06 & Find a basis for the nullspace of $A   [1,1,1;1,1,1;1,1,1]$. &\nConstruct a 2 by 2 matrix whose nullspace equals its column space. This is possible. (Ch 3.2 Q20)\n\\\\\n\\hline\n4 & MIT 18.06 & Find a basis for the nullspace of $A$ if the columns of $A$ are unit vectors, all mutually perpendicular. & \nFind A'A if the columns of A are unit vectors, all mutually perpendicular. (Ch 4.1 Q25)\n\\\\ \n\\hline\n5 & MIT 18.06 & What 2 by 2 matrix $R$ rotates every vector through 90 degrees? & \nWhat 2 by 2 matrix R rotates every vector through 45 degrees? Example: the vector [1,0] goes to [sqrt(2)/2, sqrt(2)/2].\n(Ch 2.1, Q21)\n\\\\\n\\hline\n6 & MIT 18.06 & The complete solution to $Ax   [1;3]$ is $x [1;0]+c[0;1]$. Find the nullspace of $A$. &\nConstruct a 2 by 2 matrix whose nullspace equals its column space. This is possible. (Ch 3.2 Q20)\n\\\\\n\\hline\n7 & MIT 18.06 & Find a matrix $A$ that has a negative eigenvalue and is symmetric. & \nFind a symmetric matrix [1,b;b,1] that has a negative eigenvalue. (Ch 6.4, Q9a)\n\\\\\n\\hline\n8 & MIT 18.06 & Find the best plane $C+Dt+Et^2$ to fit $b[1,2,3,4,5]$ at times $t0,1,2,3,4$. & \nFind the best line C+Dt to fit b4,3,-1,0,0 at times t-2,-1,0,1,2. (Ch 4.3, Q22)\n\\\\\n\\hline \\hline\n9 & COMS3251 & Find the eigenvalues of the matrix $A   [1, 2; -2, -3]$. & \nFind the eigenvalues of [-0.2, 0.3; 0.2, -0.3].\n\\\\\n\\hline\n10 & COMS3251 & \nCompute the determinant of the following matrix: $[-1,-2;-2,-4]$ & Compute the determinant of the following matrix:\n[3,-4,5;0,-1,-5;5,-4,3] \\\\\n\\hline\n11 & COMS3251 & \nFind the determinant of the following matrix: $[1,-2,-1;0,2,-3;-4,-5,6]$ & \nCompute the determinant of the following matrix:\n[3,-4,5;0,-1,-5;5,-4,3]\n\\\\ \n\\hline\n12 & COMS3251 & \nCompute an LU decomposition of the matrix $A   [1, 2; -2, -3]$ &\nFind an LU decomposition of the following matrix:\n[-1,-1,2;2,0,3;-3,2,-1]\n\\\\\n\\hline\n13 & COMS3251 & \nWhich of the following matrices is a left inverse to $A[1,2,-3;-1,-1,0;-2,-3,3]?$\n(a) $[-1,0,2;-2,-3,3;-6,-9,9]\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ \n(b) $[-1,-1,0;0.5,-0.5,0;1/6,-2/6,3/6]\\;\\;\\;\\;\\;$\n(c) $[-1,-2,-3;0.5,-0.5,0;1/6,-4/6,9/6]$ $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$\n(d) None of the above. & \nCompute the inverse of the following matrix:\n[-1,-2;-2,0]\n\\\\\n\\hline\n14 & COMS3251 & \nFind a combination of the vectors $[1; 2; 3]$, $[4; 5; 6]$, and $[7; 8; 9]$ that gives the vector $[12; 23]$ & \nFind a combination of the vectors [1; 2; 3], [4; 5; 6], and [7; 8; 9] that give the zero vector.\n\\\\\n\\hline\n15 & COMS3251 & \nWhat is the dimension of the subspace spanned by the following vectors?\n$[1,2,3]$,$[0,1,0]$,$[-1/2,-1/3,1]$ &\nWhat is the dimension of the subspace spanned by the following vectors?\n[2,-1/2],[1,1],[4,4]\n\\\\\n\\hline\n16 & COMS3251 & \nFind the projection matrix onto the column space of $A   [1, 2, 3; 4, 5, 6]$ & \nFind the projection matrix onto the column space of A [3, 6, 6; 4, 8, 8].\n\\\\\n\\hline\n\\end{tabular}\n\\caption{New questions generated from MIT Linear Algebra 18.06 questions and Computational Linear Algebra (COMS3251) questions, and the closest question among the existing questions.}\n\\label{tab:new18.06newCOMS3251}\n\\end{table*}\n\n\\section{Results}\n\\subsection{Performance Evaluation}\nOur dataset includes 30 questions from MIT's 18.06 and 30 questions from Columbia University's COMS3251 and gets perfect accuracy on these courses (see Appendix for detailed input and the solution output for each question in the datasets). In contrast, GPT3 yields 0\\ accuracy. We would like to quantify the extent of human effort required for achieving these perfect results. We therefore measure the similarity between the original question text and the final programming prompt that results in a correct answer. As shown in Figure \\ref{fig:course_similarityla} we observe highly similar texts. Specifically 90\\ median similarity for Columbia University's COMS3251 and 80\\ median similarity for MIT's 18.06, computed using the cosine similarity between their language embeddings. This demonstrates that only minor changes are required for turning a question into a program task that results in a correct answer. As a baseline we also include the similarity among the different   original questions in each course, to verify the validity of our metric.\n\n\\subsection{Generating New Questions}\nWe are able to generate new questions with ease. We prompt Codex by a set of $n$ numbered questions on different topics, and synthesize question number $n+1$. Table \\ref{tab:new18.06newCOMS3251} shows eight new generated questions for each course.\n\n\n\\subsection{Limitations}\nWe currently do not handle input drawings or any visual elements as input. Extending our approach to handle such inputs by using a multi-modal text and vision Transformer would help solve many diverse types of mathematical problems. While our methodology works well for numerical outputs and figures, our pipeline doesn't yet handle solutions that require multi-line derivations or proofs. We currently often modify the original question manually to form a question for which Codex returns a program which solves the question correctly making our method interactive. We plan on training a Transformer, such as T5 \\cite{T5}, for paraphrazing and performing this step automatically.\n\n\\section{Conclusion}\nOur work is the first to solve linear algebra problems at a university-level difficulty. Our results open the door for solving other STEM courses, which has the potential to disrupt higher education by: (i) automatically learning all university level STEM courses, (ii) automatically grading course, and (iii) rapidly generating new course content.\n\n\n\\newpage\n\\clearpage\n\n\\bibliography{bibliography}\n\\bibliographystyle{acl_natbib}\n\n\\onecolumn\n\\appendix\n\\include{appendix-COMS3251}\n\\include{appendix-18.06}\n\n\\end{document}\n\n\n",
  "title": "Solving Linear Algebra by Program Synthesis"
}
