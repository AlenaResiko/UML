{
  "authors": [
    "Leonard Tang",
    "Elizabeth Ke",
    "Nikhil Singh",
    "Nakul Verma",
    "Iddo Drori"
  ],
  "date_published": "2021-11-16",
  "raw_tex": " This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.\n\\pdfoutput1\n In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.\n\n\\documentclass[11pt]{article}\n\n Remove the \"review\" option to generate the final version.\n\\usepackage{acl}\n\n Standard package includes\n\\usepackage{times}\n\\usepackage{relsize}\n\\usepackage{latexsym}\n\\usepackage{graphicx}\n\\usepackage{todonotes}\n\n\\usepackage{listings}\n\\usepackage{color}\n\n\\definecolor{dkgreen}{rgb}{0,0.6,0}\n\\definecolor{gray}{rgb}{0.5,0.5,0.5}\n\\definecolor{mauve}{rgb}{0.58,0,0.82}\n\n\\lstset{languagePython,\n   aboveskip1mm,\n   belowskip1mm,\n   showstringspacesfalse,\n   columnsflexible,\n   basicstyle{\\small\\ttfamily},\n   numbersnone,\n   numberstyle\\tiny\\color{gray},\n   keywordstyle\\color{blue},\n   commentstyle\\color{dkgreen},\n   stringstyle\\color{mauve},\n   breaklinestrue,\n   breakatwhitespacetrue,\n   tabsize3,\n   framenone\n}\n\n For proper rendering and hyphenation of words containing Latin characters (including in bib files)\n\\usepackage[T1]{fontenc}\n For Vietnamese characters\n \\usepackage[T5]{fontenc}\n See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets\n\n This assumes your files are encoded as UTF8\n\\usepackage[utf8]{inputenc}\n\n This is not strictly necessary, and may be commented out,\n but it will improve the layout of the manuscript,\n and will typically save some space.\n\\usepackage{microtype}\n\n\\usepackage{fdsymbol}\n\n If the title and author information does not fit in the area allocated, uncomment the following\n\n\\setlength\\titlebox{<dim>}\n\n and set <dim> to something 5cm or larger.\n\n\\DeclareCaptionStyle{ruled}{labelfontnormalfont,labelsepcolon,strutoff}   DO NOT CHANGE THIS\n\\frenchspacing   DO NOT CHANGE THIS\n\n\\title{Solving Probability and Statistics Problems by Program Synthesis}\n\n Author information can be set in various styles:\n For several authors from the same institution:\n \\author{Author 1 \\and ... \\and Author n \\\\\n   Address line \\\\ ... \\\\ Address line}\n if the names do not fit well on one line use\n   Author 1 \\\\ {\\bf Author 2} \\\\ ... \\\\ {\\bf Author n} \\\\\n For authors from different institutions:\n \\author{Author 1 \\\\ Address line \\\\   ... \\\\ Address line\n   \\And   ... \\And\n   Author n \\\\ Address line \\\\ ... \\\\ Address line}\n To start a seperate ``row'' of authors use \\AND, as in\n \\author{Author 1 \\\\ Address line \\\\   ... \\\\ Address line\n   \\AND\n   Author 2 \\\\ Address line \\\\ ... \\\\ Address line \\And\n   Author 3 \\\\ Address line \\\\ ... \\\\ Address line}\n\n\\author{Leonard Tang \\\\\n   Harvard University\\\\\n   Mathematics\\\\\\And\n   Elizabeth Ke \\\\\n   MIT\\\\\n   Mathematics\\\\\\And\n   Nikhil Singh \\\\\n   MIT\\\\\n   Media Lab\\\\\\AND\n   Nakul Verma \\\\\n   Columbia University\\\\\n   Computer Science Department\\\\\\And\n   Iddo Drori \\\\\n   MIT\\\\\n   EECS\\\\}\n\n\\begin{document}\n\\maketitle\n\\begin{abstract}\nWe solve university level probability and statistics questions by program synthesis using OpenAI's Codex, a Transformer trained on text and fine-tuned on code. We transform course problems from MIT's 18.05 Introduction to Probability and Statistics and Harvard's STAT110 Probability into programming tasks. We then execute the generated code to get a solution. Since these course questions are grounded in probability, we often aim to have Codex generate probabilistic programs that simulate a large number of probabilistic dependencies to compute its solution. Our approach requires prompt engineering to transform the question from its original form to an explicit, tractable form that results in a correct program and solution. To estimate the amount of work needed to translate an original question into its tractable form, we measure the similarity between original and transformed questions. Our work is the first to introduce a new dataset of university-level probability and statistics problems and solve these problems in a scalable fashion using the program synthesis capabilities of large language models.\n\\end{abstract}\n\n\\section{Introduction}\nLet's say we play a game where I keep flipping a coin until I get heads. If the first time I get heads is on the $n$-th coin, then I pay you $2n - 1$ dollars. How much would you pay me to play this game?\n\nHow would one solve this problem in an automated fashion?\nExisting approaches to solving such a problem, typical in university level probability and statistics courses, overwhelmingly hinge upon directing foundation models to formulate answers in a \\textit{deductive} fashion, whether via a sequence of steps \\cite{hendrycksmath2021} or formal operations \\cite{amini-etal-2019-mathqa}. \n\nAn alternate compelling approach is to simulate a given task on a large scale and aggregate results across multiple scenarios. Such an approach, usually dubbed as probabilistic programming in the literature \\cite{pmlr-v15-wingate11a}, offers a flexible mechanism for solving a variety of probabilistic tasks.\n\nInspired by this insight, our goal is to solve probability problems both via simulation and direct methods by leveraging the power of a program synthesizer such as OpenAI's Codex \\cite{chen2021evaluating}. Codex is a Transformer model trained on text and fine-tuned on code, which has the capacity to write programs that can simulate arbitrary stochastic tasks. Our core approach is neatly demonstrated in Figure \\ref{fig:prob-workflow}. We transform raw question text into a programming task. This is then fed into Codex, generating a probabilistic program. We can then execute this code to get the correct response.\n\n\n\\begin{figure*}[!h]\n   \\centering\n   \\includegraphics[width\\textwidth]{workflow_probabilistic.png}\n   \\vspace{0.05in}\n   \\caption{Probabilistic Simulation Program Workflow Example: (i) The original problem is translated into a programming task that asks Codex to simulate a large number of probabilistic scenarios, (ii) Codex generates such a program, and (iii) the program is executed to yield an answer.}\n   \\label{fig:prob-workflow}\n\\end{figure*}\n\n\nTo the best of our knowledge, we are the first to propose such a   simulation based approach to solve probability questions. To evaluate the efficacy of our approach, we collect two sets of 20 undergraduate-level probability and statistics problems, curated from MIT's 18.05 and Harvard's STAT110.\n\nThe key to our success lies in the carefully engineered prompts we present to Codex. Critically, we introduce the notion of \\textit{Concept-Grounded Task} prompting, i.e.\\ priming Codex with related concepts and problem-solving strategies in its prompt (cf.\\ Figure \\ref{fig:concept-workflow}).\n\n\\subsection{Related Work}\n\n\\paragraph{Foundation models.}\nFoundation models \\cite{bommasani2021opportunities} such as GPT-3 \\cite{NEURIPS2020_1457c0d6} have demonstrated impressive and unforeseen emergent capabilities from their learning process, including aptitude in automatic speech recognition, vision, commonsense reasoning, and more \\cite{9054476, dosovitskiy2020vit, Bosselut2019COMETCT}. For the task of answering questions specifically, such models have recently achieved strong performance \\cite{rajpurkar-etal-2018-know}. However, when tasked with solving university-level quantitative problems, foundation model performance is poor \\cite{hendrycksmath2021}.\n\n\\paragraph{Probability benchmarks.} \nThough recent works have introduced datasets, such as MATH, MAWPS, MathQA, Math23k, and GSM8K \\cite{hendrycksmath2021, koncel-kedziorski-etal-2016-mawps, amini-etal-2019-mathqa, wang-etal-2017-deep, cobbe2021training}, that focus on benchmarking mathematical question answering, including probability questions, but all of these works only consider grade-school level question difficulty. We are the first to present two datasets of undergraduate-level probability and statistics questions.\n\n\\section{Dataset}\nWe introduce two datasets of questions from two separate undergraduate-level probability and statistics courses of varying difficulty and a set of quantitative finance interview questions. We describe the datasets below:\n\n\\begin{enumerate}\n   \\item The first dataset consists of applied questions in probability. We take 20 questions that have numerical answers from MIT's 18.05: Introduction to Probability and Statistics \\cite{18_05_ocw}. Course topics include various probability and statistics concepts, including counting, conditional probability, discrete and continuous random variables, expectation and variance, central limit theorem, joint distributions, maximum likelihood estimators, Bayesian updating, null hypothesis significance testing, and confidence intervals.\n   \\item The second dataset, in contrast to the first dataset, consists of conceptual questions in probability and statistics. We take 20 questions that have numerical answers from Harvard's STAT110: Probability \\cite{stat110home} and Brainstellar \\cite{brainstellar} online catalogue of quantitative finance probability brainteasers. Topics include: distributions, moment generating functions, expectation, variance, covariance, correlation, conditional probability, joint distributions, marginal distributions, conditional distributions, limit theorems, and Markov chains.\n\\end{enumerate}\n\nSee Appendix for the list of all questions in our datasets.\n\n\\begin{figure*}[t!]\n   \\centering\n   \\includegraphics[width\\textwidth]{workflow_deterministic.png}\n   \\vspace{0.05in}\n   \\caption{Concept-Grounded Workflow Example: (i) The original problem is translated into a programming task that includes Bayes' Theorem within its context, (ii) Codex generates a program, and (iii) the program is executed to yield an answer.}\n   \\label{fig:concept-workflow}\n\\end{figure*}\n\n\\section{Methods}\n\n\\subsection{Models and Evaluation}\nAs our core program synthesizer, we leverage OpenAI's Codex \\cite{chen2021evaluating}.   Given a raw question text, we use the following experimentation pipeline: we convert each question into a programming task, prompt Codex with the task to get a programmatic solution, and execute the program generated by Codex, comparing the execution result to the ground truth solution. \n\nA critical component of this workflow is \\textit{prompt engineering} (i.e.\\ the conversion from question to programming task). We examine prompt engineering in further detail in Section \\ref{lab:prompt_egg} and measure the degree to which we manipulate the prompt in Section \\ref{lab:measure_transform}.\n\n\\subsection{Prompt Engineering}\n\\label{lab:prompt_egg}\nLarge generative language models, including Codex, are known to be extremely sensitive to input prompts \\cite{DBLP:journals/corr/abs-2102-07350}. Below, we outline and describe three classes of prompts that we communicate to Codex with and their associated effects:\n\n\\begin{itemize}\n   \\item \\textbf{Program Task Specification:} One class of prompts converts probability questions into direct task specifications \\cite{DBLP:journals/corr/abs-2102-07350}. In this case, these specifications are explicit programming assignments. For instance, if the original question is \"What is the probability of flipping two heads in a row given a fair coin?\", the corresponding task specification would be \"Write a program that computes the probability of flipping two heads in a row given a fair coin.\"   While these prompts occasionally suffice, in many instances additional prompt manipulation is required.\n   \\item \\textbf{Probabilistic Simulation Programming}: While the above classes of prompts produce programs deterministic in nature, our third prompting technique hinges upon the power of probabilistic simulation programs, i.e. programs that simulate a large number of scenarios and aggregate results across simulations to determine an approximate answer. To trigger such simulation behavior in Codex, we include in our prompt the substring \"Write a large-scale simulation program to estimate,\" followed by the desired task. Figure \\ref{fig:prob-workflow} presents an example using such a prompting scheme.\n   \\item \\textbf{Concept-Grounded Task}: An extremely useful extension beyond \\textit{Program Task Specification} is to include relevant information pertaining to both the question and program contexts. The question context includes related topics or mathematical rules to use. This is primarily represented in the form of canonical equations, definitions, and theorems. Providing Codex with hints on problem-solving \\textit{strategy} involving these concepts is extremely helpful. For instance, if a question is related to the concept of Bayes' Theorem, appending the transformed prompt with an explicit instruction to use Bayes' Theorem results in a correct program. This is demonstrated in Figure \\ref{fig:concept-workflow}. In addition to the question context, it may be useful to specify the programming context, including which packages or libraries the program will load and use, such as packages for symbolic math, integration, or optimization.\n\\end{itemize}\n\n\\section{Results}\n\n\\subsection{Evaluating Program Output}\nIt is possible for Codex to generate code which appears to yield a correct answer, but in reality is not a correct method to solve the problem. Thus, we explicitly inspect the code generated by Codex to check for its logical correctness. See Figure \\ref{fig:wrong_logic} for an explicit example which we encountered.\n\n\\begin{figure}[ht!]\n   \\centering\n   \\includegraphics[width\\columnwidth]{wrong_logic.png}\n   \\caption{Evaluating Codex requires more than just checking the numerical answer. It requires evaluating the logic of the program, and seeing that it actually answers the program.\n   Though the computed answer matches the ground truth solution of 0.5, the Codex program is written with the intention of calculating the unconditional probability of having fraternal twins (assuming fraternal and identical twins have equal probabilities), and completely ignores the conditioning information in the problem.}\n   \\label{fig:wrong_logic}\n\\end{figure}\n\nAnother peculiar challenge in evaluating Codex's generated simulation programs lies in the approximate nature of their output. Specifically, we can only achieve perfect accuracy in the limit of simulation scale. Hence, we designate a numerical output resulting from program execution as correct when it is within $1\\$ of the ground truth solution.\n\n\\subsection{Achieving Perfect Results}\n\\label{lab:measure_transform}\n\nFollowing the methods discussed in Section \\ref{lab:prompt_egg}, we are able to generate correct programs for all questions in both datasets. See the Appendix for additional detail regarding the original question text, the corresponding prompt-engineered transformation, the generated program, and finally the program evaluation for each question in each dataset.\n\nSince our approach involves prompt engineering in the process of translating a question to a programming task, we seek a concrete measure of the effort necessary in this transformation. Our metric is computed as follows: for any given pair of original question and transformed programming task, we compute the cosine similarity between the Sentence-BERT \\cite{reimers-gurevych-2019-sentence} embedding of the question and Sentence-BERT embedding of the task. \n\nFigure \\ref{fig:course_similarity} shows that we have an average similarity of 0.80 in MIT's 18.05 and an average similarity of 0.79 in STAT110. As a baseline reference we also include the average pairwise similarity score among the original questions, thus indicating we only need minor changes to the text.\n\n\\begin{figure}[ht!]\n   \\centering\n   \\includegraphics[width\\columnwidth]{course_similarity_no_6041.png}\n   \\caption{Sentence-BERT Similarity between original questions and programming tasks by course. Each course's transformation similarities are contextualized by baseline question similarities, i.e. the average pairwise similarity between the original questions.}\n   \\label{fig:course_similarity}\n\\end{figure}\n\nFinally, since theses course are available online, we verify that Codex is not overfitting to training data, by writing and solving our own novel questions.\n\n\\subsection{Implementation Details}\nWe fix all Codex's hyperparameters to be the same for all experiments: \\textit{top-p}\\footnote{\\textit{top-p} designates the portion $p$ of the token probability mass a language model samples from at each step.} is set to $1$, sampling temperature is set to $0$ (i.e.\\ argmax), and maximum sequence length is set to $750$ tokens. Both frequency and presence penalty are set to $0$, and we do not halt on any stop sequences. We use the \\textit{davinci} engine for all of our generations.\n\n\\section{Conclusion}\nTo the best of our knowledge, this is the first work to present a state-of-the-art method that leverages the program synthesis and probabilistic simulation capabilities of foundation models to solve university-level probability and statistics problems. Through the use of prompt engineering, including priming Codex with concepts and problem-solving strategies, we achieve full correctness on our novel datasets.\nWe plan to expand our work to further understand the underlying structure of programming tasks that are amenable to Codex manipulation, and in a similar vein move towards the automatic translation of questions to programming tasks.\n\n\\newpage\n\\clearpage\n\n\\bibliography{bibliography}\n\\bibliographystyle{acl_natbib}\n\n\\onecolumn\n\\appendix\n\\include{18.05-appendix}\n\\include{STAT110-appendix}\n\n\\end{document}\n",
  "title": "Solving Probability and Statistics Problems by Program Synthesis"
}
