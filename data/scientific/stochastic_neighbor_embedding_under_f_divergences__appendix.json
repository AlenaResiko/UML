{
  "authors": [
    "Daniel Jiwoong Im",
    "Nakul Verma",
    "Kristin Branson"
  ],
  "date_published": "2018-11-03",
  "raw_tex": "\\begin{appendices}\n\n   \\onecolumn\n   \\section{Appendix}\n   \\section{Precision and Recall}\n   \\begin{proposition}\n   \\label{prop1}\n   Suppose that we have the binary neighbours of point $x_i$ and $\\delta$ is very close to $0$. \n   Then, each $f$-divergence does the following:\\\\\n   KL-divergence maximizes the recall\\footnote{Recall$(i)\\left(1-\\frac{n^i_{FN}}{r_i}\\right)$ and Precision$(i)\\left(1-\\frac{n^i_{FP}}{k_i}\\right)$.},\n   \\begin{align}\n   J_{KL}   \\frac{n^i_{FN}}{r_i}C_{0}   \n   \\end{align}\n   where $N_{FN}$ is the number of flase negatives.\\\\\n   Reverse KL-divergence maximizes the precision, \n   \\begin{align}\n   J_{RKL}   \\frac{n^i_{FP}}{k_i}C_{0}\n   \\end{align}\n   where $N_{FP}$ are number of false positives. \n   The constant term corresponse to $C_0   \\log\\frac{1-\\delta}{\\delta}$. \\\\\n   Jensen-Shannon divergence maximizes both the precision and recall.\n   \\begin{align}\n   J_{JS} &\\approx \\frac{1}{2} (J_{KL} + J_{RKL}).\n   \\end{align}\\\\\n   Similarly, Helligener and Chi-Square divergences maximize precision and recall but contains extra regularization term,\n   \\begin{align}\n   J_{HL} &\\approx \\frac{n^i_{FP}}{k_i} C_{h0}+\\left(1-\\frac{n^i_{FP}}{k_i}\\right)+\\left(1-\\frac{n^i_{FN}}{r_i}\\right)C_{h1} \\label{eqn:j_hl}\\\\\n   J_{CH} &\\approx \\frac{n^i_{FP}}{k_i} C_{c0}+\\left(1-\\frac{n^i_{FP}}{k_i}\\right)C_{c1}+\\left(1-\\frac{n^i_{FN}}{r_i}\\right)C_{c2} \\label{eqn:j_ch}\n   \\end{align} \n   The constant terms corresponds to:\\\\\n   \\begin{equation}\n   C_{h0}   \\left(\\frac{r_i}{k_i} - 1\\right)^2, \n   C_{h1}\\left(\\frac{1-\\delta}{\\delta} \\frac{k_i}{m-r_i-1}- 2\\right),\n   \\end{equation}\n   \\begin{equation}\n   C_{c0}   \\left(\\sqrt{\\frac{r_i}{k_i}} - 1\\right)^2, \n   C_{c1}   \\left( 1 - 2\\sqrt{\\frac{r_i\\delta}{1-\\delta}}\\right), \\text{ and }\n   C_{c2}   \\left( 1 - 2\\sqrt{\\frac{k_i\\delta}{1-\\delta}}\\right).\n   \\end{equation}\n   \\end{proposition}\n   \\begin{proof}\n   The probabilties of relevenat neighbours and irrelevant neighbours are defined as \n   \\[ p_{ji}   \\begin{cases} \n   a_i \\equiv \\frac{1-\\delta}{r_i}   & x_j \\in NN_\\epsilon(x_i)   \\\\\n   b_i \\equiv \\frac{\\delta}{m-r_i-1}   & x_j \\notin NN_\\epsilon(x_i)\n   \\end{cases}\n   , \\qquad \n   q_{ji}   \\begin{cases} \n   c_i \\equiv \\frac{1-\\delta}{k_i}   & y_j \\in NN_\\epsilon(y_i)   \\\\\n   d_i \\equiv \\frac{\\delta}{m-k_i-1}   & y_j \\notin NN_\\epsilon(y_i)\n   \\end{cases}\n   \\]\n   where $NN_\\epsilon(x_i)\\lbrace x_j | p_{ji} > \\epsilon \\rbrace$ is relevant neigbours for point $x_i$,\n   $r_i$ are the number of relevant points and $k_i$ are number of retrieved points.\\\\\\\\\n   \\noindent For KL: \n   \\begin{align*}\n   J_{KL}(x_i) & \\sum_j   p_{ji} \\left(\\log\\frac{p_{ji}}{q_{ji}}\\right) \\nonumber\\\\\n   & \\sum_{\\substack{j\\neq i,\\\\p_{ji}a_i,\\\\q_{ji}c_i}} a_i\\log\\left(\\frac{a_i}{c_i}\\right)\\\n   + \\sum_{\\substack{j\\neq i,\\\\p_{ji}a_i,\\\\q_{ji}d_i}} a_i\\log\\left(\\frac{a_i}{d_i}\\right) \\\n   + \\sum_{\\substack{j\\neq i,\\\\p_{ji}b_i,\\\\q_{ji}c_i}} b_i\\log\\left(\\frac{b_i}{c_i}\\right) \\\n   + \\sum_{\\substack{j\\neq i,\\\\p_{ji}b_i,\\\\q_{ji}d_i}} b_i\\log\\left(\\frac{b_i}{d_i}\\right) \\nonumber\\\\\n   & n^i_{TP}a_i\\log\\left(\\frac{a_i}{c_i}\\right) + n^i_{FN} a_i\\log\\left(\\frac{a_i}{d_i}\\right)\\\n   n^i_{FP}b_i\\log\\left(\\frac{b_i}{c_i}\\right) + n^i_{TN} b_i\\log\\left(\\frac{b_i}{d_i}\\right) \\nonumber\n   \\end{align*}\n   where $n^i_{TP}, n^i_{FN}, n^i_{FP}, n^i_{TN}$ are number of true positives, false negatives (missed points), false positives, and true negatives respectively for point $x_i$.\n   Given that $\\delta$ is close to $0$, then the coefficient of $n^i_{FN}$ and $n^i_{FP}$ dominates the other terms,\n   \\begin{align*}\n   J_{KL} & n^i_{FN} a_i\\log\\left(\\frac{a_i}{d_i}\\right) + n^i_{FP} b_i\\log\\left(\\frac{b_i}{c_i}\\right) + O(\\delta)\\\\\n   & n^i_{FN} \\frac{1-\\delta}{r_i}\\log\\left(\\frac{1-\\delta}{\\delta}\\frac{m-k_i-1}{r_i}\\right) \n   + n^i_{FP} \\frac{\\delta}{m - r_i -1 }\\log\\left(\\frac{\\delta}{1-\\delta}\\frac{k_i}{m-r_i-1}\\right) + O(\\delta).\n   & ( n^i_{FP} \\frac{1-\\delta}{k_i} - n^i_{FN} \\frac{\\delta}{m - k_i -1 }\\log\\left(\\frac{1-\\delta}{\\delta}\\frac{m-k_i-1}{\\delta}\\right) \n   \\log\\left(\\frac{\\delta}{1-\\delta}\\right) \n   \\end{align*}\n   Again, the $\\log \\frac{1-\\delta}{\\delta}$ dominates the other logarithmic terms $\\bigg(\\log \\left(\\frac{m-r_i-1}{k_i}\\right)$ and $\\log \\left(\\frac{m-k_i-1}{r_i}\\right) \\bigg)$, so we have\n   \\begin{align*}\n   J_{KL} & \\left(\\frac{n^i_{FN}}{r_i} (1-\\delta) - \\frac{n^i_{FP}}{m - r_i -1} \\delta \\right) \\log \\left(\\frac{1-\\delta}{\\delta}\\right) + O(\\delta) \\nonumber\\\\\n   & \\frac{n^i_{FN}}{r_i} C_0   (1-\\text{Recall}(i))C_0 + O(\\delta)\n   \\end{align*}\n   where $C_0   \\log \\left(\\frac{1-\\delta}{\\delta}\\right)$.\\\\\\\\\n \n   \\noindent For Reverse KL: \n   \\begin{align*}\n   J_{RKL}(x_i) & - \\sum_j   q_{ji} \\left(\\log\\frac{p_{ji}}{q_{ji}}\\right) \\nonumber\\\\\n   & - \\sum_{\\substack{j\\neq i,\\\\p_{ji}a_i,\\\\q_{ji}c_i}} c_i\\log\\left(\\frac{a_i}{c_i}\\right)\\\n   - \\sum_{\\substack{j\\neq i,\\\\p_{ji}a_i,\\\\q_{ji}d_i}} d_i\\log\\left(\\frac{a_i}{d_i}\\right) \\\n   - \\sum_{\\substack{j\\neq i,\\\\p_{ji}b_i,\\\\q_{ji}c_i}} c_i\\log\\left(\\frac{b_i}{c_i}\\right) \\\n   - \\sum_{\\substack{j\\neq i,\\\\p_{ji}b_i,\\\\q_{ji}d_i}} d_i\\log\\left(\\frac{b_i}{d_i}\\right) \\nonumber\\\\\n   & - n^i_{TP}c_i\\log\\left(\\frac{a_i}{c_i}\\right) - n^i_{FN} d_i\\log\\left(\\frac{a_i}{d_i}\\right)\\\n   - n^i_{FP} c_i\\log\\left(\\frac{b_i}{c_i}\\right) - n^i_{TN} d_i\\log\\left(\\frac{b_i}{d_i}\\right) \\nonumber\n   \\end{align*}\n   where $n^i_{TP}, n^i_{FN}, n^i_{FP}, n^i_{TN}$ are number of true positives, false negatives (missed points), false positives, and true negatives respectively for point $x_i$.\n   Given that $\\delta$ is close to $0$, then the coefficient of $n^i_{FN}$ and $n^i_{FP}$ dominates the other terms,\n   \\begin{align*}\n   J_{RKL} & - n^i_{FN} d_i\\log\\left(\\frac{a_i}{d_i}\\right) - n^i_{FP} c_i\\log\\left(\\frac{b_i}{c_i}\\right) + O(\\delta)\\\\\n   & - n^i_{FN} \\frac{\\delta}{m - k_i -1 }\\log\\left(\\frac{1-\\delta}{\\delta}\\frac{m-k_i-1}{r_i}\\right) \n   - n^i_{FP} \\frac{1-\\delta}{k_i}\\log\\left(\\frac{\\delta}{1-\\delta}\\frac{k_i}{m-r_i-1}\\right) + O(\\delta).\n   & ( n^i_{FP} \\frac{1-\\delta}{k_i} - n^i_{FN} \\frac{\\delta}{m - k_i -1 }\\log\\left(\\frac{1-\\delta}{\\delta}\\frac{m-k_i-1}{\\delta}\\right) \n   \\log\\left(\\frac{\\delta}{1-\\delta}\\right) \n   \\end{align*}\n   Again, the $\\log \\frac{1-\\delta}{\\delta}$ dominates the other logarithmic terms $\\bigg(\\log \\left(\\frac{m-r_i-1}{k_i}\\right)$ and $\\log \\left(\\frac{m-k_i-1}{r_i}\\right) \\bigg)$, so we have\n   \\begin{align*}\n   J_{RKL} & \\left(\\frac{n^i_{FP}}{k_i}(1-\\delta) - \\frac{n^i_{FN}}{m - k_i -1} \\delta \\right) \\log \\left(\\frac{1-\\delta}{\\delta}\\right) + O(\\delta) \\nonumber\\\\\n   & \\frac{n^i_{FP}}{r_i} C_0   (1-\\text{Precision}(i))C_0 + O(\\delta)\n   \\end{align*}\n   where $C_0   \\log \\left(\\frac{1-\\delta}{\\delta}\\right)$.\\\\\\\\\n   \n   \\noindent For Jensen-Shanon :\n   \\begin{align*}   \n   J_{JS}(x_i) & \\frac{1}{2}\\left(\\sum_j p_{ij} \\log\\frac{2p_{ij}}{p_{ij}+q_{ij}} + \\sum_j q_{ij} \\log\\frac{2q_{ij}}{p_{ij}+q_{ij}}\\right)\\\\\n   & -\\frac{1}{2}\\left(\\sum_j p_{ij} \\log\\frac{p_{ij}+q_{ij}}{p_{ij}} + \\sum_j q_{ij} \\log\\frac{p_{ij}+q_{ij}}{q_{ij}} + \\log 4 \\right)\\\\\n   & -\\frac{1}{2}\\left(\\sum_j p_{ij} \\log\\frac{q_{ij}}{p_{ij}} + \\sum_j q_{ij} \\log\\frac{p_{ij}}{q_{ij}} + \\log 4\\right)\\\\\n   & \\frac{1}{2}\\left(J_{KL}(x_i) + J_{RKL}(x_i) + \\log 4\\right)\n   \\end{align*}   \n\n   \\noindent For Chi-Square distance:\n   \\begin{align*}\n   J_{\\text{CS}}(x_i) & \\sum_j   q_{ji} \\left(\\frac{p_{ji}}{q_{ji}}\\right)^2 \\nonumber\\\\\n   & \\sum_{\\substack{j\\neq i,\\\\p_{ji}a_i,\\\\q_{ji}c_i}} c_i\\left(\\frac{a_i}{c_i}-1\\right)^2 \\\n   + \\sum_{\\substack{j\\neq i,\\\\p_{ji}a_i,\\\\q_{ji}d_i}} d_i\\left(\\frac{a_i}{d_i}-1\\right)^2 \\\n   + \\sum_{\\substack{j\\neq i,\\\\p_{ji}b_i,\\\\q_{ji}c_i}} c_i\\left(\\frac{b_i}{c_i}-1\\right)^2 \\\n   + \\sum_{\\substack{j\\neq i,\\\\p_{ji}b_i,\\\\q_{ji}d_i}} d_i\\left(\\frac{b_i}{d_i}-1\\right)^2 \\nonumber\\\\\n   & n^i_{TP}c_i\\left(\\frac{a_i}{c_i}-1\\right)^2 + n^i_{FN} d_i\\left(\\frac{a_i}{d_i}-1\\right)^2\\\n   + n^i_{FP} c_i\\left(\\frac{b_i}{c_i}-1\\right)^2 + n^i_{TN} d_i\\left(\\frac{b_i}{d_i}-1\\right)^2. \\nonumber\n   \\end{align*}\n   Given that $\\delta$ is near $0$, then the last term $\\left(n^i_{TN} d_i\\left(\\frac{b_i}{d_i}-1\\right)^2\\right)$ gets eliminated. So, we have\n   \\begin{align*}\n   J_{\\text{CS}}(x_i) & n^i_{TP}c_i\\left(\\frac{a_i}{c_i}-1\\right)^2 + n^i_{FN} d_i\\left(\\frac{a_i}{d_i}-1\\right)^2 + n^i_{FP} c_i\\left(\\frac{b_i}{c_i}-1\\right)^2 + O(\\delta) \\nonumber\\\\\n   &   n^i_{TP} \\frac{1-\\delta}{k_i} \\left(\\frac{r_i}{k_i} -1\\right)^2\\nonumber\\\\\n   &\\qquad \\qquad\n   +n^i_{FN} \\frac{\\delta}{m-k_i-1} \\left( \\frac{1-\\delta}{\\delta}\\frac{m-k_i-1}{r_i} -1 \\right)^2\n   +n^i_{FP} \\frac{1-\\delta}{k_i} \\left( \\frac{\\delta}{1-\\delta} \\frac{k_i}{m-r_i-1}-1\\right)^2 + O(\\delta)\\nonumber\\\\\n   & \\frac{n^i_{TP}}{k_i} (1-\\delta) C_0 \n   + \\frac{n^i_{FN}}{r_i} (1-\\delta)C_1\n   + \\frac{n^i_{FP}}{k_i} (1-\\delta)   + O(\\delta)\\\\\n   & \\text{Precision}(i) C_0+(1-\\text{Precision}(i))+(1-\\text{Recall}(i))C_1. \n   \\end{align*}\n   where $C_0   (\\frac{r_i}{k_i} - 1)^2$ and $C_1\\left(\\frac{1-\\delta}{\\delta} \\frac{m-k_i-1}{r_i}- 2\\right)$.\n\n   The proof layout is similar for Hellinger distance, except that it emphasize recall and has less strict penalities,\n   \\begin{align*}\n   J_{\\text{HL}}(x_i) & n^i_{TP}c_i\\left(\\sqrt{\\frac{a_i}{c_i}}-1\\right)^2 + n^i_{FN} d_i\\left(\\sqrt{\\frac{a_i}{d_i}}-1\\right)^2 + n^i_{FP} c_i\\left(\\sqrt{\\frac{b_i}{c_i}}-1\\right)^2\\nonumber\\\\ \n   &n^i_{TP} \\frac{1-\\delta}{k_i} \\left(\\sqrt{\\frac{r_i}{k_i}}-1\\right)^2\\nonumber\\\\\n   &\\qquad\n   +n^i_{FN} \\frac{\\delta}{m-k_i-1} \\left( \\sqrt{\\frac{1-\\delta}{\\delta}\\frac{m-k_i-1}{r_i}} -1 \\right)^2   \n   +n^i_{FP} \\frac{1-\\delta}{k_i} \\left( \\sqrt{\\frac{\\delta}{1-\\delta} \\frac{k_i}{m-r_i-1}}-1\\right)^2 + O(\\delta)\\nonumber\\\\\n   & \\frac{n^i_{TP}}{k_i} (1-\\delta) C_0 \n   + \\frac{n^i_{FN}}{r_i} (1-\\delta)C_1\n   + \\frac{n^i_{FP}}{k_i} (1-\\delta)C_2+ O(\\delta)\\\\\n   & \\text{Precision}(i) C_0+(1-\\text{Precision}(i))C_2+(1-\\text{Recall}(i))C_1 + O(\\delta).\n   \\end{align*}   \n   where $C_{0}   \\left(\\sqrt{\\frac{r_i}{k_i}} - 1\\right)^2$, \n   $C_{2}   \\left( 1 - 2\\sqrt{\\frac{r_i\\delta}{1-\\delta}}\\right)$, and\n   $C_{1}   \\left( 1 - 2\\sqrt{\\frac{k_i\\delta}{1-\\delta}}\\right)$.\n\n   \\end{proof}\n   \\subsection{Wasserstein SNE}\n   \\label{app:wsne}\n   \\begin{theorem}\n   The neighbourhood probability distribution $Q_Y$ with the embedding parameters $Y$ is locally $L(Y)$-lipshitz continous, i.e., $Q_Y(X) - Q_{Y^\\prime}(X) \\leq L(Y) (Y - Y^\\prime)$.\n   Then, $W_f(P,Q_Y)$ is continous everywhere and differentible almost everywhere in $Y$.\n   \\end{theorem}\n\n   \\begin{proof}\n   We are interested in bounding $| \\max_f W_f(P,Q_Y) - \\max_{f^\\prime} W_{f^\\prime} (P,Q_{Y^\\prime}) |$.\n   \\begin{align}\n   | \\max_f W_f(P,Q_Y) - \\max_{f^\\prime} W_{f^\\prime} (P,Q_{Y^\\prime}) | &\\leq \\max_f | W_f(P,Q_Y) - W_f(P,Q_{Y^\\prime}) |\\\\\n   & | W_{f^*} (P, Q_Y) -   W_{f^*} (P, Q_{Y^\\prime})\\\\ \n   &\\leq W_{f^*} (Q_Y, Q_{Y^\\prime}). \n   \\end{align}\n   where $f^*$ is the optimial function $f$ of $\\max_f | W_f(P,Q_Y) - W_f(P,Q_{Y^\\prime}) |$.\n   The second inequality comes triangle inequality. Consider $W_{f^*} (Q_Y, Q_{Y^\\prime})$.\n   \\begin{align} \n   W_{f^*} (Q_Y, Q_{Y^\\prime}) & \\mathbb{E}_{Q_Y}\\left[f^*(x_{ij})\\right] - \\mathbb{E}_{Q_{Y^\\prime}}\\left[f^*(x_{ij^\\prime})\\right]\\\\\n   & \\sum_{ij} Q_{y_{ij}}(x_{ij})f(x_{ij}) - \\sum_{ij^\\prime} Q_{y_{ij}^\\prime}(x_{ij^\\prime})f(x_{ij^\\prime})\\\\\n   & \\sum_{ij} f(x_{ij}) \\left[Q_{y_{ij}}(x_{ij}) -   Q_{y_{ij}^\\prime}(x_{ij})\\right]\\\\\n   &\\leq \\sum_{ij} f(x_{ij}) L(Y) (y_{ij} - y_{ij^\\prime})\n   \\end{align}\n   The last inequality is by the locally $L(y_{ij})$-lipshitz assumption of $Q_Y$ embedding function.\n   \n   We know that $Q_Y$ is continous, so there is continous path from $Y\\to Y^\\prime$ such that $\\|Q_Y - Q_{Y^\\prime}\\|\\to 0$.\n   As well, we know that $\\max_f W_f(P,Q_Y)$ is lipschitz and continous everywhere.\n   Applying Radamarcher's theorem, $W_f(P,Q_Y)$ is differentiable almost everywhere.\n   \\end{proof}\n\n   \\subsection{MMD-SNE}\n   \\label{app:mmd_appendix}\n   \\begin{theorem} (\\cite{Gretton2012}) \n   Let $\\mathcal{F}$ be a unit ball in a universal RKHS $\\mathcal{H}$, defined on the compact metric space $\\mathcal{X}$, \n   with associated continous characterstic kernel $k(\\cdot, \\cdot)$. Then $MMD_\\mathcal{F}(P,Q)   0 \\Longleftrightarrow P   Q$.\n   \\end{theorem}\n\n   \\begin{theorem}\n   The neighbourhood probability distribution $Q_Y$ with the embedding parameters $Y$ is locally $L(Y)$-lipshitz continous., i.e., $Q_Y(X) - Q_{Y^\\prime}(X) \\leq L(Y) (Y - Y^\\prime)$.\n   Let $k$ be the Gaussian kernel function and $f$ be some bounded non-linear injective function.\n   Then, $MMD_{k\\circ f_theta}(P,Q_Y)$ is continous everywhere and differentible almost everywhere in $Y$.\n   \\end{theorem}\n   \\begin{proof}\n   Similar from Theorem~\\ref{lab:thm_wwd}, we bound $|\\max_f MMD_{k\\circ f_\\theta}(P,Q_Y) - \\max_{f^\\prime} MMD_{k \\circ f^\\prime} (P,Q_{Y^\\prime})|$\n   to show that its continous and locally lipschitz.\n   \\begin{align}\n   |\\max_f MMD_{k\\circ f}(P,Q_Y) - \\max_{f^\\prime} MMD_{k \\circ f^\\prime} (P,Q_{Y^\\prime})| \n   &\\leq \\max_f | MMD_{k\\circ f}(P,Q_Y) -   MMD_{k \\circ f} (P,Q_{Y^\\prime})| \\\\\n   & | MMD_{k\\circ f^*}(P,Q_Y) -   MMD_{k \\circ f^*} (P,Q_{Y^\\prime})| \\\\\n   &\\leq MMD_{k\\circ f^*}(Q_Y, Q_{Y^\\prime})\n   \\end{align}\n   where $f^*$ is the optimal function of $| MMD_{k\\circ f}(P,Q_Y) -   MMD_{k \\circ f} (P,Q_{Y^\\prime})|$.\n   \n   \\begin{align}\n   MMD_{k\\circ f^*}(Q_Y, Q_{Y^\\prime}) \n   & \\mathbb{E}_{Q_Y}\\mathbb{E}_{Q_Y}\\left[ k\\left(f(x), f(x^\\prime)\\right) \\right] - 2\\mathbb{E}_{Q_Y}\\mathbb{E}_{Q_{Y^\\prime}} \\left[ k\\left(f(x), f(x^\\prime)\\right) \\right] \\nonumber \\\\\n   &\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad + \\mathbb{E}_{Q_{Y^\\prime}}   \\mathbb{E}_{Q_{Y^\\prime}} \\left[ k\\left(f(x^\\prime), f(x^{\\prime\\prime})\\right) \\right] \\\\\n   &\\leq \\mathbb{E}_{Q_Y} \\mathbb{E}_{Q_Y} \\mathbb{E}_{Q_{Y^\\prime}} \\left[ | k(f(x), f(x^\\prime)) - k(f(x), f(x^{\\prime\\prime})) | \\right] \\nonumber\\\\\n   &\\qquad \\qquad   + \\mathbb{E}_{Q_{Y^\\prime}} \\mathbb{E}_{Q_{Y^\\prime}} \\mathbb{E}_{Q_Y} \\left[ | k(f(x^\\prime), f(x^{\\prime\\prime})) - k(f(x^\\prime), f(x)) | \\right] \\label{eqn:mmd1}\\\\\n   &\\leq 2.\n   \\end{align}   \n   The last inequality is due to the difference of two negative exponential functions (two Gaussian kernel functions) on positive interval are bounded.\n   \n   Then, $MMD_{k\\circ f^*}(Q_Y, Q_{Y^\\prime})$ as the $Q_Y^\\prime \\rightarrow Q_Y$ due to bounded convergence theorem.\n   Then, $|\\max_f MMD_{k\\circ f}(P,Q_Y) - \\max_{f^\\prime} MMD_{k \\circ f^\\prime} (P,Q_{Y^\\prime})| \\rightarrow 0$ as $Q_Y^\\prime \\rightarrow Q_Y$.\n   Therefore $MMD_{k\\circ f}(P,Q_Y)$ is continous.\n   \n   Now, we show that $MMD_{k\\circ f^*}(P, Q_Y)$ is locally lipschitz continous.\n   As well, Equation~\\ref{eqn:mmd1} can bounded by \n   \\begin{align}\n   MMD_{k\\circ f^*}(Q_Y, Q_{Y^\\prime}) \n   & \\mathbb{E}_{Q_Y}\\mathbb{E}_{Q_Y}\\left[ k\\left(f(x), f(x^\\prime)\\right) \\right] - 2\\mathbb{E}_{Q_Y}\\mathbb{E}_{Q_{Y^\\prime}} \\left[ k\\left(f(x), f(x^\\prime)\\right) \\right] \\nonumber \\\\\n   &\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad + \\mathbb{E}_{Q_{Y^\\prime}}   \\mathbb{E}_{Q_{Y^\\prime}} \\left[ k\\left(f(x^\\prime), f(x^\\prime)\\right) \\right] \\\\\n   & \\mathbb{E}_{Q_Y} \\left[ \\mathbb{E}_{Q_Y}\\left[ k\\left(f(x), f(x^\\prime)\\right) \\right] - \\mathbb{E}_{Q_{Y^\\prime}} \\left[ k\\left(f(x), f(x^{\\prime})\\right) \\right] \\right]\\nonumber\\\\\n   &\\qquad \\qquad +\\mathbb{E}_{Q_{Y^\\prime}} \\left[ \\left[ k\\left(f(x), f(x^\\prime)\\right) \\right] -   \\mathbb{E}_{Q_Y} \\left[ k\\left(f(x), f(x^\\prime)\\right) \\right]\\right] \\\\   \n   & \\sum_{ij} \\sum_{ij^\\prime} k\\left(f(x_{ij}),f(x_{ij^\\prime})\\right) Q_{y_{ij}} \\left(Q_{y_{ij}} - Q_{y^\\prime_{ij}}\\right) \\nonumber\\\\\n   &\\qquad \\qquad + \\sum_{ij} \\sum_{ij^\\prime} k\\left(f(x_{ij}),f(x_{ij^\\prime})\\right) Q_{y^\\prime_{ij}} \\left(Q_{y^\\prime_{ij}} - Q_{y_{ij}}\\right) \\\\\n   &\\leq \\sum_{ij} \\sum_{ij^\\prime} k\\left(f(x_{ij}),f(x_{ij^\\prime})\\right) Q_{y_{ij}} L(Y) \\left(y_{ij} - y^\\prime_{ij}\\right) \\nonumber \\\\\n   &\\qquad \\qquad + \\sum_{ij} \\sum_{ij^\\prime} k\\left(f(x_{ij}),f(x_{ij^\\prime})\\right) Q_{y^\\prime_{ij}} L(Y) \\left(y^\\prime_{ij} - y_{ij}\\right) \\\\\n   & \\sum_{ij} \\sum_{ij^\\prime} k\\left(f(x_{ij}),f(x_{ij^\\prime})\\right) L(Y) \\left[ \\left(Q_{y^\\prime_{ij}}   \\left(y^\\prime_{ij} - y_{ij}\\right) \\right) - \\left( Q_{y^\\prime_{ij}} \\left(y^\\prime_{ij} - y_{ij}\\right)\\right) \\right] \\\\\n   &\\le \\infty\n   \\end{align}\n   Since Gaussian kernel $k\\left(f(x),f(x^\\prime)\\right)$ is bounded and $Q_{y_{ij}}$ are probability of $y_i$ and $y_j$ being neighbour it is bounded, the whole equation is bounded.\n   Therefore, $|\\max_f MMD_{k\\circ f}(P,Q_Y) - \\max_{f^\\prime} MMD_{k \\circ f^\\prime} (P,Q_{Y^\\prime})| \\rightarrow 0$ as $Q_Y^\\prime \\rightarrow Q_Y$ is locally lipschitz continous.\n   \n   We can apply Radamarcher's theorem, since $\\max_f MMD_{k\\circ f}(P,Q_Y)$ is locally lipschitz and continous everywhere.\n   Thus, $MMD_{k\\circ f}(P,Q_Y)$ is differentiable almost everywhere.\n   \\end{proof}\n   \n   \\subsubsection{Large-scale batch training}\n   \\label{app:mmd_largscale}\n   The kernel representation of MMD formulation in Equation~\\ref{eqn:mmd_kern} can be easily intractible in practice due to the quadratic comparisions (double summation).\n   For example, given 100 data points. The number of all pairs of points becomes 10,000.\n   Then, computing the kernel matrix of all pairs of points becomes $10,000^2$.\n   This show that the computation easily becomes intractable.\n\n   Fortunately, we can compute the sample average of Equation~\\ref{eqn:mmd_kern}, \n   \\begin{align}\n   \\widetilde{MMD}^2_{k}(P, Q_Y)   \\mathbb{E}_{x_{ij},x_{st}\\sim P} \\big[ k(x_{ij},x_{st}) \\big] \n   - 2\\mathbb{E}_{x_{ij} \\sim P,y_{st}\\sim Q} \\big[ k(x_{ij}, y_{st}) \\big] + \\mathbb{E}_{y_{ij},y_{st}\\sim Q} \\big[ k(y_{ij}, y_{st}) \\big].\n   \\end{align}\n   This is an unbiased estimator of $MMD^2_K(P,Q_Y)$.\n\n   In order to optimize the embedding parameter $Y$, we need to compute the gradient of sample average of squared MMD. \n   Note that the gradient of sample average of $MMD^2_K(P, Q_Y)$ with respect to the embedding parameter $Y$ is still not the same as sample average of the $\\nabla MMD^2_K(P, Q_Y)$.\n   Fortunately, we use the trick of $\\nabla_{Y} \\mathbb{E}_{P_Y} \\left[f(x)\\right]   \\mathbb{E}\\left[ f(x) \\nabla_Y \\log \\left( P_Y \\right)\\right]$,\n   then we get\n   \\begin{multline}\n   \\nabla_{Y} \\widetilde{MMD}^2_{k}(P, Q_Y) \n   \\mathbb{E}_{y_{ij}\\sim Q_Y}\\mathbb{E}_{y_{st}\\sim Q_Y} \\bigg[ k(y_{ij}, y_{st}) \\big[ \\nabla_Y \\log Q_Y(y_{ij}) + \\nabla_Y \\log Q_Y(y_{st}) \\big]\\bigg] \\\\\n   - 2\\mathbb{E}_{x_{ij}\\sim P}\\mathbb{E}_{y_{st}\\sim Q_Y} \\bigg[ k(x_{ij}, y_{st}) \\nabla_Y \\log Q_Y(y_{st})\\bigg]\n   \\label{eqn:grad_mmd}\n   \\end{multline}\n\n   See the appendix for the derivation of Equation~\\ref{eqn:grad_mmd}.\n   \\begin{proof}\n   \\begin{align}\n   \\nabla_{Y} \\sum_{x_{ij},x_{st}}   k(x_{ij},x_{st}) &\\bigg[ P(x_{ij})P(x_{st}) - 2 P(x_{ij})Q_Y(x_{st}) + Q_Y(x_{ij})Q_Y(x_{st}) \\bigg] \\nonumber\\\\\n   & \\sum_{x_{ij},x_{st}}   k(x_{ij},x_{st}) \\nabla_{Y} \\bigg[ P(x_{ij})P(x_{st}) - 2 P(x_{ij})Q_Y(x_{st}) + Q_Y(x_{ij})Q_Y(x_{st}) \\bigg] \\nonumber\\\\\n   & \\sum_{x_{ij},x_{st}}   k(x_{ij},x_{st}) \\bigg[ \\nabla_{Y} \\bigg(Q_Y(x_{ij})Q_Y(x_{st})\\bigg) - 2 \\nabla_{Y}P(x_{ij})Q_Y(x_{st}) \\bigg] \\nonumber\\\\\n   & \\sum_{x_{ij},x_{st}}   k(x_{ij},x_{st}) \\nabla_{Y} \\bigg(Q_Y(x_{ij})Q_Y(x_{st})\\bigg) - 2 \\sum_{x_{ij},x_{st}}   k(x_{ij},x_{st}) P(x_{ij})\\nabla_{Y}Q_Y(x_{st}) \\nonumber\\\\\n   & \\sum_{x_{ij},x_{st}}   k(x_{ij},x_{st}) \\bigg( Q_Y(x_{ij}) \\nabla_{Y} Q_Y(x_{st}) +   Q_Y(x_{st}) \\nabla_{Y} Q_Y(x_{ij})\\bigg) \\nonumber\\\\\n   &\\qquad\\qquad\\qquad\\qquad- 2 \\sum_{x_{ij},x_{st}}   k(x_{ij},x_{st}) P(x_{ij})Q_Y(x_{st}) \\nabla_{Y}\\log Q_Y(x_{st}) \\\\\n   & \\sum_{x_{ij},x_{st}}   k(x_{ij},x_{st}) Q_Y(x_{ij}) Q_Y(x_{st}) \\bigg( \\nabla_{Y} \\log Q_Y(x_{st}) + \\nabla_{Y} \\log Q_Y(x_{ij})\\bigg) \\nonumber\\\\\n   &\\qquad\\qquad\\qquad\\qquad- 2 \\sum_{x_{ij},x_{st}}   k(x_{ij},x_{st}) P(x_{ij})Q_Y(x_{st}) \\nabla_{Y}\\log Q_Y(x_{st}) \\\\\n   & \\mathbb{E}_{x_{ij},x_{st}\\sim Q_Y}   \\left [ k(x_{ij},x_{st}) \\bigg( \\nabla_{Y} \\log Q_Y(x_{st}) +   \\nabla_{Y} \\log Q_Y(x_{ij})\\bigg) \\right] \\nonumber\\\\\n   &\\qquad\\qquad\\qquad\\qquad- 2 \\mathbb{E}_{x_{ij}\\sim P,x_{st}\\sim Q_Y}   \\bigg[ k(x_{ij},x_{st}) \\nabla_{Y}\\log Q_Y(x_{st})\\bigg] \n   \\end{align}\n   The first term of third equality is applying the derivative product rule and the second term of third equality is apply the trick mentioned.\n   We apply the trick to the first term of fourth equality again. For the last equality, we express in terms of expecation over the distributions.\n   \\end{proof}\n   \\begin{multline}\n   MMD_{k\\circ f_\\theta}(P, Q_Y)   \\sum_{x_{ij},x_{ij'}\\sim P} k(f_\\theta(x_{ij}), f_\\theta(x_{ij'})) -2 \\sum_{x_{ij} \\sim P,y_{ij'}\\sim Q} k(f_\\theta(x_{ij}), f_\\theta(y_{ij'})) \\\\\n   + \\sum_{y_{ij},y_{ij'}\\sim Q} k(f_\\theta(y_{ij}), f_\\theta(y_{ij'})) \n   \\end{multline}\n   where $k\\circ f$ is a characteristic kernel function and $f_\\theta(\\cdot)$ is a function with parameter $\\theta$. \n   We will use neural networks for $f$ in practice to learn a non-linear kerenl function, $k \\circ f$.\n\n   \\clearpage\n\n   \\section{Variational $ft$-SNE}\n\n   \\begin{algorithm}[htp]\n   \\caption{Variational (Adversarial) SNE Optimization Algorithm}\\label{euclid}\n   \\label{algo:vfsne_update_rule2}\n   \\begin{algorithmic}[1]\n   \\Procedure{Optimization}{Dataset $\\{X_{tr}, X_{vl}\\}$, learning rate $\\eta$, $f$-divergence $J$}\n   \\State Initialize the discriminant parameter $\\phi$.\n   \n   \\While {$\\phi$ has not converged}\n   \\For {$j1,\\ldots, J$}\n   \\State $\\phi_{t+1}   \\phi_t + \\eta \\nabla_\\phi J$.\n   \\EndFor\n   \\For {$k1,\\ldots, K$}\n   \\State $y^i_{t+1}   y^i_t - \\eta_y \\nabla_y J$.\n   \\EndFor\n   \\EndWhile\n   \\EndProcedure\n   \\end{algorithmic}\n   \\end{algorithm}\n\n   It is standard to relax the optimization of the variational $ft$-SNE objective function in Eq~\\ref{eqn:vfsne} by alternatively optimizing the paramters $\\phi$ and $y_1, \\ldots, y_m$.\n   Algorithm~\\ref{algo:vfsne_update_rule} alternatively updates $y_1, \\ldots, y_m$ and $\\phi$.\n   The parametric hypothesis class $\\bar{\\mathcal{H}}$ is parameterized by $\\phi$ (for instance, $\\phi$ are the weights of the deep neural network).\n   Remark that this is not guaranteed to return the same solution as the original minimax objective in Eq~\\ref{eqn:vfsne}. \n   Thus it is possible that Algorithm~\\ref{algo:vfsne_update_rule} can find a different solution depending on the choice of $J$ and $K$ and under different measures.\n\n\n   \\begin{table}[h!]\n   \\centering\n   {\\small \n   \\caption{Variational $ft$-SNE. }\n   \\label{tab:vfSNE}\n   \\begin{tabular}{lcccc}\\hline\n   $D_f(P\\|Q)$   & $f(t)$   & $f^*(t)$   & $h(x)$ \\\\\\hline\\hline\n   Kullback-Leibler (KL)   & $t\\log t$   & $\\exp(t-1)$   & $x$\\\\\n   Reverse-KL (RKL)   & $-\\log t$   & $-1-\\log(-t)$   & $-\\exp(-x)$\\\\\n   Jensen-Shannon (JS)   & $-(t+1)\\log\\frac{(1+t)}{2} + t\\log t$ & $-\\log(1-\\exp(t))$   & $\\log(2) - \\log\\left(1+\\exp(-x)\\right)$\\\\\n   Hellinger distance (HL)   & $(\\sqrt{t} -1)^2$   & $\\frac{t}{1-t}$   & $1-\\exp(-x)$\\\\ \n   Chi-square ($\\mathcal{X}^2$ or CS) & $(t-1)^2$\t\t   & $\\frac{1}{4}t^2+t$   & $x$\\\\   \\hline\n   \\end{tabular}}\n   \\end{table}\n\n   \\newpage\n   \\section{Experimental Supplementary Materials}\n   \\label{app:exp}\n\n   \\begin{figure}[htp]\n   \\centering\n   \\begin{minipage}{0.4\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/blobs_data.pdf}\n   \\vspace{-0.6cm}\n   \\label{fig:syn_blobs}\n   \\subcaption{Three Gaussian clusters}\n   \\end{minipage}   \n   \\begin{minipage}{0.4\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/swiss_roll_data.pdf}\n   \\vspace{-0.6cm}\n   \\label{fig:syn_swiss}\n   \\subcaption{Swiss Roll}\n   \\end{minipage}   \n   \\caption{Synthetic Datasets}\n   \\label{fig:toy_dataset}\n   \\end{figure}\n\n   {\\em Datasets}. Throughout the experiments, we diversified our datasets by selecting manifold, cluster, and hierical datasets.\n   We first experimented with two synthetic datasets, swiss roll and three Gaussian cluter datasets (see S. M. Figure~\\ref{fig:toy_dataset}).\n   Thence, we conducted the set of experiments on FACE, MNIST, and 20 Newsgroups datasets.\n   FACE and MNIST with single digits (MNIST1) fall under manifold datasets and MNIST and 20 Newsgroups fall under cluster and hierical cluster datasets. \n   \n   \\begin{itemize}[noitemsep,nolistsep]\n   \\item FACE contains 698 64 x 64 face images. \n   The face varies smoothly with respect to light intensities and poses.\n   The face dataset used in isomap paper \\cite{Tenenbaum2000}.\n   We use face dataset as a manifold dataset.\n\n   \\item MNIST consists of 28 x 28 handwritten digits dataset with digits from 0 to 9. \n   MNIST data points were projected down to $30$ features using PCA. \n   We used MNIST as both clustering and manifold datasets. \n   For clustering dataset, we used 6,000 examples of first five digits (MNIST). \n   For manifold dataset, we used 6,000 examples of digits of ones (MNIST1).\n\n   \\item 20-NEWSGROUPS consists of 20 different news genres. Among 20 news genres, \n   some of the genres fall under the same abstract categories.\n   The 20-newsgroup data are represented using bag of words.\n   For sparse clustering dataset, we sample 6,000 examples of news articles that\n   fall under seven categroies : automobile, medical, christian, politics, forsales, and microsoft windows.\n   We used 6,000 new articles that fall under thirteen categories:\n   {\\em rec.autos, rec.motorcycles, rec.sport.baseball, rec.sport.hockey, \n   sci.crypt, sci.electronics, sci.med, sci.space, soc.religion.christian,\n   talk.politics.guns, talk.politics.mideast, talk.politics.misc, and talk.religion.misc.}\n   Hence, this dataset corresponds to sparse hierarchical clustering dataset.\n   \\end{itemize}\n\n   {\\em Optimization}. We use gradient decent method with momentum to optimize the $ft$-SNE. \n   We set our learning rate to be approximately $(\\epsilon_0) \\~ \\frac{n}$ \n   We decreased the learning rate and momentum overtime as such\n   $\\epsilon_{t+1}   \\frac{\\epsilon_{t}}{(1+\\frac{t}{\\rho})}$ and \n   $\\lambda_{t+1}   \\frac{\\lambda_{t}}{(1+\\frac{t}{\\eta})}$\n   \\begin{align}\n   \\epsilon_{t+1} & \\frac{\\epsilon_{t}}{(1+\\frac{t}{\\rho})}, \\qquad\n   \\label{eqn:lrdecay}\n   \\lambda_{t+1} & \\frac{\\lambda_{t}}{(1+\\frac{t}{\\eta})} \n   \\end{align}\n   where $\\epsilon_t$ and $\\lambda_t$ are learning rate and momentum, and $\\rho$ and $\\eta$ are learning rate decay and momentum decay parameters.\n   $t$-SNE has very tiny gradients in the beginning since all the parameters are intialize in the quite small domain (the initial embeddings are drawn from the Normal distribution with zero mean and $1*e^{-4}$ standard deviation). \n   However, once the embedding parameters spread, the gradients become relatively large compare to early stage. \n   Thus, the learning rate and momentum require to be adjusted appropriately over different stage of optimization.\n\n   \\newpage\n   \\subsection{More Experimental Results : Synthetic Data Experiments}\n\n\n   \\begin{figure}\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.16\\textwidth}\n   \\centering MNIST1\n   \\end{minipage}   \n   \\begin{minipage}{0.16\\textwidth}\n   \\centering Face\n   \\end{minipage}   \n   \\begin{minipage}{0.16\\textwidth}\n   \\centering MNIST\n   \\end{minipage}   \n   \\begin{minipage}{0.16\\textwidth}\n   \\centering GENE\n   \\end{minipage}   \n   \\begin{minipage}{0.16\\textwidth}\n   \\centering NEWS\n   \\end{minipage}   \n   \\begin{minipage}{0.16\\textwidth}\n   \\centering SBOW\n   \\end{minipage}   \n   \\end{minipage}\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.16\\textwidth}\n   \\includegraphics[width\\linewidth]{mnist1_knnVSkfn_perp25_1000epoch_c.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\begin{minipage}{0.16\\textwidth}\n   \\includegraphics[width\\linewidth]{face_knnVSkfn_50perp_500epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\begin{minipage}{0.16\\textwidth}\n   \\includegraphics[width\\linewidth]{mnist_knnVSkfn_perp100_1000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\begin{minipage}{0.16\\textwidth}\n   \\includegraphics[width\\linewidth]{gene_knnVSkfn_perp50_2000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\begin{minipage}{0.16\\textwidth}\n   \\includegraphics[width\\linewidth]{news_knnVSkfn_100perp_1000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\begin{minipage}{0.16\\textwidth}\n   \\includegraphics[width\\linewidth]{sbow_knnVSkfn_perp50_1000epoch_v0_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\subcaption{FN Precision vs. NN Precision}\n   \\end{minipage}\n   \\caption{Precision-Recall curves for each of the proposed algorithms on all datasets. Each row corresponds to a different dataset, each column to different quantitative criteria, and each line to a different algorithm.}\n   \\label{fig:results_diff_metric2}\n   \\end{figure}\n\n   \\begin{figure}[htp]\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.195\\textwidth}\n   \\subcaption*{RKL: $\\alpha0$}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\subcaption*{$\\alpha0.01$}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\subcaption*{$\\alpha0.1$}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\subcaption*{JS: $\\alpha0.5$}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\subcaption*{KL: $\\alpha1$}\n   \\end{minipage}   \n   \\end{minipage}   \n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_10perp2001epoch_initlr1000_numpoints5000_fold0_epoch2000_00.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_10perp2001epoch_initlr1000_numpoints5000_fold0_epoch2000_05.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_10perp2001epoch_initlr1000_numpoints5000_fold0_epoch2000_1.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_10perp2001epoch_initlr1000_numpoints5000_fold0_epoch2000_5.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_10perp2001epoch_initlr1000_numpoints5000_fold0_epoch2000_10.pdf}\n   \\end{minipage}   \n   \\subcaption*{Perplexity10} \n   \\end{minipage}   \n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_100perp2001epoch_initlr1000_numpoints5000_fold0_epoch100_00.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_100perp2001epoch_initlr1000_numpoints5000_fold0_epoch100_05.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_100perp2001epoch_initlr1000_numpoints5000_fold0_epoch100_1.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_100perp2001epoch_initlr1000_numpoints5000_fold0_epoch100_5.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_100perp2001epoch_initlr1000_numpoints5000_fold0_epoch2000_10.pdf}\n   \\end{minipage}   \n   \\subcaption*{Perplexity100} \n   \\end{minipage}   \n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_500perp2001epoch_initlr1000_numpoints5000_fold0_epoch2000_00.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_500perp2001epoch_initlr1000_numpoints5000_fold0_epoch2000_05.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_500perp2001epoch_initlr1000_numpoints5000_fold0_epoch2000_1.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_500perp2001epoch_initlr250_numpoints5000_fold0_epoch2000_5.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_500perp2001epoch_initlr250_numpoints5000_fold0_epoch2000_10.pdf}\n   \\end{minipage}   \n   \\subcaption*{Perplexity500} \n   \\end{minipage}   \n   \\caption{$t$-SNE embeddings on three Gaussian clusters}   \n   \\label{fig:syn_embeddings_blobs}\n   \\end{figure}\n   \\begin{figure}[htp]\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.195\\textwidth}\n   \\subcaption*{RKL: $\\alpha0$}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\subcaption*{$\\alpha0.01$}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\subcaption*{$\\alpha0.1$}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\subcaption*{JS: $\\alpha0.5$}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\subcaption*{KL: $\\alpha1$}\n   \\end{minipage}   \n   \\end{minipage}   \n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_9perp2001epoch_initlr1000_numpoints5000_fold0_epoch2000_0.pdf}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_7perp2001epoch_initlr1000_fold0_epoch2000_0.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_9perp2001epoch_initlr1000_numpoints5000_fold0_epoch2000_01.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_9perp2001epoch_initlr1000_numpoints5000_fold0_epoch2000_1.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_9perp2001epoch_initlr1000_numpoints5000_fold0_epoch2000_5.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_9perp2001epoch_initlr1000_numpoints5000_fold0_epoch2000_10.pdf}\n   \\end{minipage}   \n   \\subcaption{Perplexity10} \n   \\end{minipage}   \n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_100perp3001epoch_initlr1000_numpoints5000_fold0_epoch100_0.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_100perp3001epoch_initlr1000_numpoints5000_fold0_epoch100_05.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_100perp3001epoch_initlr1000_numpoints5000_fold0_epoch100_1.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_100perp3001epoch_initlr1000_numpoints5000_fold0_epoch3000_5.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_100perp3001epoch_initlr1000_numpoints5000_fold0_epoch3000_10.pdf}\n   \\end{minipage}   \n   \\subcaption{Perplexity100} \n   \\end{minipage}   \n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_500perp3001epoch_initlr1000_numpoints5000_fold0_epoch3000_0.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_500perp3001epoch_initlr1000_numpoints5000_fold0_epoch3000_05.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_500perp3001epoch_initlr1000_numpoints5000_fold0_epoch3000_1.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_500perp3001epoch_initlr1000_numpoints5000_fold0_epoch3000_5.pdf}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_500perp3001epoch_initlr1000_numpoints5000_fold0_epoch3000_10.pdf}\n   \\end{minipage}   \n   \\subcaption{Perplexity500} \n   \\end{minipage}   \n   \\caption{$t$-SNE embeddings on Swiss Roll}   \n   \\label{fig:swiss_roll_embeddings}\n   \\end{figure}\n\n   \\pagebreak\n\n   \\begin{table}[htp]\n   \\begin{minipage}{0.49\\textwidth}\n   \\centering\n   \\caption{F-Score on X-Y MNIST}\n   \\label{tab:results1}\n   {\\footnotesize\n   \\begin{tabular}{ccccc}\\hline\n   KL   &   RKL   &   JS   & HL   & CH \\\\ \\hline\\hline\n   0.3524   & 0.4241 & 0.4190 &   0.4155 & 0.0922 \\\\\n   0.4440   & 0.5699 & 0.5395 &   0.5332   & 0.1476 \\\\\n   0.4724   & 0.6057 & 0.5570 &   0.5546   & 0.2349 \\\\\n   0.4603   & 0.5687 & 0.5220 &   0.5217   & 0.3007 \\\\\n   0.4407   & 0.5234 & 0.4843 &   0.4843   & 0.3222 \\\\\n   0.4202   & 0.4798 & 0.4491 &   0.4503   & 0.3350 \\\\\n   0.4016   & 0.4413 & 0.4189 &   0.4210   & 0.3420 \\\\\n   0.3836   & 0.4090 & 0.3939 &   0.3964   & 0.3449 \\\\\n   0.3667   & 0.3814 & 0.3722 &   0.3745   & 0.3446 \\\\\\hline\n   \\end{tabular}}\n   \\vspace{-0.5cm}\n   \\end{table}\n   \\end{minipage}\n   \\begin{minipage}{0.49\\textwidth}\n   \\begin{table}[htp]\n   \\centering\n   \\caption{F-Score on X-Y FACE}\n   \\label{tab:results}\n   {\\footnotesize\n   \\begin{tabular}{ccccc}\\hline\n   KL   &   RKL   &   JS   & HL   & CH \\\\ \\hline\\hline\n   0.4019 & 0.4186 & 0.4156 & 0.4115 & 0.2112 \\\\\n   0.5648 & 0.6446 & 0.6216 & 0.6197 & 0.2919 \\\\\n   0.6236 & 0.7534 & 0.7200 & 0.7146 & 0.3505 \\\\\n   0.5865 & 0.6970 & 0.6793 & 0.6764 & 0.3674 \\\\\n   0.5354 & 0.6207 & 0.6105 & 0.6095 & 0.3689 \\\\\n   0.4870 & 0.5447 & 0.5441 & 0.5412 & 0.3642 \\\\\n   0.4464 & 0.4841 & 0.4862 & 0.4826 & 0.3573 \\\\ \\hline\n   \\end{tabular}}\n   \\vspace{-0.5cm}\n   \\end{minipage}\n   \\end{table}\n   \n   \\begin{table}[htp]\n   \\begin{minipage}{0.49\\textwidth}\n   \\centering\n   \\caption{F-Score on X-Y MNIST}\n   \\label{tab:results}\n   {\\footnotesize\n   \\begin{tabular}{ccccc}\\hline\n   KL   &   RKL   &   JS   & HL   & CH \\\\ \\hline\\hline\n   0.4795 & 0.4444 & 0.4693 & 0.4494 & 0.4787 \\\\\n   0.5938 & 0.5805 & 0.6006 & 0.5667 & 0.6109 \\\\\n   0.5466 & 0.5872 & 0.5834 & 0.5426 & 0.5772 \\\\\n   0.4891 & 0.5334 & 0.5256 & 0.4895 & 0.5200 \\\\\n   0.4457 & 0.4868 & 0.4783 & 0.4494 & 0.4742 \\\\\n   0.4149 & 0.4495 & 0.4409 & 0.4186 & 0.4391 \\\\\n   0.3925 & 0.4180 & 0.4103 & 0.3950 & 0.4112 \\\\\n   0.3749 & 0.3920 & 0.3856 & 0.3757 & 0.3879 \\\\\n   0.3597 & 0.3699 & 0.3657 & 0.3595 & 0.3688 \\\\\\hline\n   \\end{tabular}}\n   \\vspace{-0.5cm}\n   \\end{table}\n   \\end{minipage}\n   \\begin{minipage}{0.49\\textwidth}\n   \\begin{table}[htp]\n   \\centering\n   \\caption{F-Score on X-Y NEWS}\n   \\label{tab:results}\n   {\\footnotesize\n   \\begin{tabular}{ccccc}\\hline\n   KL   &   RKL   &   JS   & HL   & CH \\\\ \\hline\\hline\n   0.3996 & 0.3665 & 0.3922 & 0.3989 & 0.2637 \\\\\n   0.4256 & 0.4108 & 0.4328 & 0.4331 & 0.3703 \\\\\n   0.3820 & 0.3933 & 0.4001 & 0.3964 & 0.4466 \\\\\n   0.3387 & 0.3569 & 0.3559 & 0.3526 & 0.4633 \\\\\n   0.3062 & 0.3209 & 0.3188 & 0.3163 & 0.4599 \\\\\n   0.2796 & 0.2899 & 0.2877 & 0.2862 & 0.4476 \\\\\n   0.2562 & 0.2626 & 0.2610 & 0.2594 & 0.4297 \\\\\n   0.2346 & 0.2380 & 0.2370 & 0.2357 & 0.4076 \\\\\n   0.2145 & 0.2162 & 0.2157 & 0.2148 & 0.3824 \\\\\\hline \n   \\end{tabular}}\n   \\vspace{-0.5cm}\n   \\end{minipage}\n   \\end{table}\n   \\begin{table}[htp]\n   \\begin{minipage}{0.49\\textwidth}\n   \\begin{table}[htp]\n   \\centering\n   \\caption{F-Score on X-Y IMAGENET SBOW}\n   \\label{tab:results}\n   {\\footnotesize\n   \\begin{tabular}{ccccc}\\hline\n   KL   &   RKL   &   JS   & HL   & CH \\\\ \\hline\\hline\n   0.4317 & 0.3411 & 0.3456 & 0.2431 & 0.4395 \\\\\n   0.4686 & 0.3825 & 0.3823 & 0.2977 & 0.4889 \\\\\n   0.4297 & 0.3635 & 0.3638 & 0.3157 & 0.4493 \\\\\n   0.3747 & 0.3259 & 0.3280 & 0.3066 & 0.3838 \\\\\n   0.3265 & 0.2932 & 0.2941 & 0.2914 & 0.3320 \\\\\n   0.2867 & 0.2635 & 0.2659 & 0.2728 & 0.2919 \\\\\n   0.2553 & 0.2403 & 0.2421 & 0.2532 & 0.2614 \\\\\n   0.2306 & 0.2218 & 0.2222 & 0.2336 & 0.2373 \\\\\n   0.2111 & 0.2065 & 0.2061 & 0.2149 & 0.2177 \\\\\\hline \n   \\end{tabular}}\n   \\vspace{-0.5cm}\n   \\end{minipage}\n   \\end{table}\n\n\n   \\begin{table}[htp]\n   \\centering\n   \\begin{minipage}{0.49\\textwidth}\n   \\caption{F-Score on X-Z FACE}\n   \\label{tab:results}\n   {\\footnotesize\n   \\begin{tabular}{ccccc}\\hline\n   KL   &   RKL   &   JS   & HL   & CH \\\\ \\hline\\hline\n   0.1444 & 0.1461 & 0.1482 & 0.1464 & 0.0930 \\\\\n   0.2220 & 0.2318 & 0.2370 & 0.2327 & 0.1434 \\\\\n   0.3012 & 0.3284 & 0.3262 & 0.3217 & 0.2043 \\\\\n   0.3322 & 0.3511 & 0.3585 & 0.3524 & 0.2405 \\\\\n   0.3421 & 0.3401 & 0.3652 & 0.3596 & 0.2645 \\\\\n   0.3431 & 0.3267 & 0.3615 & 0.3557 & 0.2807 \\\\\n   0.3396 & 0.3171 & 0.3531 & 0.3486 & 0.2915 \\\\\n   0.3337 & 0.3068 & 0.3423 & 0.3384 & 0.2965 \\\\\n   0.3249 & 0.3015 & 0.3278 & 0.3261 & 0.2983 \\\\\\hline   \n   \\end{tabular}}\n   \\vspace{-0.5cm}\n   \\end{table}\n   \\end{minipage}\n   \\begin{minipage}{0.49\\textwidth}\n   \\begin{table}[htp]\n   \\centering\n   \\caption{F-Score on X-Z MNIST}\n   \\label{tab:results}\n   {\\footnotesize\n   \\begin{tabular}{ccccc}\\hline\n   KL   &   RKL   &   JS   & HL   & CH \\\\ \\hline\\hline\n   0.3872 & 0.3503 & 0.3810 & 0.3686 & 0.3783 \\\\\n   0.6137 & 0.5269 & 0.6007 & 0.5622 & 0.5967 \\\\\n   0.7238 & 0.6128 & 0.7023 & 0.6584 & 0.6884 \\\\\n   0.7531 & 0.6406 & 0.7246 & 0.6958 & 0.7054 \\\\\n   0.6903 & 0.5974 & 0.6649 & 0.6826 & 0.6524 \\\\\n   0.6027 & 0.5379 & 0.5795 & 0.6224 & 0.5698 \\\\\n   0.5277 & 0.4850 & 0.5073 & 0.5537 & 0.4989 \\\\\n   0.4667 & 0.4344 & 0.4472 & 0.4940 & 0.4414 \\\\\n   0.4186 & 0.3923 & 0.3994 & 0.4448 & 0.3958 \\\\\\hline\n   \\end{tabular}}\n   \\vspace{-0.5cm}\n   \\end{minipage}\n   \\end{table}\n   \\begin{table}[htp]\n   \\begin{minipage}{0.49\\textwidth}\n   \\centering\n   \\caption{F-Score on X-Z NEWS}\n   \\label{tab:results}\n   {\\footnotesize\n   \\begin{tabular}{ccccc}\\hline\n   KL   &   RKL   &   JS   & HL   & CH \\\\ \\hline\\hline\n   0.0066 & 0.0076 & 0.0074 & 0.0076 & 0.0069 \\\\\n   0.0036 & 0.0041 & 0.0038 & 0.0040 & 0.0034 \\\\\n   0.0018 & 0.0020 & 0.0020 & 0.0020 & 0.0018 \\\\\n   0.0013 & 0.0014 & 0.0015 & 0.0015 & 0.0014 \\\\\n   0.0011 & 0.0011 & 0.0012 & 0.0012 & 0.0013 \\\\\n   0.0011 & 0.0010 & 0.0011 & 0.0011 & 0.0011 \\\\\n   0.0010 & 0.0009 & 0.0010 & 0.0009 & 0.0011 \\\\\n   0.0010 & 0.0009 & 0.0009 & 0.0009 & 0.0010 \\\\\n   0.0010 & 0.0009 & 0.0009 & 0.0009 & 0.0010 \\\\\\hline\n   \\end{tabular}}\n   \\vspace{-0.5cm}\n   \\end{minipage}\n   \\begin{minipage}{0.49\\textwidth}\n   \\centering\n   \\caption{F-Score on X-Z IMAGENET SBOW}\n   \\label{tab:results12}\n   {\\footnotesize\n   \\begin{tabular}{ccccc}\\hline\n   KL   &   RKL   &   JS   & HL   & CH \\\\ \\hline\\hline\n   0.0063 & 0.0051 & 0.0044 & 0.0052 & 0.0043 \\\\\n   0.0025 & 0.0021 & 0.0025 & 0.0027 & 0.0023 \\\\\n   0.0016 & 0.0014 & 0.0017 & 0.0018 & 0.0014 \\\\\n   0.0012 & 0.0013 & 0.0013 & 0.0013 & 0.0011 \\\\\n   0.0010 & 0.0011 & 0.0011 & 0.0011 & 0.0010 \\\\\n   0.0009 & 0.0009 & 0.0009 & 0.0009 & 0.0009 \\\\\n   0.0008 & 0.0008 & 0.0008 & 0.0008 & 0.0008 \\\\\n   0.0007 & 0.0007 & 0.0007 & 0.0007 & 0.0007 \\\\\n   0.0006 & 0.0006 & 0.0007 & 0.0006 & 0.0007 \\\\\\hline\n   \\end{tabular}}\n   \\vspace{-0.5cm}\n   \\end{minipage}\n   \\end{table}\n\n\n   \\pagebreak\n   \\subsection{More Experimental Results : Optimization of the primal form versus variational form (Duality Gap) Analysis}\n\n   \\begin{table}[t]\n   \\centering\n   \\caption{Amount of time for $v$KL-SNE to achieve same level of loss as KL-SNE}\n   \\label{tab:speed_results}\n   {\\footnotesize\n   \\begin{tabular}{l|c|ccc}\\hline\n   & KL-SNE & \\multicolumn{3}{c}{$v$KL-SNE}   \\\\\\hline\n   Data   &   -   & vSNE 20 hids & vSNE 10-20 hids & vSNE 5-10-20 hids \\\\\\hline\n   MNIST (Digit 1)   & 294s   & 230.1s   & {\\bf 196.17s}   & 217.3s   \\\\\\hline\n   MNIST   & 1280s   & 1239.84s   & {\\bf 972.72s}   & 1171.05s   \\\\\\hline\n   News   & {\\bf 505.8s}   & 2003.48s   & 1910.08s   & 1676.73s   \\\\\\hline\n   \\end{tabular}}\n   \\vspace{-0.5cm}\n   \\end{table}\n   \\begin{figure}[htp]\n   \\centering\n   \\includegraphics[width0.35\\linewidth]{./figs/discriminant.pdf}\n   \\vspace{-0.2cm}\n   \\caption{Discriminant Architecture \\label{fig:disc_arch}}\n   \\end{figure}\n   \n\n   \\begin{figure}[htp]\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.325\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist_cost_curves_JJKK_ch.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{CS-SNE}\n   \\end{minipage}   \n   \\begin{minipage}{0.325\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist_cost_curves_JJKK_js.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{JS-SNE}\n   \\end{minipage}   \n   \\begin{minipage}{0.325\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist_cost_curves_JJKK_kl.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{KL-SNE}\n   \\label{fig:kl_jjkk}\n   \\end{minipage}\n   \\subcaption{MNIST}\n   \\end{minipage}   \n   \\begin{minipage}{0.5\\textwidth}\n   \\begin{minipage}{0.485\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist1_cost_curves_JJKK_ch.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{MNIST1}\n   \\end{minipage}\n   \\begin{minipage}{0.485\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/news_cost_curves_JJKK_ch.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{NEWS}\n   \\end{minipage} \n   \\subcaption{Chi-Square}\n   \\end{minipage}   \n   \\begin{minipage}{0.5\\textwidth}\n   \\begin{minipage}{0.485\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist1_cost_curves_JJKK_js.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{MNIST1}\n   \\end{minipage}\n   \\begin{minipage}{0.485\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/news_cost_curves_JJKK_js.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{NEWS}\n   \\end{minipage} \n   \\subcaption{Jensen-Shannon}\n   \\end{minipage}   \n   \\caption{Log $ft$-SNE criterion during optimization for different choices of $J$ and $K$ on MNIST, MNIST1, and NEWS. \n   Two hidden layer (10-20) deep ReLU neural network was used as the discriminator and perplexity was set to 2,000.}\n   \\label{fig:JJ_KK}\n   \\end{figure}\n\n   \\begin{figure}[htp]\n   \\centering\n   \\begin{minipage}{0.325\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/kl_loss_perplexity.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{KL-SNE}\n   \\end{minipage}   \n   \\begin{minipage}{0.325\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/js_loss_perplexity.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{JS-SNE}\n   \\end{minipage} \n   \\begin{minipage}{0.325\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/rkl_loss_perplexity.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{RKL-SNE}\n   \\end{minipage} \n   \\caption{$ft$-SNE Loss with respect to different perplexities on three Gaussian cluster and swiss roll datasets.}\n   \\label{fig:syn_perp}\n   \\end{figure}\n\n   \\begin{figure}[htp]\n   \\begin{minipage}{0.5\\textwidth}\n   \\begin{minipage}{0.485\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist1_cost_curves_perp_ch.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{MNIST1}\n   \\end{minipage}\n   \\begin{minipage}{0.485\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/news_cost_curves_perp_ch.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{NEWS}\n   \\end{minipage} \n   \\subcaption{Chi-Square}\n   \\label{fig:perp_ch}\n   \\end{minipage}   \n   \\begin{minipage}{0.5\\textwidth}\n   \\begin{minipage}{0.485\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist1_cost_curves_perp_js.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{MNIST1}\n   \\end{minipage}   \n   \\begin{minipage}{0.485\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/news_cost_curves_perp_js.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{NEWS}\n   \\end{minipage} \n   \\subcaption{Jensen-Shannon}\n   \\label{fig:perp_js}\n   \\end{minipage}   \n   \\caption{$ft$-SNE and $vft$-SNE Loss with respect to different perplexities   MNIST1 and NEWS.}\n   \\label{fig:SM_perp}\n   \\end{figure}\n\n   \\pagebreak\n   \\subsubsection{More Experimental Results with different discriminant functions \\label{app:sm_arch_exp}}\n   {\\em Architecture} Optimizing under $vf$-SNE require having a discriminant function.\n   Throughout the experiments, we used deep neural network the discriminator,\n   The architecture tology is defined as $D(x_i,x_j)   g([f(x_i) + f(x_j); f(x_i)\\odot f(x_j)])$ (depicted in S. M. Figure~\\ref{fig:disc_arch}).\n   $f(\\cdot)$ is the neural network that encodes the pair of data points, $f(x)$ and $f(y)$, and\n   $g(\\cdot)$ is the neural network that takes $[f(x)+f(y); f(x)\\odot f(y)]$ and outputs the score value.\n   Our architecture is invariant to the ordering of data points (i.e., $D(x_i,x_j)   D(x_j,x_i)$).\n   We used 10 hidden layers and 20 hidden layers for $f(\\cdot)$ and $g(\\cdot)$ in the experiments except when we experiments with expressibility of discriminant function.\n\n\n\n   \\begin{figure}[htp]\n   \\centering\n   \\vspace{0.2cm}\n   \\includegraphics[width\\linewidth]{./figs/DiscriminantSize.pdf}\n   \\vspace{-0.6cm}\n   \\caption{Discriminant Architecture : \\# of Hidden Layer Size \\label{fig:disc_arch_size}}\n\n   \\includegraphics[width\\linewidth]{./figs/DiscriminantDepth.pdf}\n   \\vspace{-0.6cm}\n   \\caption{Discriminant Architecture : \\# of Hidden Layer Depth \\label{fig:disc_arch_depth}}\n   \\end{figure}\n\n   \\begin{figure}[htp]\n   \\centering\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.32\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist_cost_curves_numhids_kl.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{KL-SNE}\n   \\end{minipage}\n   \\begin{minipage}{0.32\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist_cost_curves_numhids_js.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{JS-SNE}\n   \\end{minipage}   \n   \\begin{minipage}{0.32\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist_cost_curves_numhids_ch.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{CS-SNE}\n   \\end{minipage}\n   \\subcaption{Varying discriminator network width}\n   \\end{minipage}\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.32\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist_cost_curves_hids_depth_kl.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{KL-SNE}\n   \\end{minipage}\n   \\begin{minipage}{0.32\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist_cost_curves_hids_depth_js.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{JS-SNE}\n   \\end{minipage}   \n   \\begin{minipage}{0.32\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist_cost_curves_hids_depth_ch.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{CS-SNE}\n   \\end{minipage} \n   \\subcaption{Varying discrimnator network depth}\n   \\end{minipage}\n   \\caption{Log $ft$-SNE criterion during optimization for different discriminator network architectures on MNIST. In (a), we compare two-layer networks of different widths (legend indicates number of units in first and second hidden layers). In (b), we compare networks with different depths (legend indicates number of units in each hidden layer). \n   The number of updates were set to J:K10:10 and perplexity was set to 2,000.}\n   \\label{fig:arch}\n   \\begin{minipage}{0.495\\textwidth}\n   \\begin{minipage}{0.485\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist1_cost_curves_numhids_ch.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{MNIST1}\n   \\end{minipage}\n   \\begin{minipage}{0.485\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/news_cost_curves_numhids_ch.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{NEWS}\n   \\end{minipage} \n   \\subcaption*{CS-SNE}\n   \\end{minipage}\n   \\begin{minipage}{0.495\\textwidth}\n   \\begin{minipage}{0.485\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist1_cost_curves_numhids_js.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{MNIST1}\n   \\end{minipage}\n   \\begin{minipage}{0.485\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/news_cost_curves_numhids_js.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption*{NEWS}\n   \\end{minipage} \n   \\subcaption*{JS-SNE}\n   \\end{minipage}   \n   \\caption{Log $ft$-SNE criterion during optimization for different discriminator network architectures of different widths on MNIST. }\n   \\label{fig:sm_arch}\n   \\end{figure}\n\n\n\n   \\begin{figure}\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/mnist_perp100_pdist_qdist_hl_embedding_kl.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption{KL}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/mnist_perp100_pdist_qdist_hl_embedding_rkl.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption{RKL}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/mnist_perp100_pdist_qdist_hl_embedding_js.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption{JS}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/mnist_perp100_pdist_qdist_hl_embedding_ch.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption{CH}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/mnist_perp100_pdist_qdist_hl_embedding_hl.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption{HL}\n   \\end{minipage}   \n   \\end{minipage}   \n   \\caption{Divergences along $P_{ij}$ and $Q_{ij}$ on MNIST}   \n   \\label{fig:div_p_vs_q}\n   \\end{figure}\n\n\\section{Embeddings}\n\n   Figure~\\ref{fig:face_emb_kl_rkl} presnets the embeddings of KL-SNE and RKL-SNE.\n   Note that KL-SNE generates spurious clusters on the bottom left of the embeddings,\n   whereas RKL-SNE generated smooth embeddigns that captures the manifold structure.\n   Note that in practice, we do not want to generate such a spurious cluster \n   because the practitioners can misinterpret the visualization of the dataset. \n\n   As mentioned in Section~\\ref{sec:fSNE}, \n\n   \\begin{figure}[htp]\n   \\centering\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.49\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/kl_face}\n   \\end{minipage}\n   \\begin{minipage}{0.49\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/kl_embedding_bwr.pdf}\n   \\end{minipage}   \n   \\vspace{-0.3cm}\n   \\subcaption{RKL-SNE \\label{fig:kl_face}}\n   \\end{minipage}\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.495\\textwidth}\n   \\includegraphics[width0.85\\linewidth]{./figs/rkl_face}\n   \\end{minipage}\n   \\begin{minipage}{0.495\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/rkl_embedding_bwr.pdf}\n   \\end{minipage}   \n   \\vspace{-0.3cm}\n   \\subcaption{RKL-SNE \\label{fig:rkl_face}}\n   \\end{minipage}   \n   \\caption{Face Embeddings with KL and RKL-SNE\\label{fig:face_emb_kl_rkl}}\n   \\end{figure}\n\n\\begin{figure}[htp]\n   \\captionsetup{justificationcentering}\n   \\begin{minipage}{\\textwidth} \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{kl/face_tsne_300perp1000epoch_initlr1000pose1_epoch975.pdf}\n   \\vspace{-0.7cm}\n   \\subcaption*{KL-SNE}\n   \\end{minipage}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{rkl/face_tsne_300perp1000epoch_initlr1000pose1_epoch975.pdf}\n   \\vspace{-0.7cm}\n   \\subcaption*{RKL-SNE}\n   \\end{minipage}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{js/face_tsne_300perp1000epoch_initlr1000pose1_epoch975.pdf}\n   \\vspace{-0.75cm}\n   \\subcaption*{JS-SNE}\n   \\end{minipage}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{hl/face_tsne_300perp1000epoch_initlr10pose1_epoch975.pdf}\n   \\vspace{-0.75cm}\n   \\subcaption*{HL-SNE}\n   \\end{minipage}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{cs/face_tsne_300perp1000epoch_initlr100pose1_epoch975.pdf}\n   \\vspace{-0.7cm}\n   \\subcaption*{CH-SNE}\n   \\end{minipage}\n   \\subcaption{Coloured based on pose 1}\n   \\end{minipage}\n   \\begin{minipage}{\\textwidth} \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{kl/face_tsne_300perp1000epoch_initlr1000pose2_epoch975.pdf}\n   \\vspace{-0.7cm}\n   \\subcaption*{KL-SNE}\n   \\end{minipage}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{rkl/face_tsne_300perp1000epoch_initlr1000pose2_epoch975.pdf}\n   \\vspace{-0.7cm}\n   \\subcaption*{RKL-SNE}\n   \\end{minipage}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{js/face_tsne_300perp1000epoch_initlr1000pose2_epoch975.pdf}\n   \\vspace{-0.75cm}\n   \\subcaption*{JS-SNE}\n   \\end{minipage}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{hl/face_tsne_300perp1000epoch_initlr10pose2_epoch975.pdf}\n   \\vspace{-0.75cm}\n   \\subcaption*{HL-SNE}\n   \\end{minipage}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{cs/face_tsne_300perp1000epoch_initlr100pose2_epoch975.pdf}\n   \\vspace{-0.7cm}\n   \\subcaption*{CH-SNE}\n   \\end{minipage}\n   \\subcaption{Coloured based on pose 2}\n   \\end{minipage}\n   \\caption{Face Embeddings using $ft$-SNE. Perplexity300}\n   \\label{fig:fSNE_face}\n\\end{figure}\n\n\\begin{figure}[htp]\n   \\captionsetup{justificationcentering}\n   \\begin{minipage}{\\textwidth}\n \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{kl/mnist_tsne_2000perp2000epoch_initlr2400pca30_epoch1975.pdf}\n   \\vspace{-0.7cm}\n   \\subcaption*{KL-SNE}\n   \\end{minipage}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{rkl/mnist_tsne_2000perp2000epoch_initlr1000pca30_epoch1975.pdf}\n   \\vspace{-0.7cm}\n   \\subcaption*{RKL-SNE}\n   \\end{minipage}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{js/mnist_tsne_2000perp2000epoch_initlr1000pca30_epoch1975.pdf}\n   \\vspace{-0.75cm}\n   \\subcaption*{JS-SNE}\n   \\end{minipage}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{hl/mnist_tsne_2000perp2000epoch_initlr1000pca30_epoch1975.pdf}\n   \\vspace{-0.75cm}\n   \\subcaption*{HL-SNE}\n   \\end{minipage}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{cs/mnist_tsne_2000perp2000epoch_initlr1000pca30_epoch1975.pdf}\n   \\vspace{-0.7cm}\n   \\subcaption*{CH-SNE}\n   \\end{minipage}\n   \\end{minipage}\n   \\caption{MNIST Embeddings using $f$-SNE. Perplexity: 2000 \\& Step: 2000}\n   \\label{fig:fSNE_mnist}\n\\end{figure}\n\n\n\n\\end{appendices}\n",
  "title": "Stochastic Neighbor Embedding under $f$-divergences"
}
