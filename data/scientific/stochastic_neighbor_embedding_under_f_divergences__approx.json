{
  "authors": [
    "Daniel Jiwoong Im",
    "Nakul Verma",
    "Kristin Branson"
  ],
  "date_published": "2018-11-03",
  "raw_tex": "\\section{Approximation algorithm for the least squares problem}\n\\label{sec:approx}\n\nIn this section, we consider the least squares problem from \\Cref{eq:mle}.\nThe inputs are an arbitrary matrix $\\vX   [ \\vx_1 | \\vx_2 | \\dotsb | \\vx_n ]^\\T\n\\in \\R^{n \\times d}$ and an arbitrary vector $\\vy   (y_1,y_2,\\dotsc,y_n)^\\T \\in\n\\R^n$, and the goal is to find a vector $\\vw \\in \\R^d$ and permutation matrix\n$\\vPi \\in \\cP_n$\n(where $\\cP_n$ denotes the space of $n \\times n$ permutation matrices\\footnote{\n   Each permutation matrix $\\vPi \\in \\cP_n$ corresponds to a permutation $\\pi$ on\n   $[n]$; the $(i,j)$-th entry of $\\vPi$ is one if $\\pi(i)   j$ and is zero\n   otherwise.\n}) to minimize $\\norm[0]{\\vX\\vw - \\vPi^\\T\\vy}_2^2$.\nThis problem is NP-hard in the case where $d   \\Omega(n)$\n\\citep{pananjady2016linear} (see also \\Cref{sec:npc}).\nWe give an approximation scheme that, for any $\\epsilon \\in \\intoo{0,1}$,\nreturns a $(1+\\epsilon)$-approximation in time $(n/\\epsilon)^{O(k)} +\n\\poly(n,d)$, where $k : \\rank(\\vX) \\leq \\min\\{n,d\\}$.\n\nWe assume without loss of generality that $\\vX \\in \\R^{n \\times k}$ and $\\vX^\\T\n\\vX   \\vI_k$.\nThis is because we can always replace $\\vX$ with its matrix of left singular\nvectors $\\vU \\in \\R^{n \\times k}$, obtained via singular value decomposition\n$\\vX   \\vU \\vvarSigma \\vV^\\T$, where $\\vU^\\T\\vU   \\vV^\\T\\vV   \\vI_k$ and\n$\\vvarSigma \\succ 0$ is diagonal.\nA solution $(\\vw,\\vPi)$ for $(\\vU,\\vy)$ has the same cost as the solution\n$(\\vV\\vvarSigma^{-1}\\vw,\\vPi)$ for $(\\vX,\\vy)$,\nand a solution $(\\vw,\\vPi)$ for $(\\vX,\\vy)$ has the same cost as the solution\n$(\\vvarSigma\\vV^\\T\\vw,\\vPi)$ for $(\\vU,\\vy)$.\n\n\\subsection{Algorithm}\n\n\\begin{algorithm}[t]\n   \\renewcommand\\algorithmicrequire{\\textbf{input}}\n   \\renewcommand\\algorithmicensure{\\textbf{output}}\n   \\caption{Approximation algorithm for least squares problem}\n   \\label{alg:approx}\n   \\begin{algorithmic}[1]\n   \\REQUIRE\n   Covariate matrix $\\vX   [ \\vx_1 | \\vx_2 | \\dotsb | \\vx_n ]^\\T \\in \\R^{n\n   \\times k}$;\n   response vector $\\vy   (y_1,y_2,\\dotsc,y_n)^\\T \\in \\R^n$;\n   approximation parameter $\\epsilon \\in \\intoo{0,1}$.\n\n   \\renewcommand\\algorithmicrequire{\\textbf{assume}}\n   \\REQUIRE\n   $\\vX^\\T\\vX   \\vI_k$.\n\n   \\ENSURE\n   Weight vector $\\hat{\\vw} \\in \\R^k$ and permutation matrix $\\hat{\\vPi} \\in\n   \\cP_n$.\n\n   \\STATE\n   Run ``Row Sampling'' algorithm with input matrix $\\vX$ to obtain a matrix\n   $\\vS \\in \\R^{r \\times n}$ with $r   4k$.\n\n   \\STATE\n   Let $\\cB$ be the set of vectors $\\vb   (b_1,b_2,\\dotsc,b_n)^\\T\n   \\in \\R^n$ satisfying the following: for each $i \\in [n]$,\n   \\begin{itemize}[leftmargin2em]\n   \\item\n   if the $i$-th column of $\\vS$ is all zeros, then $b_i   0$;\n\n   \\item\n   otherwise, $b_i \\in \\cbr[0]{ y_1, y_2, \\dotsc, y_n}$.\n\n   \\end{itemize}\n\n   \\STATE Let $c : 1+4(1 + \\sqrt{n/(4k)})^2$.\n\n   \\FOR{each $\\vb \\in \\cB$}\n\n   \\STATE\n   Compute $\\tilde\\vw_{\\vb} \\in \\argmin_{\\vw \\in \\R^k} \\norm[0]{\\vS(\\vX\\vw -\n   \\vb)}_2^2$, and let $r_{\\vb} : \\min_{\\vPi \\in \\cP_n}\n   \\norm[0]{\\vX\\tilde\\vw_{\\vb} - \\vPi^\\T\\vy}_2^2$.\n\n   \\STATE\n   Construct a $\\sqrt{\\epsilon r_{\\vb}/c}$-net $\\cN_{\\vb}$ for the Euclidean\n   ball of radius $\\sqrt{cr_{\\vb}}$ around $\\tilde\\vw_{\\vb}$, so that for\n   each $\\vv \\in \\R^k$ with $\\norm[0]{\\vv - \\tilde\\vw_{\\vb}}_2 \\leq\n   \\sqrt{cr_{\\vb}}$, there exists $\\vv' \\in \\cN_{\\vb}$ such that\n   $\\norm[0]{\\vv - \\vv'}_2 \\leq \\sqrt{\\epsilon r_{\\vb}/c}$.\n\n   \\ENDFOR\n\n   \\RETURN\n   $\\displaystyle\\hat\\vw \\in \\argmin_{\\vw \\in \\bigcup_{\\vb \\in \\cB} \\cN_{\\vb}}\n   \\min_{\\vPi \\in \\cP_n} \\norm[0]{\\vX\\vw - \\vPi^\\T\\vy}_2^2$\n   and\n   $\\displaystyle\\hat\\vPi \\in \\argmin_{\\vPi \\in \\cP_n} \\norm[0]{\\vX\\hat\\vw -\n   \\vPi^\\T\\vy}_2^2$.\n\n   \\end{algorithmic}\n\\end{algorithm}\n\nOur approximation algorithm, shown as \\Cref{alg:approx}, uses a careful\nenumeration to beat the na\\\"ive brute-force running time of $\\Omega(|\\cP_n|) \n\\Omega(n!)$.\nIt uses as a subroutine a ``Row Sampling'' algorithm of\n\\citet{boutsidis2013near} (described in \\Cref{sec:approx-details}), which has\nthe following property.\n\n\\begin{theorem}[Specialization of Theorem 12 in \\citep{boutsidis2013near}]\n   \\label{thm:row-sampling}\n   There is an algorithm (``Row Sampling'') that, given any matrix $\\vA \\in \\R^{n\n   \\times k}$ with $n\\geq k$, returns in $\\poly(n,k)$ time a matrix $\\vS \\in\n   \\R^{r \\times n}$ with $r   4k$ such that the following hold.\n   \\begin{enumerate}[leftmargin2em,itemsep0ex]\n   \\item\n   Every row of $\\vS$ has at most one non-zero entry.\n\n   \\item\n   For every $\\vb \\in \\R^n$, every $\\vw' \\in \\argmin_{\\vw \\in \\R^k}\n   \\norm[0]{\\vS(\\vA\\vw - \\vb)}_2^2$ satisfies $\\norm[0]{\\vA\\vw' - \\vb}_2^2\n   \\leq c \\cdot \\min_{\\vw \\in \\R^k} \\norm[0]{\\vA\\vw - \\vb}_2^2$ for $c   1 +\n   4(1+\\sqrt{n/(4k)})^2   O(n/k)$.\n\n   \\end{enumerate}\n\\end{theorem}\n\nThe matrix $\\vS$ returned by Row Sampling determines a (weighted) subset of\n$O(k)$ rows of $\\vA$ such that solving a (ordinary) least squares problem (with\nany right-hand side $\\vb$) on this subset of rows and corresponding right-hand\nside entries yields a $O(n/k)$-approximation to the least squares problem over\nall rows and right-hand side entries.\nRow Sampling does not directly apply to our problem because (1) it does not\nminimize over permutations of the right-hand side, and (2) the approximation\nfactor is too large.\nHowever, we are able to use it to narrow the search space in our problem.\n\nAn alternative to Row Sampling is to simply enumerate all subsets of $k$ rows of\n$\\vX$.\nThis is justified by a recent result of~\\citet{derezinski2017unbiased}, which\nshows that for any right-hand side $\\vb \\in \\R^n$, using ``volume\nsampling''~\\citep{avron2013faster} to choose a matrix $\\vS \\in \\cbr[0]{0,1}^{k\n\\times k}$ (where each row has one non-zero entry) gives a similar guarantee as\nthat of Row Sampling, except with the $O(n/k)$ factor replaced by $k+1$ in\nexpectation.\n\n\\subsection{Analysis}\n\nThe approximation guarantee of \\Cref{alg:approx} is given in the following\n\\namecref{thm:approx}.\n\n\\begin{theorem}\n   \\label{thm:approx}\n   \\Cref{alg:approx} returns $\\hat{\\vw} \\in \\R^k$ and $\\hat{\\vPi} \\in \\cP_n$\n   satisfying\n   \\begin{equation*}\n   \\norm{\\vX\\hat{\\vw} - \\hat\\vPi^\\T\\vy}_2^2\n   \\ \\leq \\ (1+\\epsilon) \\min_{\\vw\n   \\in \\R^k, \\vPi \\in \\cP_n} \\norm{\\vX\\vw - \\vPi^\\T\\vy}_2^2\n   \\,.\n   \\end{equation*}\n\\end{theorem}\n\n\\begin{proof}\n   Let $\\opt : \\min_{\\vw,\\vPi} \\norm[0]{\\vX\\vw - \\vPi^\\T\\vy}_2^2$ be the optimal\n   cost, and let $(\\vw_\\star,\\vPi_\\star)$ denote a solution achieving this cost.\n   The optimality implies that $\\vw_\\star$ satisfies the normal equations\n   $\\vX^\\T\\vX\\vw_\\star   \\vX^\\T\\vPi_\\star^\\T\\vy$.\n   Observe that there exists a vector $\\vb_\\star \\in \\cB$ satisfying\n   $\\vS\\vb_\\star   \\vS\\vPi_\\star^\\T\\vy$.\n   By \\Cref{thm:row-sampling} and the normal equations, the vector\n   $\\tilde{\\vw}_{\\vb_\\star}$ and cost value $r_{\\vb_\\star}$ satisfy\n   \\begin{equation*}\n   \\opt\n   \\ \\leq \\\n   r_{\\vb_\\star}\n   \\ \\leq \\\n   \\norm{\\vX\\tilde{\\vw}_{\\vb_\\star} - \\vPi_\\star^\\T\\vy}_2^2\n   \\   \\\n   \\norm{\\vX(\\tilde{\\vw}_{\\vb_\\star} - \\vw_\\star)}_2^2\n   + \\opt\n   \\ \\leq \\ c \\cdot \\opt \\,.\n   \\end{equation*}\n   Moreover, since $\\vX^\\T\\vX   \\vI_k$, we have that\n   $\\norm[0]{\\tilde{\\vw}_{\\vb_\\star} - \\vw_\\star}_2 \\leq \\sqrt{(c-1)\\opt} \\leq\n   \\sqrt{c r_{\\vb_\\star}}$.\n   By construction of $\\cN_{\\vb_\\star}$, there exists $\\vw \\in \\cN_{\\vb_\\star}$\n   satisfying $\\norm[0]{\\vw - \\vw_\\star}_2^2   \\norm[0]{\\vX(\\vw - \\vw_\\star)}_2^2\n   \\leq \\epsilon r_{\\vb_\\star} / c \\leq \\epsilon \\opt$.\n   For this $\\vw$, the normal equations imply\n   \\begin{equation*}\n   \\min_{\\vPi \\in \\cP_n} \\norm[0]{\\vX\\vw - \\vPi^\\T\\vy}_2^2\n   \\ \\leq \\\n   \\norm[0]{\\vX\\vw - \\vPi_\\star^\\T\\vy}_2^2\n   \\   \\ \\norm[0]{\\vX(\\vw - \\vw_\\star)}_2^2 + \\opt\n   \\ \\leq \\ (1+\\epsilon) \\opt\n   \\,.\n   \\end{equation*}\n   Therefore, the solution returned by \\Cref{alg:approx} has cost no more than\n   $(1+\\epsilon) \\opt$.\n\\end{proof}\n\nBy the results of \\citet{pananjady2016linear} for maximum likelihood estimation,\nour algorithm enjoys recovery guarantees for $\\bar{\\vw}$ and $\\bar{\\pi}$ when\nthe data come from the Gaussian measurement model~\\eqref{eq:measurements}.\nHowever, the approximation guarantee also holds for worst-case inputs without\ngenerative assumptions.\n\n\\paragraph{Running time.}\n\nWe now consider the running time of \\Cref{alg:approx}.\nThere is the initial cost for singular value decomposition (as discussed at the\nbeginning of the section), and also for ``Row Sampling''; both of these take\n$\\poly(n,d)$ time.\nFor the rest of the algorithm, we need to consider the size of $\\cB$ and the\nsize of the net $\\cN_{\\vb}$ for each $\\vb \\in \\cB$.\nFirst, we have $|\\cB| \\leq n^r   n^{O(k)}$, since $\\vS$ has only $4k$ rows and\neach row has at most a single non-zero entry.\nNext, for each $\\vb \\in \\cB$, we construct the $\\delta$-net $\\cN_{\\vb}$ (for\n$\\delta : \\sqrt{\\epsilon r_{\\vb}/c}$) by constructing a $\\delta/\\sqrt{k}$-net\nfor the $\\ell_\\infty$-ball of radius $\\sqrt{cr_{\\vb}}$ centered at\n$\\tilde{\\vw}_{\\vb}$ (using an appropriate axis-aligned grid).\nThis has size $|\\cN_{\\vb}| \\leq (4c^2k/\\epsilon)^{k/2}   (n/\\epsilon)^{O(k)}$.\nFinally, each $\\argmin_{\\vw \\in \\R^k}$ computation takes $O(nk^2)$ time, and\neach $(\\arg)\\min_{\\vPi \\in \\cP_n}$ takes $O(nk + n \\log n)$\ntime~\\citep{pananjady2016linear} (also see \\Cref{sec:approx-details}).\nSo, the overall running time is $(n/\\epsilon)^{O(k)} + \\poly(n,d)$.\n\n",
  "title": "Stochastic Neighbor Embedding under $f$-divergences"
}
