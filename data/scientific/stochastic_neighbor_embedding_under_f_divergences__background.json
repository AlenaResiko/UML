{
  "authors": [
    "Daniel Jiwoong Im",
    "Nakul Verma",
    "Kristin Branson"
  ],
  "date_published": "2018-11-03",
  "raw_tex": "\n   \\begin{table*}[t]\n   \\centering\n   {\\small \n\t   \\caption{A list of commonly used $f$-divergences (along with their generating function) and their corresponsing $t$-SNE objective (which we refer to as $ft$-SNE). The last column describes what kind of distance relationship gets emphasized by different choices of $f$-divergence.\n   }\n   \\label{tab:fSNE}\n   \\begin{tabular}{lccc}\\hline\n   $D_f(P\\|Q)$   & $f(t)$   & $ft$-SNE objective   & Emphasis \\\\\n\\hline\n\\hline\n   Kullback-Leibler (KL)   & $t\\log t$   & $\\sum p_{ij} \\left(\\log \\frac{p_{ij}}{q_{ij}}\\right)$   & Local \n\\\\\n   Chi-square ($\\mathcal{X}^2$ or CH)   & $(t-1)^2$   & $\\sum \\frac{(p_{ij} - q_{ij})^2}{q_{ij}} $   & Local \n\\\\ \n   Reverse-KL (RKL)   &   $-\\log t$   & $\\sum q_{ij} \\left(\\log \\frac{q_{ij}}{p_{ij}}\\right) $   & Global   \n\\\\\n   Jensen-Shannon (JS)   & $(t+1)\\log\\frac{2}{(t+1)} + t\\log t$ & $\\frac{1}{2}(\\textrm{KL}(p_{ij}\\|\\frac{p_{ij}+q_{ij}}{2}) + \\textrm{KL}(q_{ij}|\\frac{p_{ij}+q_{ij}}{2}))$ & Both \n\\\\   \n   Hellinger distance (HL)   & $(\\sqrt{t} -1)^2$\t   & $\\sum(\\sqrt{p_{ij}} - \\sqrt{q_{ij}}   )^2 $ & Both \n\\\\   \n \\hline\n   \\end{tabular}}\n   \\vspace{-0.4cm}\n   \\end{table*}\n\n\n\n\\section{Stochastic Neighbor Embedding for Low-Dimensional Visualizations}\n\\label{sec:sne_background}\n\nGiven a set of $m$ high-dimensional datapoints $x_1,\\ldots, x_m \\in \\mathbb{R}^D$,\nthe goal of Stochastic Neighbor Embedding (SNE) is to represent these\ndatapoints in one- two- or three-dimensions in a way that faithfully\ncaptures important intrinsic structure that may be present in the given\ninput. It aims to achieve this by first modelling neighboring pairs of points based on distance in the original, high-dimensional space. Then, SNE\naims to find a low-dimensional representation of the input datapoints whose\npairwise similarities induce a probability distribution that is as \\emph{close}\nto the original probability distribution as possible. More specifically,\nSNE computes $p_{ij}$, the probability of selecting a pair of neighboring points $i$ and $j$, as\n$$p_{ij}   \\frac{p_{i|j} + p_{j|i}}{2m},$$\nwhere $p_{j|i}$ and $p_{i|j}$ represent the probability that $j$ is $i$'s neighbor and $i$ is $j$'s neighbor, respectively. These are modeled as\n$$p_{j|i} : \\frac{\\exp \\left(   - \\| x_{i} - x_{j} \\|^2 / 2\\sigma^2_i \\right)}{\\sum_{k\\neq i} \\exp \\left(   - \\| x_{i} - x_k \\|^2 / 2\\sigma^2_i \\right)}.$$\nThe parameters $\\sigma_i$ control the effective neighborhood size for the\nindividual datapoints $x_i$. In practical implementations the neighborhood\nsizes are controlled by the so-called \\emph{perplexity} parameter, which can be\ninterpreted as the effective number of neighbors for a given datapoint and is\nproportional to the neighborhood size \\cite{Maaten2008}.\n \nThe pairwise similarities between the corresponding low-dimensional datapoints $y_1,\\ldots,y_m \\in \\mathbb{R}^d$ (where $d   1, 2$ or $3$ typically), are modelled as\nStudent's $t$-distribution\n$$\nq_{ij} :   \\frac{(1+\\|y_i-y_j\\|^2)^{-1}}{\\sum_{k\\neq i} (1+\\|y_i-y_k\\|^2)^{-1}}. \n$$\nThe choice of a heavy-tailed $t$-distribution to model the low-D similarities is\ndeliberate and is key to circumvent the so-called \\emph{crowding\nproblem} \\cite{Maaten2008}, hence the name $t$-SNE. \n\nThe locations of the mapped $y_i$'s are determined by minimizing the discrepancy between the original high-D pairwise similarity distribution $P   (p_{ij})$ and the corresponding low-D distribution $Q(q_{ij})$.\n$t$-SNE prescribes minimizing the KL-divergence ($ D_\\textrm{KL}$) between distributions $P$ and $Q$ to find an optimal configuration of the mapped points\n$$\nJ_{\\textrm{KL}}(y_1,\\ldots,y_m) : D_\\textrm{KL}(P||Q)   \\sum_{i\\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}.\n$$\nWhile it is reasonable to use KL-divergence to compare the pairwise distributions $P$ and $Q$, there is no compelling reason why it should be preferred over other measures. \nIn fact we will demonstrate that using KL-divergence is restrictive for some types of structure discovery, and one should explore other divergence-based measures as well to gain a wholistic \nunderstanding of the input data.\n\n\n\n\n\\iffalse\n\nThe main objective is to visualize the high-dimensional data points in two or three dimensional space.\nStochastic neighbour embeddings (SNE) mainly cares about distance relationship in the dataset where \nit aims capture neighbourhood distance relationship between the points even in low dimensional space.\n\n\\subsection{Student-t Stochastic Neighbour Embeddings}\nLet $\\mathcal{P}$ be the class of all distributions $P$ over the space $\\mathcal{X}$. \nLet set $S\\lbrace x_1, x_2, \\cdots, x_m\\rbrace$ such that $x_i \\sim \\rho$.\nConstruct neighbourhood probability distribution $P$ such that\n\\begin{itemize}\n   \\item $P$ assigns zero probability to all sets not intersecting $S$.\n   \\item The probability of $x_i$ picking $x_j$ as its neighbour of is \n   \\begin{align}\n   p(x_j| x_i)   \\frac{\\exp \\left(   - \\| x_{i} - x_{j} \\|^2_2 / 2\\sigma^2_i \\right)}{\\sum_{k\\neq i} \\exp \\left(   - \\| x_{i} - x_k \\|^2_2 / 2\\sigma^2_i \\right)}\n   \\end{align}\n   \\item   The probability of $x_i$ and $x_J$ being neighbour (the joint distribution of the pairs) is \n   $p(x_i,x_j)   \\frac{p(x_i | x_j) + p(x_j|x_i)}{2}$.\n\\end{itemize}\n\nConstruct neighbourhood probability distribution $Q$ over $Y   \\lbrace y_1, y_2, \\cdots, y_m \\rbrace$ such that\n\\begin{itemize}\n   \\item $q$ assigns zero probability to all sets not intersecting $Y$.\n   \\item The probability of $y_i$ picking $y_j$ as its neighbour is \n   \\begin{align}\n   q(y_{j}| y_i)   \\frac{(1+\\|y_i-y_j\\|^2)^{-1}}{\\sum_{k\\neq i} (1+\\|y_i-y_k\\|^2)^{-1}} \\label{eqn:Q_Y}\n   \\end{align}\n   \\item The probability of $y_i$ and $y_j$ being neighbour is $q(x_i,x_j)   \\frac{q(x_i | x_j) + q(x_j|x_i)}{2}$.\n\\end{itemize}\nAssume that the maximum and minimum distance between the two points are bounded, \nwhere $\\max_S \\| x_i - x_j\\|^2_2/\\sigma_i^2 \\leq B_U$ and $\\min_S \\| x_i - x_j\\|^2_2/\\sigma_i^2 \\leq B_L$ for all $i$ and $j$.\nWe use simple notation $x_{ij}   (x_i,x_j)$ and $y_{ij}   (y_i,y_j)$, as well, \nwe will denote $p_{ij}   p(x_i,x_j)$ and $q_{ij}   q(x_i, x_j)$ from now.\n\nThe neighbour distance between the points in set $S$ can be preserved by matching the distribution $Q$ to be same as $P$.\nThe difference between $Q$ and $P$ is minimized using sum of Kullback-Leibler divergence,\n\\begin{align}\n   J_{KL}   \\sum_{i,j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}.\n   \\label{eq:kl-sne}\n\\end{align}\nThe subscript $Y$ on distribution $Q$ denotes that $Y$ is parameters.\nThis objective function minimizes the difference between conditional probability distributions, \nwhich preserves neighbour distance relationship for each data point $x_i$.\nThis formulation has tendency to capture the local structure of the data points, because \nit has high penalty for points that are nearby each other in original space but far apart in low dimensional space\nand small penalty for points that are far away in original space but close in low dimensional space.\nThe intensity of penalty can be adjusted by tunning $\\sigma_i$ for each point $i$, although this varies depending on the region $\\mathcal{X}$.\n\nRemark that the formulation of distribution $P$ and $Q$ are designers choice.\nSince we are using student-t distribution for $Q$ distribution, the particular formulation is known as {\\em Student-t SNE} (tSNE).\n\n\n\\begin{itemize}\n\\item Given $\\lbrace x_1, x_2, \\cdots, x_m \\rbrace \\sim P(\\mathcal{X})$.\nLet $X   \\lbrace x_{ij}   (x_i,x_j) |\\ \\forall\\ i,j \\rbrace$.\nLet $Y   \\lbrace y_{ij}   (y_i,y_j) |\\ y_i \\in R^2\\ \\forall\\ i,j \\rbrace$.\nThe objective is to have $Q_Y(X)$ to be distributed similar to $P(X)$ by preserving neighbourhood distances.\n\\begin{align}\n   P(x_{ij})   & p_{ij}   \\frac{\\exp \\left(   - \\| x_i - x_j \\|^2_2 / 2\\sigma^2_i \\right)}{\\sum_k \\exp \\left(   - \\| x_i - x_k \\|^2_2 / 2\\sigma^2_i \\right)}\\\\\n   Q_{y_{ij}}(x_{ij}) & q_{ij}   \\frac{(1+\\|y_i-y_j\\|^2)^{-1}}{\\sum_{k\\neq m} (1+\\|y_k-y_m\\|^2)^{-1}}   \\label{eqn:Q_Y}\\\\\n\\end{align}\n\n\n\\fi\n5\n\n",
  "title": "Stochastic Neighbor Embedding under $f$-divergences"
}
