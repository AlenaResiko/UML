{
  "authors": [
    "Daniel Jiwoong Im",
    "Nakul Verma",
    "Kristin Branson"
  ],
  "date_published": "2018-11-03",
  "raw_tex": "\n\\vspace{-0.2in}\n\n\\section{Discussion and Related Work}\n\\label{sec:discussion}\n\\vspace{-0.1in}\nOther divergences for $t$-SNE optimization have been explored previously.\nPerhaps the first detailed study was done by \n\\citet{tsne_div_bunte} where they explored divergences from various families\n(Gamma- Bregman- and $f$-divergences) and their corresponding visualizations on\nsome image processing datasets.\n\\citet{div_improves_tsne} and \n\\citet{alpha_beta_div_improves_micro_macro_struct} recently discussed how\ndifferent divergences can be used to find micro and macro relationships in\ndata. , but do not focus on faithful discovery of intrinsic structures.\n\nAn interesting line of work by \\citet{multiscale_tsne} and\n\\citet{trustworthy_dim_redux} highlights the issues of trustworthy structure\ndiscovery and multi-scale visualizations to find local and global structures.\n\nThe work by \\citet{Amid2015} is closely related where they study $\\alpha$-divergences\nfrom an informational retrieval perspective. Our work extends it to the general class of $f$-divergences and \nexplores the relationships between data structure and the type of divergence used. \n\nIt is worth emphasizing that no previous study makes an explicit connection\nbetween the choice of divergence and the type of structure discovery. Our work makes this explicit\nand should help a practitioner gain better insights about their data in the data exploration phase.\n Our work\ngoes a step further and attempts to\nameliorate the issues non-convex objective function in the $ft$-SNE criterion.\nBy studying the variational dual form, we can achieve \nbetter quality (locally optimal) solutions, which would be extremely beneficial to the practitioner. \n\n\n\n\nand also attempts to improve upon the quality of the (locally optimal) sol\n \nRELATED WORK\n\nBunte paper -- computed bunch of divergences with tsne \\cite{tsne_div_bunte} \n\nicml paper1   -- compare properly with \\cite{div_improves_tsne}\n\nicml paper2   -- alpha/beta divergence discover micro/macro structure   \\cite{alpha_beta_div_improves_micro_macro_struct}\n\nmultiscale tsne -- \\cite{multiscale_tsne}\n\ntrustworthy dimensionality reduction -- \\cite{trustworthy_dim_redux}\n\n\n\n\\iffalse\n\n\\subsection{Comparison between $ft$-SNEs and $f$-GANs}\n\n$f$-Generative Adversarial Networks ($f$-GAN) is another type of model uses $f$-divergence as its objective function\nand its goal is to train a generative model that learns to generate data like samples \\cite{Nowozin2016}.\nOther then the fact that $f$-GAN generate data like samples and $f$-SNE generates embeddings and that\none works in the continous set of domain and other one works in the discrete set of domain,\nthe two models are extremely similar.\nBoth $f$-GAN and $ft$-SNE have several common sharing:\ni) they fall under the category of unsupervised learning, \nii) share same variational $f$-divergence formula in Equation~\\ref{eqn:vfdiv2},\niii) use deep neural networks for the discriminant function, and\niv) use same learning schema to update parameters in Algorithm~\\ref{algo:vfsne_update_rule}.\nThe main difference is that $f$-GAN generates samples from continuous space \nwhereas $ft$-SNE generates finite set of embedding points.\n$ft$-SNE consists of individual parameters per data points whereas the generator of $f$-GAN is a deep neural network.\nHowever, this is not a problem since we can simply use deep neural network to output embedding points. \n\nEven though \\cite{Nowozin2016} demonstrated that any $f$-divergence can be used for training GANs, \nthey do not provide any guidance on how to select which $f$-divergence function to use nor show which $f$-divergences generate best samples.\nBecause it is impossible to visualize the entire domain of samples, they instead inspect by visualizing handful set of unordered samples. \nThis make difficult to evaluate which $f$-GAN models perform the best.\nThis is because it is notoriously difficult to evaluate the performance of GANs. \nIn contrast, we analyzed the primal form of $f$-divergence and delivered insights with respect to precision and recall.\nWe were able to measure the precision and recall to show which $f$-divergences work the best for which type of dataset.\nAdditionally, our insights of precision and recall transfer to GANs as well.\nThere has been several work claiming that GANs suffer from model collapse (same diversity problem)\nand more importantly, the reason for mode collapsing remain mysterious for GANs \\cite{}.\nOur view of precision and recall between the neighbours of $P$ and $Q$ directly applies to model collapse and model hallucinations in GAN.\nThe precision corresponds to the quality of mode collapsing problem and the recall corresponds to the quality of model hallucination.\nIn our paper, we precisely show the trade-off between weighting of precision versus recall for different $f$-divergence.\n\nThe mainly due to difficulty in evaluation and visualization of the distribution over the samples (since it lies on high dimensional space), \nand the difficulties in debugging.\nThus, it is empirically and notoriously difficult to study the stability of GAN objectives and the performance of Algorithm~\\ref{algo:update_rule} under different objective functions. \nGANs are known to be difficult to train. This is because there are no principled way of training them such as early stopping but also it is not straightforward to debugging and evaluation. \nNot only we cannot visualize the entire samples, we cannot access or visualize the probability distribution.\nIn contrast, because the embedding parameters lie in two or three dimensional space and there are finite set of data points, we visualize the whole domain   in t-SNE as shown.\nAdditionally, we can also view the probabilist distribution over the domain.\nThis makes easier to evaluate the algorithm since we have the entire picture of how the probability mass are distributed ove the domain.\n\nAlthough the primal and dual forms are mathematically the same, the dynamics over the iterative optimization process of two forms can be dramatically different.\nAs opposite to dueal form, the primal form ought to be numerically stable since it does not involve differentiating the dual solution.\nHowever, GAN formulations require using dual form (or lower bound of dual form) of distance or divergence formula to train their generative model.\nThis is because we cannot minimize the primal form of $f$-divergence directly, since Equation~\\ref{eqn:fdivergence} is intractable for GANs.\n\\footnote{KL-divergence is an exception among $f$-divergences. \nBesides $f$-divergence, many of other primal form of distances used to train GANs are intractable such as Wasserstein distance.}.\nHowever, both the primal form of $f$-divergence and dual form of $f$-divergence becomes tractable for t-SNE.\nThis allow us to compare the difference in two optimization and analyze existing optimization algorithms.\nFor example, Algorithm~\\ref{algo:vfsne_update_rule} is commonly used as a heuristic update rule in practice.\nIt updates the discriminant parameters $J$ times and update the embedding (generator) parameters $K$ times.\nBecause the loss function is non-convex and we are alternating the minimax updates, we do not know what kind of solution this algorithm lead us to.\nAlgorithm~\\ref{algo:vfsne_update_rule} can find a different solution depending on the choice of $J$ and $K$ and under different measures.\nThis makes understanding the stability and robustness of Algorithm~\\ref{algo:vfsne_update_rule} are very important.\nIn our experiments, we analyze it by comparing the loss value between the solutions from optimizing primal versus variational $f$-divergence. \nAs far as we know this is first time empirically comparison of the performance of two optimizations.\n\n\n\n\nDespite the success of both $f$-GAN and $f$-SNE, we do not have good understanding of the optimization both in terms of computational and algorithmic levels.\nComputationallly, we do not know if Nash equilibrium exist, or if its degenerate, or the discrepancy between local Nash equilibria if exists.\nAlgorithically, we do not know how to seek for Nash equilibria and what type of solutions existing methods find us (following Algorithm~\\ref{algo:vfsne_update_rule} guides to a good saddle point or not).\n\n\nMore broadly speaking\nNevertheless, knowing whether these methods mitigate the problem or by how much are not well understood.\nThis is mainly due to difficulty in evaluation and visualization of the distribution over the samples (since it lies on high dimensional space), and the difficulties in debugging.\nThus, it is empirically and notoriously difficult to study the stability of GAN objectives and the performance of Algorithm~\\ref{algo:update_rule} under different objective functions. \n\n\\fi\n\n\n",
  "title": "Stochastic Neighbor Embedding under $f$-divergences"
}
