{
  "authors": [
    "Daniel Jiwoong Im",
    "Nakul Verma",
    "Kristin Branson"
  ],
  "date_published": "2018-11-03",
  "raw_tex": "   \\begin{table*}[t]\n   \\centering\n   \\caption{Best $ft$-SNE method for each dataset and criterion, according to maximum F-score in Figure~\\ref{fig:results_diff_metric}.}\n   \\label{tab:results}\n   {\\footnotesize\n   \\begin{tabular}{l|c|cc|c|c}\\hline\n   &   & \\multicolumn{3}{|c|}{Data-Embeddings} & Class-Embedings   \\\\\\hline\n   Data   & Type   & K-Nearest & K-Farthest   & F-Score on X-Y   & F-Score on Z-Y   \\\\\\hline\\hline\n   MNIST (Digit 1)   & Manifold   & {\\color{green} RKL}   & {\\color{green}RKL}   & {\\color{green}RKL}   & - \\\\\n   Face   & Manifold   & {\\color{gray}{HL},\\color{green}RKL} &   {\\color{green}RKL}   &   {\\color{green}RKL}   & {\\color{blue}JS} \\\\\n   MNIST   & Clustering   & {\\color{red} KL}   & {\\color{red} KL}   &   {\\color{brown}CS}   & {\\color{red}KL}   \\\\   \n   GENE   & Clustering   & {\\color{red} KL}   & {\\color{red} KL}   &   {\\color{red}KL}   & {\\color{red}KL}   \\\\   \n   \\multirow{2}{*}{20 News Groups}   & Sparse \\& & \\multirow{2}{*}{{\\color{brown} CS}}   & \\multirow{2}{*}{{\\color{brown}CS}}   & \\multirow{2}{*}{{\\color{brown}CS}} & \\multirow{2}{*}{{\\color{gray}HL}}   \\\\\n   \\multirow{2}{*}{ImageNet (sbow)}   & Sparse \\& & \\multirow{2}{*}{{\\color{green} RKL}} & \\multirow{2}{*}{{\\color{green}RKL}}   & \\multirow{2}{*}{{\\color{brown}CS}} & \\multirow{2}{*}{{\\color{red}KL}}   \\\\\n   & Hierachical   & & & \\\\\\hline\n   \\end{tabular}}\n   \\vspace{-0.5cm}\n   \\end{table*}\n\n\n\n\n\n\\section{Experiments}\n\\label{sec:expts}\n   \nIn this section, we compare the performance of the proposed $ft$-SNE methods in preserving different types of structure present in selected data sets. Next, we compare the efficacy of optimizing the primal versus the dual form of the $ft$-SNE. Details about the datasets, optimization parameters, and architectures are described in the Supplementary Material (S.M.).\n\\\\\n\n\\subsection{Datasets}\n\\noindent \\textbf{Datasets. }\nWe compared the proposed $ft$-SNE methods on a variety of datasets with different latent structures. The MNIST dataset consists of images of handwritten digits from 0 to 9~\\cite{lecun1998gradient}, thus the latent structure is clusters corresponding to each digit. We also tested on just MNIST images of the digit 1, which corresponds to a continuous manifold. The Face dataset, proposed in~\\cite{Tenenbaum2000}, consists of rendered images of faces along a 3-dimensional manifold corresponding to up-down rotation, left-right rotation, and left-right position of the light source. The Gene dataset consists of RNA-Seq gene expression levels for patients with five different types of tumors~\\cite{weinstein2013cancer}, and thus has a cluster latent structure. The 20-Newsgroups dataset consisted of text articles from a hierarchy of topics~\\cite{joachims1996probabilistic}, and thus the latent structure corresponded to a hierarchical clustering. In addition, we used a bag-of-words representation of the articles, thus the feature representation is sparse (many of the features in the original representation will be 0). We also examined two synthetic datasets: the Swiss Roll dataset~\\cite{Tenenbaum2000} which has a continuous manifold latent structure, and a simple dataset consisting of 3 Gaussian clusters in 2 dimensions, which has a cluster latent structure. Details of these datasets can be found in Appendix. \\TODO{Section} \\ref{app:exp}. \n\n\\subsection{Comparison of $f$-divergences for SNE}\n\nWe developed several criteria for quantifying the performance of the different $ft$-SNE methods. Our criteria are based on the observation that, if the local structure is well-preserved, then the nearest neighbours in the original data space $\\mathcal{X}$ should match the nearest neighbours in the embedded space $\\mathcal{Y}$. In addition, many of our datasets include a known latent variable, e.g.~the discrete digit label for MNIST and the continuous head angle for Face. Thus, we also measure how well the embedded space captures the known structure of the latent space $\\mathcal{Z}$. We define the neighbors $N_\\epsilon(x_i)$, $N_\\epsilon(y_i)$, and $N_\\epsilon(z_i)$ of points $x_i$, $y_i$, and $z_i$ by thresholding the pairwise similarity $p_{j|i}$, $q_{j|i}$ and $r_{j|i}$, respectively, at a selected threshold $\\epsilon$. Here, $r_{j|i}   r(z_j|z_i)$ is the pairwise similarity in the latent space $\\mathcal{Z}$. For discrete labels, we define $r_{j|i} \\propto \\mathbb{I}(z_i   z_j)$. For continuous latent spaces, we use a t-distribution.\n\nUsing these definitions of neighbors, we can define precision and recall, considering the original $\\mathcal{X}$ or latent $\\mathcal{Z}$ spaces as true and the embedded space $\\mathcal{Y}$ as the predicted:\n\\begin{align}\n\t\\allowdisplaybreaks\n\\nonumber\n\\text{Precision}_X(\\epsilon) & \\frac{1}{N}\\sum^{N}_{i} \\frac{| N_\\epsilon(y_i) \\cap N_\\epsilon(x_i) |}{|N_\\epsilon(y_i)|},   \\quad \n &\\text{Precision}_Z(\\epsilon)\\frac{1}{N}\\sum^{N}_{i} \\frac{| N_\\epsilon(y_i) \\cap N_\\epsilon(z_i) |}{|N_\\epsilon(y_i)|} \\\\\n\\nonumber\n\\text{Recall}_X(\\epsilon) & \\frac{1}{N}\\sum^{N}_{i} \\frac{| N_\\epsilon(y_i) \\cap N_\\epsilon(x_i) |}{|N_\\epsilon(x_i)|}, \\quad \n &\\text{Recall}_Z(\\epsilon)\\frac{1}{N}\\sum^{N}_{i} \\frac{| N_\\epsilon(y_i) \\cap N_\\epsilon(z_i) |}{|N_\\epsilon(z_i)|}.\n\\end{align}\n\nAlternatively, we can measure how well the embedded space preserves the nearest and farthest neighbor structure. Let $NN_K(x_i)$ and $NN_K(y_i)$ indicate the $K$ nearest neighbors and $FN_K(x_i)$ and $FN_K(y_i)$ indicate the $K$ farthest neighbors. We define\n\\begin{align}\n{\\scriptsize \n\\nonumber\n\\text{NN-Precision}(K) & \\frac{1}{NK}\\sum^{N}_{i}|\\text{NN}_K(y_i) \\cap \\text{NN}_K(x_i)|, \\\\   \\quad\n\\nonumber\n   \\text{FN-Precision}(K) & \\frac{1}{NK}\\sum^{N}_{i}|\\text{FN}_K(y_i) \\cap \\text{FN}_K(X_i)|\n}\n\\end{align}\n   If the local structure is well-preserved, then the nearest neighbours in the data space should match the nearest neighbours in the embedded space. \n   Based on this assumption, we calculate neighbourhood preversing precision in order to quantiatively measure the amount of neighbours retained.\n\nFor each of the datasets, we produced Precision$_X(\\epsilon)$-Recall$_X(\\epsilon)$ and Precision$_Z(\\epsilon)$-Recall$_Z(\\epsilon)$ curves by varying $\\epsilon$, and $\\text{NN Precision}(K)$-$\\text{FN Precision}(K)$ curves by varying $K$. Results are shown in Figure~\\ref{fig:results_diff_metric1} and \\ref{fig:results_diff_metric2}. Table~\\ref{tab:results1}-\\ref{tab:results12} summarizes these results by presenting the algorithm with the highest maximum f-score per criterion. Full results are available in the Appendix.\nFor the two manifold datasets, MNIST-Digit-1 and Face, RKL and JS outperformed KL. This reflects the analysis from \\TODO{Section}~\\ref{sec:fSNE} \n(see Proposition 1) that RKL and JS emphasize global structure more than KL, and global structure preservation is more important for manifolds. Conversely, KL performs best on the two cluster datasets, MNIST and GENE. Finally, CH and HL performed best on the hierarchical dataset, News (cf.\\ \\citealp{bow_chi_sq}).\n\n\n\n\n   \\begin{figure}\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.328\\textwidth}\n   \\includegraphics[width\\linewidth]{mnist1_knnVSkfn_perp25_1000epoch_c.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\begin{minipage}{0.328\\textwidth}\n   \\includegraphics[width\\linewidth]{mnist1_recallVSPrec_perp25_80perc_2000epoch_c.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\begin{minipage}{0.328\\textwidth}\n   \\hspace{0.329\\textwidth}\n   \\vspace{-0.3cm}\n   \\end{minipage} \n   \\vspace{-0.3cm}\n   \\subcaption*{MNIST 1-Digit}\n   \\end{minipage}   \n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.328\\textwidth}\n   \\includegraphics[width\\linewidth]{face_knnVSkfn_50perp_500epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\begin{minipage}{0.328\\textwidth}\n   \\includegraphics[width\\linewidth]{face_recallVSPrec_25perp_500epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\begin{minipage}{0.328\\textwidth}\n   \\includegraphics[width\\linewidth]{face_recallVSPrecZ_25perp_500epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage} \n   \\vspace{-0.3cm}\n   \\subcaption*{Face}\n   \\end{minipage}   \n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.328\\textwidth}\n   \\includegraphics[width\\linewidth]{mnist_knnVSkfn_perp100_1000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\begin{minipage}{0.328\\textwidth}\n   \\includegraphics[width\\linewidth]{mnist_recallVSPrec_perp100_80perc_2000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\begin{minipage}{0.328\\textwidth}\n   \\includegraphics[width\\linewidth]{mnist_recallVSprecisionZ_perp100_2000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage} \n   \\vspace{-0.3cm}\n   \\subcaption*{MNIST}\n   \\end{minipage}   \n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.328\\textwidth}\n   \\includegraphics[width\\linewidth]{gene_knnVSkfn_perp50_2000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\begin{minipage}{0.328\\textwidth}\n   \\includegraphics[width\\linewidth]{gene_recallVSPrec_perp50_80perc_2000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\begin{minipage}{0.328\\textwidth}\n   \\includegraphics[width\\linewidth]{gene_recallVSprecisionZ_perp50_2000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage} \n   \\vspace{-0.3cm}\n   \\subcaption*{Gene}\n   \\end{minipage}   \n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.328\\textwidth}\n   \\includegraphics[width\\linewidth]{news_knnVSkfn_100perp_1000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\begin{minipage}{0.328\\textwidth}\n   \\includegraphics[width\\linewidth]{news_recallVSPrec_100_1000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\begin{minipage}{0.328\\textwidth}\n   \\includegraphics[width\\linewidth]{news_recallVSPrecZ_100_1000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage} \n   \\vspace{-0.3cm}\n   \\subcaption*{News}   \n   \\end{minipage}   \n   \\caption{Precision-Recall curves for each of the proposed algorithms on all datasets. Each row corresponds to a different dataset, each column to different quantitative criteria, and each line to a different algorithm.}\n   \\label{fig:results_diff_metric}\n   \\end{figure}\n\n \\subsection{Synthetic Data Experiments}\n   First, we observed the divergence values and gradients over different $f$-divergence \n   with respect to $p$ and $q$ (see Figure~\\ref{fig:div_p_vs_q} and \\ref{fig:f_div_loss_grad}).\n   Our observation matches with the theory in Section~\\ref{sec:fSNE}.\n   KL and CH are sensitive to high $p$ and low $q$, whereas RKL is sensitive to low $p$ and high $q$, and\n   JS and HL are symmetric.\n   Gradients of $f$-divergences w.r.t $q$ show that all divergence are generally sensitive to when $p$ is high and $q$ is low.\n   However, we see that RKL, JS, and HL provide much smoother gradient signals over $p > q$ space.\n   KL penalize strictly towards high $p$ and low $q$ and CH is much stricter towards $p >> q$ space.\n\n\n   \\begin{figure*}[t]\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.16\\textwidth}\n   \\centering MNIST1\n   \\end{minipage}   \n   \\begin{minipage}{0.16\\textwidth}\n   \\centering Face\n   \\end{minipage}   \n   \\begin{minipage}{0.16\\textwidth}\n   \\centering MNIST\n   \\end{minipage}   \n   \\begin{minipage}{0.16\\textwidth}\n   \\centering GENE\n   \\end{minipage}   \n   \\begin{minipage}{0.16\\textwidth}\n   \\centering NEWS\n   \\end{minipage}   \n   \\begin{minipage}{0.16\\textwidth}\n   \\centering SBOW\n   \\end{minipage}   \n   \\end{minipage}\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.16\\textwidth}\n   \\includegraphics[width\\linewidth]{mnist1_recallVSPrec_perp25_80perc_2000epoch_c.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}\n   \\begin{minipage}{0.16\\textwidth}\n   \\includegraphics[width\\linewidth]{face_recallVSPrec_25perp_500epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\begin{minipage}{0.16\\textwidth}\n   \\includegraphics[width\\linewidth]{mnist_recallVSPrec_perp100_80perc_2000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\begin{minipage}{0.16\\textwidth}\n   \\includegraphics[width\\linewidth]{gene_recallVSPrec_perp50_80perc_2000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\begin{minipage}{0.16\\textwidth}\n   \\includegraphics[width\\linewidth]{news_recallVSPrec_100_1000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage}   \n   \\begin{minipage}{0.16\\textwidth}\n   \\includegraphics[width\\linewidth]{sbow_recallVSPrec_perp100_80perc_1000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage} \n   \\vspace{-0.2cm}\n   \\subcaption{Precision-Recall curve for XY ($\\text{Precision}_X(\\epsilon)$ vs. $\\text{Recall}_X(\\epsilon)$)}\n   \\end{minipage}   \n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.16\\textwidth}\n   \\hspace{0.19\\textwidth}\n   \\end{minipage} \n   \\begin{minipage}{0.16\\textwidth}\n   \\includegraphics[width\\linewidth]{face_recallVSPrecZ_25perp_500epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage} \n   \\begin{minipage}{0.16\\textwidth}\n   \\includegraphics[width\\linewidth]{mnist_recallVSprecisionZ_perp100_2000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage} \n   \\begin{minipage}{0.16\\textwidth}\n   \\includegraphics[width\\linewidth]{gene_recallVSprecisionZ_perp50_2000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage} \n   \\begin{minipage}{0.16\\textwidth}\n   \\includegraphics[width\\linewidth]{news_recallVSPrecZ_100_1000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage} \n   \\begin{minipage}{0.16\\textwidth}\n   \\includegraphics[width\\linewidth]{sbow_recallVSprecisionZ_perp100_2000epoch_nolegend.pdf}\n   \\vspace{-0.3cm}\n   \\end{minipage} \n   \\vspace{-0.2cm}\n   \\subcaption{Precision-Recall curve for ZY ($\\text{Precision}_Z(\\epsilon)$ vs. $\\text{Recall}_Z(\\epsilon)$)}\n   \\end{minipage}   \n   \\caption{Precision-Recall curves for each of the proposed algorithms on all datasets. \n   Each row corresponds to different quantitative criteria, each column to a different dataset, and each line to a different algorithm.}\n   \\label{fig:results_diff_metric1}\n   \\vspace{-0.4cm}\n   \\end{figure*}\n\n\n\nTo better understand the relative strengths of KL and RKL, we qualitatively compared the embeddings resulting from interpolating between them: \n\\begin{align}\n\\alpha KL\\text{-SNE} + (1-\\alpha) RKL\\text{-SNE}\n\\end{align}\nfor $\\alpha   0, 0.05, 0.1, 0.5, 1.0$ ($\\alpha.5$ corresponds to JS).\nFigure~\\ref{fig:syn_embeddings} presents the embedding results for two\nsynthetic datasets: the Swiss Roll which is a continuous manifold, and three\nGaussian clusters, for a range of perplexity and $\\alpha$ values.\n\nFor each value of $\\alpha$, we tried a range of\nperplexities between $5$ and $1000$ and visually selected the best embedding\nto show. Results for all perplexities are shown in\nSupp.~Fig.~\\ref{fig:swiss_roll_embeddings}. \nWe observe that RKL worked better for manifolds\nwith low perplexity while KL worked better clusters with larger perplexity (as\npredicted in Section \\ref{sec:fSNE}. In\naddition, KL broke up the continuous Swiss Roll manifold into disjoint pieces,\nwhich produces smoother embeddings compare to KL-SNE under low\nperplexity. Finally, we did not see a continuous gradient in embedding results\nas we changed $\\alpha$. Instead, even for $\\alpha   0.1$, the Swiss Roll\nembedding was more similar to the discontinuous KL embedding. For this\ndataset, the embedding produced by JS was more similar to that produced by KL\nthan RKL. For the three Gaussian dataset, all algorithms separated the three\nclusters, however KL and JS correctly formed circular clusters, while smaller\nvalues of $\\alpha$ resulted in differently shaped clusters. \n\n   \\begin{figure}\n   \\begin{center}\n   \\begin{minipage}{.49\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blob_cropped.pdf}\n   \\vspace{-0.4cm}\n   \\subcaption{3 well-separated Gaussian clusters}\n   \\end{minipage}\n   \\begin{minipage}{.49\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_cropped.pdf}\n   \\vspace{-0.4cm}\n   \\subcaption{Swiss roll manifold}\n   \\end{minipage}\n   \\end{center}\n   \\vspace{-0.2cm}\n   \\caption{$ft$-SNE embeddings obtained with interpolated divergences between KL and RKL.} \\label{fig:syn_embeddings}\n   \\end{figure}\n\n   \\begin{figure}\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_500perp2001epoch_initlr1000_numpoints5000_fold0_epoch2000_00.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption{Perplexity500 \\\\ RKL: $\\alpha0$}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_500perp2001epoch_initlr1000_numpoints5000_fold0_epoch2000_05.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{Perplexity500 \\\\ $\\alpha0.05$}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_500perp2001epoch_initlr1000_numpoints5000_fold0_epoch2000_1.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{Perplexity500 \\\\ $\\alpha0.1$}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_1000perp2001epoch_initlr250_numpoints5000_fold0_epoch2000_5.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{Perplexity500 \\\\ $\\alpha0.5$ (JS)}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/blobs/tsne_500perp2001epoch_initlr250_numpoints5000_fold0_epoch2000_10.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{Perplexity500 \\\\ KL:$\\alpha1$}\n   \\end{minipage}   \n   \\subcaption*{Three Gaussian clusters} \n   \\end{minipage}   \n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_7perp2001epoch_initlr1000_fold0_epoch2000_0.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{Perplexity10 \\\\ RKL: $\\alpha0$}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_9perp2001epoch_initlr1000_numpoints5000_fold0_epoch2000_01.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{Perplexity10 \\\\ $\\alpha0.01$}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_1000perp3001epoch_initlr250_numpoints5000_fold0_epoch3000_1.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{Perplexity1000 \\\\ $\\alpha0.1$}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_500perp3001epoch_initlr1000_numpoints5000_fold0_epoch3000_5.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{Perplexity500\\\\ JS: $\\alpha0.5$}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/swiss_roll/tsne_500perp3001epoch_initlr1000_numpoints5000_fold0_epoch3000_10.pdf}\n   \\vspace{-0.6cm} \n   \\subcaption*{Perplexity500\\\\ KL: $\\alpha1$}\n   \\end{minipage}   \n   \\subcaption*{Swiss Roll} \n   \\end{minipage}   \n   \\caption{$ft$-SNE embeddings obtained with interpolated divergences between KL and RKL.} \\label{fig:syn_embeddings}\n   \\end{figure}\n\n   \\begin{figure}[t]\n   \\begin{minipage}{0.495\\textwidth}\n   \\begin{minipage}{0.325\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist_cost_curves_perp_kl.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{KL}\n   \\end{minipage}\n   \\begin{minipage}{0.325\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist_cost_curves_perp_js.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{JS}\n   \\end{minipage}   \n   \\begin{minipage}{0.325\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist_cost_curves_perp_ch.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{CH}\n   \\end{minipage} \n   \\vspace{-0.2cm}\n   \\caption{Log of the primal $ft$-SNE loss for the $ft$-SNE and $vft$-SNE algorithms for different perplexities on MNIST. \n   The number of updates were set to J:K10:10 and two hidden layer (10-20) deep ReLU neural network.}\n   \\label{fig:opt_perp}\n   \\end{minipage}\n   \\vspace{-0.2cm}\n   \\end{figure}\n   \\begin{figure*}[t]\n   \\centering\n   \\begin{minipage}{0.28\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist_cost_curves_JJKK_kl.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption{Number of updates ($J$:$K$)}\n   \\end{minipage}   \n   \\begin{minipage}{0.33\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist_cost_curves_numhids_kl.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption{Network depth}\n   \\end{minipage}\n   \\begin{minipage}{0.35\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/opt/mnist_cost_curves_hids_depth_kl.pdf}\n   \\vspace{-0.5cm}\n   \\subcaption{Network width}\n   \\end{minipage}\n   \\caption{Comparison of log $t$-SNE criterion for different parameter choices. All plots show results for the KL divergence on the MNIST dataset (perplexity 2,000), results for other divergences and datasets are in S.M. (a) Different numbers of updates to the discriminator and embedding weights with fixed network architecture (2 layers of 10 and 20 hidden units). (b) Different network widths, with fixed $J$:$K$   10:10. (c) Different network depths (widths specified in legend), $J$:$K$   10:10.}\n   \\label{fig:opt_fig}\n   \\vspace{-0.4cm}\n   \\end{figure*}\n \n\\subsection{Optimization of the primal vs.\\ variational forms}\n\nIn this section, we quantitatively and qualitatively compared the efficacy of optimizing the primal, $ft$-SNE, versus the variational, $vft$-SNE, forms of the criteria. Quantitatively, we compared the primal $ft$-SNE criteria at solutions found using both methods during and after optimization.\n\nFigure~\\ref{fig:opt_perp} shows the log primal $ft$-SNE criteria of the final solutions using both optimization methods for different $f$-divergences and different perplexities for MNIST (Supp. Fig.~\\ref{fig:SM_perp} shows results for other datasets). We found that for small perplexities $vft$-SNE outperforms $ft$-SNE, while this difference decreases as perplexity increases $ft$-SNE and $vft$-SNE converges to same loss values as the perplexity increases. However, even at perplexity $2000$, $vft$-SNE achieves a slightly lower loss than $ft$-SNE. This is surprising since $vft$-SNE minimizes a lower bound of $ft$-SNE, the criterion we are using for comparison, and suggests that optimizing the primal form using gradient descent can result in bad local minima. We also observed that $vft$-SNE resulted in more consistent embeddings than $ft$-SNE when rerun from different initializations \\TODO{add evidence}. \n\nWe next evaluated the performance of the $vft$-SNE algorithm as we vary some of the parameters of the method. Figure~\\ref{fig:opt_fig}a compares the results as we vary the number of updates $J$ and $K$ to perform to the discriminator and embedding weights (Algorithm~\\ref{algo:vfsne_update_rule}). For the KL divergence, we found that optimizing the variational form performed better for all choices of $J$ and $K$, both in terms of the rate of convergence and the final solution found. For the CH and JS divergences, nearly all choices of $J$ and $K$ resulted in faster optimization (see Supp. Fig.~\\ref{fig:JJ_KK}). This plot is in terms of the number of updates, wall clock time is shown in Table~\\ref{tab:speed_results} (S.M.). \n\nFigure~\\ref{fig:opt_fig}b and~\\ref{fig:opt_fig}c compares the results as we change the architecture of the discriminator. We experimented with a linear classifier and neural networks with 1-3 hidden layers of varying numbers of hidden units (network width). Figure~\\ref{fig:opt_fig}a compares results results as we vary network width (architecture shown in Supp. Fig.~\\ref{fig:disc_arch_size}) and Figure~\\ref{fig:opt_fig}b compares results as we change network depth (architecture shown in Supp. Fig.~\\ref{fig:disc_arch_depth}). We observed that the performance was largely consistent as we changed network architecture. The results for JS and CH-SNE are shown in Supp. Fig.~\\ref{fig:arch}a and~\\ref{fig:arch}b. \n\n   \n   \\begin{table}[t]\n   \\centering\n   \\caption{Amount of time for $vft$-SNE to achieve same level of loss (0.3271) as $ft$-SNE}\n   \\label{tab:results}\n   {\\footnotesize\n   \\begin{tabular}{l|cccc}\\hline\n   & $t$-SNE & \\multicolumn{3}{c}{$vft$-SNE}   \\\\\\hline\n   Data   &   -   & vSNE 20 hids & vSNE 10-20 hids & vSNE 5-10-20 hids \\\\\\hline\n   MNIST (Digit 1)   & 294s & 230.1s   & 196.17s   & 217.3s   \\\\\\hline\n   MNIST   & 1280s   & 1601.85s   & 1367.15s   & 1389.41s   \\\\\\hline\n   NEWS   & 505.8s & 2003.48s   & 1910.08s   & 1676.73s   \\\\\\hline\n   \\end{tabular}}\n   \\end{table}\n   \n\n\n",
  "title": "Stochastic Neighbor Embedding under $f$-divergences"
}
