{
  "authors": [
    "Daniel Jiwoong Im",
    "Nakul Verma",
    "Kristin Branson"
  ],
  "date_published": "2018-11-03",
  "raw_tex": "\\section{Introduction}\n\nA key aspect of exploratory data analysis is to study two-dimensional\nvisualizations of the given high-dimensional input data. In order to gain\ninsights about the data, one hopes that such visualizations faithfully depict\nsalient structures that may be present in the input.   $t$-distributed Stochastic\nNeighbor Embedding ($t$-SNE) introduced by \n\\citet{Maaten2008} is a prominent and popular visualization\ntechnique that has been applied successfully in several application domains \\citep{chem_app,physics_app,security_app,music_app,cancer_app,bio_app1}.\n\nArguably, alongside PCA, $t$-SNE has now become the de facto method of choice used by\npractitioners for 2D visualizations to study and unravel the structure present\nin data.   Despite its immense popularity, very little work has been done to\nsystematically understand the power and limitations of the $t$-SNE method, and\nthe quality of visualizations that it produces. Only recently researchers\nshowed that if the high-dimensional input data does contain prominent clusters\nthen the 2D $t$-SNE visualization will be able to successfully capture the cluster\nstructure \\citep{Linderman2017,arora2018}. While these results are a promising\nstart, a more fundamental question remains unanswered: \n\\begin{center}\n\\emph{what kinds of intrinsic\nstructures can a $t$-SNE visualization reveal?} \n\\end{center}\n \n\nIntrinsic structure in data can take many forms. While clusters are a common structure to \nstudy, there may be several other important structures such as manifold, sparse or hierarchical structures that are \npresent in the data as well. How does the $t$-SNE optimization criterion fare at discovering these other structures?\n\nHere we take a largely experimental approach to answer this question. Perhaps\nnot surprisingly,   minimizing $t$-SNE's KL-divergence criterion is \\emph{not}\nsufficient to discover all these important types of structure. We adopt the\nneighborhood-centric precision-recall analysis proposed by \\citet{Venna2010},\nwhich showed that KL-divergence maximizes recall at the expense of precision.\nWe show that this is geared specifically towards revealing cluster structure\nand performs rather poorly when it comes to finding manifold or hierarchical\nstructure. In order to discover these other types of structure effectively, one\nneeds a better balance between precision and recall, and we show that this can\nbe achieved by minimizing $f$-divergences other than the KL-divergence. \n\nWe prescribe that data scientists create and explore low-dimensional\nvisualizations of their data corresponding to several different\n$f$-divergences, each of which is geared toward different types of structure.\nTo this end, we provide efficient code for finding $t$-SNE embeddings based on\nfive different $f$-divergences\\footnote{The code is available at $github.com/jiwoongim/ft-SNE$.}. Users can even provide their own specific instantiation of an\n$f$-divergence, if needed. Our code can optimize either the standard criterion,\nor a variational lower bound based on convex conjugate of the $f$-divergence.\nEmpirically, we found that minimizing this dual variational\nform was computationally more efficient and produced better quality embeddings,\neven for the standard case of KL-divergence. To our knowledge, this is the\nfirst work that explicitly compares the\noptimization of both the primal and dual form of $f$-divergences, which would be of independent\ninterest to the reader.\n\n\n\n\\iffalse\n\nfind to look for precision-recall \n \n\n\nmanifold sparse cluster hierarchical. \n\n\n\n   We show that the choice of divergence affects the trade off between precision and recall.\n   Furthermore, we demonstrate that the trade off between precision and recall may depend on the types of datasets. \n\n\n\n\n\n\n\n\\begin{itemize}\n\n\n\n   \\item We study $f$-divergence measure for t-Stochastic Neighbour Embedding.\n\n   \\item Our first contributions: \n   We show that the choice of divergence affects the trade off between precision and recall.\n   Furthermore, we demonstrate that the trade off between precision and recall may depend on the types of datasets. \n   Based on our knowledge, this is first work that studies the property for the choice of $f$-divergence.\n   \\item Our second contribution: \n   We empirically study the optimization of primal and dual form $f$-divergence.\n   We show that optimizing the dual (variational) form is more advantangeous.\n   Based on our knowledge, this is first work that compares the optimization of both primal and dual form of $f$-divergence.\n\\end{itemize}\n\n\n{\\color{blue} could you emphasis the two contributions equally if possible? }\n\n\\fi\n\n\n\n\n",
  "title": "Stochastic Neighbor Embedding under $f$-divergences"
}
