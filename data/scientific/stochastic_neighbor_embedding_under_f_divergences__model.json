{
  "authors": [
    "Daniel Jiwoong Im",
    "Nakul Verma",
    "Kristin Branson"
  ],
  "date_published": "2018-11-03",
  "raw_tex": "   \\begin{table*}[t]\n   \\centering\n   {\\small \n   \\caption{$ft$-SNE}\n   \\label{tab:fSNE}\n   \\begin{tabular}{lccc}\\hline\n   $D_f(P\\|Q)$   & $f(t)$   & $ft$-SNE objective   & Emphasis \\\\\n\\hline\n\\hline\n   Kullback-Leibler (KL)   & $t\\log t$   & $\\sum p_{ij} \\left(\\log \\frac{p_{ij}}{q_{ij}}\\right)$   & Local \n\\\\\n   Chi-square ($\\mathcal{X}^2$ or CS)   & $(t-1)^2$   & $\\sum \\frac{(p_{ij} - q_{ij})^2}{q_{ij}} $   & Local \n\\\\ \n   Reverse-KL (RKL)   &   $-\\log t$   & $\\sum q_{ij} \\left(\\log \\frac{q_{ij}}{p_{ij}}\\right) $   & Global   \n\\\\\n   Jensen-Shannon (JS)   & $(t+1)\\log\\frac{2}{(t+1)} + t\\log t$ & $\\frac{1}{2}(J_{\\textrm{KL}} + J_{\\textrm{RKL}})$   & Both \n\\\\   \n   Hellinger distance (HL)   & $(\\sqrt{t} -1)^2$\t   & $\\sum(\\sqrt{p_{ij}} - \\sqrt{q_{ij}}   )^2 $ & Both \n\\\\   \n \\hline\n   \\end{tabular}}\n   \\vspace{-0.4cm}\n   \\end{table*}\n\n\n\n\\begin{figure*}[t]\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/pdist_qdist_kl_heatmap_1.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{KL}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/pdist_qdist_rkl_heatmap_1.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{RKL}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/pdist_qdist_js_heatmap_1.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{JS}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/pdist_qdist_ch_heatmap_1.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{CH}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/pdist_qdist_hl_heatmap_1.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{HL}\n   \\end{minipage}   \n   \\end{minipage}   \n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_heatmap/mnist_perp100_pdist_qdist_kl_embedding_kl_1.png}\n   \\vspace{-0.8cm}\n   \\subcaption*{KL}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_heatmap/mnist_perp100_pdist_qdist_rkl_embedding_rkl_1.png}\n   \\vspace{-0.8cm}\n   \\subcaption*{RKL}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_heatmap/mnist_perp100_pdist_qdist_js_embedding_js_1.png}\n   \\vspace{-0.8cm}\n   \\subcaption*{JS}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_heatmap/mnist_pdist_qdist_hl_embedding_ch_1.png}\n   \\vspace{-0.8cm}\n   \\subcaption*{CH}\n   \\end{minipage} \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_heatmap/mnist_perp100_pdist_qdist_ch_embedding_hl_1.png}\n   \\vspace{-0.8cm}\n   \\subcaption*{HL}\n   \\end{minipage} \n   \\end{minipage}   \n   \\vspace{-0.2cm}\n   \\caption{Gradient of f-Divergences}   \n   \\label{fig:f_div_loss_embed}\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/grad_pdist_qdist_kl_heatmap_1.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{KL}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/grad_pdist_qdist_rkl_heatmap_1.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{RKL}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/grad_pdist_qdist_js_heatmap_1.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{JS}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/grad_pdist_qdist_ch_heatmap_1.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{CH}\n   \\end{minipage} \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/grad_pdist_qdist_hl_heatmap_1.pdf}\n   \\vspace{-0.6cm}\n   \\subcaption*{HL}\n   \\end{minipage} \n   \\end{minipage}   \n   \\vspace{-0.2cm}\n   \\caption{Top: $f$-divergence loss. Bottom: gradient of $f$-divergence.\n   The color limit represents the magnitude of $f$-divergence (resp.\\ gradient of $f$-divergence)\n   of $p_{ij}$ and $q_{ij}$.}   where they lie in between $[0,1]$} \n   \\label{fig:div_p_vs_q}\n   \\caption{Gradient of f-Divergences}   \n   \\label{fig:f_div_loss_grad}\n   \\vspace{-0.2cm}\n\\end{figure*}\n\n\n\n\n\n\\section{$f$-Divergence-based Stochastic Neighbor Embedding \\label{sec:fSNE}}\n\nOnly a limited number of studies have explored the use of criteria other than the KL-divergence with the $t$-SNE optimization \n(see our detailed discussion in \\TODO{Section} \\ref{sec:discussion}) \n(see our detailed discussion in Related Work) and even fewer\nanalyze their effects on structure discovery. \nKL-divergence is a special case of a broader class of divergences called $f$-divergences. A few popular special cases of $f$-divergences include\nthe reverse KL divergence, Jenson-Shannon divergence, Hellinger distance (HL), total variation distance and $\\chi^2$-divergence. Of course, each instantiation compares discrepancy between the distributions differently \\cite{nguyen2008} \nand \nit would be instructive to study what effects, if any, do these other divergences have on low-D visualizations of a given input. \nFormally $f$-divergence between two distributions $P$ and $Q$ (over the same measurable space $\\Omega$) is defined as \n$$ \\begin{align}\n   D_f(P||Q) : \\int_\\Omega f\\left(\\frac{P(x)}{Q(x)}\\right)dQ(x),\n   \\label{eqn:fdivergence}\n$$ \\end{align}\nwhere $f$ is a convex function such that $f(1)0$. Intuitively, $f$-divergence tells us the average odds-ratio between $P$ and $Q$ weighted by the function $f$. For the $t$-SNE objective, the \ngeneric form of $f$-divergence simplifies to \n\\begin{align}\n   J_{f}(y_1,\\ldots,y_m) : D_f(P||Q)   \\sum_{i\\neq j} q_{ij} f\\left(\\frac{p_{ij}}{q_{ij}}\\right).\n   \\label{eqn:fSNE_primal}\n\\end{align}\nTable~\\ref{tab:fSNE} shows a list of common instantiations of $f$-divergences and their corresponding $t$-SNE objectives, which we shall call $ft$-SNE.\n   \nObviously, one expects different optimization objectives (i.e.\\ different choices\nof $f$) to produce different results. A more significant question is whether these differences have any significant qualitative effects on types of structure discovery.\n\nAn indication towards why the choice of $f$ might affect the type of structure revealed is to notice that $f$-divergences are typically asymmetric, and penalize \nthe ratio $p_{ij}/q_{ij}$ (cf.\\ Eq.\\ \\ref{eqn:fSNE_primal}) differently. KL-SNE (i.e.\\ $f$ taken as KL-divergence, cf.\\ Table \\ref{tab:fSNE}) for instance \npenalizes pairs of nearby points in the original space getting mapped far away in the embedded space more heavily than faraway points being mapped nearby (since the corresponding $p_{ij} \\gg q_{ij} \\approx 0$). Thus KL-SNE optimization prefers visualizations that don't distort local neighborhoods. \nIn contrast, SNE with the reverse-KL-divergence criterion, RKL-SNE, as the name suggests, emphasizes the opposite, and better captures global structure in the corresponding visualizations.\n\nA nice balance between the two extremes is achieved by the JS- and HL-SNE (cf.\\ Table \\ref{tab:fSNE}), where JS is simply an arithmetic mean of the KL and RKL penalties, and HL is a sort of aggregated geometric mean. Meanwhile, CH-SNE can be viewed as relative version of the (squared) $L_2$ distance between the distributions, and is a popular choice for comparing bag-of-words models \\cite{bow_chi_sq}. \n\nWe can empirically observe how $p$ and $q$ similarities are penalized by divergence (see Figure~\\ref{fig:div_p_vs_q}).\nOur observation matches with our intuition: KL and CH are sensitive to high $p$ and low $q$, whereas RKL is sensitive to low $p$ and high $q$, and\nJS and HL are symmetric.\nThe corresponding gradients w.r.t.\\ $q$ show that all divergence are generally sensitive to when $p$ is high and $q$ is low.\nHowever, RKL, JS, and HL provide much smoother gradient signals over $p > q$ space.\nKL penalize strictly towards high $p$ and low $q$ and CH is much stricter towards $p \\gg q$ space.\n\\\\\n\n\n\n\n\n\n\n   \\begin{table}[t]\n   \\centering\n   {\\small \n   \\caption{$ft$-SNE}\n   \\label{tab:fSNE}\n   \\begin{tabular}{lccc | cc}\\hline\n   $D_f(P\\|Q)$   & $f(t)$   & $ft$-SNE objective   & Emphasis & $f^*(t)$ & $h(x)$ \\\\\\hline\\hline\n   Kullback-Leibler (KL)   & $t\\log t$   & $\\sum p_{ij} \\left(\\log \\frac{p_{ij}}{q_{ij}}\\right)$   & Local &   $\\exp(t-1)$   & $x$ \n\\\\\n   Chi-square ($\\mathcal{X}^2$ or CS)   & $(t-1)^2$   & $\\sum \\frac{(p_{ij} - q_{ij})^2}{q_{ij}} $   & Local & $\\frac{1}{4}t^2+t$   & $x$\n\\\\ \n   Reverse-KL (RKL)   &   $-\\log t$   & $\\sum q_{ij} \\left(\\log \\frac{q_{ij}}{p_{ij}}\\right) $   & Global &   $-1-\\log(-t)$   & $-\\exp(-x)$\n\\\\\n   Jensen-Shannon (JS)   & $(t+1)\\log\\frac{2}{(t+1)} + t\\log t$ & $\\frac{1}{2}(J_{\\textrm{KL}} + J_{\\textrm{RKL}})$   & Both & $-\\log(1-\\exp(t))$   & $\\log(2) - \\log\\left(1+\\exp(-x)\\right)$\n\\\\   \n   Hellinger distance (HL)   & $(\\sqrt{t} -1)^2$\t   & $\\sum(\\sqrt{p_{ij}} - \\sqrt{q_{ij}}   )^2 $ & Both &   $\\frac{t}{1-t}$   & $1-\\exp(-x)$\n\\\\   \n \\hline\n   \\end{tabular}}\n   \\end{table}\n\n\n\n\\subsection{A Neighborhood-level Precision-Recall Analysis}\n\\noindent \\textbf{A Neighborhood-level Precision-Recall Analysis.} \nThe optimization criterion of $ft$-SNE is a complex non-convex function that is\nnot conducive to a straightforward analysis without simplifying assumptions. \nTo simplify the analysis, we consider pairs of points in a \\emph{binary neighborhood}\nsetting, where, for each datapoint, other datapoints are either in its neighborhood, or not in its neighborhood.\n\nLet $N_\\epsilon(x_i)$ and $N_\\epsilon(y_i)$ denote the neighbors of points\n$x_i$ and $y_i$ by thesholding the pairwise similarities $p_{j|i}$ and $q_{j|i}$\nat a fixed threshold $\\epsilon$, respectively. Let $r_i : |N_\\epsilon(x_i)|$ and $k_i : |N_\\epsilon(y_i)|$ denote the number of true and retrieved neighbors. Our simplifying binary neighborhood assumption can be formalized as: \n\\begin{flalign*}\n   \\[ \n\\begin{aligned}\np_{ij} &: \\begin{cases}\na_i, & x_j \\in N_\\epsilon(x_i)   \\\\\nb_i, & x_j \\notin N_\\epsilon(x_i)\n\\end{cases},\n\\end{aligned}\n\\begin{aligned}\nq_{ij} &: \\begin{cases} \nc_i, & y_j \\in N_\\epsilon(y_i)   \\\\\nd_i, & y_j \\notin N_\\epsilon(y_i)\n\\end{cases}.\n\\end{aligned}\n\\end{flalign*}\nwhere $a_i$ and $c_i$ are large ($a_i \\geq \\frac{1-\\delta}{r_i}$, $c_i \\geq \\frac{1-\\delta}{k_i}$) and $b_i$ and $d_i$ are small ($b_i \\leq \\frac{\\delta}{m-r_i-1}$, $d_i \\leq \\frac{\\delta}{m-k_i-1}$), for small $\\delta$. \n\n\\begin{figure*}[htb]\n\\begin{center}\n\\begin{minipage}{.49\\textwidth}\n\\includegraphics[width\\linewidth]{./figs/blob_cropped.pdf}\n   \\vspace{-0.4cm}\n\\subcaption{3 well-separated Gaussian clusters}\n\\end{minipage}\n\\begin{minipage}{.49\\textwidth}\n\\includegraphics[width\\linewidth]{./figs/swiss_cropped.pdf}\n   \\vspace{-0.4cm}\n\\subcaption{Swiss roll manifold}\n\\end{minipage}\n\\end{center}\n\\caption{$ft$-SNE embeddings obtained with interpolated divergences between KL and RKL.\n   The perplexity for each row corresponds to 10, 100, and 500 respectively.} \\label{fig:syn_embeddings}\n\\vspace{-0.2cm}\n\\end{figure*}\n\nIn this binary formulation, we can rewrite each of the $f$-divergences in terms related to the embedding precision, the fraction of embedding-neighbors are true neighbors, and the recall, the fraction of true neighbors are also embedding-neighbors. Define $n_{\\TP}^i : |N_\\epsilon(x_i) \\cap N_\\epsilon(y_i)|$, $n_\\textrm{FP}^i : |N_\\epsilon(y_i) \\setminus N_\\epsilon(x_i)|$ and\n$n_\\textrm{FN}^i : |N_\\epsilon(x_i) \\setminus N_\\epsilon(y_i)|$ to denote the\nnumber of true-positive, false-positive and false-negative neighbors\nrespectively. In this notation, per-neighborhood precision is $n_{\\TP}^i / k_i\n 1 - n_{\\FP}^i/k_i$ and recall is $n_{\\TP}^i/r_i   1 - n_{\\FN}^i/r_i$. This\ninformation retrieval analysis has previously been performed for KL-SNE\n\\cite{Venna2010}. Novelly, we extend it to other $f$-divergences to understand\ntheir assumptions. \n\n\\begin{proposition}\n   \\label{prop1}\n   Under the binary-neighborhood assumption, for $\\delta$ sufficiently small,\n\\vspace{-0.1in}\n\\begin{enumerate}[(i)]\n\\item $J_{\\KL} \\propto   \\Big( \\sum_i \\underbrace{{n^i_{\\FN}}/{r_i}}_{1-\\textrm{recall}} \\Big)$, maximizes recall.\n\\item $J_{\\RKL} \\propto \\Big( \\sum_i \\underbrace{{n^i_{\\FP}}/{k_i}}_{1-\\textrm{precision}} \\Big)$ maximizes precision.\n\\item $J_{\\JS}   \\frac{1}{2}(\\textrm{KL}(p_{ij}\\|m_{ij}) + \\textrm{KL}(q_{ij}\\|m_{ij}))$ balances precision and recall,\n   where $m_{ij}\\frac{1}{2}(p_{ij}+q_{ij})$\n\\item $J_{\\JS} \\propto J_{\\KL}+J_{\\RKL}$ balances precision and recall,\n\\item The first two terms of HL-SNE balance precision and recall (the coefficients are close to 1, since $\\delta$ is small). The last term forces preservation of neighborhood sizes, and strongly penalizes small embedding neighborhoods when precision is high. \n\\begin{align*}\nJ_{\\HL} \\propto \\sum_i &\\Big[ \\underbrace{ \\Big( \\frac{n_{\\FN}^i}{r_i} \\Big)}_{1-\\textrm{recall}}   \\cdot ( 1 - O({(\\delta r_i)}^{\\frac{1}{2}})) \\Big] \n   +\\Big[ \\underbrace{ \\Big( \\frac{n_{\\FP}^i}{k_i} \\Big)}_{1-\\textrm{precision}}   \\cdot ( 1 - O({(\\delta k_i)}^{\\frac{1}{2}})) \\Big] \n   +\\underbrace{\\Big( \\frac{n_{\\TP}^i}{k_i} \\Big) }_{\\textrm{precision}} \\cdot \\underbrace{\\Big(\\sqrt{\\frac{r_i}{k_i}} - 1 \\Big)^2 }_{\\substack{\\textrm{neighborhood}\\\\\\textrm{size ratio}} }.\n\\end{align*} $$\n\\item \nCH-SNE is biased towards maximizing recall, since the multiplier of recall is much larger than that on precision. Like HL-SNE, the last term forces preservation of neighborhood sizes, and strongly penalizes small embedding neighborhoods when precision is high. \n\\begin{align*}\nJ_{\\CH} \\propto \\sum_i & \\Big[ \\underbrace{ \\Big( \\frac{n_{\\FN}^i}{r_i} \\Big)}_{1-\\textrm{recall}}   \\cdot \\Big( \\frac{m-k_i}{r_i \\delta} \\Big) \\Big]   \n+   \\underbrace{ \\Big( \\frac{n_{\\FP}^i}{k_i} \\Big)}_{1-\\textrm{precision}}   \n +   \\Big[ \\underbrace{\\Big( \\frac{n_{\\TP}^i}{k_i} \\Big) }_{\\textrm{precision}} \\cdot \\underbrace{\\Big({\\frac{r_i}{k_i}} - 1 \\Big)^2 }_{\\substack{\\textrm{neighborhood} \\\\ \\textrm{size ratio}}} \\Big].\n\\end{align*} $$\n\\end{enumerate}\n\\end{proposition}\nThis proposition corroborates our intuition (see also Table \\ref{tab:fSNE}), and provides a relationship between the proposed $ft$-SNE criteria and the types of neighborhood similarities that are preserved. KL-SNE maximizes neighborhood recall, while RKL-SNE maximizes neighborhood precision. All other criteria balance precision and recall in different ways. JS-SNE provides equal weight to precision and recall. HL-SNE gives approximately equal weight to precision and recall, with an extra term encouraging the original and embedding neighborhood sizes to match. This regularization term gives more severe penalties if the embedding neighborhood is much smaller than the original neighborhood than the reverse, and thus HL-SNE can be viewed as a regularized version of JS-SNE. CH-SNE gives more weight to maximizing recall, again with an extra term encouraging the original and embedding neighborhood sizes to match, and is and thus similar to a regularized version of KL-SNE.\n\n\\TODO{KB: I don't understand this part. I think it would be helpful to have someone go over this with me.}\nNext, we connect these precision-recall interpretations of the various criteria to the types of intrinsic structure they preserve. Suppose the intrinsic structure within the data is clusters. A good embedding of this data would have points belonging to the same true cluster all grouped together in the visualization, but the specific locations of the embedded points within the cluster do not matter. Thus, cluster discovery requires good neighborhood recall, and one might expect KL-SNE to perform well. For neighborhood sizes similar to true cluster sizes, this argument is corroborated both theoretically by previous work \\citep{Linderman2017,arora2018} and\nempirically by our experiments (Experiments Section). Both theoretically and practically, the perplexity parameter---which is a proxy for neighborhood size---needs to be set so that the effective neighborhood size matches the cluster size for successful cluster discovery.\n\nIf the intrinsic structure within the data is a continuous manifold, a good embedding would preserve the smoothly varying structure, and not introduce artificial breaks in the data that lead to the appearance of clusters. Having a large neighborhood size (i.e.\\ large perplexity) may not be conducive to this goal, again because\nthe SNE optimization criterion does not care about the specific mapped locations of\nthe datapoints within the neighborhood. Instead, it is more preferable to have\nsmall enough neighborhood where the manifold sections are approximately\nlinear one require high precision in these small neighborhoods. Thus one might expect RKL-SNE to fare well manifold discovery tasks.\nIndeed, this is also corroborated practically in our experiments. \\TODO{Section} \\ref{sec:expts}. \n(To best of our knowledge, no theory work exists on this.)\n\\\\\n\n\n\n\n\n\n\n\\iffalse\n\n   \\begin{table}[t]\n   \\centering\n   {\\small \n   \\caption{$ft$-SNE}\n   \\label{tab:fSNE}\n   \\begin{tabular}{lccc}\\hline\n   $D_f(P\\|Q)$   & $f(\\cdot)$   & $ft$-SNE   & Structure Preservation \\\\\\hline\\hline\n   KL   & $t\\log t$   & $\\sum_{x_{ij}} p_{ij} \\left(\\log \\frac{p_{ij}}{q_{ij}}\\right)$   & Local\\\\\n   Reverse KL   &   $-\\log t$   & $\\sum_{x_{ij}}-q_{ij} \\left(\\log \\frac{p_{ij}}{q_{ij}}\\right) $   & Global\\\\\n   Jensen-Shannon   & $-(t+1)\\log\\frac{(1+t)}{2} + t\\log t$ & $\\frac{1}{2}(J_{KL} + J_{RKL})$   & Both\\\\   \n   Hellinger distance\t& $(\\sqrt{t} -1)^2$\t\t   & $\\sum_{x_{ij}} q_{ij} \\left(\\sqrt{\\frac{p_{ij}}{q_{ij}}}-1\\right)^2$ & Both\\\\   \n   Chi-square ($\\mathcal{X}^2$)   & $(t-1)^2$   & $\\sum_{x_{ij}} q_{ij} \\left(\\frac{p_{ij}}{q_{ij}}-1\\right)^2$   & Local\\\\   \\hline\n   \\end{tabular}}\n   \\end{table}\n\n\n   The famous tSNE uses KL divergence as an loss measure and no other measure has been explored up until now.\n   Nonetheless, there are many other divergence functions that can be explored, such as reverse KL, Jensen-Shannon, $\\mathcal{X}^2$ distance, Hellinger distance, and $\\alpha$-divergence.\n   All of these fall under the umbrella of $f$-divergence, which measures the difference between two probability distributions \\cite{Nguyen2008}.\n   The $f$-divergence is defined as \n   \\begin{align}\n   D_f(P\\|Q)   \\int_M f\\left(\\frac{P(x)}{Q(x)}\\right)dQ(x)\n   \\label{eqn:fdivergence}\n   \\end{align}\n   where $f$ is a convex function such that $f(1)0$.\n   $f$-divergence tells us the average odd ratio between $P$ and $Q$ that is weighted by function $f$. \n   \n   It is obvious that we can replace $KL$ loss with other ones and still optimize the embeddings. \n   The generic form of the SNE objective becomes\n   \\begin{align}\n   J_{f}   \\sum_{i,j} q_{ij} f\\left(\\frac{p_{ij}}{q_{ij}}\\right).\n   \\label{eqn:fSNE_primal}\n   \\end{align}\n\n   Besides the KL-SNE in Equation~\\ref{eq:kl-sne}, here are list of other SNE objectives based on $f$-divergence functions in Table~\\ref{tab:fSNE}. \n   {\\bf Reverse KL-SNE}\n   \\begin{align}\n   J_{RKL}   \\sum_{x_{ij}}-q_{ij} (\\log p_{ij} - \\log q_{ij}) \n   \\end{align}\n   {\\bf Jensen-Shannon-SNE (JS-SNE)}\n   \\begin{align}\n   J_{RKL}   \\frac{1}{2} J_{KL} + \\frac{1}{2} J_{RKL} \n   \\end{align}\n   {\\bf $\\mathcal{X}^2$-SNE}\n   \\begin{align}\n   J_{C2}   \\sum_{x_{ij}} q_{ij} \\left(\\sqrt{\\frac{p_{ij}}{q_{ij}}}-1\\right)^2 \n   \\end{align}\n   {\\bf Hellinger-SNE}\n   \\begin{align}\n   J_{C2}   \\sum_{x_{ij}} q_{ij} \\left(\\frac{p_{ij}}{q_{ij}}-1\\right)^2 \n   \\end{align}\n   Table~\\ref{tab:fSNE} shows the list of SNE objective functions based on different $f$-divergence functions. \n   We call these set of SNE objectives {\\em the $ft$-SNE}.\n   Hence, we dubbed the name of the following set of SNE objectives to be {\\em the $f$-SNE}.\n   \n   It is important to understand that $f$-divergences are not symmetric except Jensen-Shannon divergence.\n   For example, KL-SNE penalizes on widely spread embedding points to represent nearby data points, i.e., small $q_{ij}$ to model a large $p_{ij}$ \\cite{Maaten2008}.\n   This means that KL-SNE focus on capture the local structure of the data points. \n   In contrast, RKL-SNE penalizese on nearby embedding points to represent widely spread out data points, i.e., large $q_{ij}$ to model a small $p_{ij}$.\n   Hence, RKL-SNE targets to capture global structure of the data points. \n   These illustrate that the two methods to behave totally opposite of each other. \n   A nice compromise of the two methods is JS-SNE. \n   JS-SNE is the average of KL-SNE and RKL-SNE, which means that will penalize both widely spread out embedding points to represent nearby data points and nearby embedding points to represent widely spread out data points. \n\n   The other two $ft$-SNE, CH-SNE and HL-SNE cares about retaining both the local and global structure of the data points.\n   For both Hellinger distance and Chi-Square divergence, the ratio term dominates the weight scaling term, where $\\left(\\frac{p_{ij}}{q_{ij}}-1\\right)^2$ grows [decays] faster than $q_{ij}$ when $\\frac{p_{ij}}{q_{ij}} > 1$ $\\left[\\frac{p_{ij}}{q_{ij}} < 1\\right]$.\n   It is important to note that KL-SNE, RKL-SNE, JS-SNE are much more sensitive to the small $q_{ij}$ with large $p_{ij}$ and largre $q_{ij}$ with small $p_{ij}$ respectively compare to CS-SNE and HL-SNE.\n   This is because the log term in their loss is more sensitive than the quadratic term.\n\n   Similar to KL and RKL-divergence, we can obtain opposite characteristics of $D_f(P\\|Q)$ for any $f$-divergence by swapping $P$ and $Q$\n   It is possible that one may want to preserve only the local structure, global structure, or both.\n   Depending on the objective and the choice of divergence function, we can always obtain the structure that we want by   \n   \\begin{align}\n   D_f(P\\|Q) \\rightarrow D_f(Q\\|P).\n   \\end{align}\n   if $D_f(P\\|Q)$ has the opposite characteristics. \n   If the objective is to capture both local and global structure, similar to JS-SNE, we can simply average the two divergences:\n   \\begin{align}\n   D_f(P,Q) : \\frac{1}{2}(D_f(P\\|Q) + D_f(Q\\|p))\n   \\end{align}\n   Following form makes $D_f(P,Q)$ to be a metric since it satisfies all the metric conditions, identity, non-negativity, symmetry, and triangle inequality.\n\n\\begin{figure}\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_heatmap/pdist_qdist_kl_perp100_heatmap_points.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{KL}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_heatmap/pdist_qdist_rkl_perp100_heatmap_points.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{RKL}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_heatmap/pdist_qdist_js_perp100_heatmap_points.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{JS}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_heatmap/pdist_qdist_ch_perp100_heatmap_points.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{CH}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_heatmap/pdist_qdist_hl_perp100_heatmap_points.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{HL}\n   \\end{minipage}   \n   \\end{minipage}   \n   \\vspace{-0.2cm}\n   \\caption{f-Divergences}   \n   \\label{fig:div_p_vs_q}\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_heatmap/grad_pdist_qdist_kl_perp100_heatmap_points.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{KL}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_heatmap/grad_pdist_qdist_rkl_perp100_heatmap_points.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{RKL}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_heatmap/grad_pdist_qdist_js_perp100_heatmap_points.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{JS}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_heatmap/grad_pdist_qdist_ch_perp100_heatmap_points.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{CH}\n   \\end{minipage} \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_heatmap/grad_pdist_qdist_hl_perp100_heatmap_points.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{HL}\n   \\end{minipage} \n   \\end{minipage}   \n   \\vspace{-0.2cm}\n   \\caption{Gradient of f-Divergences}   \n   \\label{fig:f_div_loss_grad}\n\\end{figure}\n\n\\begin{figure}\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/pdist_qdist_kl_heatmap.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{KL}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/pdist_qdist_rkl_heatmap.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{RKL}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/pdist_qdist_js_heatmap.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{JS}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/pdist_qdist_ch_heatmap.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{CH}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/pdist_qdist_hl_heatmap.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{HL}\n   \\end{minipage}   \n   \\end{minipage}   \n   \\vspace{-0.2cm}\n   \\caption{f-Divergences}   \n   \\label{fig:div_p_vs_q}\n   \\begin{minipage}{\\textwidth}\n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/grad_pdist_qdist_kl_heatmap.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{KL}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/grad_pdist_qdist_rkl_heatmap.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{RKL}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/grad_pdist_qdist_js_heatmap.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{JS}\n   \\end{minipage}   \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/grad_pdist_qdist_ch_heatmap.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{CH}\n   \\end{minipage} \n   \\begin{minipage}{0.195\\textwidth}\n   \\includegraphics[width\\linewidth]{./figs/div_p_vs_q/grad_pdist_qdist_hl_heatmap.pdf}\n   \\vspace{-0.8cm}\n   \\subcaption*{HL}\n   \\end{minipage} \n   \\end{minipage}   \n   \\vspace{-0.2cm}\n   \\caption{Gradient of f-Divergences}   \n   \\label{fig:f_div_loss_grad}\n\\end{figure}\n\n\\subsection{Precision \\& Recall Analysis}\nIf the local structure is well-preserved, then the nearest neighbours in the data space should match the nearest neighbours in the embedded space. \nOur analysis setup follows ``the binary neighbourhoods'' definition from \\cite{Venna2010} where (i) the point of interest has some number of relevant neighbours and \nall the other points are compeletely irrelevant, and (ii) the points that are relevant are all equally relevant.\n\nThe probabilties of relevenat neighbours and irrelevant neighbours are defined by \n   \\[ p_{j|i}   \\begin{cases} \n   a_i \\equiv \\frac{1-\\delta}{r_i}   & x_j \\in NN_\\epsilon(x_i)   \\\\\n   b_i \\equiv \\frac{\\delta}{N-r_i-1}   & x_j \\notin NN_\\epsilon(x_i)\n   \\end{cases}\n   , \\qquad \n   q_{j|i}   \\begin{cases} \n   c_i \\equiv \\frac{1-\\delta}{k_i}   & y_j \\in NN_\\epsilon(y_i)   \\\\\n   d_i \\equiv \\frac{\\delta}{N-k_i-1}   & y_j \\notin NN_\\epsilon(y_i)\n   \\end{cases}\n   \\]\nwhere $NN_\\epsilon(x_i)\\lbrace x_j | p_{j|i} > \\epsilon \\rbrace$ is relevant neigbours for point $x_i$,\n$r_i$ are the number of relevant points and $k_i$ are number of retrieved points.\nNote that we set $r_ik_i$ given that we use $k$-nearest neighbour assignments for both $x_i$ and $y_i$. \n\n\\begin{proposition}\n   \\label{prop11}\n   Suppose that we have the binary neighbours of point $x_i$ and $\\delta$ is very close to $0$. \n   Then, each $f$-divergence does the following:\\\\\n   KL-divergence maximizes the recall\\footnote{Recall$(i)\\left(1-\\frac{N_{MISS,i}}{r_i}\\right)$ and Precision$(i)\\left(1-\\frac{N_{FP,i}}{k_i}\\right)$.},\n   \\begin{align}\n   J_{KL}   \\frac{N_{MISS,i}}{r_i}C_{0}   \n   \\end{align}\n   where $N_{MISS}$ is the number of flase negatives.\\\\\n   Reverse KL-divergence maximizes the precision, \n   \\begin{align}\n   J_{RKL}   \\frac{N_{FP,i}}{k_i}C_{0}\n   \\end{align}\n   where $N_{FP}$ are number of false positives. \n   The constant term corresponse to $C_0   \\log\\frac{1-\\delta}{\\delta}$. \\\\\n   Jensen-Shannon divergence maximizes both the precision and recall.\n   \\begin{align}\n   J_{JS} &\\approx \\frac{1}{2} (J_{KL} + J_{RKL}).\n   \\end{align}\\\\\n   Similarly, Helligener and Chi-Square divergences maximize precision and recall but contains extra regularization term,\n   \\begin{align}\n   J_{HL} &\\approx \\frac{N_{FP,i}}{k_i} C_{h0}+\\left(1-\\frac{N_{FP,i}}{k_i}\\right)+\\left(1-\\frac{N_{MISS,i}}{r_i}\\right)C_{h1} \\label{eqn:j_hl}\\\\\n   J_{CH} &\\approx \\frac{N_{FP,i}}{k_i} C_{c0}+\\left(1-\\frac{N_{FP,i}}{k_i}\\right)C_{c1}+\\left(1-\\frac{N_{MISS,i}}{r_i}\\right)C_{c2} \\label{eqn:j_ch}\n   \\end{align} \n   The constant terms corresponds to:\\\\\n   \\begin{equation}\n   C_{h0}   \\left(\\frac{r_i}{k_i} - 1\\right)^2, \n   C_{h1}\\left(\\frac{1-\\delta}{\\delta} \\frac{k_i}{N-r_i-1}- 2\\right),\n   \\end{equation}\n   \\begin{equation}\n   C_{c0}   \\left(\\sqrt{\\frac{r_i}{k_i}} - 1\\right)^2, \n   C_{c1}   \\left( 1 - 2\\sqrt{\\frac{r_i\\delta}{1-\\delta}}\\right), \\text{ and }\n   C_{c2}   \\left( 1 - 2\\sqrt{\\frac{k_i\\delta}{1-\\delta}}\\right).\n   \\end{equation}\n\\end{proposition}\nHellinger and Chi-Square penalize precision when $\\frac{r_i}{k_i} > 1$.\nIn other words when there are more number of neighbours than number of retrieved points,\nits objective encourages to find more number of retrieved points.\nIn contrast, it maximize the precision when there are more number of retrieved points than number of neighbours.\n\nIn the case of when we employ K-nearest neighbour methods for retrieving neighour points in data space and embedding space,\nthen Hellinger and Chi-Square divergence omit the penality terms.\n\n\\begin{proposition}\n   Under proposition~\\ref{prop11}, \n   when the number of relevant neighbours of $x_i$ and number of relevant neighbours of $y_i$ are the same $(r_ik_i)$,\n   then the Helliger distance and Chi-Square divergence becomes\n   \\begin{align}\n   J & \\left(1-\\frac{N_{FP,i}}{k_i}\\right)+\\left(1-\\frac{N_{MISS,i}}{r_i}\\right)C \n   \\end{align}\n   where $CC_{h1}$ and $C_{c2}$ for Helliger and Chi-Square respectively.\n\\end{proposition}\nThe constant term $C_0   (\\frac{r_i}{k_i} - 1)^2   0$ in Equation~\\ref{eqn:j_hl} and \\ref{eqn:j_ch} since $r_i   k_i$. \nBecause $\\frac{1-\\delta}{\\delta}$ is the large value that dominates $C_1$, \nHellinger distance emphasize much more on recall than precision compare to Chi-Square divergence.\nwhereas Chi-Square and Jensen-Shannon divergences put equal weights to precision and recall.\n\n\n\n\\fi\n\n",
  "title": "Stochastic Neighbor Embedding under $f$-divergences"
}
