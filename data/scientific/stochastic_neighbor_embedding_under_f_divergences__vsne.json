{
  "authors": [
    "Daniel Jiwoong Im",
    "Nakul Verma",
    "Kristin Branson"
  ],
  "date_published": "2018-11-03",
  "raw_tex": "   \\begin{table}[t]\n   \\centering\n   {\\small \n   \\caption{Variational $ft$-SNE. }\n   \\label{tab:vfSNE}\n   \\begin{tabular}{lcccc}\\hline\n   $D_f(P\\|Q)$   & $f(t)$   & $f^*(t)$   & $h(x)$ \\\\\\hline\\hline\n   Kullback-Leibler (KL)   & $t\\log t$   & $\\exp(t-1)$   & $x$\\\\\n   Reverse-KL (RKL)   & $-\\log t$   & $-1-\\log(-t)$   & $-\\exp(-x)$\\\\\n   Jensen-Shannon (JS)   & $-(t+1)\\log\\frac{(1+t)}{2} + t\\log t$ & $-\\log(1-\\exp(t))$   & $\\log(2) - \\log\\left(1+\\exp(-x)\\right)$\\\\\n   Hellinger distance (HL)   & $(\\sqrt{t} -1)^2$   & $\\frac{t}{1-t}$   & $1-\\exp(-x)$\\\\ \n   Chi-square ($\\mathcal{X}^2$ or CS) & $(t-1)^2$\t\t   & $\\frac{1}{4}t^2+t$   & $x$\\\\   \\hline\n   \\end{tabular}}\n   \\end{table}\n\n\n\n\\section{Variational $ft$-SNE}\n\\noindent \\textbf{Variational $ft$-SNE for practical usage and improved optimization.}\nThe $ft$-SNE criteria can be optimized using gradient descent or one of its variants, e.g.~stochastic gradient descent, and KL-SNE is classically optimized in this way. The proposed $ft$-SNE criteria (including KL-SNE) are non-convex, and gradient descent may not converge to a good solution. We explored minimizing the $ft$-SNE criteria by expressing it in terms of its conjugate dual \\citep{Nowozin2016,nguyen2008}:\n$$\n\\begin{align*}\nD_f(P||Q)   \\sum_{i\\neq j} \\left[ q_{ij} \\left(\\sup_{h \\in \\mathcal{H}} h((x_i,x_j))\\frac{p_{ij}}{q_{ij}} - f^*(h((x_i,x_j))) \\right)\\right]\n   \\end{align*}\n$$\nwhere $\\mathcal{H}$ is the space of real-valued functions on the underlying measure space and $f^*$ is the Fenchel conjugate of $f$.\nIn this equation, the maximum operator acts per data point, making optimization infeasible. Instead, we optimize the variational lower bound\n$$ \n   \\begin{align}\n   D_f(P||Q)\\geq   \\sup_{h \\in \\mathcal{H}} \\sum_{i \\neq j} \\left[ h((x_i,x_j))p_{ij} - f^*(h((x_i, x_j)))q_{ij}\\right],\n   \\label{eqn:vfdiv}\n   \\end{align}\n$$\nwhich is tight for sufficiently expressive $\\mathcal{H}$. \n   Optimizing the variational form of $ft$-SNE in Eq~\\ref{eqn:vfdiv} involves replacing the $\\sup_{h \\in \\mathcal{H}}$ by $\\max$ operator with some function class $f$.\n   In practice, one uses a parameteric hypothesis class $\\bar{\\mathcal{H}}$, and we use multilayer, fully-connected neural networks.\n   Table~\\ref{tab:vfSNE} shows a list of common instantiations of $f$-divergences and their corresponding $h(x)$ functions.\n   Our variational form of $ft$-SNE objective (or $vft$-SNE) finally becomes the following minimax problem\n   \\begin{align}\n   \\min_{y_{ij}}\\max_{\\phi} \n\\nonumber\nJ(&y_1,\\ldots,y_m)   \n\\min_{y_1, \\ldots, y_m} \\max_{\\bar{h} \\in \\bar{\\mathcal{H}}} \\sum_{i \\neq j} \\left[ \\bar{h} ((x_i, x_j))p_{ij} - f^*\\left( \\bar{h}((x_i,x_j))\\right)q_{ij}\\right].\n   \\label{eqn:vfsne}\n   \\end{align}   \nWe alternatively optimize $y_{1}, \\ldots, y_{m}$ and $\\bar{h}$   (see Algorithm~\\ref{algo:vfsne_update_rule},\nmore details available in S.M.). \n   \\begin{algorithm}[htp]\n   \\caption{Variational (Adversarial) SNE Optimization Algorithm}\\label{euclid}\n   \\label{algo:vfsne_update_rule}\n   \\begin{algorithmic}[1]\n   \\Procedure{Optimization}{Dataset $\\{X_{tr}, X_{vl}\\}$, learning rate $\\eta$, $f$-divergence $J$}\n   \\State Initialize the discriminant parameter $\\phi$.\n   \n   \\While {$\\phi$ has not converged}\n   \\For {$j1,\\ldots, J$}\n   \\State $\\phi_{t+1}   \\phi_t + \\eta \\nabla_\\phi J$.\n   \\EndFor\n   \\For {$k1,\\ldots, K$}\n   \\State $y^i_{t+1}   y^i_t - \\eta_y \\nabla_y J$.\n   \\EndFor\n   \\EndWhile\n   \\EndProcedure\n   \\end{algorithmic}\n   \\vspace{-0.1cm}\n   \\end{algorithm}\n\n\n\n   \\begin{table*}[t]\n   \\centering\n   \\caption{Best $ft$-SNE method for each dataset and criterion, according to maximum F-score in Figure~\\ref{fig:results_diff_metric1} and \\ref{fig:results_diff_metric2}.}\n   \\label{tab:results}\n   {\\footnotesize\n   \\begin{tabular}{l|c|cc|c|c}\\hline\n   &   & \\multicolumn{3}{|c|}{Data-Embeddings} & Class-Embedings   \\\\\\hline\n   Data   & Type   & K-Nearest & K-Farthest   & F-Score on X-Y   & F-Score on Z-Y   \\\\\\hline\\hline\n   MNIST (Digit 1)   & Manifold   & {\\color{green} RKL}   & {\\color{green}RKL}   & {\\color{green}RKL}   & - \\\\\n   Face   & Manifold   & {\\color{gray}{HL},\\color{green}RKL} &   {\\color{green}RKL}   &   {\\color{green}RKL}   & {\\color{blue}JS} \\\\\n   MNIST   & Clustering   & {\\color{red} KL}   & {\\color{red} KL}   &   {\\color{brown}CS}   & {\\color{red}KL}   \\\\   \n   GENE   & Clustering   & {\\color{red} KL}   & {\\color{red} KL}   &   {\\color{red}KL}   & {\\color{red}KL}   \\\\   \n   \\multirow{2}{*}{20 News Groups}   & Sparse \\& & \\multirow{2}{*}{{\\color{brown} CS}} & \\multirow{2}{*}{{\\color{brown}CS}}   & \\multirow{2}{*}{{\\color{brown}CS}} & \\multirow{2}{*}{{\\color{gray}HL}}   \\\\\n   & Hierachical   & & & \\\\\n   \\multirow{2}{*}{ImageNet (sbow)}   & Sparse \\& & \\multirow{2}{*}{{\\color{brown} CS}} & \\multirow{2}{*}{{\\color{brown}CS}}   & \\multirow{2}{*}{{\\color{brown}CS}} & \\multirow{2}{*}{{\\color{red}KL}}   \\\\\n   & Hierachical   & & & \\\\\\hline\n   \\end{tabular}}\n   \\vspace{-0.3cm}\n   \\end{table*}\n\n\n\n   where $Y\\lbrace y_{ij}\\rbrace$ is embedding parameters for point $x_{ij}$.\n\n   We now construct variational versions of the $ft$-SNE based on Table~\\ref{tab:vfSNE}.\n   \\begin{itemize}\n   \\item {\\em Example 1: Variational JS-SNE} \n   Setting $f$-divergence function to be $f(x)   x\\log x - (x+1)\\log(x+1)$, the conjugate of $f$ correponds to $f^*(t)   -\\log (1 - \\exp(t))$.\n   Then, we get\n   \\begin{align}\n   J_{JS}(Y)   \\min_{y_{ij}}\\max_{\\phi} \\sum_{x_{ij}}\\left[ \\log\\left(D_\\phi(x_{ij})\\right)p_{ij} - \\log\\left(1-D_\\phi(x_{ij})\\right)q^Y_{ij}\\right]\n   \\label{eqn:vjs-tse}\n   \\end{align}\n   \\item {\\em Example 2: Variational CH-SNE} \n   Setting $f$-divergence function to be $f(x)   (\\sqrt{t} -1)^2$, the conjugate of $f$ correponds to $f^*(t)   \\frac{1}{4}t^2+t$. \n   Then, we get\n   \\begin{align}\n   J_{CH}(Y)   \\min_{y_{ij}}\\max_{\\phi} \\sum_{x_{ij}} \\left[ D_\\phi(x_{ij})p_{ij} - \\left[D_\\phi(x_{ij}) \\left( \\frac{1}{4} D_\\phi(x_{ij})+1\\right)\\right]q^Y_{ij}\\right]\n   \\end{align}\n   \\end{itemize}\n   We work with variational JS-SNE (VJS-SNE) and variational CH-SNE (VCH-SNE) in the experiment section.\n\n\\subsection{Other Adversarial SNE}\nWe introduced adversarial t-SNE, which are SNE objectives that are formulated as in Equation~\\ref{eqn:generic_minmax}.\nIn particular, there are two widely studied distance and divergence measures, $f$-divergence \\cite{Nguyen2008} and the Integral Probability Metric (IPM) \\cite{Muller1997}.\nHere, We formulate t-SNE objectives under both varational lower bound of $f$-divergence and IPM.\n\n\n\n\\section{Variational SNE}\n\n   \\begin{table}[t]\n   \\centering\n   \\caption{Variational $f$-SNE. }\n   \\label{tab:vfSNE}\n   \\begin{tabular}{lccc}\\hline\n   $D_f(P\\|Q)$   & $f^*(\\cdot)$   & $t(x)$ \\\\\\hline\\hline\n   KL   & $\\exp(t-1)$   & $x$\\\\\n   Reverse KL   & $-1-\\log(-t)$   & $-\\exp(-x)$\\\\\n   Jensen-Shannon   & $-\\log(1-\\exp(t))$   & $\\log(2) - \\log\\left(1+\\exp(-x)\\right)$\\\\   \n   Chi-square ($\\mathcal{X}^2$)\t& $\\frac{1}{4}t^2+t$   & $x$\\\\   \n   Hellinger distance   & $\\frac{t}{1-t}$   & $1-\\exp(-x)$\\\\   \\hline\n   \\end{tabular}\n   \\end{table}\n\n\n   Because $f$-divergence has the dual form, it is possible to formulate $f$-SNE in variational form. \n   The dual form of $f$-divergence in Equation~\\ref{eqn:fdivergence} is\n   \\begin{align}\n   D(P||Q) & \\sum_{x_{ij}} \\left[ q_{ij} \\sup_{t \\in \\mathcal{T}} t(x_{ij})\\frac{p_{ij}}{q_{ij}} - f^*(t(x_{ij})) \\right]\\\\\n   \\geq & \\sup_{t \\in \\mathcal{T}} \\sum_{x_{ij}} \\left[ t(x_{ij})p_{ij} - f^*(t(x_{ij}))q_{ij}\\right]\n   \\label{eqn:vfdiv}\n   \\end{align}\n   where $\\mathcal{T}$ is a class of bounded real-valued measurable functions on measurable space M, and\n   $f$ is any $f$-divergence functions and $f^*$ corresponds to conjugate of $f$. \n   \n   We turn the variational form of $f$-SNE in Eq~\\ref{eqn:vfdiv} into optimization problem, we replace $\\sup_{t \\in \\mathcal{T}}$ by $\\min$ operator with neural network function class,\n   \\begin{align}\n   D(P||Q) \\geq \\max_{\\phi} \\sum_{x_{ij}} \\left[ D_\\phi(x_{ij})p_{ij} - f^*\\left(t(D_\\phi(x_{ij}))\\right)q_{ij}\\right]\n   \\end{align}\n   where $D_\\phi(\\cdot):\\mathcal{X}\\rightarrow \\mathbb{R}$ is a neural network function that takes input and ouputs in real domain, and $\\phi$ is the parameters of neural network. \n   Hence, the problem became {\\em an min-max problem}\n   \\begin{equation}\n   J(Y)   \\min_{y_{ij}}\\max_{\\phi} \\sum_{x_{ij}} \\left[ D_\\phi(x_{ij})p_{ij} - f^*\\left(t\\left(D_\\phi(x_{ij})\\right)\\right)q^Y_{ij}\\right]\n   \\label{eqn:vfsne}\n   \\end{equation}   \n   where $Y\\lbrace y_{ij}\\rbrace$ is embedding parameters for point $x_{ij}$.\n\n   Now, we can construct the objective function for variational versions of $f$-SNE based on Table~\\ref{tab:vfSNE}:\n   \\begin{itemize}\n   \\item {\\em Example 1: Variational JS-SNE} \n\n   Setting $f$-divergence function to be $f(x)   x\\log x - (x+1)\\log(x+1)$, the conjugate of $f$ correponds to $f^*(t)   -\\log (1 - \\exp(t))$.\n   Then, we get\n   \\begin{align}\n   J_{JS}(Y)   \\min_{y_{ij}}\\max_{\\phi} \\sum_{x_{ij}}\\left[ \\log\\left(D_\\phi(x_{ij})\\right)p_{ij} - \\log\\left(1-D_\\phi(x_{ij})\\right)q^Y_{ij}\\right]\n   \\label{eqn:vjs-tse}\n   \\end{align}\n   This corresponds to GAN objective upto the constant factor \\cite{Goodfellow2014}.\n   \\item {\\em Example 2: Variational CH-SNE} \n\n   Setting $f$-divergence function to be $f(x)   (\\sqrt{t} -1)^2$, the conjugate of $f$ correponds to $f^*(t)   \\frac{1}{4}t^2+t$. \n   Then, we get\n   \\begin{align}\n   J_{CH}(Y)   \\min_{y_{ij}}\\max_{\\phi} \\sum_{x_{ij}} \\left[ D_\\phi(x_{ij})p_{ij} - \\left[D_\\phi(x_{ij}) \\left( \\frac{1}{4} D_\\phi(x_{ij})+1\\right)\\right]q^Y_{ij}\\right]\n   \\end{align}\n   \\end{itemize}\n   We work with variational JS-SNE (VJS-SNE) and variational CH-SNE (VCH-SNE) in the experiment section.\n\n\n\\section{Integral Probability Metric based Stochastic Neighbour Embeddings}\n\n   We turn our interest to finding a SNE using class of distance measure called Integral Probability Metric (IPM).\n   IPM is a class of distance measure on probabilities,\n   \\begin{align}\n   D(P, Q)   \\sup_{f \\in \\mathcal{F}} \\left|\\int_M   f dP -   \\int_M fdQ \\right|\n   \\end{align}\n   where $\\mathcal{F}$ is a class of bounded real-valued measurable functions on measurable space M.\n   IPM considers largest difference in feature means of the certain function class.\n   Because we are dealing with discrete set of domains, we simply sum over all possible pairs of points,\n   \\begin{align}\n   D(P, Q_Y)   \\sup_{f \\in \\mathcal{F}} \\left| \\sum_{x_{ij}} f(x_{ij}) \\left[ p_{ij} - q_{ij} \\right] \\right|.\n   \\end{align}\n   \n   Type of popular distance measures that fall under IPM includes the maximum mean discrepancy, Wasserstein distance, Dudley, and Kolmogorov distance.\n   We optimize SNE through MMD and Wasserstein distance measure. \n\n   \\subsection{Wasserstein SNE}\n\n   The Wasserstein SNE (W-SNE) objective is defined as \n   \\begin{align}\n   W(P, Q)   \\sup_{f \\in \\mathcal{F}_1}\\sum_{x_{ij}} f(x_{ij}) \\left[ p_{ij} - q_{ij} \\right] \n   \\label{eqn:wsne}\n   \\end{align}\n   where $\\mathcal{F}_1$ is a class of 1-lipschitz functions of $\\mathcal{F}$.\n\n   In practice, we consider $\\mathcal{F}_1$ by a parameteric model that satisfies lipschitz constraints.   \n   Then, we use envelope theorem to re-parameterize the Wasserstein metric $W(P, Q)$ in terms of $V(\\phi, Y)$\n   where change in the optimizer of the $W(P,Q)$ do not contribute to the change in the objective function $V(\\phi, Y)$.\n   Thus, the function class $\\mathcal{F}_1$ is parameterized by $\\phi$ and the embedidng distribution $Q$ is parameterized by $Y$.\n   The following theorem show that $V(\\phi, Y)$ is differentible almost everywhere with respect to embedding parameter $Y$.\n\n   \\begin{theorem}\n   \\label{lab:thm_wwd}\n   The neighbourhood probability distribution $Q_Y$ with the embedding parameters $Y$ is locally $L(Y)$-lipshitz continous, i.e., $Q_Y(X) - Q_{Y^\\prime}(X) \\leq L(Y) (Y - Y^\\prime)$.\n   Then, $W_f(P,Q_Y)$ is continous everywhere and differentible almost everywhere in $Y$.\n   \\end{theorem}\n   The proof is in the Appendix~\\ref{app:wsne}.\n   We can gaurantee that as W-SNE objective approaches zero, our embedding distribution $Q_Y$ approaches to data distribution $P$:\n   \\begin{theorem} (weak topology \\cite{villani2009}) \n   Let $P$ be the distribution on a compact space $\\mathcal{X}$ and $\\lbrace P_n \\rbrace$ be a sequence of distributions.\n   Then, \n   \\begin{align}\n   \\lim_{n\\rightarrow \\infty} W(P_n, P) \\rightarrow 0 \\Longleftrightarrow \\lim_{n\\rightarrow \\infty} P_n \\xrightarrow{\\text{D}} P \n   \\end{align}\n   where $\\xrightarrow{\\text{D}}$ implies convergence in distribution.\n   \\end{theorem} \n\n   \\subsection{MMD-SNE}\n   The maximum mean discrepancy metric considers the largest difference in the expectations over unit ball of reproducing kernel Hilbert space (RKHS) $\\mathcal{H}$,\n   \\begin{align}\n   MMD(P, Q) & \\sup_{h \\in \\mathcal{H}}   \\mathbb{E}_{P}\\left[h(x)\\right] - \\mathbb{E}_{Q}\\left[h(x)\\right].\n   \\end{align}\n   We can understand the squared MMD distance as the distance of mean embeddings in $\\mathcal{H}$,\n   \\begin{align}\n   MMD^2(P, Q_Y) & \\left[ \\sup_{h \\in \\mathcal{H}}\\sum_{x_{ij}} h(x_{ij}) \\left[ p_{ij} - q_{ij} \\right] \\right]^2   \\| \\mu_p - \\mu_q\\|^2_\\mathcal{H}\n   \\end{align}\n   The main advantage of MMD is that it allows us to consider mean of infinitely many features.\n   Moreover, we can express the sequared MMD in terms of kernel function\n   \\begin{align}\n   MMD^2(P, Q_Y) & \\left[ \\sum_{x_{ij}} \\sum_{x_{st}}k(x_{ij}, x_{st}) \\left[ p_{ij} - q_{st} \\right] \\right]^2 \n   \\label{eqn:mmd_kern}\n   \\end{align}\n   where $\\mathcal{H}$ is kernel function from Reproducing Kernel Hilbert Space and $\\|h\\| \\leq 1$. $k(x_i,x_j)$ is kernel function.\n\n   Theorem 5 in \\cite{Gretton2012} gaurantees that our embedding distribution $Q_Y$ will match the data distribution $P$ when $MMD_{k\\circ f_theta}(P,Q_Y)0$ (Theorem 5 can be found in Appendix~\\ref{app:mmd_appendix}).\n   For the optimization purpose, it is important that $MMD_{k\\circ f_theta}(P,Q_Y)$ is differentible almost everywhere with respect to embeeding parameters $Y$\n   and $Q_Y$ approaches $P$ as $MMD_{k\\circ f_theta}(P,Q_Y)$ approaches zero. \n   The following two theorems gaurantees them:\n   \\begin{theorem}\n   The neighbourhood probability distribution $Q_Y$ with the embedding parameters $Y$ is locally $L(Y)$-lipshitz continous., i.e., $Q_Y(X) - Q_{Y^\\prime}(X) \\leq L(Y) (Y - Y^\\prime)$.\n   Let $k$ be the Gaussian kernel function and $f$ be some bounded non-linear injective function.\n   Then, $MMD_{k\\circ f_theta}(P,Q_Y)$ is continous everywhere and differentible almost everywhere in $Y$.\n   \\end{theorem}\n   The proof of the first thoerem is in the Appendix~\\ref{app:mmd_appendix}.\n\n   \\begin{theorem} (weak topology \\cite{Li2017}) \n   Let $P$ be the distribution on a compact space $\\mathcal{X}$ and $\\lbrace P_n \\rbrace$ be a sequence of distributions.\n   Let $k$ be the Gaussian kernel function and $f_\\theta$ be some bounded non-linear injective function that is parameterized by $\\theta$.\n   Then, \n   \\begin{align}\n   \\lim_{n\\rightarrow \\infty} MMD_{k\\circ f_\\theta}(P_n, P) \\rightarrow 0 \\Longleftrightarrow \\lim_{n\\rightarrow \\infty} P_n \\xrightarrow{\\text{D}} P \n   \\end{align}\n   where $\\xrightarrow{\\text{D}}$ implies convergence in distribution.\n   \\end{theorem} \n   \n   Note that the kernel representation of MMD formulation in Equation~\\ref{eqn:mmd_kern} can be easily intractible in practice due to the quadratic comparisions (double summation).\n   We introduce a large-scale batch training for MMD-SNE in Appendix~\\ref{app:mmd_largscale}.\n\n   \\subsection{Kernel weighted Least-Square SNE}\n   We consider a simple least square distance loss with kernel witness function.\n   \\begin{align}\n   KLS(P, Q)   & \\left[\\mathbb{E}_{P}\\left[h(x) k(x)\\right] - \\mathbb{E}_{Q}\\left[h(x) k(x)\\right]\\right]^2\\\\   \n   & \\sum_{x_{ij}} k(x_i,x_j) \\left[ p(x_{ij}) - q(x_{ij})\\right]^2\n   \\end{align}\n\n\n\n\n\n\\subsection{Wassestein SNE}\n\\begin{theorem} Given $P$ distribution. Let $Q_Y$ be the embedding distribution with respect to the emebedding parameters $Y \\lbrace y_{ij} | \\forall i,\\ j\\rbrace$. \n   Supppose that $Q_Y$ in Equation~\\ref{eqn:Q_Y} is locally $L$-Lipchitze continuous.\n   when the $f\\circ q$ are well-defined. \n\\end{theorem}\n\\subsection{Maximum Mean Discrepancy SNE}\n\\section{Generalization bound for SNE}\n\n\\begin{theorem} \n   \\begin{align}\n   \\left| \\sup_f \\int f(x)\\left[P(x)-Q(x)\\right]dx - \\sup_f \\sum_i f(x_i)\\left[P(x_i)-Q(x_i)\\right] \\right| \\leq \\epsilon\n   \\end{align}\n   with probability at least $1-\\frac{\\delta}{4}$.\n\\end{theorem}\n\n\n\\begin{proof}\n   \\begin{align}\n   & \\left| \\sup_f   \\int f(x)\\left[Q(x)-P(x)\\right] dx - \\sum_f \\sum_i f(x_i) \\left[P(x_i) - Q(x_i)\\right] \\right| \\\\\n   \\leq & \\sup_f \\left| \\int f(x) \\left[P(x)-Q(x)\\right] dx - \\sum_i f(x_i) \\left[P(x_i) - Q(x_i)\\right] \\right| \\\\\n   & \\sup_f \\left| \\int f(x) P(x)dx - \\sum_i f(x_i) P(x_i) + \\sum_i f(x_i)Q(x_i) - \\int f(x)Q(x) dx \\right| \\\\\n   \\leq & \\sup_f \\left( \\left| \\int f(x) P(x)dx - \\sum_i f(x_i) P(x_i) \\right| + \\left| \\int f(x)Q(x) dx - \\sum_i f(x_i)Q(x_i)   \\right| \\right) \\\\\n   \\leq & \\sup_f \\left| \\int f(x) P(x)dx - \\sum_i f(x_i) P(x_i) \\right| + \\sup_f \\left| \\int f(x)Q(x) dx - \\sum_i f(x_i)Q(x_i)   \\right| \\label{eqn:abc}\n   \\end{align}\n   Let $l(S)$ be the number of distinct elemnts of $S$.\n   We can further bound Equation~\\ref{eqn:abc}.\n   \\begin{multline}\n   \\sup_f \\left| \\int f(x) P(x)dx - \\sum_i f(x_i) P(x_i) \\right| + \\sup_f \\left| \\sum_i f(x_i)Q(x_i) - \\int f(x)Q(x) dx \\right| \\\\\n   \\leq   \\sup_f \\left| \\int f(x) P(x)dx - \\sum_i^m f(x_i) \\right| + \\sup_f \\left|   \\sum_i^m f(x_i) - \\sum^{l(S)}_j f(x_j) P(x_j) \\right| \\\\\n   + \\sup_f \\left| \\int f(x) Q(x)dx - \\sum_i^m f(x_i) \\right| + \\sup_f \\left|   \\sum_i^m f(x_i) - \\sum^{l(S)}_j f(x_j) Q(x_j) \\right|\n   \\end{multline}\n   By applying Theorem 11 from \\cite{Bharath2009} to the first two terms,\n   \\begin{multline}\n   \\sup_f \\left| \\int f(x) P(x)dx - \\sum_i^m f(x_i) \\right| + \\sup_f \\left| \\int f(x) Q(x)dx - \\sum_i^m f(x_i) \\right|\n   \\leq 4\\mathcal{R}_m (\\mathcal{F}:S) \\sqrt{18 v^2 \\log \\frac{4}{\\delta}} \\frac{2}{\\sqrt{m}}\n   \\end{multline}\n   where $\\mathcal{R}_m (\\mathcal{F}:S)$ is Rademacher complexity over $\\mathcal{F}$.\n   Corollary 12 from \\cite{Bharath2009} tells us that   \n   \\begin{multline}\n   \\sup_f \\left| \\int f(x) P(x)dx - \\sum_i^m f(x_i) \\right| + \\sup_f \\left| \\int f(x) Q(x)dx - \\sum_i^m f(x_i) \\right|   O(2 r_m) \\text{ for W-SNE}\n   \\end{multline}\n   where \n   \\[ r_m   \n   \\begin{cases} \n   m^{-1}{2}\\log m & d1\\\\\n   m^{-1/(d+1)} & d\\geq2\\\\\n   \\end{cases}\n   \\], and \n   \\begin{align}\n   \\sup_f \\left| \\int f(x) P(x)dx - \\sum_i^m f(x_i) \\right| + \\sup_f \\left|   \\int f(x) Q(x)dx - \\sum_i^m f(x_i) \\right|   O(2 m^{-1/2}) \\text{ for MMD-SNE}\n   \\end{align}\n   with probability at least $1-\\frac{\\delta}{4}$.\n\n   Now we apply Lemma 1 and Lemma 2 to bound to third and fourth terms.\n   ...\n\\end{proof}\n\n\\begin{lemma} \n   \\begin{align}\n   \\sup_f \\left|   \\sum_i^m f(x_i) - \\sum^{l(S)}_j f(x_j) P(x_j) \\right|   \\leq [something]\n   \\end{align}\n   with probability at least $1-\\frac{\\delta}{4}$.\n\\end{lemma}\n\\begin{proof}\n   Let $l(S)$ be the number of distinct elemnts of $S$.\n   \\begin{multline}\n   \\sup_f \\left|   \\sum_i^m f(x_i) - \\sum^{l(S)}_j f(x_j) P(x_j) \\right| \n   \\end{multline}\n\\end{proof}\n\n\\section{Algorithm for IPM-SNE}\n\\begin{algorithm}[H]\n   \\caption{Algorithm 1 : IPM-SNE Training Procedure}\\label{euclid}\n   \\label{algo:update}\n   \\begin{algorithmic}[1]\n   \\Procedure{Full-batch Training}{learning rate $\\eta$}\n   \\While {$(\\theta, Y)$ has not converged}\n   \\State \\# Single-Step Gradient Update\n   \\State $\\theta \\leftarrow \\theta + \\eta \\nabla_\\theta V(\\theta, Y)$\n   \\State $Y \\leftarrow Y + \\eta \\nabla_Y V(\\theta, Y)$\n   \\EndWhile\n   \\EndProcedure\n   \\end{algorithmic}\n\\end{algorithm}\n\n\n\n\\begin{theorem} \n   Let $\\pi   (\\theta, Y)$ be two sets of parameters for the training objective function $V(\\theta, Y)$,\n   \\[ \\min_Y \\max_\\theta V(\\theta, Y)   \n   \\begin{cases} \n   \\min_Y W_{f_\\theta}(P,Q_Y) & \\text{for Wasserstein-SNE}\\\\\n   \\min_Y MMD_{f_\\theta}(P,Q_Y) & \\text{for MMD-SNE}\\\\\n   \\end{cases}\n   \\]\n   Assume that $V(\\theta, Y)$ is strongly convex in $\\theta$, \n   $\\nabla_\\theta V(\\pi^*)   0$ and $\\nabla_\\theta V(\\pi) \\geq \\delta I$,\n   and strongly concave in $Y$, i.e.,\n   $\\nabla_Y V(\\pi^*)   0$ and $\\nabla_Y V(\\pi) \\leq \\delta I$, where $\\pi^*$ is an locally optimal solution.\n   Consider   $J(\\pi)   \\frac{1}{2}\\|\\nabla V(\\pi)\\|^2_2$ and $J(\\pi)$ is locally L-lipschitz function.\n   Then, \n   \\begin{align}\n   J(\\pi^t) \\leq \\left(1 - \\frac{\\delta^2}{2L}\\right)^t J(\\pi^0)\n   \\end{align}   \n   for step-size $\\eta   \\delta/L$ in Algorithm~\\ref{algo:update}, \n\\end{theorem}\n\n\n\n\\subsection{Stochastic Optimization}\n\\begin{itemize}\n\\item Problem: We can't do naive stochastic / batch update with respect to $Y\\lbrace y_{ij}\\ \\forall\\ i,j \\rbrace $, because\n   \\begin{align}\n   \\nabla_Y W(P, Q_Y)   - \\nabla_Y \\mathbb{E}_{Q_Y} \\left[f(x_{ij})\\right] \\neq - \\mathbb{E}_{Q_Y} \\left[\\nabla_Y f(x_{ij})\\right]\n   \\end{align}\n\n\\item We need to do is \n   \\begin{align}\n   \\nabla_Y \\mathbb{E}_{Q_Y} \\left[f(x_{ij})\\right]   \\mathbb{E}_{Q_Y} \\left[f(x_{ij}) \\frac{\\partial \\log Q_Y(x_{ij}) }{ \\partial Y } \\right]\n   \\mathbb{E}_{Q_Y} \\left[\\nabla f(q_{ij}) + f(q_{ij}) \\nabla \\log (q_{ij}) \\right]\n   \\end{align}\n   \\begin{align}\n   \\nabla_{y_{ij}} \\left [ \\mathbb{E}_{p_{ij} \\sim P } - \\mathbb{E}_{q_{ij} \\sim Q_Y} \\left[f(q_{ij}) \\right] \\right]\n   & -\\nabla_{y_{ij}}   \\mathbb{E}_{q_{ij}~\\sim Q_Y } \\left[f(q_{ij}) \\right]\\\\   \n   & -\\sum_{ij} \\nabla_{y_{ij}} \\left[ q_{ij} f(q_{ij}) \\right]\\\\\n   & -\\sum_{ij} \\left[ \\nabla_{y_{ij}} q_{ij} f(q_{ij}) + q_{ij} \\nabla_{y_{ij}} f(q_{ij}) \\right]\\\\\n   & -\\sum_{ij} \\left[ q_{ij} \\nabla_{y_{ij}} f(q_{ij}) + q_{ij} f(q_{ij})   \\nabla_{y_{ij}} \\left(\\log q_{ij}\\right) \\right]\\\\\n   & -\\sum_{ij} q_{ij} \\left[ \\nabla_{y_{ij}} f(q_{ij}) + f(q_{ij}) \\nabla_{y_{ij}} \\log q_{ij}\\right]\\\\\n   & - \\mathbb{E}_{q_{ij}\\sim Q_Y} \\left[ \\nabla_{y_{ij}}f(q_{ij}) + f(q_{ij}) \\nabla_{y_{ij}} \\log q_{ij}\\right]\n   \\end{align}\n\\item Our gradients have high variance. We need to do variance reduction.\n   \\begin{align}\n   -\\nabla_{y_{ij}}   \\mathbb{E}_{q_{ij}~\\sim Q_Y } \\left[f(q_{ij}) \\right]   - \\mathbb{E}_{q_{ij}\\sim Q_Y} \\left[ \\nabla_{y_{ij}}f(q_{ij}) + (f(q_{ij}) - b)\\nabla_{y_{ij}} \\log q_{ij}\\right]\n   \\end{align}\n   where $b$ is some baseline variable that does not depend on $\\theta$.\n\\end{itemize}\n\n\n\n\n\n\n\\iffalse   \n   We have introduced $ft$-SNE in the previous section. We have two possible ways to optimze the $ft$-SNE.\n   Equation~\\ref{eqn:fSNE_primal} corresponds to the primal form, here, we will introduce dual form of $ft$-SNE.\n\n   Because $f$-divergences consists of dual form, we can consider optimizing the embeddings from its dual formulation. \n   The dual form of $f$-divergence in Equation~\\ref{eqn:fdivergence} is\n   \\begin{align}\n   D(P||Q) & \\sum_{x_{ij}} \\left[ q_{ij} \\left(\\max_{h \\in \\mathcal{H}} h(x_{ij})\\frac{p_{ij}}{q_{ij}} - f^*(h(x_{ij})) \\right)\\right]\n   \\end{align}\n   where $\\mathcal{H}$ is a real-valued measurable functions on measurable space M, and\n   $f$ is any $f$-divergence functions and $f^*$ corresponds to Fenchel conjugate of $f$. \n   The maximum operator\\footnote{the supremum operator is replaced with maximum operator in our settings.} in dual form acts per data point, which makes infeasible to optimize in practice.\n   Instead, we consider the variatonal form (the lower bound of the dual form),\n   \\begin{align}\n   D(P||Q)\\geq & \\max_{h \\in \\mathcal{H}} \\sum_{x_{ij}} \\left[ h(x_{ij})p_{ij} - f^*(h(x_{ij}))q_{ij}\\right].\n   \\label{eqn:vfdiv}\n   \\end{align}\n   \n   \\begin{table}[t]\n   \\centering\n   {\\small \n   \\caption{Variational $ft$-SNE. }\n   \\label{tab:vfSNE}\n   \\begin{tabular}{lcccc}\\hline\n   $D_f(P\\|Q)$   & $f(\\cdot)$   & $f^*(\\cdot)$   & $h(x)$ \\\\\\hline\\hline\n   KL   & $t\\log t$   & $\\exp(t-1)$   & $x$\\\\\n   Reverse KL   & $-\\log t$   & $-1-\\log(-t)$   & $-\\exp(-x)$\\\\\n   Jensen-Shannon   & $-(t+1)\\log\\frac{(1+t)}{2} + t\\log t$ & $-\\log(1-\\exp(t))$   & $\\log(2) - \\log\\left(1+\\exp(-x)\\right)$\\\\\n   Hellinger distance   & $(\\sqrt{t} -1)^2$   & $\\frac{t}{1-t}$   & $1-\\exp(-x)$\\\\ \n   Chi-square ($\\mathcal{X}^2$)\t& $(t-1)^2$\t\t   & $\\frac{1}{4}t^2+t$   & $x$\\\\   \\hline\n   \\end{tabular}}\n   \\end{table}\n\n   Eq~\\ref{eqn:vfdiv} becomes the equality under flexible $\\mathcal{H}$. \n   Optimizing the variational form of $ft$-SNE in Eq~\\ref{eqn:vfdiv} involves replacing the $\\sup_{h \\in \\mathcal{H}}$ by $\\max$ operator with some function class $f$.\n   In the case of using parameteric model for $\\mathcal{H}$, \n   \\begin{align}\n   D(P||Q) \\geq \\max_{\\phi} \\sum_{x_{ij}} \\left[ D_\\phi(x_{ij})p_{ij} - f^*\\left(D_\\phi(x_{ij})\\right)q_{ij}\\right].\n   \\label{eqn:vfdiv2}\n   \\end{align}\n   where $D_\\phi(\\cdot):\\mathcal{X}\\rightarrow \\mathbb{R}$ is a parameterized model that takes input and ouputs in real domain,\n   and $h(\\cdot)$ refers to the function shown in Table~\\ref{tab:vfSNE}.\n   Finally our variational SNE objective becomes {\\em an minimax problem}\n   \\begin{equation}\n   \\min_{y_{ij}}\\max_{\\phi} J(Y)   \\min_{y_{ij}}\\max_{\\phi} \\sum_{x_{ij}} \\left[ D_\\phi(x_{ij})p_{ij} - f^*\\left(t\\left(D_\\phi(x_{ij})\\right)\\right)q^Y_{ij}\\right]\n   \\label{eqn:vfsne}\n   \\end{equation}   \n   where $Y\\lbrace y_{ij}\\rbrace$ is embedding parameters for point $x_{ij}$.\n\n   We now construct variational versions of the $ft$-SNE based on Table~\\ref{tab:vfSNE}.\n   \\begin{itemize}\n   \\item {\\em Example 1: Variational JS-SNE} \n   Setting $f$-divergence function to be $f(x)   x\\log x - (x+1)\\log(x+1)$, the conjugate of $f$ correponds to $f^*(t)   -\\log (1 - \\exp(t))$.\n   Then, we get\n   \\begin{align}\n   J_{JS}(Y)   \\min_{y_{ij}}\\max_{\\phi} \\sum_{x_{ij}}\\left[ \\log\\left(D_\\phi(x_{ij})\\right)p_{ij} - \\log\\left(1-D_\\phi(x_{ij})\\right)q^Y_{ij}\\right]\n   \\label{eqn:vjs-tse}\n   \\end{align}\n   \\item {\\em Example 2: Variational CH-SNE} \n   Setting $f$-divergence function to be $f(x)   (\\sqrt{t} -1)^2$, the conjugate of $f$ correponds to $f^*(t)   \\frac{1}{4}t^2+t$. \n   Then, we get\n   \\begin{align}\n   J_{CH}(Y)   \\min_{y_{ij}}\\max_{\\phi} \\sum_{x_{ij}} \\left[ D_\\phi(x_{ij})p_{ij} - \\left[D_\\phi(x_{ij}) \\left( \\frac{1}{4} D_\\phi(x_{ij})+1\\right)\\right]q^Y_{ij}\\right]\n   \\end{align}\n   \\end{itemize}\n   We work with variational JS-SNE (VJS-SNE) and variational CH-SNE (VCH-SNE) in the experiment section.\n\n\\subsection{Other Adversarial SNE}\nWe introduced adversarial t-SNE, which are SNE objectives that are formulated as in Equation~\\ref{eqn:generic_minmax}.\nIn particular, there are two widely studied distance and divergence measures, $f$-divergence \\cite{Nguyen2008} and the Integral Probability Metric (IPM) \\cite{Muller1997}.\nHere, We formulate t-SNE objectives under both varational lower bound of $f$-divergence and IPM.\n\n\\begin{algorithm}[t]\n   \\caption{Variational (Adversarial) SNE Optimization Algorithm}\\label{euclid}\n   \\label{algo:vfsne_update_rule}\n   \\begin{algorithmic}[1]\n   \\Procedure{Optimization}{Dataset $\\{X_{tr}, X_{vl}\\}$, learning rate $\\eta$, $f$-divergence $D$}\n   \\State Initialize the discriminant parameter $\\phi$.\n\n   \\While {$\\phi$ has not converged}\n   \\For {$j1\\cdots J$}\n   \\State $\\phi^{t+1}   \\phi^t + \\eta_\\phi \\nabla_\\phi D$.\n   \\EndFor\n   \\For {$k1\\cdots K$}\n   \\State $y^{t+1}   w^t - \\eta_y \\nabla_y D$.\n   \\EndFor\n   \\EndWhile\n   \\EndProcedure\n   \\end{algorithmic}\n\\end{algorithm}\n\n\n\n\n\n\n\n\n\\section{Variational SNE}\n\n   \\begin{table}[t]\n   \\centering\n   \\caption{Variational $f$-SNE. }\n   \\label{tab:vfSNE}\n   \\begin{tabular}{lccc}\\hline\n   $D_f(P\\|Q)$   & $f^*(\\cdot)$   & $t(x)$ \\\\\\hline\\hline\n   KL   & $\\exp(t-1)$   & $x$\\\\\n   Reverse KL   & $-1-\\log(-t)$   & $-\\exp(-x)$\\\\\n   Jensen-Shannon   & $-\\log(1-\\exp(t))$   & $\\log(2) - \\log\\left(1+\\exp(-x)\\right)$\\\\   \n   Chi-square ($\\mathcal{X}^2$)\t& $\\frac{1}{4}t^2+t$   & $x$\\\\   \n   Hellinger distance   & $\\frac{t}{1-t}$   & $1-\\exp(-x)$\\\\   \\hline\n   \\end{tabular}\n   \\end{table}\n\n\n   Because $f$-divergence has the dual form, it is possible to formulate $f$-SNE in variational form. \n   The dual form of $f$-divergence in Equation~\\ref{eqn:fdivergence} is\n   \\begin{align}\n   D(P||Q) & \\sum_{x_{ij}} \\left[ q_{ij} \\sup_{t \\in \\mathcal{T}} t(x_{ij})\\frac{p_{ij}}{q_{ij}} - f^*(t(x_{ij})) \\right]\\\\\n   \\geq & \\sup_{t \\in \\mathcal{T}} \\sum_{x_{ij}} \\left[ t(x_{ij})p_{ij} - f^*(t(x_{ij}))q_{ij}\\right]\n   \\label{eqn:vfdiv}\n   \\end{align}\n   where $\\mathcal{T}$ is a class of bounded real-valued measurable functions on measurable space M, and\n   $f$ is any $f$-divergence functions and $f^*$ corresponds to conjugate of $f$. \n   \n   We turn the variational form of $f$-SNE in Eq~\\ref{eqn:vfdiv} into optimization problem, we replace $\\sup_{t \\in \\mathcal{T}}$ by $\\min$ operator with neural network function class,\n   \\begin{align}\n   D(P||Q) \\geq \\max_{\\phi} \\sum_{x_{ij}} \\left[ D_\\phi(x_{ij})p_{ij} - f^*\\left(t(D_\\phi(x_{ij}))\\right)q_{ij}\\right]\n   \\end{align}\n   where $D_\\phi(\\cdot):\\mathcal{X}\\rightarrow \\mathbb{R}$ is a neural network function that takes input and ouputs in real domain, and $\\phi$ is the parameters of neural network. \n   Hence, the problem became {\\em an min-max problem}\n   \\begin{equation}\n   J(Y)   \\min_{y_{ij}}\\max_{\\phi} \\sum_{x_{ij}} \\left[ D_\\phi(x_{ij})p_{ij} - f^*\\left(t\\left(D_\\phi(x_{ij})\\right)\\right)q^Y_{ij}\\right]\n   \\label{eqn:vfsne}\n   \\end{equation}   \n   where $Y\\lbrace y_{ij}\\rbrace$ is embedding parameters for point $x_{ij}$.\n\n   Now, we can construct the objective function for variational versions of $f$-SNE based on Table~\\ref{tab:vfSNE}:\n   \\begin{itemize}\n   \\item {\\em Example 1: Variational JS-SNE} \n\n   Setting $f$-divergence function to be $f(x)   x\\log x - (x+1)\\log(x+1)$, the conjugate of $f$ correponds to $f^*(t)   -\\log (1 - \\exp(t))$.\n   Then, we get\n   \\begin{align}\n   J_{JS}(Y)   \\min_{y_{ij}}\\max_{\\phi} \\sum_{x_{ij}}\\left[ \\log\\left(D_\\phi(x_{ij})\\right)p_{ij} - \\log\\left(1-D_\\phi(x_{ij})\\right)q^Y_{ij}\\right]\n   \\label{eqn:vjs-tse}\n   \\end{align}\n   This corresponds to GAN objective upto the constant factor \\cite{Goodfellow2014}.\n   \\item {\\em Example 2: Variational CH-SNE} \n\n   Setting $f$-divergence function to be $f(x)   (\\sqrt{t} -1)^2$, the conjugate of $f$ correponds to $f^*(t)   \\frac{1}{4}t^2+t$. \n   Then, we get\n   \\begin{align}\n   J_{CH}(Y)   \\min_{y_{ij}}\\max_{\\phi} \\sum_{x_{ij}} \\left[ D_\\phi(x_{ij})p_{ij} - \\left[D_\\phi(x_{ij}) \\left( \\frac{1}{4} D_\\phi(x_{ij})+1\\right)\\right]q^Y_{ij}\\right]\n   \\end{align}\n   \\end{itemize}\n   We work with variational JS-SNE (VJS-SNE) and variational CH-SNE (VCH-SNE) in the experiment section.\n\n\n\\section{Integral Probability Metric based Stochastic Neighbour Embeddings}\n\n   We turn our interest to finding a SNE using class of distance measure called Integral Probability Metric (IPM).\n   IPM is a class of distance measure on probabilities,\n   \\begin{align}\n   D(P, Q)   \\sup_{f \\in \\mathcal{F}} \\left|\\int_M   f dP -   \\int_M fdQ \\right|\n   \\end{align}\n   where $\\mathcal{F}$ is a class of bounded real-valued measurable functions on measurable space M.\n   IPM considers largest difference in feature means of the certain function class.\n   Because we are dealing with discrete set of domains, we simply sum over all possible pairs of points,\n   \\begin{align}\n   D(P, Q_Y)   \\sup_{f \\in \\mathcal{F}} \\left| \\sum_{x_{ij}} f(x_{ij}) \\left[ p_{ij} - q_{ij} \\right] \\right|.\n   \\end{align}\n   \n   Type of popular distance measures that fall under IPM includes the maximum mean discrepancy, Wasserstein distance, Dudley, and Kolmogorov distance.\n   We optimize SNE through MMD and Wasserstein distance measure. \n\n   \\subsection{Wasserstein SNE}\n\n   The Wasserstein SNE (W-SNE) objective is defined as \n   \\begin{align}\n   W(P, Q)   \\sup_{f \\in \\mathcal{F}_1}\\sum_{x_{ij}} f(x_{ij}) \\left[ p_{ij} - q_{ij} \\right] \n   \\label{eqn:wsne}\n   \\end{align}\n   where $\\mathcal{F}_1$ is a class of 1-lipschitz functions of $\\mathcal{F}$.\n\n   In practice, we consider $\\mathcal{F}_1$ by a parameteric model that satisfies lipschitz constraints.   \n   Then, we use envelope theorem to re-parameterize the Wasserstein metric $W(P, Q)$ in terms of $V(\\phi, Y)$\n   where change in the optimizer of the $W(P,Q)$ do not contribute to the change in the objective function $V(\\phi, Y)$.\n   Thus, the function class $\\mathcal{F}_1$ is parameterized by $\\phi$ and the embedidng distribution $Q$ is parameterized by $Y$.\n   The following theorem show that $V(\\phi, Y)$ is differentible almost everywhere with respect to embedding parameter $Y$.\n\n   \\begin{theorem}\n   \\label{lab:thm_wwd}\n   The neighbourhood probability distribution $Q_Y$ with the embedding parameters $Y$ is locally $L(Y)$-lipshitz continous, i.e., $Q_Y(X) - Q_{Y^\\prime}(X) \\leq L(Y) (Y - Y^\\prime)$.\n   Then, $W_f(P,Q_Y)$ is continous everywhere and differentible almost everywhere in $Y$.\n   \\end{theorem}\n   The proof is in the Appendix~\\ref{app:wsne}.\n   We can gaurantee that as W-SNE objective approaches zero, our embedding distribution $Q_Y$ approaches to data distribution $P$:\n   \\begin{theorem} (weak topology \\cite{villani2009}) \n   Let $P$ be the distribution on a compact space $\\mathcal{X}$ and $\\lbrace P_n \\rbrace$ be a sequence of distributions.\n   Then, \n   \\begin{align}\n   \\lim_{n\\rightarrow \\infty} W(P_n, P) \\rightarrow 0 \\Longleftrightarrow \\lim_{n\\rightarrow \\infty} P_n \\xrightarrow{\\text{D}} P \n   \\end{align}\n   where $\\xrightarrow{\\text{D}}$ implies convergence in distribution.\n   \\end{theorem} \n\n   \\subsection{MMD-SNE}\n   The maximum mean discrepancy metric considers the largest difference in the expectations over unit ball of reproducing kernel Hilbert space (RKHS) $\\mathcal{H}$,\n   \\begin{align}\n   MMD(P, Q) & \\sup_{h \\in \\mathcal{H}}   \\mathbb{E}_{P}\\left[h(x)\\right] - \\mathbb{E}_{Q}\\left[h(x)\\right].\n   \\end{align}\n   We can understand the squared MMD distance as the distance of mean embeddings in $\\mathcal{H}$,\n   \\begin{align}\n   MMD^2(P, Q_Y) & \\left[ \\sup_{h \\in \\mathcal{H}}\\sum_{x_{ij}} h(x_{ij}) \\left[ p_{ij} - q_{ij} \\right] \\right]^2   \\| \\mu_p - \\mu_q\\|^2_\\mathcal{H}\n   \\end{align}\n   The main advantage of MMD is that it allows us to consider mean of infinitely many features.\n   Moreover, we can express the sequared MMD in terms of kernel function\n   \\begin{align}\n   MMD^2(P, Q_Y) & \\left[ \\sum_{x_{ij}} \\sum_{x_{st}}k(x_{ij}, x_{st}) \\left[ p_{ij} - q_{st} \\right] \\right]^2 \n   \\label{eqn:mmd_kern}\n   \\end{align}\n   where $\\mathcal{H}$ is kernel function from Reproducing Kernel Hilbert Space and $\\|h\\| \\leq 1$. $k(x_i,x_j)$ is kernel function.\n\n   Theorem 5 in \\cite{Gretton2012} gaurantees that our embedding distribution $Q_Y$ will match the data distribution $P$ when $MMD_{k\\circ f_theta}(P,Q_Y)0$ (Theorem 5 can be found in Appendix~\\ref{app:mmd_appendix}).\n   For the optimization purpose, it is important that $MMD_{k\\circ f_theta}(P,Q_Y)$ is differentible almost everywhere with respect to embeeding parameters $Y$\n   and $Q_Y$ approaches $P$ as $MMD_{k\\circ f_theta}(P,Q_Y)$ approaches zero. \n   The following two theorems gaurantees them:\n   \\begin{theorem}\n   The neighbourhood probability distribution $Q_Y$ with the embedding parameters $Y$ is locally $L(Y)$-lipshitz continous., i.e., $Q_Y(X) - Q_{Y^\\prime}(X) \\leq L(Y) (Y - Y^\\prime)$.\n   Let $k$ be the Gaussian kernel function and $f$ be some bounded non-linear injective function.\n   Then, $MMD_{k\\circ f_theta}(P,Q_Y)$ is continous everywhere and differentible almost everywhere in $Y$.\n   \\end{theorem}\n   The proof of the first thoerem is in the Appendix~\\ref{app:mmd_appendix}.\n\n   \\begin{theorem} (weak topology \\cite{Li2017}) \n   Let $P$ be the distribution on a compact space $\\mathcal{X}$ and $\\lbrace P_n \\rbrace$ be a sequence of distributions.\n   Let $k$ be the Gaussian kernel function and $f_\\theta$ be some bounded non-linear injective function that is parameterized by $\\theta$.\n   Then, \n   \\begin{align}\n   \\lim_{n\\rightarrow \\infty} MMD_{k\\circ f_\\theta}(P_n, P) \\rightarrow 0 \\Longleftrightarrow \\lim_{n\\rightarrow \\infty} P_n \\xrightarrow{\\text{D}} P \n   \\end{align}\n   where $\\xrightarrow{\\text{D}}$ implies convergence in distribution.\n   \\end{theorem} \n   \n   Note that the kernel representation of MMD formulation in Equation~\\ref{eqn:mmd_kern} can be easily intractible in practice due to the quadratic comparisions (double summation).\n   We introduce a large-scale batch training for MMD-SNE in Appendix~\\ref{app:mmd_largscale}.\n\n   \\subsection{Kernel weighted Least-Square SNE}\n   We consider a simple least square distance loss with kernel witness function.\n   \\begin{align}\n   KLS(P, Q)   & \\left[\\mathbb{E}_{P}\\left[h(x) k(x)\\right] - \\mathbb{E}_{Q}\\left[h(x) k(x)\\right]\\right]^2\\\\   \n   & \\sum_{x_{ij}} k(x_i,x_j) \\left[ p(x_{ij}) - q(x_{ij})\\right]^2\n   \\end{align}\n\n\n\n\n\n\\subsection{Wassestein SNE}\n\\begin{theorem} Given $P$ distribution. Let $Q_Y$ be the embedding distribution with respect to the emebedding parameters $Y \\lbrace y_{ij} | \\forall i,\\ j\\rbrace$. \n   Supppose that $Q_Y$ in Equation~\\ref{eqn:Q_Y} is locally $L$-Lipchitze continuous.\n   when the $f\\circ q$ are well-defined. \n\\end{theorem}\n\\subsection{Maximum Mean Discrepancy SNE}\n\\section{Generalization bound for SNE}\n\n\\begin{theorem} \n   \\begin{align}\n   \\left| \\sup_f \\int f(x)\\left[P(x)-Q(x)\\right]dx - \\sup_f \\sum_i f(x_i)\\left[P(x_i)-Q(x_i)\\right] \\right| \\leq \\epsilon\n   \\end{align}\n   with probability at least $1-\\frac{\\delta}{4}$.\n\\end{theorem}\n\n\n\\begin{proof}\n   \\begin{align}\n   & \\left| \\sup_f   \\int f(x)\\left[Q(x)-P(x)\\right] dx - \\sum_f \\sum_i f(x_i) \\left[P(x_i) - Q(x_i)\\right] \\right| \\\\\n   \\leq & \\sup_f \\left| \\int f(x) \\left[P(x)-Q(x)\\right] dx - \\sum_i f(x_i) \\left[P(x_i) - Q(x_i)\\right] \\right| \\\\\n   & \\sup_f \\left| \\int f(x) P(x)dx - \\sum_i f(x_i) P(x_i) + \\sum_i f(x_i)Q(x_i) - \\int f(x)Q(x) dx \\right| \\\\\n   \\leq & \\sup_f \\left( \\left| \\int f(x) P(x)dx - \\sum_i f(x_i) P(x_i) \\right| + \\left| \\int f(x)Q(x) dx - \\sum_i f(x_i)Q(x_i)   \\right| \\right) \\\\\n   \\leq & \\sup_f \\left| \\int f(x) P(x)dx - \\sum_i f(x_i) P(x_i) \\right| + \\sup_f \\left| \\int f(x)Q(x) dx - \\sum_i f(x_i)Q(x_i)   \\right| \\label{eqn:abc}\n   \\end{align}\n   Let $l(S)$ be the number of distinct elemnts of $S$.\n   We can further bound Equation~\\ref{eqn:abc}.\n   \\begin{multline}\n   \\sup_f \\left| \\int f(x) P(x)dx - \\sum_i f(x_i) P(x_i) \\right| + \\sup_f \\left| \\sum_i f(x_i)Q(x_i) - \\int f(x)Q(x) dx \\right| \\\\\n   \\leq   \\sup_f \\left| \\int f(x) P(x)dx - \\sum_i^m f(x_i) \\right| + \\sup_f \\left|   \\sum_i^m f(x_i) - \\sum^{l(S)}_j f(x_j) P(x_j) \\right| \\\\\n   + \\sup_f \\left| \\int f(x) Q(x)dx - \\sum_i^m f(x_i) \\right| + \\sup_f \\left|   \\sum_i^m f(x_i) - \\sum^{l(S)}_j f(x_j) Q(x_j) \\right|\n   \\end{multline}\n   By applying Theorem 11 from \\cite{Bharath2009} to the first two terms,\n   \\begin{multline}\n   \\sup_f \\left| \\int f(x) P(x)dx - \\sum_i^m f(x_i) \\right| + \\sup_f \\left| \\int f(x) Q(x)dx - \\sum_i^m f(x_i) \\right|\n   \\leq 4\\mathcal{R}_m (\\mathcal{F}:S) \\sqrt{18 v^2 \\log \\frac{4}{\\delta}} \\frac{2}{\\sqrt{m}}\n   \\end{multline}\n   where $\\mathcal{R}_m (\\mathcal{F}:S)$ is Rademacher complexity over $\\mathcal{F}$.\n   Corollary 12 from \\cite{Bharath2009} tells us that   \n   \\begin{multline}\n   \\sup_f \\left| \\int f(x) P(x)dx - \\sum_i^m f(x_i) \\right| + \\sup_f \\left| \\int f(x) Q(x)dx - \\sum_i^m f(x_i) \\right|   O(2 r_m) \\text{ for W-SNE}\n   \\end{multline}\n   where \n   \\[ r_m   \n   \\begin{cases} \n   m^{-1}{2}\\log m & d1\\\\\n   m^{-1/(d+1)} & d\\geq2\\\\\n   \\end{cases}\n   \\], and \n   \\begin{align}\n   \\sup_f \\left| \\int f(x) P(x)dx - \\sum_i^m f(x_i) \\right| + \\sup_f \\left|   \\int f(x) Q(x)dx - \\sum_i^m f(x_i) \\right|   O(2 m^{-1/2}) \\text{ for MMD-SNE}\n   \\end{align}\n   with probability at least $1-\\frac{\\delta}{4}$.\n\n   Now we apply Lemma 1 and Lemma 2 to bound to third and fourth terms.\n   ...\n\\end{proof}\n\n\\begin{lemma} \n   \\begin{align}\n   \\sup_f \\left|   \\sum_i^m f(x_i) - \\sum^{l(S)}_j f(x_j) P(x_j) \\right|   \\leq [something]\n   \\end{align}\n   with probability at least $1-\\frac{\\delta}{4}$.\n\\end{lemma}\n\\begin{proof}\n   Let $l(S)$ be the number of distinct elemnts of $S$.\n   \\begin{multline}\n   \\sup_f \\left|   \\sum_i^m f(x_i) - \\sum^{l(S)}_j f(x_j) P(x_j) \\right| \n   \\end{multline}\n\\end{proof}\n\n\\section{Algorithm for IPM-SNE}\n\\begin{algorithm}[H]\n   \\caption{Algorithm 1 : IPM-SNE Training Procedure}\\label{euclid}\n   \\label{algo:update}\n   \\begin{algorithmic}[1]\n   \\Procedure{Full-batch Training}{learning rate $\\eta$}\n   \\While {$(\\theta, Y)$ has not converged}\n   \\State \\# Single-Step Gradient Update\n   \\State $\\theta \\leftarrow \\theta + \\eta \\nabla_\\theta V(\\theta, Y)$\n   \\State $Y \\leftarrow Y + \\eta \\nabla_Y V(\\theta, Y)$\n   \\EndWhile\n   \\EndProcedure\n   \\end{algorithmic}\n\\end{algorithm}\n\n\n\n\\begin{theorem} \n   Let $\\pi   (\\theta, Y)$ be two sets of parameters for the training objective function $V(\\theta, Y)$,\n   \\[ \\min_Y \\max_\\theta V(\\theta, Y)   \n   \\begin{cases} \n   \\min_Y W_{f_\\theta}(P,Q_Y) & \\text{for Wasserstein-SNE}\\\\\n   \\min_Y MMD_{f_\\theta}(P,Q_Y) & \\text{for MMD-SNE}\\\\\n   \\end{cases}\n   \\]\n   Assume that $V(\\theta, Y)$ is strongly convex in $\\theta$, \n   $\\nabla_\\theta V(\\pi^*)   0$ and $\\nabla_\\theta V(\\pi) \\geq \\delta I$,\n   and strongly concave in $Y$, i.e.,\n   $\\nabla_Y V(\\pi^*)   0$ and $\\nabla_Y V(\\pi) \\leq \\delta I$, where $\\pi^*$ is an locally optimal solution.\n   Consider   $J(\\pi)   \\frac{1}{2}\\|\\nabla V(\\pi)\\|^2_2$ and $J(\\pi)$ is locally L-lipschitz function.\n   Then, \n   \\begin{align}\n   J(\\pi^t) \\leq \\left(1 - \\frac{\\delta^2}{2L}\\right)^t J(\\pi^0)\n   \\end{align}   \n   for step-size $\\eta   \\delta/L$ in Algorithm~\\ref{algo:update}, \n\\end{theorem}\n\n\n\n\\subsection{Stochastic Optimization}\n\\begin{itemize}\n\\item Problem: We can't do naive stochastic / batch update with respect to $Y\\lbrace y_{ij}\\ \\forall\\ i,j \\rbrace $, because\n   \\begin{align}\n   \\nabla_Y W(P, Q_Y)   - \\nabla_Y \\mathbb{E}_{Q_Y} \\left[f(x_{ij})\\right] \\neq - \\mathbb{E}_{Q_Y} \\left[\\nabla_Y f(x_{ij})\\right]\n   \\end{align}\n\n\\item We need to do is \n   \\begin{align}\n   \\nabla_Y \\mathbb{E}_{Q_Y} \\left[f(x_{ij})\\right]   \\mathbb{E}_{Q_Y} \\left[f(x_{ij}) \\frac{\\partial \\log Q_Y(x_{ij}) }{ \\partial Y } \\right]\n   \\mathbb{E}_{Q_Y} \\left[\\nabla f(q_{ij}) + f(q_{ij}) \\nabla \\log (q_{ij}) \\right]\n   \\end{align}\n   \\begin{align}\n   \\nabla_{y_{ij}} \\left [ \\mathbb{E}_{p_{ij} \\sim P } - \\mathbb{E}_{q_{ij} \\sim Q_Y} \\left[f(q_{ij}) \\right] \\right]\n   & -\\nabla_{y_{ij}}   \\mathbb{E}_{q_{ij}~\\sim Q_Y } \\left[f(q_{ij}) \\right]\\\\   \n   & -\\sum_{ij} \\nabla_{y_{ij}} \\left[ q_{ij} f(q_{ij}) \\right]\\\\\n   & -\\sum_{ij} \\left[ \\nabla_{y_{ij}} q_{ij} f(q_{ij}) + q_{ij} \\nabla_{y_{ij}} f(q_{ij}) \\right]\\\\\n   & -\\sum_{ij} \\left[ q_{ij} \\nabla_{y_{ij}} f(q_{ij}) + q_{ij} f(q_{ij})   \\nabla_{y_{ij}} \\left(\\log q_{ij}\\right) \\right]\\\\\n   & -\\sum_{ij} q_{ij} \\left[ \\nabla_{y_{ij}} f(q_{ij}) + f(q_{ij}) \\nabla_{y_{ij}} \\log q_{ij}\\right]\\\\\n   & - \\mathbb{E}_{q_{ij}\\sim Q_Y} \\left[ \\nabla_{y_{ij}}f(q_{ij}) + f(q_{ij}) \\nabla_{y_{ij}} \\log q_{ij}\\right]\n   \\end{align}\n\\item Our gradients have high variance. We need to do variance reduction.\n   \\begin{align}\n   -\\nabla_{y_{ij}}   \\mathbb{E}_{q_{ij}~\\sim Q_Y } \\left[f(q_{ij}) \\right]   - \\mathbb{E}_{q_{ij}\\sim Q_Y} \\left[ \\nabla_{y_{ij}}f(q_{ij}) + (f(q_{ij}) - b)\\nabla_{y_{ij}} \\log q_{ij}\\right]\n   \\end{align}\n   where $b$ is some baseline variable that does not depend on $\\theta$.\n\\end{itemize}\n\n\\fi\n\n\n\n\n",
  "title": "Stochastic Neighbor Embedding under $f$-divergences"
}
