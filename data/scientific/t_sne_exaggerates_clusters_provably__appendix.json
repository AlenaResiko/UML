{
  "authors": [
    "Noah Bergam",
    "Szymon Snoeck",
    "Nakul Verma"
  ],
  "date_published": "2025-10-09",
  "raw_tex": "\\section{Appendix: Misrepresentation of Cluster Structure}\\label{sec:appendix_false_pos}\n\n\n\\subsection{Additional Experiments}\n\\todo[inline]{Make Figure 6 appear here.}\n\\iffalse\n\\begin{figure}\n   \\centering\n   \\includegraphics[width0.49\\linewidth]{images/Same_Output_Diff_Input_in_same_dim_9_19_25.png}\n   \\caption{Mixture of Gaussians of Increasing Dimension.} \\label{fig:DiffIn_SameOut}\n   \\todo[inline]{Update caption.}\n   The t-SNE visualization and interpoint distance matrix of 50 points sampled from $N\\left(\\vec{0}, \\frac{1}{d^{3/4}}I_d\\right)$ and 50 points sampled from $N\\left(\\vec{e}_1,   \\frac{1}{d^{3/4}}I_d\\right)$ for $d \\in [100, 1000, 10000].$ Notice that as the dimension increases, the interpoint distance matrix of the input approaches a simplex but the t-SNE visualization becomes more clustered.\n\\end{figure}\n\\fi\n\nIn Figure \\ref{fig:higher_dim_tighter_clusters}, we plot a sample from a mixture of two Gaussians in 250, 500, 1000, 2000, and 4000 dimensions. Notice that as the dimension of the Gaussian increases, the interpoint distance matrix of the input points (bottom) approaches a simplex but the t-SNE corresponding visualization (top) remains qualitatively unchanged. \n\n\\begin{figure}[h!]\n   \\centering\n   \\includegraphics[width\\linewidth]{images/appendix_figs/Same_Output_Diff_Input_9_22_25.png}\n   \\caption{\\small t-SNE's interplay with Gaussian concentration of measure. \n   \\todo[inline]{Decide whether to keep or remove this figure.}]\n}\\label{fig:higher_dim_tighter_clusters}\n\\end{figure}\n\n\n\\begin{figure}[h!]\n   \\centering\n   \\includegraphics[width\\linewidth]{./iclr2026/images/appendix_figs/dist_hist.png}\n   \\caption{\\small Higher dimensional data looks more like a regular simplex; an example in NLP. We plot the histogram of interpoint distances of a term frequency-inverse document frequency (tf-idf) vectorization of the BBC news dataset as we increase the number of features used. }\\label{fig:NLP_simplex}\n\\end{figure}\n\n\\textcolor{red}{rework}\n It is also an intuitive property in practice: collecting more measurements and continually normalizing the resulting vector of measurements tends to push the minimum and maximum interpoint distances between data points closer together, see Figure \\ref{fig:NLP_simplex} for a manifestation of this phenomenon in the NLP domain. \n\n\n\\subsection{Calinski-Harabasz Index}\\label{sec:CHindex}\nFor an $n$-point dataset $X   \\{x_1, \\dots, x_n\\} \\subset \\mathbb{R}^{n-1}$ and a partition of the dataset into clusters $C_1 \\sqcup C_2 \\sqcup \\dots \\sqcup C_k   [n]$ with $n > k > 1$, the Calinski-Harabasz Index is defined as the ratio of the distance between cluster centers to the   internal distance to a cluster's center. Let $E$ be the function sending $S \\subseteq [n]$ to $\\mathbb{R}^{n-1}$ such that:\n$$E(S)   \\frac{1}{|S|}\\sum_{i \\in S} x_i.$$\nThen the Calinski-Harabasz Index is defined as\\footnote{If the denominator and numerator are $0$, then $\\text{CH}(X; C_{m \\in [k]}) : 1$. If only the denominator is 0, then $\\text{CH}(X; C_{m \\in [k]}) : \\infty$.}:\n$$\\text{CH}(X; C_{m \\in [k]})   \\frac{\\frac{1}{k-1}\\sum_{m \\in [k]} |C_m|\\cdot \\lVert E(C_m) - E([n])\\rVert^2}{\\frac{1}{n-k}\\sum_{m \\in [k]} \\sum_{i \\in C_m} \\lVert x_i - E(C_m)\\rVert^2}.$$\n\nIt ranges from $0$ to $\\infty$ with a score of $\\infty$ being assigned to perfectly clustered data, 1 to unclustered data and 0 to incorrectly clustered data. \n\nNow we provide an analogue to Theorem \\ref{thm:unclustHammer} with respect to the Calinski-Harabasz Index:\n\n\n\\begin{theorem}\n\\label{thm:unclustHammerCH}\n   Fix any $n > k > 1$, and $n$-point dataset ${X} \\subset \\mathbb{R}^{n-1}$ with partition $C_1 \\sqcup \\cdots \\sqcup C_k   [n]$ such that $\\textup{CH}({X}; C_{m\\in[k]}) > 1$. For all $1 < \\epsilon \\leq \\textup{CH}({X}; C_{m\\in[k]})$, there exists $n$-point dataset ${X}_\\epsilon \\subset \\mathbb{R}^{n-1}$ such that \n   $$\\textup{CH}({X}_\\epsilon; C_{m\\in[k]})   \\epsilon, $$\n   yet, for any $\\rho \\in (1, n-1)$:\n   $${\\TSNE}_\\rho({X})   {\\TSNE}_\\rho({X}_\\epsilon).$$\n\\end{theorem}\n\n\\begin{corollary}\n\\label{cor:twoCluster_n_UnclusteredCH}\n   For all $n \\geq 4$ even, and partition $C_1 \\sqcup C_2   [n]$ such that $|C_1||C_2|\\frac{n}{2}$. There exist a sequence of $n$-point datasets in $\\mathbb{R}^{n-1}$, $\\{ {X}_\\epsilon\\}_{1 < \\epsilon \\leq \\infty}$, with \n   $$\\textup{CH}({X}_\\epsilon; C_1, C_2)   \\epsilon$$\n   such that for any $\\rho \\in (1, n-1)$, $\\bigcap_{1<\\epsilon \\leq \\infty}\\text{t-SNE}_\\rho({X}_\\epsilon)$ contains $n$-point dataset ${Y} \\subseteq \\mathbb{R}^2$ with\n   $$\\textup{CH}(Y; C_1,C_2)   \\infty.$$ \n\\end{corollary}\n\n\\begin{proof}[\\textbf{Proof of Theorem \\ref{thm:unclustHammerCH}}]\n   \\textcolor{red}{for clarity you should call it \\(g_X(C)\\), also maybe use a different letter than \\(C\\), even little \\(c\\) is better}\n   First, let us assume that $\\text{CH}(X; C_{m \\in [k]}) < \\infty$. Let $g$ be the function from Corollary \\ref{cor:g_exists}, and let $f(C)   \\text{CH}(g(C); C_{m \\in [k]})$. Note that $f$ is continuous whenever the denominator of $\\text{CH}(\\ \\cdot\\ ; C_{m \\in [k]})$ \\textcolor{red}{maybe give the denominator a name}\n   is non-zero which is always the case for $C \\in [0,1]$. Therefore, the image of $f$ on $[0,1)$ contains the interval \\textcolor{red}{you did not establish it is a decreasing function of \\(C\\)} \n   $(f(1), f(0)]   (1, \\text{CH}(X ; C_{m \\in [k]})]$. \\textcolor{red}{at least show \\(f(1)   1\\)} \n   Thus, for all $\\epsilon \\in (1, \\text{CH}(X ; C_{m \\in [k]})]$, there exists $C \\in [0,1)$ such that $X_\\epsilon   g(C)$ satisfies the hypothesis.\n\n   If $\\text{CH}(X; C_{m \\in [k]})   \\infty,$ then $f$ is continuous on $(0,1)$ only. Thus for all $\\epsilon \\in (1, \\text{CH}(X ; C_{m \\in [k]}))$, there exists $C \\in (0,1)$ such that $X_\\epsilon   g(C)$ satisfies the hypothesis and for $\\epsilon \\text{CH}(X ; C_{m \\in [k]}))$, $X_\\epsilon   X$ satisfies the hypothesis.\n\\end{proof}\n\nFor proof of Corollary \\ref{cor:twoCluster_n_UnclusteredCH} see Appendix \\ref{sec:proofs_clust}.\n\n\\subsection{Dunn Index}\nFor an $n$-point dataset $X   \\{x_1, \\dots, x_n\\} \\subset \\mathbb{R}^{n-1}$ and a partition of the dataset into clusters $C_1 \\sqcup C_2 \\sqcup \\dots \\sqcup C_k   [n]$ with $|C_{m\\in[k]}| > 1$, the Dunn index measures the ratio between the minimum inter-cluster distance and maximum intra-cluster distance. Specifically, the Dunn index is given by the expression\\footnote{If the denominator and numerator are $0$, then $\\text{DI}(X; C_{m \\in [k]}) : 1$. If only the denominator is 0, then $\\text{DI}(X; C_{m \\in [k]}) : \\infty$.}\n$$\\text{DI}(X; C_{m \\in [k]})   \\frac{\\min_{m,l \\in [k], m\\neq l, i \\in C_m, j \\in C_l} \\lVert x_i - x_j \\rVert}{\\max_{m \\in [k], i,j \\in C_m} \\lVert x_i - x_j \\rVert}.$$\n\n\nIt ranges from $0$ to $\\infty$ with a score of $0$ being assigned to incorrectly clustered data, $1$ to unclustered data, and $\\infty$ to perfectly clustered data.\n\nNow we provide an analogue to Theorem \\ref{thm:unclustHammer} with respect to the Dunn Index:\n\n\\begin{theorem}\n\\label{thm:unclustHammerDunn}\n   Fix any $n > k > 1$, and $n$-point dataset ${X} \\subset \\mathbb{R}^{n-1}$ with partition $C_1 \\sqcup \\cdots \\sqcup C_k   [n]$ such that $|C_{m\\in[k]}| > 1$ and $\\textup{DI}({X}; C_{m\\in[k]}) > 1$. For all $1 < \\epsilon \\leq \\textup{DI}({X}_\\epsilon; C_{m\\in[k]})$, there exists $n$-point dataset ${X}_\\epsilon \\subset \\mathbb{R}^{n-1}$ such that \n   $$\\textup{DI}({X}_\\epsilon; C_{m\\in[k]})   \\epsilon, $$\n   yet, for any $\\rho \\in (1, n-1)$:\n   $${\\TSNE}_\\rho({X})   {\\TSNE}_\\rho({X}_\\epsilon).$$\n\\end{theorem}\n\n\\begin{corollary}\n\\label{cor:twoCluster_n_UnclusteredDunn}\n   For all $n \\geq 4$ even, and partition $C_1 \\sqcup C_2   [n]$ such that $|C_1||C_2|\\frac{n}{2}$. There exist a sequence of $n$-point datasets in $\\mathbb{R}^{n-1}$, $\\{ {X}_\\epsilon\\}_{1 < \\epsilon \\leq \\infty}$, with \n   $$\\textup{DI}({X}_\\epsilon; C_1, C_2)   \\epsilon$$\n   such that for any $\\rho \\in (1, n-1)$, $\\bigcap_{1<\\epsilon \\leq \\infty}\\text{t-SNE}_\\rho({X}_\\epsilon)$ contains $n$-point dataset ${Y} \\subseteq \\mathbb{R}^2$ with\n   $$\\textup{DI}(Y; C_1,C_2)   \\infty.$$ \n\\end{corollary}\n\n\\begin{proof}[\\textbf{Proof of Theorem \\ref{thm:unclustHammerDunn}}]\n   First, let us assume that $\\text{DI}(X; C_{m \\in [k]}) < \\infty.$ Let $g$ be the function from Corollary \\ref{cor:g_exists}, and $f(C)   \\text{DI}(g(C); C_{m \\in [k]}).$ Fix $i,j \\in [n]$ such that:\n   $$\\min_{m,l \\in [k], m\\neq l, i' \\in C_m, j' \\in C_l} \\lVert x_{i'} - x_{j'} \\rVert   \\lVert x_i - x_j \\rVert,$$\n   and $t,r \\in [n]$ such that:\n   $$\\max_{m \\in [k], i',j' \\in C_m} \\lVert x_{i'} - x_{j'} \\rVert   \\lVert x_{r} - x_{t} \\rVert,$$\n   Then:   \\textcolor{red}{you should emphaszie that the \\(g\\) transformation doesn't change the identity of max and min; DI is NOT continuous in X because of the max/miin   }\n   $$f(C)   \\frac{ \\sqrt{(1-C)\\cdot\\lVert x_i - x_j \\rVert+C}}{ \\sqrt{(1-C)\\cdot\\lVert x_r - x_t \\rVert+C}},$$ \n   \\textcolor{red}{note the continuity}\n   since $g$ preserves the ordering of the distances.   from min to max.\n   Thus, $f$ is continuous on $[0,1)$ and the image of $f$ on $[0,1)$ is $(f(0), f(1)]   (1, \\text{DI}(X; C_{m \\in [k]})]$. Therefore, for all $\\epsilon \\in (1,\\text{DI}(X; C_{m \\in [k]})]$, there exists $C \\in [0,1)$ such that $X_\\epsilon   g(C)$ satisfies the hypothesis.\n\n   If $\\text{DI}(X; C_{m \\in [k]})   \\infty,$ then $f$ is continuous on $(0,1)$ only. Thus for all $\\epsilon \\in (1, \\text{DI}(X; C_{m \\in [k]})),$ there exist $C \\in (0,1)$ such that $X_\\epsilon   g(C)$ satisfies the hypothesis and for $\\epsilon   \\text{DI}(X; C_{m \\in [k]}),$ $X_\\epsilon   X$ satisfies the hypothesis.\n\\end{proof}\n\nFor proof of Corollary \\ref{cor:twoCluster_n_UnclusteredDunn} see Appendix \\ref{sec:proofs_clust}.\n\n\\iffalse\n\\subsection{Davies\u2013Bouldin index}\nFor an $n$-point dataset $X   \\{x_1, \\dots, x_n\\} \\subset \\mathbb{R}^{n-1}$ and a partition of the dataset into clusters $C_1 \\sqcup C_2 \\sqcup \\dots \\sqcup C_k   [n]$ with $|C_{m\\in[k]}| \\geq 1$, the Davies\u2013Bouldin index measures the average ratio of the distance between a cluster and a cluster centroid to the distance to the next closet cluster centroid. Let $E$ be as perviously defined in Appendix \\ref{sec:CHindex} and fix $p,q \\geq 1$. For $m,l \\in [k]$ define:\n\n$$S_m : \\left( \\frac{1}{|C_m|} \\sum_{j \\in C_m} \\lVert x_j - E(C_m) \\rVert_2^q\\right)^{1/q}$$\n\nand\n\n$$M_{ml}   \\lVert E(C_m) - E(C_l) \\rVert_p.$$\n\nThen the Davies\u2013Bouldin index is:\\footnote{If the denominator and numerator are 0, then $\\frac{S_m+S_l}{M_{lm}} : ??$. If only the denominator is 0, then\n$\\frac{S_m+S_l}{M_{lm}} : \\infty$.}\n\n$$\\bar{R}_{p,q}(X; C_{m\\in[k]})   \\frac{1}{k} \\sum_{m \\in [k]} \\max_{l \\neq m} \\frac{S_m +S_l}{M_{lm}}.$$\n\nThe index ranges from 0 to $\\infty$ with a score of $0$ being assigned to perfectly clustered data, and $\\infty$ to misclustered data. Let $\\Dela \\subset \\mathbb{R}^{n-1}$ be a regular simplex of any side length. Then, the \n\nNow we restate Theorem \\ref{thm:perturbhammer} with respect to the Davies-Bouldin index:\n\n\\fi\n\n\n\n\n\n\n\n\n\\subsection{Proofs}\\label{sec:proofs_clust} \n\n\n\nThe main effort of this section will be to prove Lemma \\ref{lem:image_of_tSNE} which gives us Theorems \\ref{thm:unclustHammer} and \\ref{thm:perturbhammer}. We first introduce a number of technical lemmas that collectively show that t-SNE is invariant under additive and multiplicative scaling of the input.\n\n\\begin{lemma}\\label{lem:unique_neighborhood} Let \\(H(\\cdot)\\) denote the entropy function. For any $n>2$, \\(X   \\{x_1,\\ldots,x_n\\}\\subset \\mathbb{R}^{n-1}\\) and \\(\\rho \\in (1,n-1)\\), there is a unique \\(\\sigma_i \\geq 0\\) that minimizes\n\\[\\Big|   H(P_{\\cdot |i}(X; \\sigma_i) ) - \\log_2 \\rho \\Big|.\\]\n\\end{lemma}\n\\begin{proof}\n   This follows easily from the fact that \\(H(P_{\\cdot |i}(X; \\sigma))\\) is a continuous, strictly increasing function of \\(\\sigma\\) (see e.g.\\ Lemma 4.2 of \\citet{jeong2024convergence}), where \\(\\lim_{\\sigma\\to \\infty}H(P_{\\cdot |i}(X; \\sigma))   \n   \\log_2(n-1)\\) and \\(H(P_{\\cdot |i}(X; 0)) \\in (0,\\log_2(n-1))\\). If \\(\\log_2 \\rho \\in (h, n-1)\\), then there exists a unique \\(\\sigma_i\\) achieving zero gap. Otherwise, the \n\\end{proof}\n\n\\begin{definition}\n   For any $n \\geq 1$, dataset $X   \\{x_1, \\dots, x_n\\} \\subset \\mathbb{R}^{n-1},$ and $C \\geq 0$, define ${X_{+C}   \\{x'_1, \\dots, x'_n\\} \\subset \\mathbb{R}^{n-1}}$ such that for all $i\\neq j$ \n   $$\\lVert x'_i-x_j' \\rVert^2   \\lVert x_i-x_j \\rVert^2 + C.$$\n\\end{definition}\n\n\\begin{lemma}\\label{lem:X+C_exists}\n   Fix any $n \\geq 1$. For all $n$-point datasets $X   \\{x_1, \\dots, x_n\\} \\subset \\mathbb{R}^{n-1}$ and $C \\geq 0$, there exists $X_{+C}   \\{x'_1, \\dots, x'_n\\} \\subset \\mathbb{R}^{n-1}$ such that for all $i\\neq j$, $\\lVert x'_i-x_j' \\rVert^2   \\lVert x_i-x_j \\rVert^2 + C$.\n\\end{lemma}\n\\begin{proof}\n   Let $D$ be the squared inter-point distance matrix of $X$. Thus, the inter-point squared distance matrix of $X_{+C}$ is $D_{+C}   D + C\\cdot (11^T-I_{n})$. By a famous theorem by \\citet{schoenberg1938metric}, $X_{+C}$ is isometrically embeddable in $\\mathbb{R}^{n-1}$ with respect to $\\ell_2$ metric if and only if $\\forall u \\in \\mathbb{R}^n$ with $u^T \\vec{1}   0$, $u^T D_{+C} u \\leq 0$ holds. Indeed,\n   $$u^T D_{+C} u   u^T D u + C\\cdot(u^T\\vec{1})(\\vec{1}^T u) - C \\cdot u^T u   u^T D u - C \\cdot \\lVert u \\rVert^2 \\leq 0,$$\n   where the final inequality uses the fact that $D$ is embeddable.\n\\end{proof}\n\n\\begin{lemma}\\label{lem:tSNE_additive_invariance}\n   Fix any $n > 2$. For all $n$-point datasets $X \\subset \\mathbb{R}^{n-1}$, $\\rho \\in (1, n-1)$, and $C \\geq 0$:\n   $${\\TSNE}_\\rho(X)   {\\TSNE}_\\rho(X_{+C}).$$\n\\end{lemma}\n\\begin{proof}\n   It is sufficient to show that the input affinity matrices for $X$ and $X_{+C}$ are identical. Indeed, for all $i,j \\in [n], i\\neq j$ and for all \\(\\sigma_i > 0\\)\n   \\begin{align*}\n   P_{j|i}(X ; \\sigma_i) & \\frac{\\exp\\left(-\\lVert x_i - x_j \\rVert^2_2 / (2\\sigma_i^2)\\right)}{\\sum_{k \\neq i} \\exp\\left(-\\lVert x_i - x_k \\rVert^2_2 / (2\\sigma_i^2)\\right)} \\\\\n   & \\frac{\\exp\\left(-(\\lVert x_i - x_j \\rVert^2_2+C) / (2\\sigma_i^2)\\right)}{\\sum_{k \\neq i} \\exp\\left(-(\\lVert x_i - x_k \\rVert^2_2+C) / (2\\sigma_i^2)\\right)}   P_{j|i}(X_{+C}; \\sigma_i).\n   \\end{align*}\n   If \\(\\sigma_i   0\\), then \\(P_{j|i}(X)\\) is purely a function of the ordering of the squared interpoint distances, which is unaffected by adding a constant.\n\\end{proof}\n\n\n\n\\begin{lemma}\\label{lem:tSNE_multiplicative_invariance}\n   Fix any $n > 2$. For all $n$-point datasets $X \\subset \\mathbb{R}^{n-1}$, $\\rho \\in (1, n-1)$, and $C > 0$:\n   $${\\TSNE}_\\rho(X)   {\\TSNE}_\\rho(C \\cdot X).$$\n\\end{lemma}\n\n\\begin{proof} \n   First note that for any dataset $X$ and its scaling $C\\cdot X$, and all $\\sigma_i\\geq 0$, we have the following:\n   It suffices to show that the input affinity matrices for $X$ and $C \\cdot X$ are identical. Note that, for all datasets \\(X\\) and \\(\\sigma_i > 0\\):\n   \\iffalse For all \\(\\sigma_i>   0\\), the entropy of \\(P_{\\cdot |i}(X; \\sigma_i )\\) is given by:Let \\(\\sigma_i\\) be any value achieving the perplexity condition, and let \\(H(P_{\\cdot | i})\\) be the corresponding entropy of the conditional affinities. Observe:\n   \\begin{align*}\n   &\\log_2(\\rho) \n   &H_i   \n   &\\sum_{j 1, j\\neq i}^n P_{j|i} \\log_2(1/P_{j|i}) \\\\\n   &\n   -\\sum_{j 1, j\\neq i}^n \\frac{\\exp(-\\lVert x_i-x_j\\rVert^2/(2\\sigma_i^2))}{\\sum_{k1, k\\neq j}^n \\exp(-\\lVert x_i-x_k\\rVert^2/(2\\sigma_i^2))} \\log_2\\left(\\frac{\\exp(-\\lVert x_i-x_j\\rVert^2/(2\\sigma_i^2))}{\\sum_{k1, k\\neq j}^n \\exp(-\\lVert x_i-x_k\\rVert^2/(2\\sigma_i^2))}\\right).\n   \\end{align*}\n   Since \\cite{jeong2024convergence} showed the above uniquely defines $\\sigma_i$ (see Lemma 4.2), it must be the case that for $C\\cdot X$, the optimal neighborhood size for $i$ is $C \\cdot \\sigma_i$. \\fi\n   Therefore for any $j \\in [n]\\setminus \\{i\\}$:\n   \\begin{align*}\n   P_{j|i}(C\\cdot X; C\\cdot \\sigma_i)   \\frac{\\exp(-C^2\\cdot\\lVert x_i-x_j\\rVert^2/(2C^2\\cdot\\sigma_i^2))}{\\sum_{k1, k\\neq j}^n \\exp(-C^2\\cdot\\lVert x_i-x_k\\rVert^2/(2C^2\\cdot\\sigma_i^2))}   P_{j|i}(X; \\sigma_i).\n   \\end{align*}\nLet \\(H(\\cdot)\\) denote the entropy function. By the above, \\({H(P_{\\cdot | i}(X;\\sigma_i))   H(P_{\\cdot | i}(C\\cdot X;C\\cdot \\sigma_i))}\\). Note that \\(H(P_{\\cdot|i}(X; \\sigma_i))\\) is a strictly decreasing function of \\(\\sigma_i > 0\\) as shown in \\citep{jeong2024convergence} Lemma 4.2 (and correspondingly for \\(H(P_{\\cdot|i}(C\\cdot X; \\gamma_i))\\)).\nLet $\\sigma^*_i$ and correspondingly $\\gamma^*_i$ be the (unique, per Lemma \\ref{lem:unique_neighborhood}) neighborhood scalings that satisfy the perplexity condition for \\(X\\) and \\(C\\cdot X\\) respectively (see Section \\ref{sec:prelims}). \nFor any input perplexity value $\\rho \\in (1,n-1)$, let $\\sigma^*_i$ (and correspondingly $\\gamma^*_i)$ be the (unique) neighborhood scalings that minimize the gap between $H(P_{\\cdot | i}(X;\\sigma_i))$ and $\\log \\rho$ (c.f.\\ Section \\ref{sec:prelims}) (correspondingly $H(P_{\\cdot | i}(C\\cdot X;\\gamma_i))$). \nThen \\(\\gamma_i^*   C\\cdot \\sigma_i^*\\).\n\nTherefore \\(P_{\\cdot |i}(C\\cdot X; \\gamma_i^*)   P_{\\cdot |i}(C\\cdot X; C\\cdot \\sigma_i^*) P_{\\cdot |i}(X; \\sigma_i^*) \\), yielding the result. Clearly,\n\\[ \\min | \\log \\rho - H()|\\]\nThat is,\n\\begin{align*}\n\\Sigma^*_i &: \\argmin_{\\sigma > 0} \\Big| \\log \\rho - H(P_{\\cdot | i}(X;\\sigma)_i) \\Big|   \\\\\n\\Gamma^*_i &: \\argmin_{\\gamma > 0} \\Big| \\log \\rho - H(P_{\\cdot | i}(C\\cdot X;\\gamma_i)) \\Big|.   \n\\end{align*}\nWe will show \\(\\Sigma_i^*   \\Gamma_i^*\\)\nFor all \\(\\gamma_i \\in \\Gamma_i^*\\), there exists \\(\\sigma_i \\in \\Sigma_i^*\\) such that \\(\\gamma_i^*   C\\cdot \\sigma_i^*\\). If this were not the case, we would have a contradiction: \nBy \\citep{jeong2024convergence} Lemma 4.2, there exists a unique \\(\\sigma_i^*\\) that determines \\(P_{j|i}(X)\\) and a unique \\(\\gamma_i^*\\) that determines \\(P_{j|i}(C\\cdot X)\\). Therefore \\(\\gamma_i^*   C\\sigma_i\\) and hence \\(P_{\\cdot|i}(C\\cdot X)   P_{\\cdot|i}(X) \\).\n   Let \\(H(\\cdot)\\)By \\citep{jeong2024convergence} Lemma 4.2\n\\end{proof}\n\nUsing the additive and multiplicative invariance of t-SNE, we now prove Lemma \\ref{lem:image_of_tSNE}:\n\n\\todo[inline]{Redo labeling / maybe restatable?}\n\n\\begin{lemma}\n   Fix any $n \\geq 2$ and $\\rho \\in [1, n-1]$. For all $\\epsilon > 0$, Let $S_\\epsilon   \\{ \\mathcal{X}\\{x_1, \\dots, x_n\\} \\subset \\mathbb{R}^{n-1}: \\forall i,j \\in [n], \\lVert x_i - x_j \\rVert^2 \\in [1-\\epsilon, 1+\\epsilon]\\}$, then:\n   $${\\TSNE}_\\rho(n)   {\\TSNE}_\\rho(S_\\epsilon).$$\n\\end{lemma}\n\n\\begin{proof}[\\textbf{Proof of Lemma \\ref{lem:image_of_tSNE}}] \n   Fix any $\\epsilon>0$. It suffices to show that ${\\IMTSNE} \\subseteq {\\TSNE}_\\rho(\\Delta_\\epsilon).$ Fix any $Y \\in {\\IMTSNE}$, there exists a $n$-point dataset $X   \\{x_1,\\ldots,x_n\\} \\subset \\mathbb{R}^{n-1}$ such that:\n   $$Y \\in {\\TSNE}_\\rho(X).$$\n   Using additive and multiplicative invariance, we will manipulate $X$ such that it is in $\\Delta_\\epsilon$ which by Lemma \\ref{lem:tSNE_additive_invariance} and Lemma \\ref{lem:tSNE_multiplicative_invariance} will not change the output. Let $D_{\\max}   \\max_{i\\neq j}\\lVert x_i - x_ j \\rVert^2$ and $D_{\\min}   \\min_{i,j \\in [n], i\\neq j}\\lVert x_i - x_ j \\rVert^2$. WLOG, assume that $D_{\\max} \\neq 0$ otherwise $X_{+1} \\in \\Delta_\\epsilon.$ Set $A   \\frac{1}{2\\epsilon}\\big \\lvert (1-\\epsilon)D_{\\max} - (1+\\epsilon)D_{\\min} \\big \\rvert$   and $B   \\frac{1+\\epsilon}{D_{\\max} + A}.$ Note that since $A \\geq 0$ and $D_{\\max}>0$, $B$ is well defined and strictly greater than 0. Then the dataset $B \\cdot (X_{+A})   \\{x'_1, \\dots, x'_n\\}$ exists by Lemma \\ref{lem:X+C_exists} and is such that:\n   $$\\text{t-SNE}_\\rho(X)   \\text{t-SNE}_\\rho(B \\cdot (X_{+A}))$$\n   by Lemma \\ref{lem:tSNE_additive_invariance} and Lemma \\ref{lem:tSNE_multiplicative_invariance}. Moreover, for all $i \\neq j$:\n   \\begin{align*}\n   \\lVert x'_i - x'_j \\rVert^2 & \\frac{1+\\epsilon}{D_{\\max}+A} \\cdot (\\lVert x_i - x_j \\rVert^2+A) \\leq   1+\\epsilon,\n   \\end{align*}\n   and\n   \\begin{align*}\n   \\lVert x'_i - x'_j \\rVert^2 &\\geq (1+\\epsilon)\\frac{D_{\\min}+A}{D_{\\max}+A}\\\\\n   &\\geq (1+\\epsilon)\\frac{D_{\\min}+\\frac{1}{2\\epsilon}\\big ( (1-\\epsilon)D_{\\max} - (1+\\epsilon)D_{\\min} \\big )}{D_{\\max}+\\frac{1}{2\\epsilon}\\big ( (1-\\epsilon)D_{\\max} - (1+\\epsilon)D_{\\min} \\big )}\\\\\n   &\\geq (1+\\epsilon)\\frac{(1-\\epsilon)(D_{\\max} - D_{\\min})}{(1+\\epsilon)(D_{\\max} - D_{\\min})}   1-\\epsilon,\n   \\end{align*}\n\n   where the second inequality follows because $A \\geq \\frac{1}{2\\epsilon}\\big ( (1-\\epsilon)D_{\\max} - (1+\\epsilon)D_{\\min} \\big ) > -D_{\\max}.$\n\\end{proof}\n\nThe above lemmas give us the following useful corollary that will allow us to prove Theorem \\ref{thm:unclustHammer}, Theorem \\ref{thm:unclustHammerCH}, and Theorem \\ref{thm:unclustHammerDunn}.\n\n\\begin{corollary}\\label{cor:g_exists}\n   Fix any $n > 2$, and $X   \\{x_1, \\dots, x_n\\} \\subset \\mathbb{R}^{n-1}.$ There exists a well-defined, continuous function $g: [0,1] \\to \\mathbb{R}^{n \\times n-1}$ such that:\n   $$C \\mapsto ((1-C)\\cdot X)_{+C},$$\n   and for all $\\rho \\in (1, n-1)$ and $C \\in [0,1):$\n   $${\\TSNE}_\\rho(X)   {\\TSNE}_\\rho(g(C)).$$\n   Moreover, $g$ is such that for all $i \\neq j, l \\neq m \\in [n]:$\n\n   $$\\lVert x_i - x_j \\rVert \\leq \\lVert x_l - x_k \\rVert \\implies \\lVert g(C)_i - g(C)_j \\rVert \\leq \\lVert g(C)_l - g(C)_k \\rVert$$\n\\end{corollary}\n\n\\begin{proof}\n   $g$ is well defined by Lemma \\ref{lem:X+C_exists} and WLOG continuous since it continuously transforms the distances in $X$:\n   $$\\forall i,j \\in [n], i\\neq j, \\hspace{1cm} \\lVert g(C)_i - g(C)_j \\rVert   \\sqrt{(1-C) \\cdot \\lVert x_i - x_j \\rVert^2 + C}.$$\n   Moreover, by Lemmas \\ref{lem:tSNE_additive_invariance} and \\ref{lem:tSNE_multiplicative_invariance}, for all $C \\in [0,1):$\n   $${\\TSNE}_\\rho(X)   {\\TSNE}_\\rho(g(C)).$$\n\\end{proof}\n\nUsing the above lemmas, Theorem \\ref{thm:unclustHammer}, Corollary \\ref{cor:twoCluster_n_Unclustered}, and Theorem \\ref{thm:perturbhammer} are proven.\n\n\\UnclustHammer*\n\n\\begin{proof}[\\textbf{Proof of Theorem \\ref{thm:unclustHammer}}]\n   Let $g$ be the function from Corollary \\ref{cor:g_exists}, and $f(C)   \\bar{\\mathcal{S}}(g(C); C_{m \\in [k]}).$ Note that $f$ is continuous for $C \\in [0,1]$ since $g$ is continuous, and $\\bar{\\mathcal{S}}(\\ \\cdot\\ ; C_{m \\in [k]})$ is continuous whenever for all $i\\in[n]$, $a(i), b(i) \\neq 0$ which follows from $\\bar{\\mathcal{S}}(X ; C_{m \\in [k]})$ being well-defined and the definition of $g$. Therefore, the image of $f$ on $[0,1)$ contains the interval $(f(1), f(0)]   (0, \\bar{\\mathcal{S}}(X ; C_{m \\in [k]})]$ (or if $\\bar{\\mathcal{S}}(X ; C_{m \\in [k]}) \\leq 0,$ $[\\bar{\\mathcal{S}}(X ; C_{m \\in [k]}), 0)$). Thus, for all $\\epsilon \\in (0,1]$, there exists $C \\in [0,1)$ such that $X_\\epsilon   g(C)$ satisfies the hypothesis.\n\\end{proof}\n\nNow we can prove Corollary \\ref{cor:twoCluster_n_Unclustered}, Corollary \\ref{cor:twoCluster_n_UnclusteredCH}, and Corollary \\ref{cor:twoCluster_n_UnclusteredDunn} simultaneously: \n\n\\iffalse\n\\todo[inline]{fix numbering of this corollary.}\n\\TwoClusterNUnclustered*\n\\fi\n\n\\begin{proof}[\\textbf{Proof of Corollaries \\ref{cor:twoCluster_n_Unclustered}, \\ref{cor:twoCluster_n_UnclusteredCH}, and \\ref{cor:twoCluster_n_UnclusteredDunn}}]\n   The proof proceeds by showing a dataset and its output who have an average silhouette score of 1, Calinski-Harabasz index of $\\infty$, and Dunn index of $\\infty$, and then applies Theorem \\ref{thm:unclustHammer}, Theorem \\ref{thm:unclustHammerCH}, and Theorem \\ref{cor:twoCluster_n_UnclusteredDunn} respectively. WLOG fix partition $C_1 \\sqcup C_2   [n]$ with $C_1[1,n/2]$ and $C_2   [n/2+1,n]$. Consider the $n$-point dataset, $\\mathcal{X}\\{x_1, \\dots, x_n\\} \\subseteq \\mathbb{R}^{n-1}$, such that for all $i \\in C_1$, $x_i   \\vec{0}$, and for all $i \\in C_2$, $x_i   \\vec{e_1}$.\n   \n   Routine calculations show that the conditional input affinities are:\n\n   $$P_{i|j}   \\begin{cases} \n   \\frac{1}{\\frac{n}{2}-1+\\frac{n}{2}\\exp\\left(-\\frac{1}{2\\sigma^2_j} \\right)} & i \\in C(j), i \\neq j \\\\\n   \\frac{\\exp\\left(-\\frac{1}{2\\sigma^2_j} \\right)}{\\frac{n}{2}-1+\\frac{n}{2}\\exp\\left(-\\frac{1}{2\\sigma^2_j} \\right)} & i \\not\\in C(j)\\\\\n   0 & i   j.\n   \\end{cases}$$\n\n   By symmetry, $\\sigma_j   \\sigma_i$ for all $i,j \\in [n].$ Hence, let $\\sigma$ be the neighborhood size for all $j \\in [n]$ which is non-zero and well defined for $\\rho \\in [1, n-1].$ Thus the symmetrized input affinities are:\n\n   $$P_{ij}   \\begin{cases} \n   \\frac{1}{\\frac{n^2}{2}-n+\\frac{n^2}{2}\\exp\\left(-\\frac{1}{2\\sigma^2} \\right)} & i \\in C(j), i\\neq j \\\\\n   \\frac{\\exp\\left(-\\frac{1}{2\\sigma^2}\\right)}{\\frac{n^2}{2}-n+\\frac{n^2}{2}\\exp\\left(-\\frac{1}{2\\sigma^2} \\right)} & i \\in C_1, j \\in C_2 \\\\\n   0 & i   j.\n   \\end{cases}$$\n\n   Any set $\\mathcal{Y}   \\{ y_1, \\dots, y_n\\} \\subseteq \\mathbb{R}$ is a global minimizer if $P_{ij}   Q_{ij}$ for all $i,j \\in [n]$. In this case, this is achieved if $y_{i \\in C_1}   0$ and $y_{i \\in C_2}   \\sqrt{\\exp\\left( \\frac{1}{2\\sigma^2}\\right) - 1}.$ Furthermore, since $\\mathcal{Y}$ can be isometrically embedded in $\\mathbb{R}^d$ for all $d \\geq 1$, this result holds for t-SNE embeddings of all dimensions. \n\n   To finish the proof note that for all $i \\in [n]$, $a(i)0$ when defined with respect to $\\mathcal{Y}$ and partition $C_1 \\sqcup C_2.$\n\\end{proof}\n\n\\Perturbhammer*\n\\begin{proof}[\\textbf{Proof of Theorem \\ref{thm:perturbhammer}}]\n   The proof is immediate by application of Lemma \\ref{lem:image_of_tSNE}.   \n\\end{proof}\n\n\\newpage\n\n\\subsection{Impostor dataset construction}\n\\label{app:impostor_construction}\n\nThe construction of an impostor dataset based on an input dataset is done as follows.\n\n\\begin{algorithm}[H]\n\\caption{Impostor Dataset Creation}\n\\label{alg:impostor}\n\\begin{algorithmic}[1]\n\\Require Dataset $X   \\{x_1, \\ldots, x_n\\}$ with at least two distinct points, and tolerance $\\epsilon > 0$\n\\Return Dataset $X_\\epsilon   \\{x_1', \\ldots, x_n'\\}$ such that $\\mathrm{t\\text{-}SNE}(X_\\epsilon)   \\mathrm{t\\text{-}SNE}(X)$ and aspect ratio$(X_\\epsilon) < 1+\\epsilon$\n\\State Construct squared interpoint distance matrix of $X$, denote it by $D$\n\\State Form $D' \\gets \\frac{\\epsilon}{\\max_{i,j} D_{ij}} \\cdot D + (11^\\top - I_n)$\n\\State Run classical multidimensional scaling on $D'$ to obtain its Euclidean embedding $$X_\\epsilon   \\{x_1', \\ldots, x_n'\\} \\subset \\mathbb{R}^{n-1}.$$\n\\State \\Return $X_\\epsilon$\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\\newpage\n\n\\section{Appendix: Misrepresentation of Outliers}\\label{sec:appendix_outliers}\n\n\n\\subsection{Additional Experiments}\\label{sec:appendix_outliers_experiment}\nWe provide a comparison of t-SNE and PCA on the BBC news dataset. For ease of presentation, we take a three-cluster, \\((n   1204)\\)-size subset (business, sports, tech) and we analyze what happens under injection of $120$ poison points versus $120$ far outliers. \n\n\\begin{figure}[h!]\n   \\centering\n   \\includegraphics[width\\linewidth]{images/appendix_figs/bbc_appendix.png}\n   \\caption{\\small t-SNE vs.\\ PCA on poison points versus outlier points on a three-cluster subset of the BBC news dataset. The label on the bottom left is silhouette score of the plot (sans injected points) with respect to their ground-truth labels.}\n   \\label{fig:bbc_appendix}\n\\end{figure}\n\n\nIn both Figure \\ref{fig:outliers_real_world} and Figure \\ref{fig:bbc_appendix}, poison points are picked as follows: first we run a $k$-means algorithm on the original dataset; then, for each poison point, we pick one of the these means and $10$ random points of the dataset, and we average these two quantities (the idea is to connect the points in a way that contradicts the ground-truth three-clustering). We found \\(k2\\) worked well. We pick outlier points as normal vectors centered at the mean of the dataset with variance \\(32\\) (the diameter of the original dataset is roughly \\(1.5\\)).\n\n\\subsection{Proofs}\n\n\\begin{lemma}\\label{lem:Qsum_bound} Fix $n\\geq 2$ and \\(Y   \\{y_0,\\ldots,y_{n-1}\\} \\subset \\mathbb{R}^d\\). Let ${\\beta : \\diam(Y\\setminus \\{y_0\\})}$ and ${\\gamma : \\min_{j\\in [n]} \\|y_0-y_j\\|}$. Then\n\\begin{align*}\n   \\sum_{i1}^n Q_{0i} &\\leq \\frac{1}{2 + (n-2)\\cdot \\frac{1+\\gamma^2}{1+\\beta^2}} . \\\\\n   & \\frac{1 +   D_Y^2}{2(1+D_Y^2 ) + (n-1)(1+\\alpha^2 D_Y^2)} \n\\end{align*}\n\\end{lemma}\n\\begin{proof}\n   Let \\(Z_0   \\sum_{i1}^n \\frac{1}{1+\\|y_i-y_0\\|^2}\\) and \\(Z_{1:n}   \\sum_{i, j: i\\neq j}   \\frac{1}{1+\\|y_i-y_j\\|^2}\\). Then\n   \\[\\sum_{i1}^n Q_{0i}   \\frac{Z_0}{2Z_0 + Z_{1:n}}   \\frac{1}{2 + Z_{1:n}/Z_0}.\\]\nNow observe that\n\\begin{align*}\n   \\frac{Z_{1:n}}{Z_0} & \\frac{\\sum_{i,j:i\\neq j} (1 + \\|y_i-y_j\\|^2)^{-1}}{\\sum_{j1}^n (1 + \\|y_0 -y_j\\|^2)^{-1}}   \\\\\n &\\geq \\frac{(n-1)(n-2)(1+\\max_{i,j \\in[n]} \\|y_i-y_j\\|^2)^{-1}}{(n-1)(1+\\min_{j \\in[n]} \\|y_0-y_j\\|^2)^{-1}}   \\\\\n   & \\frac{(n-2)(1+\\gamma^2)}{1+\\beta^2}.   \\\\\n   & \\frac{(n^2-n)(1+\\beta^2)^{-1}}{n(1+\\alpha^2)^{-1}}   \\\\\n   & (n-1)\\cdot \\frac{1 + \\alpha^2}{1 + \\beta^2} .\n   &\\geq \\frac{(n^2-n)(1+ D_Y^2)}{n( 1 + \\alpha^2 D_Y^2)}\\geq (n-1)/\\alpha^2 \n\\end{align*}\nNote that \\(\\min_{j\\in [n]}\\|y_0-y_j\\|^2   \\geq \\alpha\\max\\{\\beta,1\\}\\).\n{\\color{red} Under new definition (margin-based) of $\\alpha$-outlier, it is not obvious that $\\alpha^2   \\min_{j \\in [n]} \\lVert y_0 - y_j \\rVert^2$. Maybe do a case by case thing on whether $\\diam > 1$. Also doesn't this margin definition add a factor $2$ somewhere to the inequality?}\nPlugging this back into the previous equation gives the statement. \n\\end{proof}\n\n\\textcolor{red}{UNDER CONSTRUCTION}\n\n\\begin{lemma}\\label{lem:r_split}\n   Fix \\(n\\geq 2\\) and \\(Y   \\{y_0,y_1,...,y_{n-1}\\} \\subset \\mathbb{R}^d\\). If \\(Y\\) is a \\((\\alpha, y_0)\\)-outlier configuration such that $\\alpha   \\alpha(Y)$, then there exists $v \\in \\mathbb{R}^d$ such that for all $i \\in [n]$:\n   \\[\\|y_i-y_0\\| \\cdot \\frac{ \\alpha}{\\sqrt{1+\\alpha^2}} \\leq (y_i - y_0)\\cdot v \\leq \\|y_i-y_0\\|.\\]\n\\end{lemma}\n\n\\begin{proof}\n   Fix $i \\in [n]$, let ${\\beta : \\diam(Y\\setminus \\{y_0\\})}$, and WLOG let $y_0   0.$ Take $v$ as in Definition \\ref{def:alpha_outlier}. Then by Cauchy-Schwarz, $(y_i-y_0) \\cdot v \\leq \\| y_i-y_0 \\|.$ To prove the other side of the inequality, we only need to lower bound the cosine of the angle between $y_i-y_0$ and $v$:\n   $$(y_i - y_0)\\cdot v   \\| y_i - y_0 \\| \\cdot \\cos(\\angle({v, y_i})).$$\n   Since $v$ is the maximum-margin hyperplane between $y_00$ and $Y\\setminus \\{y_0\\}$, it holds that $u   v \\cdot (\\alpha \\max\\{1, \\beta\\})$ is in the convex hull of $Y\\setminus \\{y_0\\}.$ Indeed, $\\| u \\|   \\inf_{y \\in \\conv(Y\\setminus \\{y_0\\})} \\| y \\|.$ Thus, we know that the closed ball $\\overline{B_\\beta (u)}$ contains $\\conv(Y\\setminus\\{y_0\\}).$ Therefore, there exists $t \\in \\mathbb{R}^d$ such that $\\| t \\| \\leq \\beta, u + t   y_i,$ and $u\\cdot t \\geq 0$. Hence\n   $$\\cos(\\angle(v, y_i))   \\frac{v \\cdot y_i}{\\lVert y_i \\rVert}   \\frac{v\\cdot(u+t)}{\\sqrt{\\lVert u \\rVert^2 + \\lVert t\\rVert^2 -2u \\cdot t}} \\geq \\frac{\\alpha \\max(\\beta, 1)}{\\sqrt{\\alpha^2 \\max(\\beta, 1)^2 + \\beta^2}} \\geq \\frac{\\alpha}{\\sqrt{1+\\alpha^2}},$$\n   completing the proof.\n\\end{proof}\n\n\n\\OutlierAbs*\n\\begin{proof}\nFix $Y \\in \\IMTSNE$ and define \\(\\gamma   \\min_i \\|y_i-y_0\\|\\). WLOG, let \\(y_0\\) be the outlier point and assume $\\gamma > 0$ otherwise the hypothesis goes through trivially.\n   Since \\(Y\\) is stationary, \\(\\frac{\\partial L}{\\partial y_0}   0\\). Pick \\(v\\) as in Lemma \\ref{lem:r_split} and observe:\n   \\begin{align*}\n   0   \\frac{\\partial \\mathcal{L}}{\\partial y_0}\\cdot v & \\sum_{i1}^{n-1} \\frac{(P_{i0} - Q_{i0})(y_0-y_i)\\cdot v}{1 + \\|y_0-y_i\\|^2} \\\\\n   &\\geq \\frac{\\alpha}{\\sqrt{1 + \\alpha^2}}\\sum_{i1}^{n-1}   P_{i0} \\frac{\\|y_0-y_i\\|}{1 + \\|y_0-y_i\\|^2}- \\sum_{i1}^{n-1} Q_{i0} \\frac{\\|y_0-y_i\\|}{1 + \\|y_0-y_i\\|^2}\\\\\n   &\\geq \\frac{\\alpha}{\\sqrt{1 + \\alpha^2}}\\sum_{i1}^{n-1}   P_{i0} \\frac{\\|y_0-y_i\\|}{1 + \\|y_0-y_i\\|^2}- \\sum_{i1}^{n-1} Q_{i0} \\frac{\\|y_0-y_i\\|}{1 + \\|y_0-y_i\\|^2}\\\\\n   &\\geq \\frac{\\alpha}{\\sqrt{\\alpha^2 + 1}}   \\frac{\\gamma}{1+(\\gamma+\\beta)^2}\\sum_{i1}^{n-1} P_{i0}\n   - \\frac{\\gamma + \\beta}{1 + \\gamma^2}\\sum_{i1}^{n-1} Q_{i0} \\\\\n   &\\geq   \\frac{\\alpha}{\\sqrt{1 + \\alpha^2}}\\frac{\\gamma}{1+(\\gamma+\\beta)^2}\\frac{1 + \\sum_{i1}^{n-1} P_{0|i}}{2n}\n   - \\frac{\\gamma + \\beta}{1 + \\gamma^2}\\frac{1}{2 + (n-2)\\frac{1+\\gamma^2}{1 + \\beta^2}}\n   \\end{align*}\nwhere, in the third line, we use Lemma \\ref{lem:Qsum_bound} and the fact that \\(\\sum_{i1}^n P_{i|0}   1\\). Multiplying by $\\frac{1+\\gamma^2}{\\gamma+\\beta}\\cdot \\frac{2n}{1 + \\sum_{i1}^{n-1} P_{0|i}} > 0$ and rearranging, we get that:\n   \nIf the above expression is positive, this yields a contradiction with the stationary condition \\(\\partial \\mathcal{L} / \\partial y_0   0\\). Therefore, the above expression must be non-positive. This implies We can manipulate the expression as follows:\n\n\n\\begin{align*}\n   \\frac{\\alpha}{\\sqrt{1 + \\alpha^2}} \\cdot \\frac{1+\\gamma^2}{\\gamma+\\beta} \\cdot \\frac{\\gamma}{1+(\\gamma+\\beta)^2} \n   &\\leq \\frac{1}{2 + (n-2)\\cdot \\frac{1+\\gamma^2}{1+\\beta^2}}\\cdot \\frac{2n}{1 + \\sum_{i1}^{n-1} P_{0|i}} \\\\\n   &\\leq \\frac{1+\\beta^2}{(n-2)(1+\\gamma^2)}\\cdot \\frac{2n}{1 + \\sum_{i1}^{n-1} P_{0|i}} \\\\\n   & \\frac{1+\\beta^2}{1+\\gamma^2}\\cdot \\Big(1 + \\frac{2}{n-2}\\Big)\\cdot \\frac{2}{1+\\sum_{i1}^{n-1} P_{0|i}}.\n\\end{align*}\nRecall, by definition of \\(\\alpha\\)-outlier configuration, that $\\gamma \\geq \\alpha \\cdot \\max\\{\\beta,1\\}$. Rearranging, we have:\n\\begin{align*}\n   \\Big(1 + \\frac{2}{n-2}\\Big)\\cdot \\frac{2}{1+\\sum_{i1}^{n-1} P_{0|i}} &\\geq \\frac{\\alpha}{\\sqrt{1 + \\alpha^2}} \\cdot \\frac{\\gamma}{\\gamma+\\beta} \\cdot \\frac{1+\\gamma^2}{1+(\\gamma+\\beta)^2} \\cdot \\frac{1+\\gamma^2}{1+\\beta^2} \\\\ \n   &\\geq \\frac{\\alpha}{\\sqrt{1 + \\alpha^2 }}\\frac{\\gamma^3}{(\\gamma+\\beta)^3}\\frac{1 + \\gamma^2}{1+\\beta^2} \\\\ \n   &\\geq \\frac{\\alpha}{\\sqrt{1 + \\alpha^2 }}\\frac{\\alpha^3\\max\\{\\beta,1\\}^3}{(\\alpha\\max\\{\\beta,1\\}+\\beta)^3}\\frac{1 + \\alpha^2\\max\\{\\beta,1\\}^2}{1+\\beta^2} \\\\\n   &\\geq \\frac{\\alpha}{\\sqrt{1 + \\alpha^2 }}\\frac{\\alpha^3}{(\\alpha+\\frac{\\beta}{\\max\\{\\beta,1\\}})^3}\\frac{1 + \\alpha^2}{2} \\\\\n   &\\geq \\frac{\\alpha}{\\sqrt{1 + \\alpha^2 }}\\frac{\\alpha^3}{(1+\\alpha)^3}\\frac{1 + \\alpha^2}{2}\\\\\n   & \\frac{\\alpha^4\\sqrt{1 + \\alpha^2 }}{2(1+\\alpha)^3}.\n\\end{align*}\n\nAssume $\\alpha \\geq 3$ (or else the hypothesis holds trivially), then the above is lower-bounded by ${(\\alpha^2 - 1)/4}$. Solving for $\\alpha$, we find \\(\\alpha \\leq \\sqrt{1 + \\Big(1 + \\frac{2}{n-2}\\Big)\\cdot \\Big( \\frac{8}{1+\\sum_{i1}^{n-1} P_{0|i}}\\Big) }\\).\n\n\\end{proof}",
  "title": "t-SNE exaggerates clusters, provably"
}
