{
  "authors": [
    "Noah Bergam",
    "Szymon Snoeck",
    "Nakul Verma"
  ],
  "date_published": "2025-10-09",
  "raw_tex": "Our \\sout{mathematical} \nstudy of t-SNE has established in considerable generality that one cannot infer the \\textit{degree} of cluster structure or the \\textit{extremity} of outliers from a t-SNE plot, see Theorems \\ref{thm:unclustHammer}, \\ref{thm:perturbhammer}, and \\ref{thm:outlier_abs}. The proofs and intuitions behind these statements guided us to the surprising empirical observation that one cannot even infer the \\textit{existence} of clusters or outliers. In particular, the injection of a small subset of adversarially chosen points can largely mask the cluster structure, while sizable injections of outlier points are masked within the cluster structure, \nsee Figures \\ref{fig:one_pt_perturb}, \\ref{fig:outliers1}, \\ref{fig:outliers_real_world}, and \\ref{fig:bbc_appendix}. Further work should seek to formalize these latter set of empirical observations. C\n\nThese behaviors are more pronounced as data becomes more high-dimensional: the damage done by an adversarially chosen poison point is largest on high-aspect-ratio data, while more faraway outliers can fly under the radar in the high-dimensional regime.   t-SNE on high-dimensional data is susceptible to certain adversarial attacks and \n\nWe have identified two properties of t-SNE that give rise to these idiosyncratic behaviors: (1) additive invariance with respect to the squared interpoint distances, and (2) the asymmetry between the input and output affinity matrices.   (in that the former probes nearest-neighbors and the latter probes radius-neighbors)\n While we have uncovered significant false positive failure modes that arise from these properties, we cannot completely rule out their utility. Additive invariance, while brittle under certain adversarial perturbations, may be robust to certain random perturbations. Indeed, adding random noise to a dataset is approximately equivalent to adding a constant to the interpoint distances due to concentration of measure. Additive invariance effectively allows t-SNE to ignore such noise, see Figure \\ref{fig:higher_dim_tighter_clusters}. This phenomenon is worthy of further study.\n \n while additive invariance is susceptible to adversarial perturbations, it is perhaps very robust to random perurbation\n \n additive invariance has desirable denoising properties, considering the concentration effects of high-dimensional noise (see Figure \\ref{fig:higher_dim_tighter_clusters}).   Indeed, Gaussian noise \n\nThe holy grail of d\nThe gold standard result for data visualization is a guarantee that clustered output implies clustered input in a suitable sense.\n\n\n\nt-SNE belongs to a larger collection of practical data visualization techniques \\citep{mcinnes2018umap, jacomy2014forceatlas2, tang2016visualizing, amid2019trimap}. Beyond characterizing the intrinsic strengths and limitations of t-SNE, it would be instructive to taxonomize this zoo of techniques.\n\nt-SNE belongs to a wide selection of recently developed \ndata visualization techniques that are yet to be understood fully \\citep{mcinnes2018umap, jacomy2014forceatlas2, tang2016visualizing, amid2019trimap}. Our hope is that this work inspires the reader to explore this fascinating landscape further and pursue the essential question: what can be provably deduced from a visualization?\n\nOur hope is that this work inspires the reader to explore this fertile frontier further and quest after the quintessential question: what can be provably deduced from a visualization?\n\nfor future work is to systematically chart this landscape. to explore this landscape further.\n\nwhose false positives remain largely unstudied, both in theory and practice.\n\nwe would like to provide a taxonomy to understand this zoo of modern techniques.\n\n Our broader vision is to taxonomize the array \n \n understand the intrinsic strengths and limitations of \n\ns compared to other methods;\u2013\u2013\u2013the gold standard here is separation, not just non sequitir performance guarantee\n \n taxonomize of other data visualization techniques. Can \n\n dissect the favorable and unfavorable qualities\n\n\nt-SNE is one of many visualization methods that, in practice, rely purely on a gradient-based optimization of the output points. Related methods in this regard include UMAP \\citep{mcinnes2018umap}, ForceAtlas \\citep{jacomy2014forceatlas2}, LargeVis \\citep{tang2016visualizing}, and TriMap \\citep{amid2019trimap}. This is in contrast to an earlier generations of data visualization methods, from PCA and classical MDS   to \\citet{tenenbaum2000global} and Laplacian Eigenmaps \\citet{belkin2003laplacian}, which usually reduce to eigenvalue problems). Future work should focus on understanding this landscape of ``force-based'' visualization methods and their seemingly unreasonable effectiveness.\n\n \nIt is understood on some heuristic level, for instance, that the affinity matrix asymmetry helps alleviate the ``crowding problem.'' Likewise, \n\nLikewise, there is reason to believe that additive invariance has \n\nThe grand vision is an axiomatic approach to data visualization, analogous to impossibility theorem for data visualization. Can we rigorously define certain desiderate of a data visualization method a la Arora\n\n\nCan the benefits of additive invariance be rigorously understood from a denoising perspective?   we leave it open to future work to quantify the strengths. \n\nFORMALIZE POISON POINTS\n\nIt would be instructive to extend our false positive and outlier type studies to other data visualization methods. \n\n\\textcolor{red}{why the glitch?} \nOne of the distinctive features of t-SNE that arose in our analysis is the property of additive invariance. While we have explored how this property relates to the exaggeration of cluster structure and the sensitivity of t-SNE to poison points, there is reason to believe that additive invariance is beneficial in the processing of high-dimensional, simplex-like data. Additive invariance data processing removes this high-dimensional signature and zeros in on the interrelations between interpoint distances. It is worth noting that UMAP \\citep{mcinnes2018umap} have a feature similar (but not quite identical) to t-SNE's additive invariance, involving the subtraction of the minimum interpoint distance.   This is because high-dimensional data tends to be close to a simplex. This is a well-known and easily observed principle in practice, see Figure \\ref{fig:NLP_simplex}, as well as in theory, from the perspective of concentration of measure. It is possible that additive-invariance is key to designing good data visualization algorithms. Indeed, it is strikingly prevalent among popular visualization algorithms such as t-SNE, UMAP, LargeVis, Laplacian Eigenmaps, and Diffusion Maps. \n\\todo[inline]{Add discussion of: $\\E_{x'\\sim N(x,I_d), y'\\sim N(y,I_d)}[\\lVert x' - y' \\rVert_2^2]   \\lVert x - y \\rVert_2^2 + C $ phenomenon.}\n\n\\todo[inline]{Include in one form or another in discussion:\n\\begin{theorem}\\label{thm:additiveInvariance}\n   Laplacian Eigenmaps, LargeVis, SNE, t-SNE, and diffusion maps with Gaussian kernel all are invariant under additive scaling.\n\\end{theorem}}\n\nIndeed, this property may be useful for other endeavors involving high-dimensional data such as measuring cluster significance. Commonly used measures of cluster significance, such as Silhouette Score, rely on comparing the gap between inter-clusters and intra-cluster distances ``multiplicatively'' (i.e. comparing these distances as a ratio). Therefore, high-dimensional datasets which tend to have close to 1 aspect ratios will not present as highly clustered to these measures even though the data may have a far more statistically significant clustering than similar datasets in low dimensions. Our hope is that comparing inter- and intra-cluster distances ``additively'' would overcome this issue and provide a computationally-friendly way to measure cluster significance that is more aligned with the nature of high-dimensional data.\n\n[additive invariance paragraph?]\n\n\n\nt-SNE is one of many visualization methods that, in practice, rely purely on a gradient-based optimization of the output points. Related methods in this regard include UMAP \\citep{mcinnes2018umap}, ForceAtlas \\citep{jacomy2014forceatlas2}, LargeVis \\citep{tang2016visualizing}, and TriMap \\citep{amid2019trimap}. This is in contrast to an earlier generations of data visualization methods, from PCA and classical MDS   to \\citet{tenenbaum2000global} and Laplacian Eigenmaps \\citet{belkin2003laplacian}, which usually reduce to eigenvalue problems). Future work should focus on understanding this landscape of ``force-based'' visualization methods and their seemingly unreasonable effectiveness.\n\n\n\\textcolor{red}{i want a sentence name-dropping all the ``force-based'' methods- trimap \\citep{amid2019trimap}- umap \\citep{mcinnes2018umap}- forceatlas \\citep{jacomy2014forceatlas2}- largevis \\citep{tang2016visualizing}}\n\n\n More generally, what information can be deduced about the input given a visualization? ",
  "title": "t-SNE exaggerates clusters, provably"
}
