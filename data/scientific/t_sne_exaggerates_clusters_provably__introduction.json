{
  "authors": [
    "Noah Bergam",
    "Szymon Snoeck",
    "Nakul Verma"
  ],
  "date_published": "2025-10-09",
  "raw_tex": "\n\n\nt-SNE and related data visualization methods have become staples in modern exploratory data analysis. They just seem to work: practitioners find that these techniques effortlessly tease out interesting cluster structures in datasets. Consequently they are now used ubiquitously in a wide array of fields, ranging from single-cell genomics to language model interpretability \\citep{kobak2019art, petukhova2025text}. The practical success of these techniques has naturally piqued some interest in the theoretical computer science community as well. \\citet{arora2018analysis} and \\citet{linderman2019clustering} for instance show that t-SNE does indeed have the capability to reveal the latent clusters present in the input data. For UMAP XYZ shows blah. thus corroborating? the impirics.\n\nWhile such studies answer the true positive discovery question, \nthe false postive discovery question remains un.\n\na are crucial in understanding how t-SNE and UMAP works, \n\n\nthey visualize the right clusters on many labeled dataset (say, MNIST) and they produce convincing clusters on many unlabeled datasets (say, single-cell genomics readings). As a result of their appealing behavior and relatively efficient implementations, these methods have gained a substantial user base throughout the sciences, and particularly in molecular biology \\citep{kobak2019art}, as tools for hypothesis generation and exploratory data analysis. \n\nThere is some theoretical evidence that t-SNE \n\n\\textcolor{red}{should glorify more, include positive theory results, THEN go negative, talk about our goal of stress-testing, ``there are many good questions to still ask.'' }\n\nOn the other hand, there is much to question about these methods. For one, they are both produced via gradient-based optimization of (reasonable, yet) complicated objective functions. It is extremely unlikely that t-SNE or UMAP outputs actually constitute globally optimal solutions to their respective optimization problems\\footnote{In fact, considering that standard t-SNE and UMAP implementations typically run for a fixed number of steps rather than until convergence, we don't even know if most outputs are local minima!}. Even if we did have access to optimal outputs, it is not at all obvious what one can definitely conclude from them. it is not at all obvious that the solution to this objective function should give us something \nThese visualizations are simply not as ``interpretable'' as other unsupervised learning methods, such as, say, a principal component analysis (PCA) projection, where we can exactly quantify the amount of variance preserved in the picture, or a hierarchical clustering, where we can compare (both quantitatively and visually) the persistence of clustering patterns at different scales. \n\nThese are just some of the concerns that motivate a more careful theoretical analysis of t-SNE and UMAP, aimed at identifying performance guarantees or a lack thereof.\n\n\n\nData visualization is a controversial enterprise. On one hand, there is so much demand to \\textit{see} data. This is evidenced by the sheer popularity of t-distributed stochastic neighbor embedding (t-SNE) and uniform manifold approximation (UMAP), which have become standard exploratory analysis tools in various fields, but particularly single-cell biology \\citep{kobak2019art}. are widely used for visualizing cluster structure in high-dimensional data, These methods, and the enterprise they represent,rank among the most widely-used yet under-studied data analysis methods. It makes sense: in exploratory data analysis, there is a distinct desire to \\textit{see} the data.\n These methods have received relatively little theoretical attention. These methods, in a word, are non-linear embedding techniques, typically   These methods produce non-linear metric embeddings into two or three dimensional space by optimizing\nThese methods are essentially non-linear metric embeddings which arise as the solution to non-convex optimization problems. As such, they are difficult to analyze mathematically and there is relatively little theoretical understanding of each method, especially as compared to other unsupervised learning models like spectral clustering and \\(k\\)-means. \n\nExisting analysis of t-SNE\n(to the author's knowledge, UMAP has received essentially no such attention) \n has established that, given high-dimensional data with spherical, well-separated cluster structure, t-SNE outputs a visualization which preserves that cluster structure   \\citep{arora2018analysis,linderman2019clustering}. In other words, t-SNE is provably good at generating \\textit{true positives} in its visualization of clusters. Curiously, t-SNE's susceptibility to generate \\textit{false positives}, i.e.\\ fabricated clusters in the output visualization, has remained largely unstudied. One should note that this is not a purely academic curiosity, since the interpretation of t-SNE outputs have important consequences downstream in the sciences, influencing hypothesis generation, experimental design, and scientific conclusions.   and which experiments to run next. Our results show that one should avoid taking t-SNE clusterings at face value. \\textbf{There is a unique danger to false positives.}\n\n\nAs an illustration of the danger of false positives, consider the 2D t-SNE visualization of a $100$-point dataset residing in \\(\\mathbb{R}^{100}\\) (depicted on the right). \n\n\\begin{wrapfigure}{r}{0.4\\textwidth}\n\\begin{figure}[h!]\n   \\centering\n   \n   \\vspace{-0.15in}\n   \\includegraphics[width0.35\\textwidth]\n   {images/tsne_plot.png}\n   \\vspace{-0.2in}\\includegraphics[width0.4\\linewidth]{images/umap_plot.png}\n   \\includegraphics[width0.3\\linewidth]{images/umap_demo.png} \\label{fig:tsneumap_demo}\n\\end{figure}\n\\end{wrapfigure}\n\nThe following are true:\n\\begin{itemize}\n   \\item \\(k\\)-means on the original dataset produces a decidedly poor clustering by standard metrics, \n   \\item \\(k\\)-means on the t-SNE and UMAP projections produce the same partition of points. \n\\end{itemize}\n\n\nBased on this plot, it is tempting to conclude that the input dataset obviously contains two distinct clusters. In this case, one understandably, any practitioner \nwould likely design their subsequent data analysis workflow guided by these two salient clusters. However a closer examination of the original (high-dimensional) dataset reveals that the situation perhaps may not be as clear-cut. By standard cluster saliency metrics, for instance, the input dataset appears quite poorly clustered according to the partition that t-SNE so strongly suggests, see Table \\ref{table:demo}.\n\nthat perhaps the 100-point dataset contains two distinct clusters. In this case, understandably, our unsuspecting naive data analyst would design their subsequent data analysis workflow guided by the two salient clusters they see. But alas even a basic cluster saliency test of the original (high-dimensional) dataset reveals that these clusters are spurious.\n\n\n\\begin{table}[h!]\\label{table:demo}\n\\centering\n\\caption{\\small Clustering scores (with respect to $k$-means) according to various popular cluster saliency metrics. The range in the first column specifies the possible values that can be attained. A higher value indicates data being highly clustered.   }\nThe direction indicator in the first column denotes what score values (higher or lower) imply data being highly clustered and the range indicates the possible values given the cluster partition is optimal.}\n\\vspace{0.3cm}\n\\begin{tabular}{lccc}\n\\hline\n\\textbf{Cluster Score (range)} & \\textbf{t-SNE (2D)} &   \\textbf{Original Data (100D)} \\\\\n\\hline\nSilhouette $ [-1,1]$ & .918 & .006 \\\\\nCalinski-Harabasz $ [0,\\infty]$   & 5590 .40\n& 1.61 \\\\\nDunn Index $[0,\\infty]$   & 3.65 & .998 \\\\\n Davies\u2013Bouldin   $\\downarrow (0,\\infty)$   & 0.1188 & 7.79 \\\\\n\\hline\n\\vspace{0.05cm}\n\\end{tabular}\n\\end{table}\n\n\\begin{wrapfigure}{r}{0.5\\textwidth}\n\\begin{figure}[h!]\n   \\centering\n   \\vspace{-0.1in}\n   \\includegraphics[width0.45\\textwidth]{images/demo/DEMO_pca_tsne_umap_distances.png}\n   \\includegraphics[width0.3\\linewidth]{images/umap_distances.png}\n   \\hspace{-0.5in}\n   \\includegraphics[width0.32\\textwidth]{images/original_distances.png}\n   \\vspace{-0.3in}\n\\end{figure}\n\\end{wrapfigure}\n\n\nThe interpoint distance matrix plots on the right further elucidate this discrepancy. t-SNE's two-dimensional visualization (right) features a sizable separation between small intra-cluster and large inter-cluster distances. This separation is not present in the original input data (left), where interpoint distances are near-uniform.\n\n\nwhereas in the original dataset, the points are all roughly equidistant from one another. \na clear partition of the points is present\n\n. In the high-dimensional dataset, all the points are roughly equidistant from one another. This stands in stark contrast with the interpoint distance matrix of the two-dimensional visualizations, where the partition of points is very obvious.\n\n\n\nThe behavior that t-SNE and UMAP exhibit in this case are the result of a particular invaraince property. \n\nAre t-SNE and UMAP completely fabricating this cluster structure? Not quite. Not quite. However, there is a sense in which t-SNE and UMAP are very much exaggerating the cluster structure in this dataset.\n\\vspace{0.09in}\n\nOur work formalizes this phenomenon and other ``cluster-happy'' behaviors exhibited by t-SNE. Our theoretical analysis, suffused with experiments, shows that one should take positively clustered outputs with a grain of salt. Our contributions are as follows:\n\nMore broadly, we are interested in to what extent we can rely on the outputs generated by such techniques. \n\n\\begin{itemize}\n\n   \\item \\textbf{Misrepresentation of clusters:} We prove that both highly-clustered and arbitrarily \\textit{un}-clustered datasets \\footnote{According to standard metrics such as silhouette score.} \n   can produce the same maximally clustered visualization,\n   We find that input data which is arbitrarily \\textit{un}-clustered (according to standard metrics such as silhouette score) can have maximally clustered visualizations\n   see Theorem \\ref{thm:unclustHammer} and Corollary \\ref{cor:twoCluster_n_Unclustered}. Indeed, any visualization produced by t-SNE can be produced by an arbitrarily un-clustered dataset. \n   Moreover, we prove that arbitrarily close inputs can have vastly distinct visualizations, see Theorem \\ref{thm:perturbhammer}. We identify the peculiar property of t-SNE that explains these behaviors. We use this understanding to design a targeted adversarial attack , the injection of ``poison'' points, \n   that disrupts cluster structure in the output, see Figure \\ref{fig:one_pt_perturb}.\n   \\item \\textbf{Misrepresentation of outliers}: We prove that, regardless of input structure, the resulting t-SNE output is incapable of depicting extreme outliers, in the sense of depicting one point as substantially far away from all the others, see Theorem \\ref{thm:outlier_abs}. In practice, on both synthetic and real datasets, we observe a more concerning phenomenon that faraway outliers are often subsumed into the cluster structure of the bulk of points, see Figures \\ref{fig:outliers1} and \\ref{fig:outliers_real_world}. We then show that, if the input to t-SNE consists of a mixture of Gaussians plus some number of extreme outliers, the output will depict these outliers as part of the Gaussians.\n\\end{itemize}\n\nIt is worth emphasizing that these behaviors are somewhat peculiar to t-SNE, UMAP, and related methods. \n\nWhile there has been some work investigating the shortcomings of t-SNE in various practical settings (see Section \\ref{sec:related_work_limitations} for a detailed discussion of the relevant literature), to the best of our knowledge this is the first work which theoretically analyzes some of the key limitations of t-SNE. \n\n\ninformal theorem 1: take high-dimensional Gaussian noise, just a small perturbation gives you ANY bi-partition; OR, more generally, un-clustered, make it clustered (ACCORDING TO WELL-ESTABLISHED cluster metrics like rand index, dunn index, silhouette score)\n\nFIGURE 1: datasets which are epsilon apart but produce extremely different outputs\n\nFIGURE 2: clustered and highly unclustered producing the same output\n\n\ninformation theorem 2: arbitrarily extreme outliers are visualized very close to the bulk of the data\n\n\n\n",
  "title": "t-SNE exaggerates clusters, provably"
}
