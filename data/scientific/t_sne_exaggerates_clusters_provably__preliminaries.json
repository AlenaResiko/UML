{
  "authors": [
    "Noah Bergam",
    "Szymon Snoeck",
    "Nakul Verma"
  ],
  "date_published": "2025-10-09",
  "raw_tex": "\n\nLet \\(\\Delta_X   \\Delta(x_1,...,x_n) : \\max \\|x_i-x_j\\| / \\min \\|x_i-x_j\\|\\) denote the aspect ratio of a point set in Euclidean space. For \\(X \\in \\mathbb{R}^{n\\times d}\\) a matrix of points, let \\(D_X \\in \\mathbb{R}^{n\\times n}\\) denote its corresponding interpoint distance matrix. \n\nThe problem of producing a t-SNE plot proceeds as follows: \n\\paragraph{Basic Notation}\n\n\\paragraph{Notation} Unless otherwise specified \\(\\|\\cdot\\|\\) refers to the Euclidean norm; big-O notation is with respect to \\(n\\), the number of points; \n\n\\paragraph{t-SNE} \nGiven an input dataset\\footnote{Without loss of generality, we shall assume that the input dimension $Dn-1$.} \\(X   \\{x_1,\\ldots, x_n\\}\\subset \\mathbb{R}^{D}\\), the goal of t-SNE is to find an embedding \\(Y   \\{y_1,\\ldots,y_n\\}\\subset \\mathbb{R}^{d}\\) (where \\(d\\ll D\\), typically \\(d2\\)) that approximately maintains the neighborhood structure in $X$. t-SNE accomplishes this by assigning affinities to input data points which encode how likely an input point is to be a neighbor to a given point. The goal then is to find a configuration of the embedded points $Y$ that induces a similar neighborhood affinity. \nSpecifically, for \\(n>2\\), let \\(P   P(X) \\in \\mathbb{R}_+^{n\\times n}\\) and \\(Q   Q(Y) \\in \\mathbb{R}_+^{n\\times n}\\) be the input and embedded\n\\textit{affinity matrices} describing the pairwise neighborhood similarities in the input and the output, respectively.\nan \\emph{distribution} that   on the neighbors minimize some joint function of \\(X\\) and \\(Y\\). This objective function operates not directly on \\(X\\) and \\(Y\\) but instead on \\textit{affinity matrices} \\(P   P(X) \\in \\mathbb{R}_+^{n\\times n}\\) and \\(Q   Q(Y) \\in \\mathbb{R}_+^{n\\times n}\\) describing the pairwise similarities in the input and output, respectively.\nt-SNE was introduced as a modification to a method called stochastic neighbor embedding (SNE), developed by \\citet{hinton2002stochastic}. The general principle of stochastic neighbor embedding is to a construct kernel similarity matrices over the input and output and then iteratively update the output points such that their affinities match those of the input. t-SNE notably uses a Gaussian-based kernel on the input points and a t-distribution-based kernel on the output points (hence the name t-SNE).   The design choices of t-SNE worked significantly better in practice, and the impressive user base followed.\nt-SNE constructs \\(P\\) \nby first computing the affinities for each point $i$ defined as (for any $j\\neq i$)\\footnote{When $X$ and $\\sigma_i^*$ are clear from context, we will often drop it from the notation.}\nbased on a symmetrized Gaussian kernel with varying neighborhood sizes \\(\\{\\sigma_i: i\\in [n]\\}\\) for the input affinities. For any input data point $i$, define the neighborhood affinities (with respect to $i$) as\nconditional Let us first define for distinct \\(i,j \\in [n]\\) the \\textit{conditional Gaussian kernel} \n\\begin{equation}\n   P_{j|i}(X; \\sigma_i) : \\frac{\\exp(-\\lVert x_i-x_j\\rVert^2/(2\\sigma_i^2))}{\\sum_{k\\neq i} \\exp(-\\lVert x_i-x_k\\rVert^2/(2\\sigma_i^2))} \\hspace{0.5in} P_{i|i} : 0,\n\\end{equation}\nwhere $\\sigma_i \\geq 0$ encodes the (point-dependent) neighborhood scalings\\footnote{We define \\(P_{j|i}(X; 0) : \\lim_{\\sigma_i \\to 0} P_{j|i}(X; \\sigma_i)\\).}. It is worth noting that $P_{\\cdot|i}$ is a valid probability distribution over $[n]$.\nThe matrix \\(P\\) is then constructed based on a crucial parameter called the \\emph{perplexity}, which is denoted by \\(\\rho \\in (1,n-1)\\) and can be viewed as a proxy for effective number of neighbors, as follows.   (denoted as \\(\\rho\\) and taking values in \\([1,n-1]\\)), as follows: \\footnote{Notably, \\citet{jeong2024convergence} established that the procedure is only well-defined for \\(\\rho\\in [1,n-1]\\)} \n\\begin{itemize}\n   \\item[(1)] For each $i \\in [n]$, select the (unique, see Lemma \\ref{lem:unique_neighborhood}) neighborhood scale \\(\\sigma_i^* \\geq 0\\) that minimizes the gap between the entropy of \\(P_{\\cdot | j}(X;\\sigma^*_i)\\) and \\(\\log_2 \\rho\\)., (for any \\(\\rho \\subset [1,n-1]\\) for which the entropy is achievable).\n   \\item[(2)] Define \\(P   [P_{ij}]_{i,j\\in [n]}\\) where \\(P_{ij} : \\frac{1}{2n}(P_{i|j}(\\sigma^*_j) + P_{j|i}(\\sigma^*_i))\\) if \\(i\\neq j\\) and zero otherwise.\n\\end{itemize}\nThe above procedure is well-defined for \\(\\rho \\in [1, n-1]\\).\nLet \\(P_X\\) be the set of \\(\\rho\\) for which the above procedure is defined.\n\nTo avoid the so-called \\emph{the crowding problem} (see \\cite{van2008visualizing} for details), the output affinity matrix \\(Q\\) is computed based on a t-distribution. Specifically, for \\(i\\neq j\\)\n\\begin{equation}\n   Q_{ij}(Y) : \\frac{(1 + \\|y_i -y_j\\|^2)^{-1}}{\\sum_{k,l ; k\\neq l} (1 + \\|y_k-y_l\\|^2)^{-1}} \\hspace{0.5in}\n   Q_{ii} :0.\n\\end{equation}\nAs indicated before, the objective then is to minimize the gap between the input and the output affinities. This is accomplished by penalizing the relative entropy (KL-divergence) from \\(P\\) to \\(Q\\), where these affinity matrices are viewed as probability distributions.   \\[\\textup{minimize}_Y \\; \\mathcal{L}_X(Y): \\KL(P(X)\\|Q(Y))   \\sum_{\\substack{i,j\\\\ i\\neq j}} P_{ij}(X) \\log\\Big( \\frac{P_{ij}(X) }   {Q_{ij}(Y)}\\Big).\\]\n\\todo[inline]{L(Y) needs to have a reference to $X$}\nThis highly non-convex objective is usually optimized by initializing at a good starting point via an \\emph{early exaggeration phase}, followed by performing standard gradient descent methods and returning an embedding $Y$ that corresponds to a local minimum of the objective. Our central task is to study the nature of the these (local minimum) embeddings returned by t-SNE and their relation to the space of input datasets.\nWe can now define the main object of interest in this paper: optimal t-SNE embeddings.\n\\begin{definition}\n   For an $(n>2)$-point dataset $X \\subset \\mathbb{R}^{n-1}$ and perplexity parameter $\\rho \\in (1,n-1)$, define\n   $${\\TSNE}_{\\rho}(X) : \\{ Y \\subset \\mathbb{R}^d : \\nabla_Y \\mathcal{L}_X(Y)   0 \\} $$\n   as the set of outputs $Y\\subset \\mathbb{R}^d$ that are stationary to the t-SNE objective on a given input $X$.\n   \n   Furthermore, for a set of $n$-point datasets $\\mathcal{X}_n$, we define ${\\TSNE}_{\\rho}(\\mathcal{X}_n)   \\bigcup_{X \\in \\mathcal{X}_n}{\\TSNE}_{\\rho}(X).$\n   If $\\mathcal{X}_n$ is the set of \\emph{all} $n$-point datasets, we denote ${\\TSNE}_\\rho(\\mathcal{X}_n)$ as ${\\IMTSNE}$.\n\\end{definition}\nAll the supporting proofs for our formal statements can be found in the Appendix, and the code related to our empirical demonstrations is available on Github at \n\\href{https://github.com/njbergam/tsne-exaggerates-clusters}{\\texttt{https://github.com/njbergam/tsne-exaggerates-clusters}}.\\href{https://github.com/anon594/iclr26_submission8125}{\\texttt{https://github.com/anon594/iclr26\\_submission8125}}.\n\n\\paragraph{Code} The code related to our experimental demonstrations is publicly available on \\href{https://github.com/anon594/iclr26_submission8125/commits/main/}{GitHub}.\n\n",
  "title": "t-SNE exaggerates clusters, provably"
}
