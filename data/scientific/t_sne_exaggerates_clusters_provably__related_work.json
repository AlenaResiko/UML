{
  "authors": [
    "Noah Bergam",
    "Szymon Snoeck",
    "Nakul Verma"
  ],
  "date_published": "2025-10-09",
  "raw_tex": "\n\nThere are two notably distinct ways of contextualizing t-SNE, the data visualization method introduced by \\citet{van2008visualizing}. In a chronological sense, \n\n\n\nConfidence in the data visualizations produced by t-SNE and related methods is a somewhat contentious subject in data science \\citep{marx2024seeing}. Some works argue that these methods have merit in terms of preserving cluster structure, while others warn us about the fundamental issues with them and the broader goal of data visualization. \n\n\n\\subsection{Data Visualization}\n\n\\subsection{Performance Guarantees and Analysis of t-SNE}\n\n\\citet{shaham2017stochastic} were among the first to provide a guarantee on the visualization produced by optimal SNE embeddings of well-clustered data. Works by\n\\citet{linderman2019clustering} and \n\\citet{arora2018analysis} refined and extended this analysis, showing that t-SNE outputs produced using gradient descent yield well-clustered visualizations so long as the input is sufficiently well-clustered. The latter work established this guarantee in considerable generality, including cases where the input is sampled from a mixture of well-separated log-concave distributions.\n\nAlong with these algorithmic performance guarantee results, there is a line of work that seeks to establish a more fundamental understanding of t-SNE as an optimization problem. \\citet{cai2022theoretical}, for instance, characterized the distinct phases of gradient-based optimization of t-SNE, and proved an asymptotic equivalence between the early exaggeration phase and spectral clustering. \\citet{auffinger2023equilibrium} proved a consistency result for a continuous analogue of t-SNE, viewing the optimization problem as producing a map between distributions rather than just point sets. \\citet{jeong2024convergence} and \\citet{weinkove2024stochastic} studied the gradient flow of t-SNE. The former showed mild assumptions under which optima exist, and the latter showed that, even in cases where the gradient flow diverges the relative interpoint distances stabilize in the limit.\\todo{Pretty sure that result was actually established by Jeong and Wu. Weinkove showed SNE does not have this property and bounded the rate of divergence.}.\n\nuncovered examples where the t-SNE gradient flow pushes the points infinitely far apart. \\textcolor{red}{the relative inerpoint distances stabilize, mention gradient flow} \n\\todo{fix}\n\nshowed existence of data distributions and output initializations for which the t-SNE output via gradient descent may diverge. \n\n\\citet{auffinger2023equilibrium} proved the existence of a t-SNE optimizer in an asymptotic sense: if the input points are sampled from a measure then the t-SNE optimal approaches some unique measure. \\citet{jeong2024convergence} use gradient flow techniques to prove the existence and boundedness of t-SNE optimal embeddings under minimal assumptions on the input. \\citet{weinkove2024stochastic} showed that \n\n\\citet{auffinger2023equilibrium} were the first to provide a sort of consistency result, guaranteeing that t-SNE embeddings generated by i.i.d.\\ samples from a fixed probability distribution converge in the large sample limit. Furthermore, this limit is related to an optimal measure in the output space: a dimension-reduced version of the original probability measure. The result only shows the existence of this so-called \\textit{equilibrium measure}; it is up to future investigation to compute or approximate this measure explicitly.\n\n\\citet{jeong2024convergence} established the existence of a minimizer for the t-SNE optimization problem   under mild assumptions. They do so by establishing the existence of a bound on the diameter for optimal t-SNE embeddings, though they fall short of giving numerical estimates for this diameter. Their approach hinges on looking at the t-SNE update as a gradient flow. Some of their interesting intermediate results include: (1) the centroid of t-SNE points under a gradient flow does not change, and (2) if one interpoint distance escapes to infinity in a t-SNE gradient flow, then all interpoint distances escape to infinity, at roughly the same rate. \n\n\\subsection{Weaknesses and Criticisms}\n\\label{sec:related_work_limitations}\nAs t-SNE, UMAP (developed circa 2008 and 2018, respectively), and related methods have gained widespread usage, they have also attracted a fair share of criticism. \n\nBunte, K., Haase, S., Biehl, M., & Villmann, T. (2012). Stochastic neighbor embedding (SNE) for dimension reduction and visualization using arbitrary divergences. Neurocomputing, 90, 23-45\n\n\\citet{bunte_aribrary_divergences} were among the first to investigate the potential shortcomings of using KL-divergence in a t-SNE visualization and proposed a generalization to other divergences that may be better suited for specific datasets and user needs.\nBuilding upon the precision-recall framework of \\citet{venna2010information},\n\\citet{im2018stochastic} extended this result and explored specific intrinsic structures within data that may be less suited for t-SNE. They concluded that while t-SNE is more attuned to reveal intrinsic cluster structure, it usually fails to reveal intrinsic manifold structure. \n\nIn terms of analyzing cluster structure specifically, \\citet{yang2021t} provided empirical evidence that t-SNE visualizations are prone to \\emph{false negatives}. They presented a selection of well-clustered real-world datasets which t-SNE embeddings, even with reasonable parameter-tuning, do not seem to faithfully represent. They also showed that these practical datasets do not abide by the theoretical cluster separation conditions that are required by \\citet{arora2018analysis} analysis. clustered visualization guarantee to go through.\n\\citet{chari2023specious} argued that t-SNE and UMAP are unreliable tools for exploratory data analysis. Taking single-cell genomic data as an important real-world example, they provided systematic empirical evidence that these embeddings suffer high distortion, and often misrepresent neighborhood and cluster structure. Curiously, to the best of our knowledge, there is no systematic theoretical study investigating false positive behavior of t-SNE.\n\nMore recently, \\citet{snoeck_incompressibility} provided   theoretical evidence that, not just t-SNE, but any embedding technique that attempts to visualize data in constant dimensions is bound to misrepresent the neighborhood structure in most datasets. This work focuses exclusively on how misrepresentations induced by t-SNE visualizations can lead to false conclusions in terms of data analysis.\n\nis bound to be misrepresented in not just t-SNE or UMAP visualizations but any low-dimensional metric embedding of realistic data. \n\n\nIn light of the inherent limitations of data visualization procedures, several techniques have sprung up to enhance or test their efficacy. \n\n\\citet{xia2024statistical} create a statistical test which designates whether embedded points are faithfully represented or\n\n\npreserving distances and neighborhoods suffices for clusters, but not necessary\n\nFrom a theoretical perspective, the tradeoff between interpoint distance distortion and dimension of an embedding is fairly well-understood \\citep{matousek2013lectures}. [Snoeck et al, 2025] \n\n\\citep{}\n\n\\citep{kobak2019art}, for instance, advocated for a protocol for using t-SNE on single-cell data, which included PCA initialization, a high learning rate, and multi-scale similarity kernels. They acknowledge that \n\n\\citep{chari2023specious} showed that t-SNE is simply not robust to changes in the parameters.\n\n\nFALSE NEGATIVES\\citep{yang2021t}\n\n\nThere has been a wave of criticism about t-SNE and UMAP in recent years.\n\n\n\nHow to even define neighborhood preservation? jaccard, . then cite our own paper, theory treatment.\n\n\\citep{marx2024seeing} discusses a lab that developed sc-DEED which compared t-SNE and UMAP to understand whether these visualizations are actually useful. This is a bit misguided as it implicitly seems to come from the point of view that t-SNE and UMAP are independent demonstrations of the structure of the data when in fact they are very correlated.",
  "title": "t-SNE exaggerates clusters, provably"
}
