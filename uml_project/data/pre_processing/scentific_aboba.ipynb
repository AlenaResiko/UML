{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.19 (main, Oct  9 2025, 15:25:03) [Clang 16.0.0 (clang-1600.0.26.6)]\n",
      "/opt/homebrew/opt/python@3.10/bin/python3.10\n",
      "3.7.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(sys.executable)\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pyarrow\n",
    "# %pip install -U pyarrow pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas\n",
    "\n",
    "# if you ran intp 3.9 python version problem in vscode, do:\n",
    "# /opt/homebrew/bin/python3.10 -m pip install --upgrade pip\n",
    "# /opt/homebrew/bin/python3.10 -m pip install ipykernel\n",
    "# /opt/homebrew/bin/python3.10 -m ipykernel install --user --name=python310 --display-name \"Python 3.10\"\n",
    "\n",
    "# then in top right, switch Python 3.10.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip\n",
    "# %pip install torch torchvision torchaudio sentence-transformers\n",
    "# %pip install spacy==3.7.1\n",
    "# %pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/alenachan/Desktop/Columbia/Fall-2025/UML/UML/data/huggingface'),\n",
       " PosixPath('/Users/alenachan/Desktop/Columbia/Fall-2025/UML/UML/data/huggingface/REGISTRY.json'),\n",
       " PosixPath('/Users/alenachan/Desktop/Columbia/Fall-2025/UML/UML/data/models'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uml_project import *\n",
    "# note: constants.py requires python version >= 3.10\n",
    "HF_DIR, HF_REGISTRY, MODEL_DIR # DEVICE_TORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x306686c00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_tex_path = Path(\n",
    "\t\"/Users/alenachan/Desktop/Columbia/Fall-2025/UML/UML/data/scientific/stochastic_neighbor_embedding_under_f_divergences__discussion.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_tex_dict = json.loads(tsne_tex_path.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['authors', 'date_published', 'raw_tex', 'title'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_tex_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_sents1 = define_sentence(tsne_tex_dict[\"raw_tex\"], nlp=nlp, latex_mode=\"sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Other divergences for -SNE optimization have been explored previously.',\n",
       "       'Perhaps the first detailed study was done by.',\n",
       "       '(Gamma- Bregman- and -divergences) and their corresponding visualizations on.',\n",
       "       'some image processing datasets.',\n",
       "       'different divergences can be used to find micro and macro relationships in.\\ndata.',\n",
       "       ', but do not focus on faithful discovery of intrinsic structures.',\n",
       "       'An interesting line of work by and.\\ndiscovery and multi-scale visualizations to find local and global structures.',\n",
       "       'The work by is closely related where they study -divergences.',\n",
       "       'from an informational retrieval perspective.',\n",
       "       'Our work extends it to the general class of -divergences and.\\nexplores the relationships between data structure and the type of divergence used.',\n",
       "       'It is worth emphasizing that no previous study makes an explicit connection.',\n",
       "       'between the choice of divergence and the type of structure discovery.',\n",
       "       'Our work makes this explicit.\\nand should help a practitioner gain better insights about their data in the data exploration phase.',\n",
       "       'Our work.\\ngoes a step further and attempts to.',\n",
       "       'ameliorate the issues non-convex objective function in the -SNE criterion.',\n",
       "       'By studying the variational dual form, we can achieve.',\n",
       "       'better quality (locally optimal) solutions, which would be extremely beneficial to the practitioner.',\n",
       "       'and also attempts to improve upon the quality of the (locally optimal) sol.',\n",
       "       'RELATED WORK.',\n",
       "       'Bunte paper -- computed bunch of divergences with tsne.',\n",
       "       'icml paper1 -- compare properly with.',\n",
       "       'icml paper2 -- alpha/beta divergence discover micro/macro structure.',\n",
       "       'multiscale tsne --.\\ntrustworthy dimensionality reduction --.',\n",
       "       '-Generative Adversarial Networks ( -GAN) is another type of model uses -divergence as its objective function.',\n",
       "       'and its goal is to train a generative model that learns to generate data like samples.',\n",
       "       'Other then the fact that -GAN generate data like samples and -SNE generates embeddings and that.',\n",
       "       'one works in the continous set of domain and other one works in the discrete set of domain.',\n",
       "       'the two models are extremely similar.',\n",
       "       'Both -GAN and -SNE have several common sharing:.',\n",
       "       'i) they fall under the category of unsupervised learning.\\nii) share same variational -divergence formula in Equation~.\\niii) use deep neural networks for the discriminant function, and.\\niv) use same learning schema to update parameters in Algorithm~.\\nThe main difference is that -GAN generates samples from continuous space.',\n",
       "       'whereas -SNE generates finite set of embedding points.',\n",
       "       '-SNE consists of individual parameters per data points whereas the generator of -GAN is a deep neural network.',\n",
       "       'However, this is not a problem since we can simply use deep neural network to output embedding points.',\n",
       "       'Even though demonstrated that any -divergence can be used for training GANs.',\n",
       "       'they do not provide any guidance on how to select which -divergence function to use nor show which -divergences generate best samples.',\n",
       "       'Because it is impossible to visualize the entire domain of samples, they instead inspect by visualizing handful set of unordered samples.',\n",
       "       'This make difficult to evaluate which -GAN models perform the best.',\n",
       "       'This is because it is notoriously difficult to evaluate the performance of GANs.',\n",
       "       'In contrast, we analyzed the primal form of -divergence and delivered insights with respect to precision and recall.',\n",
       "       'We were able to measure the precision and recall to show which -divergences work the best for which type of dataset.',\n",
       "       'Additionally, our insights of precision and recall transfer to GANs as well.',\n",
       "       'There has been several work claiming that GANs suffer from model collapse (same diversity problem).\\nand more importantly, the reason for mode collapsing remain mysterious for GANs.',\n",
       "       'Our view of precision and recall between the neighbours of and directly applies to model collapse and model hallucinations in GAN.',\n",
       "       'The precision corresponds to the quality of mode collapsing problem and the recall corresponds to the quality of model hallucination.',\n",
       "       'In our paper, we precisely show the trade-off between weighting of precision versus recall for different -divergence.',\n",
       "       'The mainly due to difficulty in evaluation and visualization of the distribution over the samples (since it lies on high dimensional space).\\nand the difficulties in debugging.',\n",
       "       'Thus, it is empirically and notoriously difficult to study the stability of GAN objectives and the performance of Algorithm~ under different objective functions.',\n",
       "       'GANs are known to be difficult to train.',\n",
       "       'This is because there are no principled way of training them such as early stopping but also it is not straightforward to debugging and evaluation.',\n",
       "       'Not only we cannot visualize the entire samples, we cannot access or visualize the probability distribution.',\n",
       "       'In contrast, because the embedding parameters lie in two or three dimensional space and there are finite set of data points, we visualize the whole domain in t-SNE as shown.',\n",
       "       'Additionally, we can also view the probabilist distribution over the domain.',\n",
       "       'This makes easier to evaluate the algorithm since we have the entire picture of how the probability mass are distributed ove the domain.',\n",
       "       'Although the primal and dual forms are mathematically the same, the dynamics over the iterative optimization process of two forms can be dramatically different.',\n",
       "       'As opposite to dueal form, the primal form ought to be numerically stable since it does not involve differentiating the dual solution.',\n",
       "       'However, GAN formulations require using dual form (or lower bound of dual form) of distance or divergence formula to train their generative model.',\n",
       "       'This is because we cannot minimize the primal form of -divergence directly, since Equation~ is intractable for GANs.',\n",
       "       'Besides -divergence, many of other primal form of distances used to train GANs are intractable such as Wasserstein distance.}.',\n",
       "       'However, both the primal form of -divergence and dual form of -divergence becomes tractable for t-SNE.',\n",
       "       'This allow us to compare the difference in two optimization and analyze existing optimization algorithms.',\n",
       "       'For example, Algorithm~ is commonly used as a heuristic update rule in practice.',\n",
       "       'It updates the discriminant parameters times and update the embedding (generator) parameters times.',\n",
       "       'Because the loss function is non-convex and we are alternating the minimax updates, we do not know what kind of solution this algorithm lead us to.',\n",
       "       'Algorithm~ can find a different solution depending on the choice of and and under different measures.',\n",
       "       'This makes understanding the stability and robustness of Algorithm~ are very important.',\n",
       "       'In our experiments, we analyze it by comparing the loss value between the solutions from optimizing primal versus variational -divergence.',\n",
       "       'As far as we know this is first time empirically comparison of the performance of two optimizations.',\n",
       "       'Despite the success of both -GAN and -SNE, we do not have good understanding of the optimization both in terms of computational and algorithmic levels.',\n",
       "       'Computationallly, we do not know if Nash equilibrium exist, or if its degenerate, or the discrepancy between local Nash equilibria if exists.',\n",
       "       'Algorithically, we do not know how to seek for Nash equilibria and what type of solutions existing methods find us (following Algorithm~ guides to a good saddle point or not).',\n",
       "       'More broadly speaking.',\n",
       "       'Nevertheless, knowing whether these methods mitigate the problem or by how much are not well understood.',\n",
       "       'This is mainly due to difficulty in evaluation and visualization of the distribution over the samples (since it lies on high dimensional space), and the difficulties in debugging.',\n",
       "       'Thus, it is empirically and notoriously difficult to study the stability of GAN objectives and the performance of Algorithm~ under different objective functions'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_sents1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_sents2 = define_sentence(tsne_tex_dict[\"raw_tex\"], nlp=nlp, latex_mode=\"strip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Other divergences for -SNE optimization have been explored previously.',\n",
       "       'Perhaps the first detailed study was done by.',\n",
       "       'some image processing datasets.',\n",
       "       'different divergences can be used to find micro and macro relationships in.',\n",
       "       'data.', 'An interesting line of work by and.',\n",
       "       'discovery and multi-scale visualizations to find local and global structures.',\n",
       "       'The work by is closely related where they study -divergences.',\n",
       "       'from an informational retrieval perspective.',\n",
       "       'Our work extends it to the general class of -divergences and.',\n",
       "       'explores the relationships between data structure and the type of divergence used.',\n",
       "       'It is worth emphasizing that no previous study makes an explicit connection.',\n",
       "       'between the choice of divergence and the type of structure discovery.',\n",
       "       'Our work makes this explicit.',\n",
       "       'and should help a practitioner gain better insights about their data in the data exploration phase.',\n",
       "       'Our work.', 'goes a step further and attempts to.',\n",
       "       'ameliorate the issues non-convex objective function in the -SNE criterion.',\n",
       "       'By studying the variational dual form, we can achieve.',\n",
       "       'better quality (locally optimal) solutions, which would be extremely beneficial to the practitioner.',\n",
       "       'and also attempts to improve upon the quality of the (locally optimal) sol.',\n",
       "       'RELATED WORK.',\n",
       "       'Bunte paper -- computed bunch of divergences with tsne.',\n",
       "       'icml paper1 -- compare properly with.',\n",
       "       'icml paper2 -- alpha/beta divergence discover micro/macro structure.',\n",
       "       'multiscale tsne --.', 'trustworthy dimensionality reduction --.',\n",
       "       'and its goal is to train a generative model that learns to generate data like samples.',\n",
       "       'Other then the fact that -GAN generate data like samples and -SNE generates embeddings and that.',\n",
       "       'one works in the continous set of domain and other one works in the discrete set of domain.',\n",
       "       'the two models are extremely similar.',\n",
       "       'Both -GAN and -SNE have several common sharing:.',\n",
       "       'i) they fall under the category of unsupervised learning.',\n",
       "       'ii) share same variational -divergence formula in Equation~.',\n",
       "       'iii) use deep neural networks for the discriminant function, and.',\n",
       "       'iv) use same learning schema to update parameters in Algorithm~.',\n",
       "       'The main difference is that -GAN generates samples from continuous space.',\n",
       "       'whereas -SNE generates finite set of embedding points.',\n",
       "       'However, this is not a problem since we can simply use deep neural network to output embedding points.',\n",
       "       'Even though demonstrated that any -divergence can be used for training GANs.',\n",
       "       'they do not provide any guidance on how to select which -divergence function to use nor show which -divergences generate best samples.',\n",
       "       'Because it is impossible to visualize the entire domain of samples, they instead inspect by visualizing handful set of unordered samples.',\n",
       "       'This make difficult to evaluate which -GAN models perform the best.',\n",
       "       'This is because it is notoriously difficult to evaluate the performance of GANs.',\n",
       "       'In contrast, we analyzed the primal form of -divergence and delivered insights with respect to precision and recall.',\n",
       "       'We were able to measure the precision and recall to show which -divergences work the best for which type of dataset.',\n",
       "       'Additionally, our insights of precision and recall transfer to GANs as well.',\n",
       "       'There has been several work claiming that GANs suffer from model collapse (same diversity problem).',\n",
       "       'and more importantly, the reason for mode collapsing remain mysterious for GANs.',\n",
       "       'Our view of precision and recall between the neighbours of and directly applies to model collapse and model hallucinations in GAN.',\n",
       "       'The precision corresponds to the quality of mode collapsing problem and the recall corresponds to the quality of model hallucination.',\n",
       "       'In our paper, we precisely show the trade-off between weighting of precision versus recall for different -divergence.',\n",
       "       'The mainly due to difficulty in evaluation and visualization of the distribution over the samples (since it lies on high dimensional space).',\n",
       "       'and the difficulties in debugging.',\n",
       "       'Thus, it is empirically and notoriously difficult to study the stability of GAN objectives and the performance of Algorithm~ under different objective functions.',\n",
       "       'GANs are known to be difficult to train.',\n",
       "       'This is because there are no principled way of training them such as early stopping but also it is not straightforward to debugging and evaluation.',\n",
       "       'Not only we cannot visualize the entire samples, we cannot access or visualize the probability distribution.',\n",
       "       'In contrast, because the embedding parameters lie in two or three dimensional space and there are finite set of data points, we visualize the whole domain in t-SNE as shown.',\n",
       "       'Additionally, we can also view the probabilist distribution over the domain.',\n",
       "       'This makes easier to evaluate the algorithm since we have the entire picture of how the probability mass are distributed ove the domain.',\n",
       "       'Although the primal and dual forms are mathematically the same, the dynamics over the iterative optimization process of two forms can be dramatically different.',\n",
       "       'As opposite to dueal form, the primal form ought to be numerically stable since it does not involve differentiating the dual solution.',\n",
       "       'However, GAN formulations require using dual form (or lower bound of dual form) of distance or divergence formula to train their generative model.',\n",
       "       'This is because we cannot minimize the primal form of -divergence directly, since Equation~ is intractable for GANs.',\n",
       "       'Besides -divergence, many of other primal form of distances used to train GANs are intractable such as Wasserstein distance.}.',\n",
       "       'However, both the primal form of -divergence and dual form of -divergence becomes tractable for t-SNE.',\n",
       "       'This allow us to compare the difference in two optimization and analyze existing optimization algorithms.',\n",
       "       'For example, Algorithm~ is commonly used as a heuristic update rule in practice.',\n",
       "       'It updates the discriminant parameters times and update the embedding (generator) parameters times.',\n",
       "       'Because the loss function is non-convex and we are alternating the minimax updates, we do not know what kind of solution this algorithm lead us to.',\n",
       "       'Algorithm~ can find a different solution depending on the choice of and and under different measures.',\n",
       "       'This makes understanding the stability and robustness of Algorithm~ are very important.',\n",
       "       'In our experiments, we analyze it by comparing the loss value between the solutions from optimizing primal versus variational -divergence.',\n",
       "       'As far as we know this is first time empirically comparison of the performance of two optimizations.',\n",
       "       'Despite the success of both -GAN and -SNE, we do not have good understanding of the optimization both in terms of computational and algorithmic levels.',\n",
       "       'Computationallly, we do not know if Nash equilibrium exist, or if its degenerate, or the discrepancy between local Nash equilibria if exists.',\n",
       "       'Algorithically, we do not know how to seek for Nash equilibria and what type of solutions existing methods find us (following Algorithm~ guides to a good saddle point or not).',\n",
       "       'More broadly speaking.',\n",
       "       'Nevertheless, knowing whether these methods mitigate the problem or by how much are not well understood.',\n",
       "       'This is mainly due to difficulty in evaluation and visualization of the distribution over the samples (since it lies on high dimensional space), and the difficulties in debugging.',\n",
       "       'Thus, it is empirically and notoriously difficult to study the stability of GAN objectives and the performance of Algorithm~ under different objective functions'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_sents2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/alenachan/Desktop/Columbia/Fall-2025/UML/UML/uml_project/data/pre_processing/tmp.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    list_of_paths = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/alenachan/Desktop/Columbia/Fall-2025/UML/UML/data/scientific/REGISTRY.json', '/Users/alenachan/Desktop/Columbia/Fall-2025/UML/UML/data/scientific/a_neural_network_solves_explains_and_generates_university_math_problems_by_program_synthesis_and_few_shot_learning_at_human_level__generation_18_01.json', '/Users/alenachan/Desktop/Columbia/Fall-2025/UML/UML/data/scientific/a_neural_network_solves_explains_and_generates_university_math_problems_by_program_synthesis_and_few_shot_learning_at_human_level__generation_18_02.json', '/Users/alenachan/Desktop/Columbia/Fall-2025/UML/UML/data/scientific/a_neural_network_solves_explains_and_generates_university_math_problems_by_program_synthesis_and_few_shot_learning_at_human_level__generation_18_03.json', '/Users/alenachan/Desktop/Columbia/Fall-2025/UML/UML/data/scientific/a_neural_network_solves_explains_and_generates_university_math_problems_by_program_synthesis_and_few_shot_learning_at_human_level__generation_18_05.json', '/Users/alenachan/Desktop/Columbia/Fall-2025/UML/UML/data/scientific/a_neural_network_solves_explains_and_generates_university_math_problems_by_program_synthesis_and_few_shot_learning_at_human_level__generation_18_06.json', '/Users/alenachan/Desktop/Columbia/Fall-2025/UML/UML/data/scientific/a_neural_network_solves_explains_and_generates_university_math_problems_by_program_synthesis_and_few_shot_learning_at_human_level__generation_6_042.json', '/Users/alenachan/Desktop/Columbia/Fall-2025/UML/UML/data/scientific/a_neural_network_solves_explains_and_generates_university_math_problems_by_program_synthesis_and_few_shot_learning_at_human_level__generation_coms3251.json', '/Users/alenachan/Desktop/Columbia/Fall-2025/UML/UML/data/scientific/a_neural_network_solves_explains_and_generates_university_math_problems_by_program_synthesis_and_few_shot_learning_at_human_level__generation_math_algebra.json', '/Users/alenachan/Desktop/Columbia/Fall-2025/UML/UML/data/scientific/a_neural_network_solves_explains_and_generates_university_math_problems_by_program_synthesis_and_few_shot_learning_at_human_level__generation_math_counting_and_probability.json']\n"
     ]
    }
   ],
   "source": [
    "print(list_of_paths[:10])\n",
    "\n",
    "list_of_paths = list_of_paths[1:] # ignore /Users/alenachan/Desktop/Columbia/Fall-2025/UML/UML/data/scientific/REGISTRY.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping an_analysis_of_document_graph_construction_methods_for_amr_summarization__alg_person_merge: tsne_sents returned empty\n",
      "Skipping an_analysis_of_document_graph_construction_methods_for_amr_summarization__fig_example: tsne_sents returned empty\n",
      "Skipping an_analysis_of_document_graph_construction_methods_for_amr_summarization__fig_pipeline: tsne_sents returned empty\n",
      "Skipping an_analysis_of_document_graph_construction_methods_for_amr_summarization__tab_merge: tsne_sents returned empty\n",
      "Skipping main__appendixc: tsne_sents returned empty\n",
      "Skipping main__appendixd: tsne_sents returned empty\n",
      "Skipping main__appendixe: tsne_sents returned empty\n",
      "Skipping main__appendixg: tsne_sents returned empty\n",
      "Skipping main__appendixo: tsne_sents returned empty\n",
      "Skipping main__appendixp: tsne_sents returned empty\n",
      "Skipping main__appendixq: tsne_sents returned empty\n",
      "Skipping main__appendixr: tsne_sents returned empty\n",
      "Skipping main__appendixs: tsne_sents returned empty\n",
      "Skipping main__appendixt: tsne_sents returned empty\n",
      "Skipping main__appendixu: tsne_sents returned empty\n",
      "Skipping main__appendixv: tsne_sents returned empty\n",
      "Skipping main__appendixx: tsne_sents returned empty\n",
      "Skipping main__appendixy: tsne_sents returned empty\n",
      "Skipping solving_linear_algebra_by_program_synthesis__appendix_18_06: tsne_sents returned empty\n",
      "Skipping solving_linear_algebra_by_program_synthesis__appendix_coms3251: tsne_sents returned empty\n",
      "Skipping solving_probability_and_statistics_problems_by_program_synthesis__18_05_appendix: tsne_sents returned empty\n",
      "Skipping solving_probability_and_statistics_problems_by_program_synthesis__stat110_appendix: tsne_sents returned empty\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = \"data/scientific/processed\"\n",
    "OUTPUT_PARQUET = os.path.join(OUTPUT_DIR, \"papers.parquet\")\n",
    "METADATA_FILE = os.path.join(OUTPUT_DIR, \"metadata.json\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "for pth in list_of_paths:\n",
    "\t# print(pth)\n",
    "\tdoc_id = os.path.splitext(os.path.basename(pth))[0]\n",
    "\n",
    "\ttsne_tex_path = Path(pth)\n",
    "\ttsne_tex_dict = json.loads(tsne_tex_path.read_text())\n",
    "\ttsne_sents = define_sentence(tsne_tex_dict[\"raw_tex\"], nlp=nlp, latex_mode=\"sentences\")\n",
    "\n",
    "\tif len(tsne_sents) == 0:\n",
    "\t\tprint(f\"Skipping {doc_id}: tsne_sents returned empty\")\n",
    "\t\tcontinue\n",
    "\n",
    "\trecords = []\n",
    "\n",
    "\tfor i, sent in enumerate(tsne_sents):\n",
    "\t\trecords.append({\n",
    "\t\t\t\"corpus\": \"scientific_papers\",\n",
    "\t\t\t\"doc_id\": doc_id,\n",
    "\t\t\t\"sent_index\": i,\n",
    "\t\t\t\"sentence\": sent,\n",
    "\t\t\t\"split\": \"train\",  # could later vary by dataset\n",
    "\t\t\t\"token_count\": len(sent.split())\n",
    "\t\t})\n",
    "\t\n",
    "\tdf = pd.DataFrame(records)\n",
    "\ttable = pa.Table.from_pandas(df)\n",
    "\tpq.write_table(table, OUTPUT_PARQUET)\n",
    "\n",
    "\tmetadata = {\n",
    "\t\t\"corpus_name\": \"scientific_papers\",\n",
    "\t\t\"description\": \"Sentence-level dataset built from parsed scientific papers.\",\n",
    "\t\t\"num_documents\": len(df['doc_id'].unique()),\n",
    "\t\t\"num_sentences\": len(df),\n",
    "\t\t\"columns\": df.columns.tolist(),\n",
    "\t\t\"example_entry\": df.iloc[0].to_dict() if len(df) else {}\n",
    "\t}\n",
    "\n",
    "\twith open(METADATA_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "\t\tjson.dump(metadata, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              corpus                                             doc_id  \\\n",
      "0  scientific_papers  t_sne_exaggerates_clusters_provably__related_work   \n",
      "1  scientific_papers  t_sne_exaggerates_clusters_provably__related_work   \n",
      "2  scientific_papers  t_sne_exaggerates_clusters_provably__related_work   \n",
      "3  scientific_papers  t_sne_exaggerates_clusters_provably__related_work   \n",
      "4  scientific_papers  t_sne_exaggerates_clusters_provably__related_work   \n",
      "\n",
      "   sent_index                                           sentence  split  \\\n",
      "0           0  There are two notably distinct ways of context...  train   \n",
      "1           1                          In a chronological sense.  train   \n",
      "2           2  Confidence in the data visualizations produced...  train   \n",
      "3           3  Some works argue that these methods have merit...  train   \n",
      "4           4  Along with these algorithmic performance guara...  train   \n",
      "\n",
      "   token_count  \n",
      "0           16  \n",
      "1            4  \n",
      "2           20  \n",
      "3           31  \n",
      "4           27  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"/Users/alenachan/Desktop/Columbia/Fall-2025/UML/UML/data/scientific/processed/papers.parquet\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
