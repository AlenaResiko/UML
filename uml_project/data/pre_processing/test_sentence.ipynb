{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from /Users/Akseldkw/coding/Columbia/UML-Project/.env.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:JAX version 0.7.2 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Akseldkw/coding/kretsinger/data/nb_log.log\n"
     ]
    }
   ],
   "source": [
    "from kret_studies import *\n",
    "from kret_studies.notebook import *\n",
    "from kret_studies.complex import *\n",
    "\n",
    "logger = get_notebook_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/Akseldkw/coding/Columbia/UML-Project/data/huggingface'),\n",
       " PosixPath('/Users/Akseldkw/coding/Columbia/UML-Project/data/huggingface/REGISTRY.json'),\n",
       " device(type='mps'),\n",
       " PosixPath('/Users/Akseldkw/coding/Columbia/UML-Project/data/models'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uml_project import *\n",
    "\n",
    "HF_DIR, HF_REGISTRY, DEVICE_TORCH, MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB_DIR = HF_DIR / \"imdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trouble_dir = Path(\n",
    "    \"/Users/Akseldkw/coding/Columbia/UML-Project/data/music/taylor_swift/Albums/Red/IKnewYouWereTrouble.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x3241ded90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "swift_trouble = nlp(trouble_dir.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123 ContributorsTranslationsEspañolPortuguêsEnglishI Knew You Were Trouble Lyrics[Intro]\n",
       "\n",
       "[Verse 1]\n",
       "Once upon a time, a few mistakes ago\n",
       "I was in your sights, you got me alone\n",
       "You found me, you found me\n",
       "You found me-e-e-e-e\n",
       "I guess you didn't care, and I guess I liked that\n",
       "And when I fell hard, you took a step back\n",
       "Without me, without me\n",
       "Without me-e-e-e-e\n",
       "[Pre-Chorus]\n",
       "And he's long gone when he's next to me\n",
       "And I realize the blame is on me\n",
       "\n",
       "[Chorus]\n",
       "'Cause I knew you were trouble when you walked in\n",
       "So shame on me now\n",
       "Flew me to places I'd never been\n",
       "'Til you put me down, oh\n",
       "I knew you were trouble when you walked in\n",
       "So shame on me now\n",
       "Flew me to places I'd never been\n",
       "Now, I'm lying on the cold, hard ground\n",
       "\n",
       "[Post-Chorus]\n",
       "Oh, oh-oh\n",
       "Trouble, trouble, trouble\n",
       "Oh, oh-oh\n",
       "Trouble, trouble, trouble\n",
       "\n",
       "[Verse 2]\n",
       "No apologies, he'll never see you cry\n",
       "Pretends he doesn't know that he's the reason why\n",
       "You're drowning, you're drowning\n",
       "You're drowning-ing-ing-ing-ing\n",
       "And I heard you moved on from whispers on the street\n",
       "A new notch in your belt is all I'll ever be\n",
       "And now, I see, now, I see\n",
       "Now, I see-e-e-e-e\n",
       "You might also like[Pre-Chorus]\n",
       "He was long gone when he met me\n",
       "And I realize the joke is on me, yeah\n",
       "\n",
       "[Chorus]\n",
       "I knew you were trouble when you walked in (Oh)\n",
       "So shame on me now\n",
       "Flew me to places I'd never been\n",
       "'Til you put me down, oh\n",
       "I knew you were trouble when you walked in\n",
       "So shame on me now\n",
       "Flew me to places I'd never been (Yeah)\n",
       "Now, I'm lying on the cold, hard ground\n",
       "\n",
       "[Post-Chorus]\n",
       "Oh, oh-oh\n",
       "Trouble, trouble, trouble (Yeah, trouble)\n",
       "Oh, oh-oh\n",
       "Trouble, trouble, trouble\n",
       "\n",
       "[Bridge]\n",
       "And the saddest fear\n",
       "Comes creeping in\n",
       "That you never loved me\n",
       "Or her, or anyone, or anything, yeah\n",
       "[Chorus]\n",
       "I knew you were trouble when you walked in\n",
       "So shame on me now\n",
       "Flew me to places I'd never been (Never been)\n",
       "'Til you put me down, oh\n",
       "I knew you were trouble when you walked in (Knew it right there)\n",
       "So shame on me now (Knew it right there)\n",
       "Flew me to places I'd never been (Ooh)\n",
       "Now, I'm lying on the cold, hard ground\n",
       "\n",
       "[Post-Chorus]\n",
       "Oh, oh-oh\n",
       "Trouble, trouble, trouble (Oh)\n",
       "Oh, oh-oh\n",
       "Trouble, trouble, trouble\n",
       "I knew you were trouble when you walked in\n",
       "Trouble, trouble, trouble\n",
       "I knew you were trouble when you walked in\n",
       "Trouble, trouble, trouble\n",
       "[Outro]112Embed"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swift_trouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(swift_trouble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 ['Once upon a time, a few mistakes ago'\n",
      " 'I was in your sights, you got me alone' 'You found me, you found me'\n",
      " 'You found me-e-e-e-e'\n",
      " \"I guess you didn't care, and I guess I liked that\"\n",
      " 'And when I fell hard, you took a step back' 'Without me, without me'\n",
      " 'Without me-e-e-e-e' \"And he's long gone when he's next to me\"\n",
      " 'And I realize the blame is on me' 'So shame on me now'\n",
      " \"Flew me to places I'd never been\"]\n"
     ]
    }
   ],
   "source": [
    "nlp = build_sentencizer(True)  # or True if you've installed en_core_web_sm\n",
    "text = Path(\n",
    "    \"/Users/Akseldkw/coding/Columbia/UML-Project/data/music/taylor_swift/Albums/Red/IKnewYouWereTrouble.txt\"\n",
    ").read_text()\n",
    "sents = define_sentence(text, nlp=nlp)\n",
    "print(len(sents), sents[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Once upon a time, a few mistakes ago',\n",
       "       'I was in your sights, you got me alone',\n",
       "       'You found me, you found me', 'You found me-e-e-e-e',\n",
       "       \"I guess you didn't care, and I guess I liked that\",\n",
       "       'And when I fell hard, you took a step back',\n",
       "       'Without me, without me', 'Without me-e-e-e-e',\n",
       "       \"And he's long gone when he's next to me\",\n",
       "       'And I realize the blame is on me', 'So shame on me now',\n",
       "       \"Flew me to places I'd never been\",\n",
       "       'I knew you were trouble when you walked in', 'So shame on me now',\n",
       "       \"Flew me to places I'd never been\",\n",
       "       \"Now, I'm lying on the cold, hard ground\", 'Oh, oh-oh',\n",
       "       'Trouble, trouble, trouble', 'Oh, oh-oh',\n",
       "       'Trouble, trouble, trouble',\n",
       "       \"No apologies, he'll never see you cry\",\n",
       "       \"Pretends he doesn't know that he's the reason why\",\n",
       "       \"You're drowning, you're drowning\",\n",
       "       \"You're drowning-ing-ing-ing-ing\",\n",
       "       'And I heard you moved on from whispers on the street',\n",
       "       \"A new notch in your belt is all I'll ever be\",\n",
       "       'And now, I see, now, I see', 'Now, I see-e-e-e-e',\n",
       "       'He was long gone when he met me',\n",
       "       'And I realize the joke is on me, yeah',\n",
       "       'I knew you were trouble when you walked in (Oh)',\n",
       "       'So shame on me now', \"Flew me to places I'd never been\",\n",
       "       'I knew you were trouble when you walked in', 'So shame on me now',\n",
       "       \"Flew me to places I'd never been (Yeah)\",\n",
       "       \"Now, I'm lying on the cold, hard ground\", 'Oh, oh-oh',\n",
       "       'Trouble, trouble, trouble (Yeah, trouble)', 'Oh, oh-oh',\n",
       "       'Trouble, trouble, trouble', 'And the saddest fear',\n",
       "       'Comes creeping in', 'That you never loved me',\n",
       "       'Or her, or anyone, or anything, yeah',\n",
       "       'I knew you were trouble when you walked in', 'So shame on me now',\n",
       "       \"Flew me to places I'd never been (Never been)\",\n",
       "       'I knew you were trouble when you walked in (Knew it right there)',\n",
       "       'So shame on me now (Knew it right there)',\n",
       "       \"Flew me to places I'd never been (Ooh)\",\n",
       "       \"Now, I'm lying on the cold, hard ground\", 'Oh, oh-oh',\n",
       "       'Trouble, trouble, trouble (Oh)', 'Oh, oh-oh',\n",
       "       'Trouble, trouble, trouble',\n",
       "       'I knew you were trouble when you walked in',\n",
       "       'Trouble, trouble, trouble',\n",
       "       'I knew you were trouble when you walked in',\n",
       "       'Trouble, trouble, trouble'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_tex_path = Path(\n",
    "    \"/Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/stochastic_neighbor_embedding_under_f_divergences__discussion.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_tex_dict = json.loads(tsne_tex_path.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['authors', 'date_published', 'raw_tex', 'title'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_tex_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_sents1 = define_sentence(tsne_tex_dict[\"raw_tex\"], nlp=nlp, latex_mode=\"sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Other divergences for -SNE optimization have been explored previously.',\n",
       "       'Perhaps the first detailed study was done by.',\n",
       "       '(Gamma- Bregman- and -divergences) and their corresponding visualizations on.',\n",
       "       'some image processing datasets.',\n",
       "       'different divergences can be used to find micro and macro relationships in.',\n",
       "       'data. ,',\n",
       "       'but do not focus on faithful discovery of intrinsic structures.',\n",
       "       'An interesting line of work by and.',\n",
       "       'discovery and multi-scale visualizations to find local and global structures.',\n",
       "       'The work by is closely related where they study -divergences.',\n",
       "       'from an informational retrieval perspective.',\n",
       "       'Our work extends it to the general class of -divergences and.',\n",
       "       'explores the relationships between data structure and the type of divergence used.',\n",
       "       'It is worth emphasizing that no previous study makes an explicit connection.',\n",
       "       'between the choice of divergence and the type of structure discovery.',\n",
       "       'Our work makes this explicit.',\n",
       "       'and should help a practitioner gain better insights about their data in the data exploration phase.',\n",
       "       'Our work.', 'goes a step further and attempts to.',\n",
       "       'ameliorate the issues non-convex objective function in the -SNE criterion.',\n",
       "       'By studying the variational dual form, we can achieve.',\n",
       "       'better quality (locally optimal) solutions, which would be extremely beneficial to the practitioner.',\n",
       "       'and also attempts to improve upon the quality of the (locally optimal) sol.',\n",
       "       'RELATED WORK.',\n",
       "       'Bunte paper -- computed bunch of divergences with tsne.',\n",
       "       'icml paper1 -- compare properly with.',\n",
       "       'icml paper2 -- alpha/beta divergence discover micro/macro structure.',\n",
       "       'multiscale tsne --.', 'trustworthy dimensionality reduction --.',\n",
       "       '-Generative Adversarial Networks ( -GAN) is another type of model uses -divergence as its objective function.',\n",
       "       'and its goal is to train a generative model that learns to generate data like samples.',\n",
       "       'Other then the fact that -GAN generate data like samples and -SNE generates embeddings and that.',\n",
       "       'one works in the continous set of domain and other one works in the discrete set of domain.',\n",
       "       'the two models are extremely similar.',\n",
       "       'Both -GAN and -SNE have several common sharing:.',\n",
       "       'i) they fall under the category of unsupervised learning.',\n",
       "       'ii) share same variational -divergence formula in Equation~.\\niii) use deep neural networks for the discriminant function, and.',\n",
       "       'iv) use same learning schema to update parameters in Algorithm~.\\nThe main difference is that -GAN generates samples from continuous space.',\n",
       "       'whereas -SNE generates finite set of embedding points.',\n",
       "       '-SNE consists of individual parameters per data points whereas the generator of -GAN is a deep neural network.',\n",
       "       'However, this is not a problem since we can simply use deep neural network to output embedding points.',\n",
       "       'Even though demonstrated that any -divergence can be used for training GANs.',\n",
       "       'they do not provide any guidance on how to select which -divergence function to use nor show which -divergences generate best samples.',\n",
       "       'Because it is impossible to visualize the entire domain of samples, they instead inspect by visualizing handful set of unordered samples.',\n",
       "       'This make difficult to evaluate which -GAN models perform the best.',\n",
       "       'This is because it is notoriously difficult to evaluate the performance of GANs.',\n",
       "       'In contrast, we analyzed the primal form of -divergence and delivered insights with respect to precision and recall.',\n",
       "       'We were able to measure the precision and recall to show which -divergences work the best for which type of dataset.',\n",
       "       'Additionally, our insights of precision and recall transfer to GANs as well.',\n",
       "       'There has been several work claiming that GANs suffer from model collapse (same diversity problem).',\n",
       "       'and more importantly, the reason for mode collapsing remain mysterious for GANs.',\n",
       "       'Our view of precision and recall between the neighbours of and directly applies to model collapse and model hallucinations in GAN.',\n",
       "       'The precision corresponds to the quality of mode collapsing problem and the recall corresponds to the quality of model hallucination.',\n",
       "       'In our paper, we precisely show the trade-off between weighting of precision versus recall for different -divergence.',\n",
       "       'The mainly due to difficulty in evaluation and visualization of the distribution over the samples (since it lies on high dimensional space).',\n",
       "       'and the difficulties in debugging.',\n",
       "       'Thus, it is empirically and notoriously difficult to study the stability of GAN objectives and the performance of Algorithm~ under different objective functions.',\n",
       "       'GANs are known to be difficult to train.',\n",
       "       'This is because there are no principled way of training them such as early stopping but also it is not straightforward to debugging and evaluation.',\n",
       "       'Not only we cannot visualize the entire samples, we cannot access or visualize the probability distribution.',\n",
       "       'In contrast, because the embedding parameters lie in two or three dimensional space and there are finite set of data points, we visualize the whole domain in t-SNE as shown.',\n",
       "       'Additionally, we can also view the probabilist distribution over the domain.',\n",
       "       'This makes easier to evaluate the algorithm since we have the entire picture of how the probability mass are distributed ove the domain.',\n",
       "       'Although the primal and dual forms are mathematically the same, the dynamics over the iterative optimization process of two forms can be dramatically different.',\n",
       "       'As opposite to dueal form, the primal form ought to be numerically stable since it does not involve differentiating the dual solution.',\n",
       "       'However, GAN formulations require using dual form (or lower bound of dual form) of distance or divergence formula to train their generative model.',\n",
       "       'This is because we cannot minimize the primal form of -divergence directly, since Equation~ is intractable for GANs.',\n",
       "       'Besides -divergence, many of other primal form of distances used to train GANs are intractable such as Wasserstein distance.}.',\n",
       "       'However, both the primal form of -divergence and dual form of -divergence becomes tractable for t-SNE.',\n",
       "       'This allow us to compare the difference in two optimization and analyze existing optimization algorithms.',\n",
       "       'For example, Algorithm~ is commonly used as a heuristic update rule in practice.',\n",
       "       'It updates the discriminant parameters times and update the embedding (generator) parameters times.',\n",
       "       'Because the loss function is non-convex and we are alternating the minimax updates, we do not know what kind of solution this algorithm lead us to.',\n",
       "       'Algorithm~ can find a different solution depending on the choice of and and under different measures.',\n",
       "       'This makes understanding the stability and robustness of Algorithm~ are very important.',\n",
       "       'In our experiments, we analyze it by comparing the loss value between the solutions from optimizing primal versus variational -divergence.',\n",
       "       'As far as we know this is first time empirically comparison of the performance of two optimizations.',\n",
       "       'Despite the success of both -GAN and -SNE, we do not have good understanding of the optimization both in terms of computational and algorithmic levels.',\n",
       "       'Computationallly, we do not know if Nash equilibrium exist, or if its degenerate, or the discrepancy between local Nash equilibria if exists.',\n",
       "       'Algorithically, we do not know how to seek for Nash equilibria and what type of solutions existing methods find us (following Algorithm~ guides to a good saddle point or not).',\n",
       "       'More broadly speaking.',\n",
       "       'Nevertheless, knowing whether these methods mitigate the problem or by how much are not well understood.',\n",
       "       'This is mainly due to difficulty in evaluation and visualization of the distribution over the samples (since it lies on high dimensional space), and the difficulties in debugging.',\n",
       "       'Thus, it is empirically and notoriously difficult to study the stability of GAN objectives and the performance of Algorithm~ under different objective functions'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_sents1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_sents2 = define_sentence(tsne_tex_dict[\"raw_tex\"], nlp=nlp, latex_mode=\"strip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Other divergences for -SNE optimization have been explored previously',\n",
       "       'Perhaps the first detailed study was done by',\n",
       "       'some image processing datasets',\n",
       "       'different divergences can be used to find micro and macro relationships in',\n",
       "       'data. ,',\n",
       "       'but do not focus on faithful discovery of intrinsic structures',\n",
       "       'An interesting line of work by and',\n",
       "       'discovery and multi-scale visualizations to find local and global structures',\n",
       "       'The work by is closely related where they study -divergences',\n",
       "       'from an informational retrieval perspective.',\n",
       "       'Our work extends it to the general class of -divergences and',\n",
       "       'explores the relationships between data structure and the type of divergence used',\n",
       "       'It is worth emphasizing that no previous study makes an explicit connection',\n",
       "       'between the choice of divergence and the type of structure discovery.',\n",
       "       'Our work makes this explicit',\n",
       "       'and should help a practitioner gain better insights about their data in the data exploration phase',\n",
       "       'Our work', 'goes a step further and attempts to',\n",
       "       'ameliorate the issues non-convex objective function in the -SNE criterion',\n",
       "       'By studying the variational dual form, we can achieve',\n",
       "       'better quality (locally optimal) solutions, which would be extremely beneficial to the practitioner',\n",
       "       'and also attempts to improve upon the quality of the (locally optimal) sol',\n",
       "       'RELATED WORK',\n",
       "       'Bunte paper -- computed bunch of divergences with tsne',\n",
       "       'icml paper1 -- compare properly with',\n",
       "       'icml paper2 -- alpha/beta divergence discover micro/macro structure',\n",
       "       'multiscale tsne --', 'trustworthy dimensionality reduction --',\n",
       "       'and its goal is to train a generative model that learns to generate data like samples',\n",
       "       'Other then the fact that -GAN generate data like samples and -SNE generates embeddings and that',\n",
       "       'one works in the continous set of domain and other one works in the discrete set of domain',\n",
       "       'the two models are extremely similar',\n",
       "       'Both -GAN and -SNE have several common sharing:',\n",
       "       'i) they fall under the category of unsupervised learning',\n",
       "       'ii) share same variational -divergence formula in Equation~',\n",
       "       'iii) use deep neural networks for the discriminant function, and',\n",
       "       'iv) use same learning schema to update parameters in Algorithm~',\n",
       "       'The main difference is that -GAN generates samples from continuous space',\n",
       "       'whereas -SNE generates finite set of embedding points',\n",
       "       'However, this is not a problem since we can simply use deep neural network to output embedding points',\n",
       "       'Even though demonstrated that any -divergence can be used for training GANs',\n",
       "       'they do not provide any guidance on how to select which -divergence function to use nor show which -divergences generate best samples',\n",
       "       'Because it is impossible to visualize the entire domain of samples, they instead inspect by visualizing handful set of unordered samples',\n",
       "       'This make difficult to evaluate which -GAN models perform the best',\n",
       "       'This is because it is notoriously difficult to evaluate the performance of GANs',\n",
       "       'In contrast, we analyzed the primal form of -divergence and delivered insights with respect to precision and recall',\n",
       "       'We were able to measure the precision and recall to show which -divergences work the best for which type of dataset',\n",
       "       'Additionally, our insights of precision and recall transfer to GANs as well',\n",
       "       'There has been several work claiming that GANs suffer from model collapse (same diversity problem)',\n",
       "       'and more importantly, the reason for mode collapsing remain mysterious for GANs',\n",
       "       'Our view of precision and recall between the neighbours of and directly applies to model collapse and model hallucinations in GAN',\n",
       "       'The precision corresponds to the quality of mode collapsing problem and the recall corresponds to the quality of model hallucination',\n",
       "       'In our paper, we precisely show the trade-off between weighting of precision versus recall for different -divergence',\n",
       "       'The mainly due to difficulty in evaluation and visualization of the distribution over the samples (since it lies on high dimensional space)',\n",
       "       'and the difficulties in debugging',\n",
       "       'Thus, it is empirically and notoriously difficult to study the stability of GAN objectives and the performance of Algorithm~ under different objective functions',\n",
       "       'GANs are known to be difficult to train.',\n",
       "       'This is because there are no principled way of training them such as early stopping but also it is not straightforward to debugging and evaluation',\n",
       "       'Not only we cannot visualize the entire samples, we cannot access or visualize the probability distribution',\n",
       "       'In contrast, because the embedding parameters lie in two or three dimensional space and there are finite set of data points, we visualize the whole domain in t-SNE as shown',\n",
       "       'Additionally, we can also view the probabilist distribution over the domain',\n",
       "       'This makes easier to evaluate the algorithm since we have the entire picture of how the probability mass are distributed ove the domain',\n",
       "       'Although the primal and dual forms are mathematically the same, the dynamics over the iterative optimization process of two forms can be dramatically different',\n",
       "       'As opposite to dueal form, the primal form ought to be numerically stable since it does not involve differentiating the dual solution',\n",
       "       'However, GAN formulations require using dual form (or lower bound of dual form) of distance or divergence formula to train their generative model',\n",
       "       'This is because we cannot minimize the primal form of -divergence directly, since Equation~ is intractable for GANs',\n",
       "       'Besides -divergence, many of other primal form of distances used to train GANs are intractable such as Wasserstein distance.}',\n",
       "       'However, both the primal form of -divergence and dual form of -divergence becomes tractable for t-SNE',\n",
       "       'This allow us to compare the difference in two optimization and analyze existing optimization algorithms',\n",
       "       'For example, Algorithm~ is commonly used as a heuristic update rule in practice',\n",
       "       'It updates the discriminant parameters times and update the embedding (generator) parameters times',\n",
       "       'Because the loss function is non-convex and we are alternating the minimax updates, we do not know what kind of solution this algorithm lead us to',\n",
       "       'Algorithm~ can find a different solution depending on the choice of and and under different measures',\n",
       "       'This makes understanding the stability and robustness of Algorithm~ are very important',\n",
       "       'In our experiments, we analyze it by comparing the loss value between the solutions from optimizing primal versus variational -divergence',\n",
       "       'As far as we know this is first time empirically comparison of the performance of two optimizations',\n",
       "       'Despite the success of both -GAN and -SNE, we do not have good understanding of the optimization both in terms of computational and algorithmic levels',\n",
       "       'Computationallly, we do not know if Nash equilibrium exist, or if its degenerate, or the discrepancy between local Nash equilibria if exists',\n",
       "       'Algorithically, we do not know how to seek for Nash equilibria and what type of solutions existing methods find us (following Algorithm~ guides to a good saddle point or not)',\n",
       "       'More broadly speaking',\n",
       "       'Nevertheless, knowing whether these methods mitigate the problem or by how much are not well understood',\n",
       "       'This is mainly due to difficulty in evaluation and visualization of the distribution over the samples (since it lies on high dimensional space), and the difficulties in debugging',\n",
       "       'Thus, it is empirically and notoriously difficult to study the stability of GAN objectives and the performance of Algorithm~ under different objective functions'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_sents2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kret_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
