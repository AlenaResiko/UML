{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from /Users/Akseldkw/coding/Columbia/UML-Project/.env.\n",
      "/Users/Akseldkw/coding/kretsinger/data/nb_log.log\n"
     ]
    }
   ],
   "source": [
    "from kret_studies import *\n",
    "from kret_studies.notebook import *\n",
    "from kret_studies.complex import *\n",
    "\n",
    "logger = get_notebook_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/Akseldkw/coding/Columbia/UML-Project/data')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uml_project.data.constants import ROOT_DIR\n",
    "\n",
    "\n",
    "ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uml_project.data.scientific import *\n",
    "\n",
    "# Step 1: Get TeX source\n",
    "search_verma = search_arxiv(author=\"Nakul Verma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '2510.07746v1',\n",
       "  'title': 't-SNE Exaggerates Clusters, Provably',\n",
       "  'authors': ['Noah Bergam', 'Szymon Snoeck', 'Nakul Verma'],\n",
       "  'year': 2025,\n",
       "  'published': '2025-10-09',\n",
       "  'summary': 'Central to the widespread use of t-distributed stochastic neighbor embedding (t-SNE) is the conviction that it produces visualizations whose structure roughly matches that of the input. To the contrary, we prove that (1) the strength of the input clustering, and (2) the extremity of outlier points, cannot be reliably inferred from the t-SNE output. We demonstrate the prevalence of these failure modes in practice as well.',\n",
       "  'categories': ['cs.LG', 'cs.LG'],\n",
       "  'url_abs': 'http://arxiv.org/abs/2510.07746v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/2510.07746v1'},\n",
       " {'id': '2508.07119v1',\n",
       "  'title': 'Compressibility Barriers to Neighborhood-Preserving Data Visualizations',\n",
       "  'authors': ['Szymon Snoeck', 'Noah Bergam', 'Nakul Verma'],\n",
       "  'year': 2025,\n",
       "  'published': '2025-08-09',\n",
       "  'summary': 'To what extent is it possible to visualize high-dimensional datasets in a two- or three-dimensional space? We reframe this question in terms of embedding $n$-vertex graphs (representing the neighborhood structure of the input points) into metric spaces of low doubling dimension $d$, in such a way that maintains the separation between neighbors and non-neighbors. This seemingly lax embedding requirement is surprisingly difficult to satisfy. Our investigation shows that an overwhelming fraction of graphs require $d = \\\\Omega(\\\\log n)$. Even when considering sparse regular graphs, the situation does not improve, as an overwhelming fraction of such graphs requires $d= \\\\Omega(\\\\log n / \\\\log\\\\log n)$. The landscape changes dramatically when embedding into normed spaces. In particular, all but a vanishing fraction of graphs demand $d=\\\\Theta(n)$. Finally, we study the implications of these results for visualizing data with intrinsic cluster structure. We find that graphs produced from a planted partition model with $k$ clusters on $n$ points typically require $d=\\\\Omega(\\\\log n)$, even when the cluster structure is salient. These results challenge the aspiration that constant-dimensional visualizations can faithfully preserve neighborhood structure.',\n",
       "  'categories': ['cs.CG', 'cs.CG', 'math.MG'],\n",
       "  'url_abs': 'http://arxiv.org/abs/2508.07119v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/2508.07119v1'},\n",
       " {'id': '2503.19280v1',\n",
       "  'title': 'LogicLearner: A Tool for the Guided Practice of Propositional Logic Proofs',\n",
       "  'authors': ['Amogh Inamdar',\n",
       "   'Uzay Macar',\n",
       "   'Michel Vazirani',\n",
       "   'Michael Tarnow',\n",
       "   'Zarina Mustapha',\n",
       "   'Natalia Dittren',\n",
       "   'Sam Sadeh',\n",
       "   'Nakul Verma',\n",
       "   'Ansaf Salleb-Aouissi'],\n",
       "  'year': 2025,\n",
       "  'published': '2025-03-25',\n",
       "  'summary': 'The study of propositional logic -- fundamental to the theory of computing -- is a cornerstone of the undergraduate computer science curriculum. Learning to solve logical proofs requires repeated guided practice, but undergraduate students often lack access to on-demand tutoring in a judgment-free environment. In this work, we highlight the need for guided practice tools in undergraduate mathematics education and outline the desiderata of an effective practice tool. We accordingly develop LogicLearner, a web application for guided logic proof practice. LogicLearner consists of an interface to attempt logic proofs step-by-step and an automated proof solver to generate solutions on the fly, allowing users to request guidance as needed. We pilot LogicLearner as a practice tool in two semesters of an undergraduate discrete mathematics course and receive strongly positive feedback for usability and pedagogical value in student surveys. To the best of our knowledge, LogicLearner is the only learning tool that provides an end-to-end practice environment for logic proofs with immediate, judgment-free feedback.',\n",
       "  'categories': ['cs.DM', 'cs.DM', 'cs.AI', 'cs.HC'],\n",
       "  'url_abs': 'http://arxiv.org/abs/2503.19280v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/2503.19280v1'},\n",
       " {'id': '2502.09955v1',\n",
       "  'title': 'Diverse Inference and Verification for Advanced Reasoning',\n",
       "  'authors': ['Iddo Drori',\n",
       "   'Gaston Longhitano',\n",
       "   'Mao Mao',\n",
       "   'Seunghwan Hyun',\n",
       "   'Yuke Zhang',\n",
       "   'Sungjun Park',\n",
       "   'Zachary Meeks',\n",
       "   'Xin-Yu Zhang',\n",
       "   'Ben Segev',\n",
       "   'Howard Yong',\n",
       "   'Nakul Verma',\n",
       "   'Avi Shporer',\n",
       "   'Alon Amit',\n",
       "   'Madeleine Udell'],\n",
       "  'year': 2025,\n",
       "  'published': '2025-02-14',\n",
       "  'summary': \"Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant progress in mathematics and coding, yet find challenging advanced tasks such as International Mathematical Olympiad (IMO) combinatorics problems, Abstraction and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions. We use a diverse inference approach that combines multiple models and methods at test time. We find that verifying mathematics and code problems, and rejection sampling on other problems is simple and effective. We automatically verify correctness of solutions to IMO problems by Lean, and ARC puzzles by code, and find that best-of-N effectively answers HLE questions. Our approach increases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%, accuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that 948 humans could not and 26.5% of ARC puzzles that o3 high compute does not. Test-time simulations, reinforcement learning, and meta-learning with inference feedback improve generalization by adapting agent graph representations and varying prompts, code, and datasets. Our approach is reliable, robust, and scalable, and in the spirit of reproducible research, we will make it publicly available upon publication.\",\n",
       "  'categories': ['cs.AI', 'cs.AI'],\n",
       "  'url_abs': 'http://arxiv.org/abs/2502.09955v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/2502.09955v1'},\n",
       " {'id': '2409.18581v3',\n",
       "  'title': 'Deep Autoregressive Models as Causal Inference Engines',\n",
       "  'authors': ['Daniel Jiwoong Im',\n",
       "   'Kevin Zhang',\n",
       "   'Nakul Verma',\n",
       "   'Kyunghyun Cho'],\n",
       "  'year': 2024,\n",
       "  'published': '2024-09-27',\n",
       "  'summary': 'Existing causal inference (CI) models are often restricted to data with low-dimensional confounders and singleton actions. We propose an autoregressive (AR) CI framework capable of handling complex confounders and sequential actions commonly found in modern applications. Our approach accomplishes this using {\\\\em sequencification}, which transforms data from an underlying causal diagram into a sequence of tokens. Sequencification not only accommodates training with data generated from a large class of DAGs, but also extends existing CI capabilities to estimate multiple causal quantities using a {\\\\em single} model. We can directly compute probabilities from interventional distributions, simplifying inference and improving outcome prediction accuracy. We demonstrate that an AR model adapted for CI is efficient and effective in various complex applications such as navigating mazes, playing chess endgames, and evaluating the impact of certain keywords on paper acceptance rates, where we consider causal queries beyond standard reinforcement learning-type questions.',\n",
       "  'categories': ['cs.LG', 'cs.LG', 'stat.ML'],\n",
       "  'url_abs': 'http://arxiv.org/abs/2409.18581v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/2409.18581v3'},\n",
       " {'id': '2306.08221v1',\n",
       "  'title': 'Contrastive Loss is All You Need to Recover Analogies as Parallel Lines',\n",
       "  'authors': ['Narutatsu Ri', 'Fei-Tzin Lee', 'Nakul Verma'],\n",
       "  'year': 2023,\n",
       "  'published': '2023-06-14',\n",
       "  'summary': 'While static word embedding models are known to represent linguistic analogies as parallel lines in high-dimensional space, the underlying mechanism as to why they result in such geometric structures remains obscure. We find that an elementary contrastive-style method employed over distributional information performs competitively with popular word embedding models on analogy recovery tasks, while achieving dramatic speedups in training time. Further, we demonstrate that a contrastive loss is sufficient to create these parallel structures in word embeddings, and establish a precise relationship between the co-occurrence statistics and the geometric structure of the resulting word embeddings.',\n",
       "  'categories': ['cs.CL', 'cs.CL'],\n",
       "  'url_abs': 'http://arxiv.org/abs/2306.08221v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/2306.08221v1'},\n",
       " {'id': '2209.04528v1',\n",
       "  'title': 'Improving Model Training via Self-learned Label Representations',\n",
       "  'authors': ['Xiao Yu', 'Nakul Verma'],\n",
       "  'year': 2022,\n",
       "  'published': '2022-09-09',\n",
       "  'summary': 'Modern neural network architectures have shown remarkable success in several large-scale classification and prediction tasks. Part of the success of these architectures is their flexibility to transform the data from the raw input representations (e.g. pixels for vision tasks, or text for natural language processing tasks) to one-hot output encoding. While much of the work has focused on studying how the input gets transformed to the one-hot encoding, very little work has examined the effectiveness of these one-hot labels. In this work, we demonstrate that more sophisticated label representations are better for classification than the usual one-hot encoding. We propose Learning with Adaptive Labels (LwAL) algorithm, which simultaneously learns the label representation while training for the classification task. These learned labels can significantly cut down on the training time (usually by more than 50%) while often achieving better test accuracies. Our algorithm introduces negligible additional parameters and has a minimal computational overhead. Along with improved training times, our learned labels are semantically meaningful and can reveal hierarchical relationships that may be present in the data.',\n",
       "  'categories': ['cs.LG', 'cs.LG'],\n",
       "  'url_abs': 'http://arxiv.org/abs/2209.04528v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/2209.04528v1'},\n",
       " {'id': '2112.15594v4',\n",
       "  'title': 'A Neural Network Solves, Explains, and Generates University Math Problems by Program Synthesis and Few-Shot Learning at Human Level',\n",
       "  'authors': ['Iddo Drori',\n",
       "   'Sarah Zhang',\n",
       "   'Reece Shuttleworth',\n",
       "   'Leonard Tang',\n",
       "   'Albert Lu',\n",
       "   'Elizabeth Ke',\n",
       "   'Kevin Liu',\n",
       "   'Linda Chen',\n",
       "   'Sunny Tran',\n",
       "   'Newman Cheng',\n",
       "   'Roman Wang',\n",
       "   'Nikhil Singh',\n",
       "   'Taylor L. Patti',\n",
       "   'Jayson Lynch',\n",
       "   'Avi Shporer',\n",
       "   'Nakul Verma',\n",
       "   'Eugene Wu',\n",
       "   'Gilbert Strang'],\n",
       "  'year': 2021,\n",
       "  'published': '2021-12-31',\n",
       "  'summary': \"We demonstrate that a neural network pre-trained on text and fine-tuned on code solves mathematics course problems, explains solutions, and generates new questions at a human level. We automatically synthesize programs using few-shot learning and OpenAI's Codex transformer and execute them to solve course problems at 81% automatic accuracy. We curate a new dataset of questions from MIT's largest mathematics courses (Single Variable and Multivariable Calculus, Differential Equations, Introduction to Probability and Statistics, Linear Algebra, and Mathematics for Computer Science) and Columbia University's Computational Linear Algebra. We solve questions from a MATH dataset (on Prealgebra, Algebra, Counting and Probability, Intermediate Algebra, Number Theory, and Precalculus), the latest benchmark of advanced mathematics problems designed to assess mathematical reasoning. We randomly sample questions and generate solutions with multiple modalities, including numbers, equations, and plots. The latest GPT-3 language model pre-trained on text automatically solves only 18.8% of these university questions using zero-shot learning and 30.8% using few-shot learning and the most recent chain of thought prompting. In contrast, program synthesis with few-shot learning using Codex fine-tuned on code generates programs that automatically solve 81% of these questions. Our approach improves the previous state-of-the-art automatic solution accuracy on the benchmark topics from 8.8% to 81.1%. We perform a survey to evaluate the quality and difficulty of generated questions. This work is the first to automatically solve university-level mathematics course questions at a human level and the first work to explain and generate university-level mathematics course questions at scale, a milestone for higher education.\",\n",
       "  'categories': ['cs.LG', 'cs.LG', 'cs.AI'],\n",
       "  'url_abs': 'http://arxiv.org/abs/2112.15594v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/2112.15594v4'},\n",
       " {'id': '2111.13993v1',\n",
       "  'title': 'An analysis of document graph construction methods for AMR summarization',\n",
       "  'authors': ['Fei-Tzin Lee',\n",
       "   'Chris Kedzie',\n",
       "   'Nakul Verma',\n",
       "   'Kathleen McKeown'],\n",
       "  'year': 2021,\n",
       "  'published': '2021-11-27',\n",
       "  'summary': 'Meaning Representation (AMR) is a graph-based semantic representation for sentences, composed of collections of concepts linked by semantic relations. AMR-based approaches have found success in a variety of applications, but a challenge to using it in tasks that require document-level context is that it only represents individual sentences. Prior work in AMR-based summarization has automatically merged the individual sentence graphs into a document graph, but the method of merging and its effects on summary content selection have not been independently evaluated. In this paper, we present a novel dataset consisting of human-annotated alignments between the nodes of paired documents and summaries which may be used to evaluate (1) merge strategies; and (2) the performance of content selection methods over nodes of a merged or unmerged AMR graph. We apply these two forms of evaluation to prior work as well as a new method for node merging and show that our new method has significantly better performance than prior work.',\n",
       "  'categories': ['cs.CL', 'cs.CL'],\n",
       "  'url_abs': 'http://arxiv.org/abs/2111.13993v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/2111.13993v1'},\n",
       " {'id': '2111.08267v1',\n",
       "  'title': 'Solving Probability and Statistics Problems by Program Synthesis',\n",
       "  'authors': ['Leonard Tang',\n",
       "   'Elizabeth Ke',\n",
       "   'Nikhil Singh',\n",
       "   'Nakul Verma',\n",
       "   'Iddo Drori'],\n",
       "  'year': 2021,\n",
       "  'published': '2021-11-16',\n",
       "  'summary': \"We solve university level probability and statistics questions by program synthesis using OpenAI's Codex, a Transformer trained on text and fine-tuned on code. We transform course problems from MIT's 18.05 Introduction to Probability and Statistics and Harvard's STAT110 Probability into programming tasks. We then execute the generated code to get a solution. Since these course questions are grounded in probability, we often aim to have Codex generate probabilistic programs that simulate a large number of probabilistic dependencies to compute its solution. Our approach requires prompt engineering to transform the question from its original form to an explicit, tractable form that results in a correct program and solution. To estimate the amount of work needed to translate an original question into its tractable form, we measure the similarity between original and transformed questions. Our work is the first to introduce a new dataset of university-level probability and statistics problems and solve these problems in a scalable fashion using the program synthesis capabilities of large language models.\",\n",
       "  'categories': ['cs.LG', 'cs.LG', 'cs.AI', 'cs.CL', 'cs.PL'],\n",
       "  'url_abs': 'http://arxiv.org/abs/2111.08267v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/2111.08267v1'},\n",
       " {'id': '2111.08171v1',\n",
       "  'title': 'Solving Linear Algebra by Program Synthesis',\n",
       "  'authors': ['Iddo Drori', 'Nakul Verma'],\n",
       "  'year': 2021,\n",
       "  'published': '2021-11-16',\n",
       "  'summary': \"We solve MIT's Linear Algebra 18.06 course and Columbia University's Computational Linear Algebra COMS3251 courses with perfect accuracy by interactive program synthesis. This surprisingly strong result is achieved by turning the course questions into programming tasks and then running the programs to produce the correct answers. We use OpenAI Codex with zero-shot learning, without providing any examples in the prompts, to synthesize code from questions. We quantify the difference between the original question text and the transformed question text that yields a correct answer. Since all COMS3251 questions are not available online the model is not overfitting. We go beyond just generating code for questions with numerical answers by interactively generating code that also results visually pleasing plots as output. Finally, we automatically generate new questions given a few sample questions which may be used as new course content. This work is a significant step forward in solving quantitative math problems and opens the door for solving many university level STEM courses by machine.\",\n",
       "  'categories': ['cs.LG', 'cs.LG', 'cs.AI', 'cs.CL'],\n",
       "  'url_abs': 'http://arxiv.org/abs/2111.08171v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/2111.08171v1'},\n",
       " {'id': '2012.02394v1',\n",
       "  'title': 'Biased Programmers? Or Biased Data? A Field Experiment in Operationalizing AI Ethics',\n",
       "  'authors': ['Bo Cowgill',\n",
       "   \"Fabrizio Dell'Acqua\",\n",
       "   'Samuel Deng',\n",
       "   'Daniel Hsu',\n",
       "   'Nakul Verma',\n",
       "   'Augustin Chaintreau'],\n",
       "  'year': 2020,\n",
       "  'published': '2020-12-04',\n",
       "  'summary': \"Why do biased predictions arise? What interventions can prevent them? We evaluate 8.2 million algorithmic predictions of math performance from $\\\\approx$400 AI engineers, each of whom developed an algorithm under a randomly assigned experimental condition. Our treatment arms modified programmers' incentives, training data, awareness, and/or technical knowledge of AI ethics. We then assess out-of-sample predictions from their algorithms using randomized audit manipulations of algorithm inputs and ground-truth math performance for 20K subjects. We find that biased predictions are mostly caused by biased training data. However, one-third of the benefit of better training data comes through a novel economic mechanism: Engineers exert greater effort and are more responsive to incentives when given better training data. We also assess how performance varies with programmers' demographic characteristics, and their performance on a psychological test of implicit bias (IAT) concerning gender and careers. We find no evidence that female, minority and low-IAT engineers exhibit lower bias or discrimination in their code. However, we do find that prediction errors are correlated within demographic groups, which creates performance improvements through cross-demographic averaging. Finally, we quantify the benefits and tradeoffs of practical managerial or policy interventions such as technical advice, simple reminders, and improved incentives for decreasing algorithmic bias.\",\n",
       "  'categories': ['econ.GN', 'econ.GN', 'cs.CY', 'q-fin.EC'],\n",
       "  'url_abs': 'http://arxiv.org/abs/2012.02394v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/2012.02394v1'},\n",
       " {'id': '1910.14134v1',\n",
       "  'title': 'Meta-Learning to Cluster',\n",
       "  'authors': ['Yibo Jiang', 'Nakul Verma'],\n",
       "  'year': 2019,\n",
       "  'published': '2019-10-30',\n",
       "  'summary': 'Clustering is one of the most fundamental and wide-spread techniques in exploratory data analysis. Yet, the basic approach to clustering has not really changed: a practitioner hand-picks a task-specific clustering loss to optimize and fit the given data to reveal the underlying cluster structure. Some types of losses---such as k-means, or its non-linear version: kernelized k-means (centroid based), and DBSCAN (density based)---are popular choices due to their good empirical performance on a range of applications. Although every so often the clustering output using these standard losses fails to reveal the underlying structure, and the practitioner has to custom-design their own variation. In this work we take an intrinsically different approach to clustering: rather than fitting a dataset to a specific clustering loss, we train a recurrent model that learns how to cluster. The model uses as training pairs examples of datasets (as input) and its corresponding cluster identities (as output). By providing multiple types of training datasets as inputs, our model has the ability to generalize well on unseen datasets (new clustering tasks). Our experiments reveal that by training on simple synthetically generated datasets or on existing real datasets, we can achieve better clustering performance on unseen real-world datasets when compared with standard benchmark clustering techniques. Our meta clustering model works well even for small datasets where the usual deep learning models tend to perform worse.',\n",
       "  'categories': ['cs.LG', 'cs.LG', 'stat.ML'],\n",
       "  'url_abs': 'http://arxiv.org/abs/1910.14134v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1910.14134v1'},\n",
       " {'id': '1910.07368v2',\n",
       "  'title': 'Model-Agnostic Meta-Learning using Runge-Kutta Methods',\n",
       "  'authors': ['Daniel Jiwoong Im', 'Yibo Jiang', 'Nakul Verma'],\n",
       "  'year': 2019,\n",
       "  'published': '2019-10-16',\n",
       "  'summary': 'Meta-learning has emerged as an important framework for learning new tasks from just a few examples. The success of any meta-learning model depends on (i) its fast adaptation to new tasks, as well as (ii) having a shared representation across similar tasks. Here we extend the model-agnostic meta-learning (MAML) framework introduced by Finn et al. (2017) to achieve improved performance by analyzing the temporal dynamics of the optimization procedure via the Runge-Kutta method. This method enables us to gain fine-grained control over the optimization and helps us achieve both the adaptation and representation goals across tasks. By leveraging this refined control, we demonstrate that there are multiple principled ways to update MAML and show that the classic MAML optimization is simply a special case of second-order Runge-Kutta method that mainly focuses on fast-adaptation. Experiments on benchmark classification, regression and reinforcement learning tasks show that this refined control helps attain improved results.',\n",
       "  'categories': ['cs.LG', 'cs.LG', 'stat.ML'],\n",
       "  'url_abs': 'http://arxiv.org/abs/1910.07368v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1910.07368v2'},\n",
       " {'id': '1902.01738v1',\n",
       "  'title': 'Metric Learning on Manifolds',\n",
       "  'authors': ['Max Aalto', 'Nakul Verma'],\n",
       "  'year': 2019,\n",
       "  'published': '2019-02-05',\n",
       "  'summary': 'Recent literature has shown that symbolic data, such as text and graphs, is often better represented by points on a curved manifold, rather than in Euclidean space. However, geometrical operations on manifolds are generally more complicated than in Euclidean space, and thus many techniques for processing and analysis taken for granted in Euclidean space are difficult on manifolds. A priori, it is not obvious how we may generalize such methods to manifolds. We consider specifically the problem of distance metric learning, and present a framework that solves it on a large class of manifolds, such that similar data are located in closer proximity with respect to the manifold distance function. In particular, we extend the existing metric learning algorithms, and derive the corresponding sample complexity rates for the case of manifolds. Additionally, we demonstrate an improvement of performance in $k$-means clustering and $k$-nearest neighbor classification on real-world complex networks using our methods.',\n",
       "  'categories': ['cs.LG', 'cs.LG', 'stat.ML'],\n",
       "  'url_abs': 'http://arxiv.org/abs/1902.01738v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1902.01738v1'},\n",
       " {'id': '1901.10837v4',\n",
       "  'title': 'Noise-tolerant fair classification',\n",
       "  'authors': ['Alexandre Louis Lamy',\n",
       "   'Ziyuan Zhong',\n",
       "   'Aditya Krishna Menon',\n",
       "   'Nakul Verma'],\n",
       "  'year': 2019,\n",
       "  'published': '2019-01-30',\n",
       "  'summary': \"Fairness-aware learning involves designing algorithms that do not discriminate with respect to some sensitive feature (e.g., race or gender). Existing work on the problem operates under the assumption that the sensitive feature available in one's training sample is perfectly reliable. This assumption may be violated in many real-world cases: for example, respondents to a survey may choose to conceal or obfuscate their group identity out of fear of potential discrimination. This poses the question of whether one can still learn fair classifiers given noisy sensitive features. In this paper, we answer the question in the affirmative: we show that if one measures fairness using the mean-difference score, and sensitive features are subject to noise from the mutually contaminated learning model, then owing to a simple identity we only need to change the desired fairness-tolerance. The requisite tolerance can be estimated by leveraging existing noise-rate estimators from the label noise literature. We finally show that our procedure is empirically effective on two case-studies involving sensitive feature censoring.\",\n",
       "  'categories': ['cs.LG', 'cs.LG', 'cs.AI', 'cs.CY', 'stat.ML'],\n",
       "  'url_abs': 'http://arxiv.org/abs/1901.10837v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1901.10837v4'},\n",
       " {'id': '1811.01247v1',\n",
       "  'title': 'Stochastic Neighbor Embedding under f-divergences',\n",
       "  'authors': ['Daniel Jiwoong Im', 'Nakul Verma', 'Kristin Branson'],\n",
       "  'year': 2018,\n",
       "  'published': '2018-11-03',\n",
       "  'summary': 'The t-distributed Stochastic Neighbor Embedding (t-SNE) is a powerful and popular method for visualizing high-dimensional data. It minimizes the Kullback-Leibler (KL) divergence between the original and embedded data distributions. In this work, we propose extending this method to other f-divergences. We analytically and empirically evaluate the types of latent structure-manifold, cluster, and hierarchical-that are well-captured using both the original KL-divergence as well as the proposed f-divergence generalization, and find that different divergences perform better for different types of structure. A common concern with $t$-SNE criterion is that it is optimized using gradient descent, and can become stuck in poor local minima. We propose optimizing the f-divergence based loss criteria by minimizing a variational bound. This typically performs better than optimizing the primal form, and our experiments show that it can improve upon the embedding results obtained from the original $t$-SNE criterion as well.',\n",
       "  'categories': ['cs.LG', 'cs.LG', 'stat.ML'],\n",
       "  'url_abs': 'http://arxiv.org/abs/1811.01247v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1811.01247v1'},\n",
       " {'id': '1505.02729v1',\n",
       "  'title': 'Sample complexity of learning Mahalanobis distance metrics',\n",
       "  'authors': ['Nakul Verma', 'Kristin Branson'],\n",
       "  'year': 2015,\n",
       "  'published': '2015-05-11',\n",
       "  'summary': \"Metric learning seeks a transformation of the feature space that enhances prediction quality for the given task at hand. In this work we provide PAC-style sample complexity rates for supervised metric learning. We give matching lower- and upper-bounds showing that the sample complexity scales with the representation dimension when no assumptions are made about the underlying data distribution. However, by leveraging the structure of the data distribution, we show that one can achieve rates that are fine-tuned to a specific notion of intrinsic complexity for a given dataset. Our analysis reveals that augmenting the metric learning optimization criterion with a simple norm-based regularization can help adapt to a dataset's intrinsic complexity, yielding better generalization. Experiments on benchmark datasets validate our analysis and show that regularizing the metric can help discern the signal even when the data contains high amounts of noise.\",\n",
       "  'categories': ['cs.LG', 'cs.LG', 'cs.AI', 'stat.ML'],\n",
       "  'url_abs': 'http://arxiv.org/abs/1505.02729v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1505.02729v1'},\n",
       " {'id': '1206.6813v1',\n",
       "  'title': 'A concentration theorem for projections',\n",
       "  'authors': ['Sanjoy Dasgupta', 'Daniel Hsu', 'Nakul Verma'],\n",
       "  'year': 2012,\n",
       "  'published': '2012-06-27',\n",
       "  'summary': \"X in R^D has mean zero and finite second moments. We show that there is a precise sense in which almost all linear projections of X into R^d (for d < D) look like a scale-mixture of spherical Gaussians -- specifically, a mixture of distributions N(0, sigma^2 I_d) where the weight of the particular sigma component is P (| X |^2 = sigma^2 D). The extent of this effect depends upon the ratio of d to D, and upon a particular coefficient of eccentricity of X's distribution. We explore this result in a variety of experiments.\",\n",
       "  'categories': ['cs.LG', 'cs.LG', 'stat.ML'],\n",
       "  'url_abs': 'http://arxiv.org/abs/1206.6813v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1206.6813v1'},\n",
       " {'id': '1205.2609v1',\n",
       "  'title': 'Which Spatial Partition Trees are Adaptive to Intrinsic Dimension?',\n",
       "  'authors': ['Nakul Verma', 'Samory Kpotufe', 'Sanjoy Dasgupta'],\n",
       "  'year': 2012,\n",
       "  'published': '2012-05-09',\n",
       "  'summary': 'Recent theory work has found that a special type of spatial partition tree - called a random projection tree - is adaptive to the intrinsic dimension of the data from which it is built. Here we examine this same question, with a combination of theory and experiments, for a broader class of trees that includes k-d trees, dyadic trees, and PCA trees. Our motivation is to get a feel for (i) the kind of intrinsic low dimensional structure that can be empirically verified, (ii) the extent to which a spatial partition can exploit such structure, and (iii) the implications for standard statistical tasks such as regression, vector quantization, and nearest neighbor search.',\n",
       "  'categories': ['stat.ML', 'stat.ML', 'cs.LG'],\n",
       "  'url_abs': 'http://arxiv.org/abs/1205.2609v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1205.2609v1'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_verma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new arXiv links to add — registry already up to date.\n"
     ]
    }
   ],
   "source": [
    "append_arxiv_results_to_registry(search_verma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = batch_fetch_tex_sources_from_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2503.19280': {'id': '2503.19280',\n",
       "  'files': [{'name': 'sn-article.tex',\n",
       "    'text': \"Version 3 December 2023\\n See section 11 of the User Manual for version history\\n\\n\\n   \\n Please do not use \\\\input{...} to include other tex files.   \\n Submit your LaTeX manuscript as one .tex document.   \\n   \\n All additional figures and files should be attached   \\n separately and not embedded in the \\\\TeX\\\\ document itself.   \\n   \\n\\n\\n\\\\documentclass[referee,sn-basic]{sn-jnl} referee option is meant for double line spacing\\n\\n\\n to print line numbers in the margin use lineno option \\n\\n\\n\\\\documentclass[lineno,sn-basic]{sn-jnl} Basic Springer Nature Reference Style/Chemistry Reference Style\\n\\n\\n to compile with pdflatex/xelatex use pdflatex option \\n\\n\\n\\\\documentclass[pdflatex,sn-basic]{sn-jnl} Basic Springer Nature Reference Style/Chemistry Reference Style\\n\\n\\nNote: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove \\x93Numbered\\x94 in the optional parenthesis. \\nThe option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst   \\n \\n\\\\documentclass[pdflatex,sn-nature]{sn-jnl} Style for submissions to Nature Portfolio journals\\n\\\\documentclass[pdflatex,sn-basic]{sn-jnl} Basic Springer Nature Reference Style/Chemistry Reference Style\\n\\\\documentclass[pdflatex,sn-mathphys-num]{sn-jnl} Math and Physical Sciences Numbered Reference Style \\n\\\\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl} Math and Physical Sciences Author Year Reference Style\\n\\\\documentclass[pdflatex,sn-aps]{sn-jnl} American Physical Society (APS) Reference Style\\n\\\\documentclass[pdflatex,sn-vancouver,Numbered]{sn-jnl} Vancouver Reference Style\\n\\\\documentclass[pdflatex,sn-apa]{sn-jnl} APA Reference Style \\n\\\\documentclass[pdflatex,sn-chicago]{sn-jnl} Chicago-based Humanities Reference Style\\n\\n Standard Packages\\n<additional latex packages if required can be included here>\\n\\n\\\\usepackage{graphicx}\\n\\\\usepackage{multirow}\\n\\\\usepackage{amsmath,amssymb,amsfonts}\\n\\\\usepackage{amsthm}\\n\\\\usepackage{mathrsfs}\\n\\\\usepackage[title]{appendix}\\n\\\\usepackage{xcolor}\\n\\\\usepackage{textcomp}\\n\\\\usepackage{manyfoot}\\n\\\\usepackage{booktabs}\\n\\\\usepackage{algorithm}\\n\\\\usepackage{algorithmicx}\\n\\\\usepackage{algpseudocode}\\n\\\\usepackage{listings}\\n\\\\usepackage{subcaption}\\n\\\\usepackage[export]{adjustbox}\\n\\n\\n\\n as per the requirement new theorem styles can be included as shown below\\n\\\\theoremstyle{thmstyleone}\\n\\\\newtheorem{theorem}{Theorem}   meant for continuous numbers\\n\\\\newtheorem{theorem}{Theorem}[section] meant for sectionwise numbers\\n optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition\\n\\\\newtheorem{proposition}[theorem]{Proposition} \\n\\\\newtheorem{proposition}{Proposition} to get separate numbers for theorem and proposition etc.\\n\\n\\\\theoremstyle{thmstyletwo}\\n\\\\newtheorem{example}{Example}\\n\\\\newtheorem{remark}{Remark}\\n\\n\\\\theoremstyle{thmstylethree}\\n\\\\newtheorem{definition}{Definition}\\n\\n\\\\newtheorem{lemma}[theorem]{Lemma}\\n\\n\\\\newcommand{{\\\\logiclearner}}{\\\\textsc{LogicLearner}}\\n\\n\\\\raggedbottom\\n\\\\unnumbered uncomment this for unnumbered level heads\\n\\n\\\\begin{document}\\n\\n\\\\title[Article Title]{LogicLearner: A Tool for the Guided Practice of Propositional Logic Proofs}\\n\\n\\\\author*[1]{\\\\fnm{Amogh} \\\\sur{Inamdar}}\\\\email{amogh.inamdar@columbia.edu}\\n\\n\\\\author[1,3]{\\\\fnm{Uzay} \\\\sur{Macar}}\\\\email{uzay@aiphabet.org}\\n\\n\\\\author[1]{\\\\fnm{Michel} \\\\sur{Vazirani}}\\\\email{mvv2114@columbia.edu}\\n\\n\\\\author[2]{\\\\fnm{Michael} \\\\sur{Tarnow}}\\\\email{m.tarnow@columbia.edu}\\n\\n\\\\author[2]{\\\\fnm{Zarina} \\\\sur{Mustapha}}\\\\email{zarina@columbia.edu}\\n\\n\\\\author[2]{\\\\fnm{Natalia} \\\\sur{Dittren}}\\\\email{nd2664@columbia.edu}\\n\\n\\\\author[2]{\\\\fnm{Sam} \\\\sur{Sadeh}}\\\\email{ss6316@columbia.edu}\\n\\n\\\\author[1]{\\\\fnm{Nakul} \\\\sur{Verma}}\\\\email{verma@cs.columbia.edu}\\n\\n\\\\author[1,3]{\\\\fnm{Ansaf} \\\\sur{Salleb-Aouissi}}\\\\email{ansaf@cs.columbia.edu}\\n\\n\\\\affil[1]{\\\\orgdiv{Computer Science}, \\\\orgname{Columbia University}, \\\\country{United States}}   , \\\\orgaddress{\\\\street{500 W 120 St.}, \\\\city{New York City}, \\\\postcode{10027}, \\\\state{New York}, \\\\country{United States}}}\\n\\n\\\\affil[2]{\\\\orgdiv{Center for Teaching and Learning}, \\\\orgname{Columbia University}, \\\\country{United States}} , \\\\orgaddress{\\\\street{Lewisohn Hall, 2970 Broadway}, \\\\city{New York City}, \\\\postcode{10027}, \\\\state{New York}, \\\\country{United States}}}\\n\\n\\\\affil[3]{\\\\orgdiv{}\\\\orgname{Aiphabet, Inc.}, \\\\country{United States}} , \\\\orgaddress{\\\\street{}, \\\\city{}, \\\\postcode{}, \\\\state{}, \\\\country{}}}\\n\\n\\\\abstract{The study of propositional logic---fundamental to the theory of computing---is a cornerstone of the undergraduate computer science curriculum. Learning to solve logical proofs requires repeated guided practice, but undergraduate students often lack access to on-demand tutoring in a judgment-free environment. In this work, we highlight the need for guided practice tools in undergraduate mathematics education and outline the desiderata of an effective practice tool. We accordingly develop {\\\\logiclearner}\\\\footnote{https://logiclearner.ctl.columbia.edu/}, a web application for guided logic proof practice. {\\\\logiclearner} consists of an interface to attempt logic proofs step-by-step and an automated proof solver to generate solutions on the fly, allowing users to request guidance as needed. We pilot {\\\\logiclearner} as a practice tool in two semesters of an undergraduate discrete mathematics course and receive strongly positive feedback for usability and pedagogical value in student surveys. To the best of our knowledge, {\\\\logiclearner} is the only learning tool that provides an end-to-end practice environment for logic proofs with immediate, judgment-free feedback.}\\n\\n\\\\keywords{Logic, Proof, Education, Mathematics, Artificial Intelligence, Pedagogy}\\n\\n\\\\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}\\n\\n\\\\maketitle\\n\\n\\\\section{Introduction}\\\\label{sec:intro}\\n\\n\\\\begin{figure}[t!]\\n\\\\centering\\n\\\\includegraphics[width\\\\textwidth]{images/introFigDemo.jpg}\\n\\\\caption{{\\\\logiclearner} is a holistic environment for guided logic proof practice.}\\\\label{fig:intro}\\n\\\\end{figure}\\n\\nThe ability to think critically and reason quantitatively is an important skill for all Computer Science (CS) students. CS students generally acquire such quantitative problem-solving skills in foundational undergraduate proof-based CS courses such as discrete mathematics. However, learning mathematics can be difficult. Students tend to find dealing with proofs challenging, and can seldom construct and communicate a long sequence of logical arguments effortlessly. Students often skip proof steps, leading to logical leaps or incorrect conclusions in their arguments. In order to promote student learning, effective course instructors identify gaps in student understanding and provide timely, high-quality feedback. Student who receive such feedback are able to build confidence, master the material, and develop valuable quantitative reasoning skills \\\\citep{evans2013making}. However, a common issue across educational institutions and grading platforms is that instructors lack the resources to provide timely and informative feedback that promotes student learning. Once the time comes for formal assessments of learning (such as graded homeworks and exams), it is usually too late for the student to master the primary objectives of such courses. This issue is exacerbated in undergraduate-level proof-based courses such as discrete mathematics. There is a wide range in the mathematical maturity of students, and feedback on assignments and quizzes is usually received after several weeks. Additionally, per our exploration of the state-of-the-art, there are currently no off-the-shelf automated grading systems for student-level mathematical proofs to speed up the feedback process. This lack of timely feedback, coupled with a general level of student anxiety in dealing with proofs, translates directly into decreased student performance---our study of exam results in an undergraduate discrete mathematics course found that exam performance was \\\\emph{10\\\\ lower on proof questions} than other types of questions.\\n\\nInteractive practice in undergraduate-level education is often limited to periodic tutorial sessions with instructors or teaching assistants (TAs). While TAs can do an excellent job in helping students, their limited availability creates a challenge in large undergraduate classes. Additionally, students may fear judgment from peers and instructors over the quality and frequency of their doubts in group settings, leading to reticence in seeking help. To alleviate these shortcomings, we propose the development and use of web-based tools for the guided practice of quantitative reasoning problems. We identify the following desiderata for such practice tools.\\n\\\\begin{itemize}\\n   \\\\item \\\\textbf{An effective practice tool is accessible and user-friendly.} Even an excellent practice tool is of little use to students who cannot access it. Tools that are difficult to use or hidden behind paywalls tend to isolate students with limited resources, despite such students being likely to benefit the most from them. \\n   \\\\item \\\\textbf{A practice tool must be a source of truth.} As a trusted authority on the subject matter, a teaching aid that provides incorrect information is especially detrimental to the learning process. Ideally, such tools would allow external experts to independently verify the correctness of these tools. We encourage open-source development as a strategy to ensure that pedagogical tools are both accessible and transparent.\\n   \\\\item \\\\textbf{An effective practice tool is non-judgmental.} While expert feedback is invaluable, a student who fears judgement from peers and instructors will likely be more willing to explore ideas in a self-guided setting. This is why a good practice tool should provide constructive feedback and encouragement to students. Overly critical feedback or high-stakes settings may discourage use and affect the learning process.\\n   \\\\item \\\\textbf{An effective practice tool provides only relevant guidance.} Effective instructors know that feedback must be catered to the problem setting and the type of mistake made. A tool that inundates the student with options or provides very basic feedback (e.g., only grading an answer as right or wrong) is likely to be less effective than one with specific, cogent, and relevant guidance.\\n\\\\end{itemize}\\n\\nWith these desiderata in mind, we tackle the challenge of improving the pedagogical experience of learning to solve logic proofs. We develop {\\\\logiclearner} (Figure \\\\ref{fig:intro}), an open-source web application that provides a holistic environment for logic proof practice. {\\\\logiclearner} minimizes the feedback turnaround time for logic proof practice, leading to greater student engagement with course materials related to theorems and proofs. Using tools from AI and machine learning, {\\\\logiclearner} validates student solutions at each steps, identifies mistakes, and solves the problem on-the-fly to provide personalized hints in a judgment-free environment. {\\\\logiclearner} is intended to supplement, not replace, the invaluable efforts of teaching assistants and tutorial sessions in a course that covers propositional logic. {\\\\logiclearner} is a free and open-source project, managed by the Center for Teaching and Learning (CTL) at Columbia University. Visuals of the application are presented in Appendix \\\\ref{sec:apxVisual}. The source code is available at \\\\url{https://github.com/ccnmtl/logiclearnertools}. \\n\\\\\\\\\\n\\nOur contributions in this work can be summarized as follows:\\n\\\\begin{itemize}\\n   \\\\item We identify a gap in the availability of on-demand, judgment-free guidance in undergraduate computer science curricula. \\n   \\\\item We highlight the need for practice tools that provide instantaneous, high-quality feedback, and identify the desiderata of these tools for pedagogical impact.\\n   \\\\item We develop {\\\\logiclearner}, a web-based application for logic proof practice with instant guidance that is mathematically sound and judgment free. At the time of writing, {\\\\logiclearner} is the only application for the end-to-end practice of logic proofs with guidance.\\n   \\\\item Through user studies across two semesters of an undergraduate discrete mathematics course, we show that {\\\\logiclearner} reduces student anxiety and improves their confidence in tackling logic proofs.\\n   \\\\item We assess the drawbacks of AI question-answering tools (a popular alternative among students for pedagogical guidance) and show that {\\\\logiclearner} provides superior pedagogical value over such tools in multiple ways.\\n\\\\end{itemize}\\n\\n\\\\section{Related Work}\\\\label{sec:related}\\n\\nOur work combines the advances in automated problem solving and in software for mathematics/logic education. In this section, we analyze the state of the art in these areas though a pedagogical lens.\\n\\n\\\\subsection{Automated logic problem solvers}\\\\label{subsec:rel1}\\n\\nTo compensate for a lack of on-demand, judgment-free pedagogical guidance, students are increasingly turning to AI question-answering systems powered by Large Language Models (LLMs), like ChatGPT\\\\footnote{OpenAI (2023). ChatGPT (Mar 14 version); GPT 3.5 backend. \\\\url{https://openai.com/blog/chatgpt}}, for pedagogical feedback. LLMs \\\\cite{achiam2023gpt, touvron2023llama, vicuna2023} are neural networks with billions of parameters that are trained for text generation on internet-scale datasets. With previously unseen prowess on natural language tasks, LLMs are now the basis of applications across domains ranging from medicine to finance \\\\cite{thirunavukarasu2023large, singhal2022large, cui2023chatlaw, webersinke2022climatebert, wu2023bloomberggpt}. However, using an LLM-based application as a learning tool for mathematics has several drawbacks. LLMs require the use of specialized prompting techniques to score well on mathematical reasoning tasks. Chain-of-Thought prompting \\\\cite{wei2022chain} formulates step-by-step LLM prompts to emulate human reasoning, improving on question-only prompts for simple arithmetic and logic problems. Extensions such as Graph of Thoughts \\\\cite{besta2023graph} and MathPrompter \\\\cite{imani2023mathprompter} develop increasingly complex input- and early-layer techniques for sophisticated users. Drori et al. \\\\cite{drori2022neural} leverage the OpenAI Codex LLM \\\\cite{chen2021evaluating} to produce impressive question-answering performance on undergraduate mathematics courses, but explicitly state that the model cannot solve ``questions with solutions that require proofs''. Datasets that have been developed to evaluate reasoning ability \\\\citep{ontanon2022logicinference, cobbe2021training, hendrycks2021measuring} primarily consist of problems with simple, few-step solutions. Additionally, LLMs have been shown to hallucinate factual information \\\\cite{huang2023survey} and are susceptible to prompt manipulation \\\\cite{cohen2024comes}. Hence, students relying on LLMs for assistance with proofs are at risk of receiving \\\\emph{confident but incorrect} answers that stunt the learning process.\\n\\nAnother focus in the literature is on tool-assisted human problem solving. Automated theorem-provers such as Isabelle \\\\cite{nipkow2002isabelle} have been used to develop human-friendly representations of logic problems \\\\cite{Villadsen_2022} and other mathematical structures \\\\cite{fuenmayor2022formalising}. Other tools---such as the popular mathematical software Wolfram \\\\cite{weisstein}---simplify or evaluate Boolean expressions using truth tables, but are not directly useful for the development of proof-solving skill.\\n\\nWhile these works make impressive strides in AI mathematical reasoning, they are not yet advanced enough to solve problems requiring several steps of reasoning. Most importantly, these systems \\\\emph{are not designed for pedagogical use}. They do not provide interfaces that benefit problem-solving practice and produce full answers, preventing students from reasoning by themselves with only subtle hints when needed. Using such systems as practice tools compounds students' frustrations with the learning process. To address this important gap and facilitate practice, we develop {\\\\logiclearner} as purpose-built learning tool that is both mathematically sound and easy to use. \\n\\n\\\\subsection{The pedagogy of propositional logic}\\\\label{subsec:rel2}\\n\\nPropositional logic (and discrete mathematics at large) are important topics of study---not only for computer science students, but also for general mathematical maturity \\\\citep{sandefur2022teaching, greefrath2022mathematical}.\\nThe literature largely asserts that these are challenging topics to learn. Multiple prior works have attempted to model student behavior in learning to solve proofs. Dawkins and Roh \\\\cite{dawkins2022aspects} posit that students have a twofold process to understanding a logical property, first generating examples and then evaluating which examples satisfy the property. This generate-and-test process is analogous to many fundamental computer search algorithms. EvoLogic \\\\cite{galafassi2020evologic, galafassi2022evologic} takes an agent-based approach to model student responses to 10 simple logic exercises. These works do not attempt to improve the learning process, as we do in this paper. \\n\\nLogical concepts also form the basis of a few learning games. \\\\textit{Proplog} and \\\\textit{Syllog} \\\\cite{ohrstrom2019teaching} require players to evaluate the logical soundness of propositions and syllogisms respectively. TrueBiters \\\\cite{de2019truebiters} aims to gamify the process of learning truth tables by representing bitwise operators as monsters that `eat' bits and return the results, with the goal of reducing a bit string to a specified target bit. We are not aware of any games or practice tools that directly aim to build proof-solving skills, as {\\\\logiclearner} does.\\n\\n\\\\section{Methods}\\\\label{sec:method}\\n\\n{\\\\logiclearner} is a web application with a gamified interface that is purpose-built for learning, backed by a parser to process Boolean expressions and an AI proof solver to provide hints when requested. In this section, we describe the working of each of its components in detail. Starting with the user interface, each subsequent sub-section describes a module that is increasingly abstracted away from the student. We reiterate that {\\\\logiclearner} is open-source and encourage community contributions to its improvement.\\n\\n\\\\subsection{User Experience}\\\\label{subsec:m1}\\n\\nAs described in Section \\\\ref{sec:intro}, ease of use is essential to a good pedagogical tool. We design {\\\\logiclearner} as a web application with an attractive but simple interface that allows students to focus on solving the problems. For first-time users, {\\\\logiclearner} contains a tutorial section with 6 simple steps describing usability. Students then attempt logic problems of varying difficulty. As they go through the learning process, {\\\\logiclearner} tracks their progress and enables them to review and re-attempt previously attempted proofs. As a web application, {\\\\logiclearner} \\\\textbf{works well on both computers and mobile devices}, allowing for practice on the go.\\n\\n\\\\subsubsection{User Interface}\\\\label{subsubsec:m11}\\n\\n\\\\begin{figure}[ht!]\\n   \\\\centering\\n   \\\\includegraphics[width0.8\\\\linewidth]{images/methodsUserFlow.jpg}\\n   \\\\caption{The user flow diagram of {\\\\logiclearner} shows that students have a linear decision making process, enabling ease of use.}\\\\label{fig:uflow}\\n\\\\end{figure}\\n\\nAs seen in Figure \\\\ref{fig:uflow}, students can choose the difficulty of the proof they will attempt. As a student attempts a proof, {\\\\logiclearner} finds solutions from the steps they have already taken and produces hints when requested to unblock their progress. Once a solution has been found (or requested), the student is able to review the full proof and attempt the next question if desired. Detailed screenshots of the application, including the tutorial section, can be found in Appendix \\\\ref{sec:apxVisual}.\\n\\nStudents use buttons to navigate the application. At each proof step, they select a logic rule to apply via a drop-down menu and enter the corresponding statement as free-form text. If incorrect, students are not explained their mistakes to allow them to reason again. Upon requesting a hint, students are provided with the correct logic rule to apply. Another hint request leads to the correct expression for the next step being provided. We use this two-step hints process to allow students to attempt problems with only partial information, just as an instructor would nudge a student in the right direction. To prevent student frustration when stuck, we also allow them to view the full solution without penalty if requested. Students are able to reset their progress and re-attempt the questions \\\\emph{ad infinitum}. The user interface of {\\\\logiclearner} is implemented as a Django\\\\footnote{\\\\url{https://www.djangoproject.com/}} web server in Python.\\n\\n\\\\subsubsection{Database}\\\\label{subsubsec:m12}\\n\\n{\\\\logiclearner} maintains a PostgreSQL\\\\footnote{\\\\url{https://www.postgresql.org/}} database as the back end of the user experience. Students do not need to explicitly log in to the application, but their progress is still preserved across sessions by tracking and storing site accesses. This provides a seamless user experience where students are not prompted to repeat proofs they have already attempted unless explicitly requested. {\\\\logiclearner} is a fully open source application, and source code is available in public repositories on GitHub\\\\footnote{UX: \\\\url{https://github.com/ccnmtl/logiclearner}, Back-end: \\\\url{https://github.com/ccnmtl/logiclearnertools}}. The application server and database are actively managed by Center for Teaching and Learning at Columbia University.\\n\\n\\n\\\\subsection{Application Back-end and Proof Solving}\\\\label{subsec:m2}\\n\\n{\\\\logiclearner} performs several computations upon receiving user input. When a student provides a logic rule and an expression as the next step of a proof, the application must \\n\\\\begin{itemize}\\n   \\\\item validate the syntax of the input expression,\\n   \\\\item check for logical entailment from the current state, and\\n   \\\\item find the subsequent steps of a full solution for hints.\\n\\\\end{itemize}\\n\\nThis process---described as a data flow diagram in Figure \\\\ref{fig:methods}---requires a computational model of Boolean expressions and logical proofs. We model Boolean expressions with a Context-Free Grammar (CFG) and describe the logic proof as a graph search problem between expressions. We then use AI search techniques to solve these proofs and provide hints to students in real time. Below, we describe this `business logic' of {\\\\logiclearner} in detail.\\n\\n\\\\begin{figure*}[ht!]\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/LLmethodsFig.jpg}\\n   \\\\caption{{\\\\logiclearner} generates a `frontier' of all possible next steps from the current expression, and subsequently a full proof for hints. User input is checked for syntax and membership in the frontier. The user receives feedback on incorrect inputs and can request a hint at any time.}\\\\label{fig:methods}\\n\\\\end{figure*}\\n\\nWe use the following notation in this and subsequent sections: $X   \\\\{x, y, z,\\\\ldots\\\\}$ is a set of Boolean variables. $E$ denotes the set containing every Boolean expression $e$ formed by any number of variables from $X$ and the constants $\\\\{T, F\\\\}$, which denote \\\\textbf{True} and \\\\textbf{False} respectively. $R$ denotes the set of logic rules, which are transformations according to the named logical equivalences between expressions (Idempotence, Associativity, etc., Table \\\\ref{tab:rules}). Every $r\\\\in R$ transforms expression $e$ into one of a set of expressions $\\\\{e_1, e_2,\\\\ldots\\\\}$, where each $e_i$ is formed by applying $r$ to a different position in $e$. If $r$ cannot be applied to $e$ then $r(e)\\\\emptyset$. To fully represent an equivalence relation, we define the inverse of a rule $r$ to be $r^{-1}\\\\in R$ such that $e\\\\in r^{-1}\\\\circ r(e)$. For example, we can apply Idempotence ($r$) to the expression $p\\\\lor q \\\\lor q$ to derive $p\\\\lor q$, and likewise apply Idempotence ($r^{-1}$) to $p\\\\lor q$ to derive $p\\\\lor q \\\\lor q$. We phrase our problems as `\\\\textit{Prove that \\\\textbf{source} is logically equivalent to \\\\textbf{target}.}', where \\\\textbf{source} and \\\\textbf{target} are Boolean expressions. When \\\\textbf{target} is \\\\textbf{True}/\\\\textbf{False}, the question is phrased `\\\\textit{---is a Tautology}'/`\\\\textit{---is a Fallacy}' respectively. Every problem is guaranteed to have a solution.\\n\\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{The rules of propositional logic}\\\\label{tab:rules}\\n\\\\begin{tabular}{@{}lll@{}}\\n\\\\toprule\\nRule & Equivalence 1 & Equivalence 2   \\\\\\\\\\n\\\\midrule\\nAbsorption   &   $p \\\\lor (p\\\\land q) \\\\equiv p$ & $p \\\\land (p\\\\lor q) \\\\equiv p$ \\\\\\\\\\nAssociativity   &   $p \\\\lor (q\\\\lor r) \\\\equiv (p \\\\lor q) \\\\lor r$ & $p \\\\land (q\\\\land r) \\\\equiv (p \\\\land q) \\\\land r$ \\\\\\\\\\nCommutativity   & $p \\\\lor q \\\\equiv q \\\\lor p$ & $p \\\\land q \\\\equiv q \\\\land p$ \\\\\\\\\\nDe Morgan's Law & $\\\\neg(p \\\\lor q) \\\\equiv \\\\neg p\\\\land \\\\neg q$ & $\\\\neg(p \\\\land q) \\\\equiv \\\\neg p\\\\lor \\\\neg q$ \\\\\\\\\\nDistributivity   & $p \\\\lor (q\\\\land r) \\\\equiv (p \\\\lor q) \\\\land (p \\\\lor r)$ & $p \\\\land (q\\\\lor r) \\\\equiv (p \\\\land q) \\\\lor (p \\\\land r)$ \\\\\\\\\\n & $p \\\\lor (q\\\\lor r) \\\\equiv (p \\\\lor q) \\\\lor (p \\\\lor r)$ & $p \\\\land (q\\\\land r) \\\\equiv (p \\\\land q) \\\\land (p \\\\land r)$ \\\\\\\\\\nDomination   &   $p \\\\lor T \\\\equiv T$ & $p \\\\land F \\\\equiv F$ \\\\\\\\\\nIdempotence   &   $p \\\\lor p \\\\equiv p$ & $p \\\\land p \\\\equiv p$ \\\\\\\\\\nIdentity   & $p \\\\lor F \\\\equiv p$ & $p \\\\land T \\\\equiv p$   \\\\\\\\\\nIff as Implication   & $p \\\\leftrightarrow q \\\\equiv (p \\\\to q) \\\\land (q \\\\to p)$ & -- \\\\\\\\\\nImplication as Disjunction &   $p \\\\to q \\\\equiv \\\\neg p \\\\lor q$ & -- \\\\\\\\\\nNegation   & $p \\\\lor \\\\neg p \\\\equiv T$ & $p \\\\land \\\\neg p \\\\equiv F$ \\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\n\\\\subsubsection{Validating user input}\\\\label{subsubsec:m21}\\n\\nTo take a step towards the solution from their current state $e_t$, a user selects a logic rule $r$ and inputs a Boolean expression $e_{t+1}$. The input is `valid' if it is a syntactically correct Boolean expression that is entailed by the selected rule of logic, i.e., $e_{t+1}\\\\in r(e_t)$. \\n\\nWe use a left-recursive context-free grammar for Boolean expressions and use the Lark\\\\footnote{\\\\url{https://pypi.org/project/lark-parser}} parser-generator to validate user syntax. The grammar accounts for variations in token representation (such as `1', `T', or `True' for the \\\\textit{true} truth value) and requires a maximum look-ahead of 1, enabling the use of Lark's fast and efficient LALR(1) parser. Each parse tree node is annotated the start and end positions of its token span.\\n\\n\\\\subsubsection{Modeling the logic proof}\\\\label{subsubsec:m23}\\n\\nSolving logic problems is challenging to students who first encounter them. To guide students who are stuck, we develop a two-level hints feature that can be triggered to provide the correct rule and subsequently the correct expression for the next step. To do this, {\\\\logiclearner} must find a path to the target expression $e_t$ from the user's current expression $e_c$. The size of the Boolean state space and unpredictable user input make pre-computed or brute-force solutions infeasible. Here, we model logic proofs as a graph search problem and use A* search \\\\cite{Hart1968} to dynamically find solutions. We define some properties and assumptions of our search graph below: \\n\\\\begin{enumerate}[1.]\\n\\\\item Every node represents a Boolean expression $e$. For a given proof, the student must derive expression $e_t$ starting from expression $e_s$.\\n\\\\item Every edge represents a transition between nodes brought about by applying a logic rule $r_i$. That is, $\\\\exists$ edge $(e_p,e_q)$ $\\\\forall i, p, q$ s.t. $r_i(e_p) \\\\supseteq e_q$.\\n\\\\item Since every rule $r_i\\\\in R$ is an equivalence relation, every edge $(e_p, e_q)$ is bidirectional.\\n\\\\item We only focus on `solvable' problems of the type \\\\textit{`Prove that $e_s$ is logically equivalent to $e_t$'}, so that at least one path (proof) from $e_s$ to $e_t$ is known to exist.\\n\\\\end{enumerate}\\n\\nThese properties ensure that every attempt of a {\\\\logiclearner} problem always has a path to success.\\n\\n\\\\begin{lemma}\\\\label{lemma:1}\\n In every {\\\\logiclearner} problem, the target expression $e_t$ is always reachable from any expression $e_c$ that a student derives from the premise $e_s$.   \\n\\\\end{lemma}\\n\\\\begin{proof}\\nSuppose the student applied logic rules $r_1, r_2,\\\\ldots,r_n$ in sequence to start expression $e_s$ to reach $e_c$ (i.e., $e_c\\\\in r_n\\\\circ r_{n-1}\\\\circ\\\\cdots\\\\circ r_1(e_s)$). Property 3 lets us apply their corresponding inverses to $e_c$ to obtain $e_s$ (i.e., $e_s\\\\in r^{-1}_1\\\\circ r^{-1}_2\\\\circ\\\\cdots\\\\circ r^{-1}_n(e_c)$). By Property 4, $e_t$ is reachable from $e_s$.\\n\\\\end{proof}\\n\\nThus, solving a logic proof is equivalent to searching a connected graph containing $e_s$ and $e_t$. Since rules like Idempotence and Absorption can be chained indefinitely, this graph has an infinite depth. Note that the method in our proof may not produce the shortest path from $e_c$ to $e_s$. Our AI search strategy is also motivated by the fact that hints that backtrack to a start state make for a poor user experience. \\n\\n\\n\\\\subsubsection{Finding a search frontier}\\\\label{subsubsec:m22}\\n\\nThe search frontier $F(e)$ of a Boolean expression $e$ is the set of all expressions obtained applying a logical equivalence to $e$, i.e., $F(e)   \\\\{e' | \\\\exists r\\\\in R: e'\\\\in r(e)\\\\}$. \\\\textbf{FRONTIER\\\\_GEN}, our efficient frontier-generation algorithm, uses Lark's \\\\textbf{Transformer} feature to traverse up the expression's annotated parse tree from leaves to root, replacing each node by the set of possible transforms at that node. This culminates at the root as a set of all potential next-steps via logical substitution. \\\\textbf{FRONTIER\\\\_GEN} visits each subtree of the parse tree exactly once and applies each logic rule to the token span at that root, keeping all valid transformations.\\n\\nA sub-expression of an expression $e$ is a contiguous span of $e$'s tokens that forms a syntactically correct expression, appearing as a sub-tree in $e$'s parse tree. The smallest sub-expression, a single literal (Boolean variable or constant), forms a leaf node in the parse tree. Thus, an expression $e$ containing $|e|$ literals ($\\\\le$ num. leaves) has at most $2|e|$ sub-expressions ($\\\\le$ num. subtrees). The number of logic rules $|R|$ is constant and we implement each rule in $\\\\mathcal{O}(1)$ operations. Hence, the size of the search frontier $F(e)$ is $\\\\mathcal{O}(|e|)$ and \\\\textbf{FRONTIER\\\\_GEN} finds this frontier in $\\\\mathcal{O}(|R|\\\\cdot|E|)\\\\mathcal{O}(|E|)$ operations, which is worst-case optimal.\\n\\n\\\\begin{algorithm}[hbt!]\\n\\\\caption{FRONTIER\\\\_GEN}\\\\label{alg:cap}\\n\\\\begin{algorithmic}\\n\\\\Require Annotated parse tree $T(e)$ of Boolean expression $e$, Rule set $R$\\n\\\\Ensure Search frontier $F(e)$\\n\\\\State $F \\\\gets \\\\emptyset$ \\n\\\\While{Transform(e)}   \\\\Comment{Lark Transformer iterates leaf-to-root}\\n   \\\\State $n \\\\gets \\\\verb|get_current_node|()$\\n   \\\\State $t_s,t_e \\\\gets \\\\verb|get_token_span|(n)$\\n   \\\\State $S_n \\\\gets \\\\verb|apply_logic_rules|(R, n)$   \\\\Comment{Set of next-step sub-expressions}\\n   \\\\While{$S_n \\\\neq \\\\emptyset$}\\n   \\\\State $s \\\\gets \\\\verb|get_element|(S_n)$\\n   \\\\State $F \\\\gets F\\\\bigcup \\\\verb|concat|(e[:t_s],s,e[t_e:])$   \\\\Comment{Sub. rule result into token span}\\n   \\\\State $S_n \\\\gets S_n - \\\\{s\\\\}$\\n   \\\\EndWhile\\n\\\\EndWhile\\n\\\\end{algorithmic}\\n\\\\end{algorithm}\\n\\n\\\\subsubsection{Solving proofs with A* Search}\\\\label{subsubsec:m24}\\n\\nThe A* graph search algorithm aims to find the lowest path between two nodes on a weighted graph using heuristic approximations of unknown path costs. A `consistent' heuristic---one that obeys the Triangle inequality---guarantees that A* search finds an optimal path between two nodes \\\\citep{Hart1968}. This makes it well-suited to solving logic proofs when structured as a Boolean graph search problem. However, we are not aware of an efficient heuristic that consistently estimates the semantic similarity (shortest path) between arbitrary Boolean expressions. Instead, we focus our efforts on a variety of heuristics that approximate the path length between expressions. \\n\\n\\\\begin{figure*}[ht!]\\n   \\\\centering\\n   \\\\begin{subfigure}[t]{0.8\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/userHintsDemo.jpg}\\n   \\\\caption{}\\n   \\\\end{subfigure}\\n   \\n   \\\\begin{subfigure}[t]{0.9\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/LLtreeFig.jpg}\\n   \\\\caption{}\\n   \\\\end{subfigure}\\n   \\\\caption{(a) A users attempts a logic proof on {\\\\logiclearner} and requests hints when stuck. (b) Search trees are calculated at every step of the proof attempt. The trees for the two requested hints in part (a) are as depicted here as numbered.}\\\\label{fig:proofTrees}\\n\\\\end{figure*}\\n\\nAs a first approximation, we use a weighted linear combination of measures of similarity between string representations of Boolean expressions (Table \\\\ref{tab:heur}). Since logic rules are not applied with uniform probability, we also consider the rule used to generate a search node. Since computing a gradient over our objective (number of questions solved) is infeasible, we optimize this combination with a genetic algorithm \\\\cite{holland1973genetic} which `evolves' a population of weighted combinations of heuristics (candidate solutions). Candidate fitness is evaluated on the training problem set and the next generation is chosen via fitness-proportionate selection with elitism. The weight ranges, crossover and mutation probabilities, and degree of elitism are determined empirically. Our genetic algorithm produces a marked improvement in performance compared to a randomly weighted ensemble, but we do not argue that the resulting heuristic is consistent.\\n\\nIn production, we implement a time-bound, depth-limited A* path-finding algorithm that searches the Boolean graph space for the target node beginning from the start node. We use the above ensemble, optimized on our question bank, as our heuristic of choice. Since {\\\\logiclearner} provides real-time hints, we constrain its search time to a few seconds and return the path to the lowest-cost node if the solution isn't found. Limiting the search depth compensates for inconsistent heuristics by preventing meandering search paths. As an illustration, stylized search trees corresponding to hints requested on the interface are shown in Figure \\\\ref{fig:proofTrees}. Details about our training methodology, hyperparameter search, and production heuristic can be found in Appendix \\\\ref{sec:apxAblate}.\\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{A* search heuristics}\\\\label{tab:heur}\\n\\\\begin{tabular}{@{}ll@{}}\\n\\\\toprule\\nHeuristic & Description (computed on a transform $(e_1, r, e_2)$ where $e_2\\\\in r(e_1)$) \\\\\\\\\\n\\\\midrule\\nUnitary function & Returns 1 regardless of input \\\\\\\\\\nLevenshtein distance & The number of single-character edits required to transform $e_1$ to $e_2$ \\\\\\\\\\nVariable mismatch & The number of variables that appear either only in $e_1$ or only in $e_2$ \\\\\\\\\\nLength difference & Absolute difference in string length between $e_1$ and $e_2$ \\\\\\\\\\nRule weight (for each $r\\\\in R$) & A prior on rule $r$ proportional to its frequency in training data \\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\n\\\\subsection{Extensions: question generation and neural embeddings}\\\\label{subsec:m3}\\n\\nWhile our production heuristic is efficient and effective, it approximates the complex semantics of Boolean algebra with only a few surface-level comparisons. A natural extension to this is to integrate \\\\emph{semantic} information into our heuristic ensemble by modeling the space of Boolean expressions in a way that allows for efficient approximations of similarity. As a proof-of-concept, we develop a neural network to learn the semantics of Boolean expressions for proof-solving though auxiliary tasks.\\n\\n\\nNeural networks are powerful machine learning models that are able learn complex relationships present in large datasets. Since these models require more data than can be manually curated, we develop \\\\textbf{PROOF\\\\_GEN} (Algorithm \\\\ref{alg:qgen}) to automatically generate logic proofs. Starting from a given target expression $e_t$, \\\\textbf{PROOF\\\\_GEN} generates a search frontier and randomly selects an expression $e_{t-1}$ from it. This procedure is repeated to obtain $e_{t-2},\\\\ldots,e_{t-k+1}e_s$. Reversed, this is now a $k$-step proof $e_s\\\\rightarrow\\\\cdots\\\\rightarrow e_t$. This also provides us with a way to expand {\\\\logiclearner}'s question bank---\\\\textbf{PROOF\\\\_GEN} does not guarantee elegance, but cherry-picking interesting proofs from its output is an easier task than manually composing novel proofs.\\n\\n\\\\begin{algorithm}[hbt!]\\n\\\\caption{PROOF\\\\_GEN}\\\\label{alg:qgen}\\n\\\\begin{algorithmic}\\n\\\\Require Boolean expression $e$, Number of proof steps $N$\\n\\\\Ensure Logic proof $P(e',e)$\\n\\\\State $P \\\\gets [\\\\ ]$ \\n\\\\State $i \\\\gets 0$\\n\\\\While{$i < N$}\\n   \\\\State $F \\\\gets \\\\verb|FRONTIER_GEN|(e)$\\n   \\\\State $e' \\\\gets \\\\verb|random_select|(F)$\\n   \\\\State $P \\\\gets \\\\verb|append|(P, e')$\\n\\\\EndWhile\\n\\\\State $P \\\\gets \\\\verb|reverse|(P)$\\n\\\\end{algorithmic}\\n\\\\end{algorithm}\\n\\nWe use \\\\textbf{PROOF\\\\_GEN} generate large amounts of data for learning Boolean semantics, and train encoder-decoder neural networks on two tasks: rule prediction and proof length prediction. To evaluate the potential of neural networks to embed Boolean expressions in metric space while preserving semantic similarity, we use the cosine similarity between encoded Boolean expressions as a heuristic to A* search on our human-curated question bank. We also evaluate an untrained baseline and a language model pre-trained on a multilingual text corpus. Details of these experiments and their results are shown in Appendix \\\\ref{sec:apxNN}. \\\\textbf{PROOF\\\\_GEN} does not have graphical interface at the time of writing, but can be accessed via the \\\\verb|logictools| library API.\\n\\n\\\\section{Results}\\\\label{sec:result}\\n\\nTo evaluate its effectiveness, we piloted {\\\\logiclearner} as an optional practice tool over two semesters of the undergraduate discrete mathematics class (COMS 3203) at Columbia University. We surveyed students on their confidence in solving logic proofs before and after covering the propositional logic unit of the course. We also recorded their assessment of {\\\\logiclearner}'s features after learning. We present these results in Section \\\\ref{subsec:r1}. In Section \\\\ref{subsec:r2}, we compare the performance of {\\\\logiclearner} with ChatGPT, an online question-answering tool powered by a Large Language Model (refer to Section \\\\ref{subsec:rel1} for an overview). Analyzing ChatGPT's most common failure modes, we show that LLMs are not currently suited for proof solving and pedagogy. We also present technical results on the performance of {\\\\logiclearner}'s AI proof solver and justify our choice of A* heuristic.\\n\\n\\\\subsection{User Study: {\\\\logiclearner} as a practice tool}\\\\label{subsec:r1}\\n\\nTo evaluate {\\\\logiclearner} as a tool for guided practice, we conducted a user study and analyzed feedback from the students of COMS 3203, the undergraduate discrete mathematics course at Columbia University, across two semesters. Before and after the propositional logic unit, we surveyed students on their confidence in solving logic proofs in various scenarios. Confidence was self-assessed along the 7-point Likert scale \\\\cite{likert1932technique}, ranging from `Not at all confident' to `Completely confident'. Scenarios ranged from \\\\emph{writing a proof with assistance} to \\\\emph{writing a proof in an exam setting}. As seen in Figure \\\\ref{fig:aconf}, student confidence in \\\\emph{writing a logical proof completely and correctly} improves dramatically after lessons in propositional logic and practice with {\\\\logiclearner}. In particular, students gained confidence in solving proofs in an exam setting, the hardest scenario in our survey. This was also reflected in the course outcomes---adjusted for overall difficulty, we observed that the mean student performance on logic proof questions in the COMS 3203 exams \\\\textbf{increased by roughly 5\\\\} after the introduction of LogicLearner as an optional practice tool. This is a notable improvement as the class sizes are large (300+) and overall performance on these exams is around 80\\\\ on average, making it unlikely that large changes in results can be observed. \\n\\n\\\\begin{figure}[ht!]\\n\\\\includegraphics[width\\\\textwidth]{images/aggregated_likert.png}\\n\\\\caption{Confidence scores for solving logic problems under various scenarios, aggregated across two semesters of a discrete mathematics course}\\\\label{fig:aconf}\\n\\\\end{figure}\\n\\nPost-learning, students were prompted to rate the user-friendliness of {\\\\logiclearner} on a scale of 1 to 5 and answer long-form questions on the application's performance. Figure \\\\ref{fig:afeat} shows students rate {\\\\logiclearner}'s highly, with every feature scoring a mean of approximately 4 out of 5 with a standard deviation of less than 1. We analyze the student responses to long-form questions for positive/negative intent using a sentiment classification pipeline from Hugging Face\\\\footnote{\\\\url{https://huggingface.co/}} (Figure \\\\ref{fig:asentiment}). Two of the questions are negative leading questions (`Missing features/improvements' and `Ease of use/challenges in usage'), and negative responses are expected. Negative responses mostly comprise of suggestions for improvement and positive responses indicate where students found nothing lacking. The two open-ended questions (`(Would you) Recommend to others' and `(Would you) Use it to practice') show overwhelmingly positive responses, showing that respondents found {\\\\logiclearner} to be a useful tool for logic practice. The phrasing of the long-form questions and full respondent statistics can be found in Appendix \\\\ref{sec:apxSurvey}.\\n\\n\\\\begin{figure}[ht!]\\n\\\\centering\\n\\\\begin{minipage}{.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width0.9\\\\linewidth]{images/aggregated_feature.png}\\n   \\\\caption{Aggregated mean feature ratings}\\n   \\\\label{fig:afeat}\\n\\\\end{minipage}\\n\\\\begin{minipage}{.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width0.8\\\\linewidth]{images/aggregate_sentiment.png}\\n   \\\\caption{Sentiment to long-form questions}\\n   \\\\label{fig:asentiment}\\n\\\\end{minipage}\\n\\\\end{figure}\\n\\nFor brevity, we only present the aggregate results of the surveys in these figures. Individual surveys (Appendix \\\\ref{sec:apxSurvey}) are highly consistent, emphasizing the significance of our results. \\n\\n\\\\subsection{Performance and AI results}\\\\label{subsec:r2}\\n\\nWe use a genetic algorithm to optimize a weighted linear ensemble of heuristics (Table \\\\ref{tab:heur}) for {\\\\logiclearner} to solve logic proofs with A* search. The details of the optimization and ablation studies involved in our heuristic selection can be found in Appendix \\\\ref{sec:apxAblate}. We compare {\\\\logiclearner} with ChatGPT, a publicly available conversational interface to the GPT series of Large Language Models (LLMs) trained for text generation on an internet-scale dataset. ChatGPT is the most easily accessible AI question-answering tool to end users, and is increasingly being used in an academic setting. However, ChatGPT is not designed as a pedagogical tool, and the user experience varies significantly from that of {\\\\logiclearner}. We find that it is also unable to produce mathematically consistent answers, and this \\\\emph{confident but incorrect} behavior could lead to maladaptation from students learning incorrect information. Figure \\\\ref{fig:flowCompare} describes one potential workflow of a student using ChatGPT for proof solving. In contrast with the {\\\\logiclearner} user flow (Figure \\\\ref{fig:uflow}), even when ChatGPT provides correct answers, it will not gradually nudge the user to a solution or take the user's progress into account unless prompted. Re-attempting and review are also not possible with such LLM-based QA systems.\\n\\n\\\\begin{figure}[ht!]\\n\\\\centering\\n\\\\includegraphics[width0.8\\\\textwidth]{images/chatGptUserFlow.jpg}\\n\\\\caption{A potential student workflow with ChatGPT for proof solving.}\\\\label{fig:flowCompare}\\n\\\\end{figure}\\n\\n\\\\subsubsection{Proof-solving performance}\\\\label{subsubsec:r21}\\n\\nWe evaluate {\\\\logiclearner} on the application's human-curated question bank of 33 logic proofs, as well as two QUESTION\\\\_GEN (Algorithm \\\\ref{alg:qgen}) generated datasets with 198 3-step proofs and 66 10-step proofs respectively. We also train and evaluate a simple neural network heuristic as a \\\\emph{proof-of-concept} as described in Section \\\\ref{subsec:m3}, and detail our experiments in Appendix \\\\ref{sec:apxNN}. Due to resource constraints, we evaluate neural models (including ChatGPT) only on our human-curated dataset. We provide the following prompt to ChatGPT: \\\\emph{`Using the rules of propositional logic, prove that \\\\textbf{premise} is logically equivalent to \\\\textbf{target}. Make sure to explain each step.'}. This prompt is manually tuned, and encourages some Chain-of-Thought reasoning \\\\cite{wei2022chain}, though we do not claim to fully use this method. The LLM's responses are well-framed, but rarely correct. A student is likely not sophisticated enough to catch these technical mistakes, making ChatGPT infeasible as a learning aid for proof solving.\\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{AI performance on logic proof question banks.}\\\\label{tab:aiRes}\\n\\\\begin{tabular}{@{}llcccc@{}}\\n\\\\toprule\\nMethod & Heuristic & Eval time & \\\\multicolumn{3}{c}{Score on Dataset (questions)} \\\\\\\\\\n & & (seconds) & Curated & \\\\multicolumn{2}{c}{Generated} \\\\\\\\ \\n & & & (33 mixed) & (198 small) & (66 mixed) \\\\\\\\\\n\\\\midrule\\n\\\\multirow{2}{*}{{\\\\logiclearner}} & Comparator ensemble & 3 & \\\\textbf{28} & \\\\textbf{143} & \\\\textbf{43} \\\\\\\\\\n & Neural Network & 15 & 13 & - & - \\\\\\\\\\n\\\\midrule\\nChatGPT & Relevant prompt & 15 - 30 & 5 & - & -\\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\nAs seen in Table \\\\ref{tab:aiRes}, {\\\\logiclearner} is able to solve almost every question in our question bank in real time, providing a seamless user experience to students requesting hints. The {\\\\logiclearner} heuristic is also able to generalize well to previously unseen and computer-generated questions of varying lengths, reinforcing its strength as a heuristic. In contrast, ChatGPT is unable to solve any proofs that are longer than 1-2 steps. Our inspection of ChatGPT's proofs shows some recurring error modes:\\n\\\\begin{itemize}\\n   \\\\item ChatGPT does not model the semantics of Boolean variables, resulting in false equivalences. For example, it does not distinguish between $q$ and $r$ in the bottom of Figure \\\\ref{fig:chatFails}(a).\\n   \\\\item It often hallucinates variable or rule names, even for correct equivalences. Figure \\\\ref{fig:chatFails}(b) shows instances of ChatGPT confusing the Absorption and Domination laws, and incorrectly spelling `Commutative'.\\n   \\\\item As seen in Figure \\\\ref{fig:chatFails}(c), ChatGPT fails to correctly perform parity-dependent operations like negation and matching parentheses. Parity matching is known to be a hard problem for Transformer \\\\cite{vaswani2017attention} based architectures like LLMs \\\\cite{Hahn_2020}, and is one of the most frequent failure modes we observed.\\n   \\\\item ChatGPT cannot accurately parse long expression sequences or maintain context over the span of a long answer, resulting in it hallucinating a conclusion to an incorrect target (Figure \\\\ref{fig:chatFails}(d)).\\n\\\\end{itemize} \\n\\nAdditional details of our ChatGPT analysis (such as prompt selection) are presented in Appendix \\\\ref{sec:apxChat}. We hypothesize that using {\\\\logiclearner}'s proofs for Retrieval-Augmented Generation \\\\citep{lewis2020retrieval} to LLMs could produce well-explained, mathematically sound proofs, but we leave this to future work.\\n\\n\\\\begin{figure*}[ht!]\\n   \\\\centering\\n   \\\\begin{subfigure}[t]{0.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/chatBadVar.png}\\n   \\\\caption{}\\n   \\\\end{subfigure}\\n   ~ \\n   \\\\begin{subfigure}[t]{0.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/chatBadLaw.png}\\n   \\\\caption{}\\n   \\\\end{subfigure}\\n\\n   \\\\begin{subfigure}[t]{0.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/chatBadNeg.png}\\n   \\\\caption{}\\n   \\\\end{subfigure}\\n   ~ \\n   \\\\begin{subfigure}[t]{0.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/chatWrongTarget.png}\\n   \\\\caption{}\\n   \\\\end{subfigure}\\n   \\\\caption{A variety of ChatGPT's failure modes on logic proofs.}\\\\label{fig:chatFails}\\n\\\\end{figure*}\\n\\n\\\\section{Limitations and future work}\\\\label{sec:limit}\\n\\nOur surveys across two semesters of undergraduate discrete mathematics show a strongly positive response to {\\\\logiclearner}. However, voluntary student surveys are susceptible to only receiving responses from conscientious students who tend to perform well regardless of training method. Additionally, it is challenging to disentangle the benefits of {\\\\logiclearner} from unaided student learning, since denying access to a practice tool a control group of students is unfair to that group. More concrete results would likely require a multi-year, multi-university effort. While {\\\\logiclearner} is guaranteed to be mathematically precise, we use a simple AI heuristic that does not guarantee accuracy. Future work could explore the potential of large models as heuristics to {\\\\logiclearner}'s graph search framework for proof solving---we explore such ideas in Appendix \\\\ref{sec:apxNN}. {\\\\logiclearner} is open-source, and we encourage the development of more sophisticated search methods. We also note that such improvements may make for interesting student projects.\\n\\n\\\\section{Conclusion}\\\\label{sec:conclusion}\\n\\nThe study of propositional logic benefits from judgment-free guidance during practice. Building on existing work on the inability of Large Language Models (LLMs) to perform deep reasoning, we analyze the failure modes of ChatGPT, an LLM-based application that is increasingly used for pedagogical guidance. We outline the requirements of an effective practice tool for proof solving, and accordingly develop {\\\\logiclearner}, an interactive application to practice propositional logic proofs with real-time an AI proof solver for real-time pedagogical guidance. We pilot {\\\\logiclearner} as a practice tool over two semesters of the undergraduate discrete mathematics course at Columbia University and receive strongly positive feedback on its design and utility in student surveys. To the best of our knowledge, {\\\\logiclearner} is the first and only application for fully-automated guided proof-solving practice at the time of writing. {\\\\logiclearner} is free and open-source, and we look forward to the continued development of tools that democratize access to mathematical understanding.\\n\\n\\\\backmatter\\n\\n\\\\bmhead{Acknowledgements}\\n\\nWe thank the Center for Teaching and Learning (CTL) at Columbia University for lending their expertise in designing {\\\\logiclearner} as an effective pedagogical tool, and for continuing to manage the {\\\\logiclearner} application server and its open-source code base.\\n\\n\\\\section*{Declarations}\\n\\n\\\\begin{itemize}\\n\\\\item \\\\textbf{Funding} This work was supported by a generous grant from the Columbia University Provost's Faculty Committee on Educational Innovation. \\n\\\\item \\\\textbf{Competing interests} All authors declare that they have no conflicts of interest in relation to this work.\\n\\\\item \\\\textbf{Ethics approval and consent to participate} Student surveys were approved by the Institutional Review Board. Approval number: IRB-AAAU0354.\\n\\\\item \\\\textbf{Consent for publication} All authors consent to the publication of this manuscript.\\n\\\\item \\\\textbf{Data availability} Student survey data is presented in aggregate form in Appendix \\\\ref{sec:apxSurvey}. Full experimental results are in Appendix \\\\ref{sec:apxAblate}.\\n\\\\item \\\\textbf{Materials availability} Not applicable.\\n\\\\item \\\\textbf{Code availability} {\\\\logiclearner} is a free and open-source application under the GNU GPLv3 license. Source code is available at \\\\url{https://github.com/ccnmtl/logiclearnertools}.\\n\\\\item \\\\textbf{Author contribution} AI implemented the business logic (Parser, Search, API), designed and conducted the technical experiments, and authored the manuscript. MV and UM designed and implemented an earlier version of the {\\\\logiclearner} parser. The staff at the CTL (MT, ZM, ND, SS) designed the {\\\\logiclearner} web application user experience and database. The application server and git repositories are managed by the CTL. As the PIs of this project, NV and AS developed the vision for {\\\\logiclearner}, oversaw the entirety of its design and implementation, and mentored all student contributors who worked on the application.\\n\\\\end{itemize}\\n \\n\\n\\n\\\\clearpage\\n\\n\\\\bibliography{sn-bibliography} common bib file\\n if required, the content of .bbl file can be included here once bbl is generated\\n\\\\input sn-article.bbl\\n\\n\\\\clearpage\\n\\n\\n\\\\begin{appendices}\\n\\n\\\\section{Visuals of the LogicLearner Web Application}\\\\label{sec:apxVisual}\\n\\nWe designed the user interface and user experience of {\\\\logiclearner} in collaboration with the Columbia University Center for Teaching and Learning\\\\footnote{\\\\url{https://ctl.columbia.edu}} (CTL). The CTL specializes in designing the user experience of educational tools, and hosts the {\\\\logiclearner} application and database on its server. \\n\\n\\\\begin{figure}[ht!]\\n\\\\includegraphics[width0.99\\\\textwidth, frame]{images/introScreen.png}\\n\\\\caption{The {\\\\logiclearner} home screen.}\\\\label{fig:appIntro}\\n\\\\end{figure}\\n\\n\\\\begin{figure}[ht!]\\n\\\\includegraphics[width0.99\\\\textwidth, frame]{images/levelsScreen.png}\\n\\\\caption{Level selection according to proof difficulty.}\\\\label{fig:appLevel}\\n\\\\end{figure}\\n\\n\\n\\\\begin{figure}[ht!]\\n\\\\includegraphics[width0.99\\\\textwidth, frame]{images/guideScreen.png}\\n\\\\caption{A tutorial on using {\\\\logiclearner}.}\\\\label{fig:appGuide}\\n\\\\end{figure}\\n\\n\\n\\\\begin{figure}[ht!]\\n\\\\includegraphics[width0.99\\\\textwidth, frame]{images/noviceScreen.png}\\n\\\\caption{Selecting questions in the Novice level.}\\\\label{fig:appNovice}\\n\\\\end{figure}\\n\\n\\\\begin{figure*}[t!]\\n   \\\\centering\\n   \\\\begin{subfigure}[t]{0.48\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/oneHintScreen.png}\\n   \\\\caption{One hint requested.}\\n   \\\\end{subfigure}\\n   ~ \\n   \\\\begin{subfigure}[t]{0.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/twoHintScreen.png}\\n   \\\\caption{Two hints requested}\\n   \\\\end{subfigure}\\n   \\\\caption{Requesting hints during a question attempt.}\\n\\\\end{figure*}\\n\\n\\\\begin{figure}[ht!]\\n\\\\includegraphics[width0.99\\\\textwidth]{images/completedHintsScreen.png}\\n\\\\caption{A completed logic proof.}\\\\label{fig:appCompleted}\\n\\\\end{figure}\\n\\n\\\\section{AI Experiments and Ablations}\\\\label{sec:apxAblate}\\n\\nWe train a genetic algorithm to optimize the weights of a linear combination of our heuristics defined in Table \\\\ref{tab:heur}. Weights were constrained to floating point numbers in $[-10,10]$, which produced the best results empirically. The hyperparameters explored were the per-question time $\\\\tau$, number of training questions $|Q|$, elitism $\\\\epsilon$, and the crossover and mutation probabilities $P_c,P_m$. A score (questions solved) was calculated for three datasets: human-curated $(S_v)$, short generated questions ($S_t$), and mixed-length generated questions $(S_l)$. We choose a small subset of our human-curated question bank to train our algorithm, and present the final performance on all questions in \\\\ref{tab:ablate1}. The weights in our production heuristic are presented in Table \\\\ref{tab:prodheur}. Note that this heuristic was committed to production prior to some of the later AI improvements. \\n\\nExperiments were conducted on modest hardware with a limited search depth, and better hardware may lead to better performance. Despite this, our genetic algorithm shows a considerable improvement over the random baseline.\\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{Genetic algorithm ablation trials}\\\\label{tab:ablate1}\\n\\\\begin{tabular}{@{}llllllllll@{}}\\n\\\\toprule\\nPopulation & Total & $\\\\tau$ & $|Q|$ & $\\\\epsilon$ & $P_c$ & $P_m$ & $S_v$ & $S_t$ & $S_l$ \\\\\\\\\\nSize & Generations & (s) & & & & & (33) & (198) & (66) \\\\\\\\ \\n\\\\midrule\\n5 & 5 & 3 & 5 & 1 & 0.8 & 0.2 & 22 & 119 & 29 \\\\\\\\\\n10 & 10 & 3 & 10 & 1 & 0.8 & 0.2 & 21 & 114 & 32 \\\\\\\\\\n10 & 10 & 3 & 20 & 1 & 0.8 & 0.2 & 27 & 129 & 38 \\\\\\\\\\n10 & 10 & 3 & 33 & 1 & 0.8 & 0.2 & 27 & 141 & 43 \\\\\\\\\\n10 & 10 & 5 & 33 & 1 & 0.8 & 0.2 & 26 & 135 & 40 \\\\\\\\\\n10 & 10 & 3 & 33 & 3 & 0.8 & 0.2 & \\\\textbf{29} & 137 & 42 \\\\\\\\\\n10 & 20 & 3 & 33 & 3 & 0.6 & 0.2 & 28 & \\\\textbf{143} & \\\\textbf{43} \\\\\\\\\\n20 & 10 & 3 & 33 & 1 & 0.8 & 0.2 & 28 & 140 & 43 \\\\\\\\\\n20 & 20 & 3 & 33 & 1 & 0.8 & 0.2 & 28 & 133 & 43 \\\\\\\\\\n20 & 10 & 33 & 30 & 3 & 0.8 & 0.2 & 27 & 127 & 40 \\\\\\\\\\n\\\\midrule\\nProduction & & & & & & & & &   \\\\\\\\\\n\\\\midrule\\n20 & 8 & 1 & 7 & 3 & 0.8 & 0.5 & 27 & 129 & 37 \\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{A* search heuristics}\\\\label{tab:prodheur}\\n\\\\begin{tabular}{@{}ll|ll@{}}\\n\\\\toprule\\nHeuristic & Weight & Heuristic & Weight \\\\\\\\\\n\\\\midrule\\nLevenshtein distance & 3.36 & Distributivity & 3.94 \\\\\\\\\\nUnitary function & 3.76 & Domination & 4.09 \\\\\\\\\\nVariable Mismatch & 6.09 & Idempotence & -7.03 \\\\\\\\\\nLength difference & 1.53 & Identity & -9.85 \\\\\\\\\\nAbsorption & -3.88 & Iff as Implication & -4.20 \\\\\\\\\\nAssociativity & 1.94 & Implication as Disjunction & 6.92 \\\\\\\\\\nCommutativity & -8.07 & Negation & -0.55 \\\\\\\\\\nDe Morgan's Law & 3.71 & Start state & 1.44 \\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\nWe also trained our model on our dataset of 66 randomly generated solutions of depths 2-10 evaluated performance on the original question bank. These results, presented in Table \\\\ref{tabR2}, show that our heuristic generalizes from our randomly generated dataset to the human-generated question bank.\\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{Train on randomly generated questions}\\\\label{tabR2}\\n\\\\begin{tabular}{@{}lllllllllll@{}}\\n\\\\toprule\\nPopulation & Total & $\\\\tau$ & $|Q|$ & $\\\\epsilon$ & $\\\\tau_\\\\mathrm{eval}$ & $P_c$ & $P_m$ & $S_v$ & $S_t$ & $S_l$ \\\\\\\\\\nSize & Generations & (s) & & & (s) & & & (33) & (198) & (66) \\\\\\\\ \\n\\\\midrule\\n5 & 10 & 3 & 5 & 1 & 1 & 0.8 & 0.3 & 25 & 136 & 41 \\\\\\\\\\n5 & 10 & 3 & 5 & 1 & 3 & 0.8 & 0.3 & 27 & 143 & 42 \\\\\\\\\\n10 & 10 & 3 & 10 & 1 & 1 & 0.8 & 0.3 & 28 & 140 & 43 \\\\\\\\\\n10 & 10 & 3 & 10 & 1 & 3 & 0.8 & 0.3 & 29 & 147 & 46 \\\\\\\\\\n10 & 10 & 3 & 10 & 1 & 1 & 0.8 & 0.5 & 26 & 141 & 45 \\\\\\\\\\n10 & 10 & 3 & 10 & 1 & 3 & 0.8 & 0.5 & 27 & 143 & 45 \\\\\\\\\\n10 & 10 & 3 & 10 & 1 & 1 & 0.9 & 0.3 & 27 & 126 & 41 \\\\\\\\\\n10 & 10 & 3 & 10 & 1 & 3 & 0.9 & 0.3 & 29 & 138 & 42 \\\\\\\\\\n20 & 5 & 3 & 66 & 1 & 1 & 0.8 & 0.3 & 27 & 131 & 44 \\\\\\\\\\n20 & 5 & 3 & 66 & 1 & 3 & 0.8 & 0.3 & 29 & 139 & 45 \\\\\\\\\\n20 & 10 & 3 & 66 & 1 & 1 & 0.8 & 0.3 & 28 & 142 & 41 \\\\\\\\\\n20 & 10 & 3 & 66 & 1 & 3 & 0.8 & 0.3 & 29 & 147 & 44 \\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\n\\\\section{Student Survey Results}\\\\label{sec:apxSurvey}\\n\\n\\\\begin{figure}[ht!]\\n\\\\includegraphics[width\\\\textwidth]{images/survey1_likert.png}\\n\\\\caption{Survey 1: confidence scores}\\\\label{fig:s1conf}\\n\\\\end{figure}\\n\\n\\\\begin{figure}[ht!]\\n\\\\includegraphics[width\\\\textwidth]{images/survey2_likert.png}\\n\\\\caption{Survey 2: confidence scores}\\\\label{fig:s2conf}\\n\\\\end{figure}\\n\\nAs seen in Figures \\\\ref{fig:s1conf} and \\\\ref{fig:s2conf}, the confidence scores are highly similar across semesters. There were no changes to the {\\\\logiclearner} application in production between evaluations.\\n\\n\\\\begin{figure}[ht!]\\n\\\\centering\\n\\\\begin{minipage}{.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width0.9\\\\linewidth]{images/survey1_feature.png}\\n   \\\\caption{Survey 1: feature ratings}\\n   \\\\label{fig:s1feat}\\n\\\\end{minipage}\\n\\\\begin{minipage}{.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width0.8\\\\linewidth]{images/survey1_sentiment.png}\\n   \\\\caption{Survey 1: long-form sentiment}\\n   \\\\label{fig:s1sentiment}\\n\\\\end{minipage}\\n\\\\end{figure}\\n\\n\\\\begin{figure}[ht!]\\n\\\\centering\\n\\\\begin{minipage}{.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width0.9\\\\linewidth]{images/survey2_feature.png}\\n   \\\\caption{Survey 2: feature ratings}\\n   \\\\label{fig:s2feat}\\n\\\\end{minipage}\\n\\\\begin{minipage}{.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width0.8\\\\linewidth]{images/survey2_sentiment.png}\\n   \\\\caption{Survey 2: long-form sentiment}\\n   \\\\label{fig:s2sentiment}\\n\\\\end{minipage}\\n\\\\end{figure}\\n\\nThe feature scores and long-form sentiment (Figures \\\\ref{fig:s1feat}, \\\\ref{fig:s2feat}, \\\\ref{fig:s1sentiment}, \\\\ref{fig:s2sentiment}) show a similar consistency across surveyed groups, bolstering our confidence in these results.\\n\\nTable \\\\ref{tab:longform} shows the number of respondents per question, with the long-form questions written as queried. The number of respondents is significantly higher in the pre-learning, but there are enough respondents (especially when aggregated) to show significant results for the post-learning evaluation. \\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{Survey questions and number of respondents}\\\\label{tab:longform}\\n\\\\begin{tabular}{@{}llll@{}}\\n\\\\toprule\\nQuestion & Survey 1 & Survey 2 & Total \\\\\\\\\\n\\\\midrule\\nPre-learning confidence (6 questions) & 65 & 213 & 278 \\\\\\\\\\nPost-learning confidence (6 questions) & 28 & 52 & 80 \\\\\\\\\\n\\\\midrule\\nFeature ratings (scale of 1-5) & 27 & 43 & 70 \\\\\\\\\\n\\\\midrule\\nWhat features are we missing? & 10 & 13 & 23 \\\\\\\\ \\nHow could we improve our app? & & & \\\\\\\\\\n\\\\midrule\\nHow easy is it to use our app? & & & \\\\\\\\ \\nWhat were some challenges you faced & 14 & 16 & 30 \\\\\\\\\\nin using our app? & & & \\\\\\\\\\n\\\\midrule\\nWould you recommend Logic Learner & & & \\\\\\\\ \\nto others for practicing propositional logic? & 14 & 20 & 34 \\\\\\\\\\nWhy or why not? & & & \\\\\\\\\\n\\\\midrule\\nWould you use this app for practicing & 15 & 21 & 36 \\\\\\\\ \\nfor an exam or homework? & & & \\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\n\\\\section{Deep Boolean Metric Learning}\\\\label{sec:apxNN}\\n\\n{\\\\logiclearner}'s hints feature requires a robust and efficient heuristic for proof solving with A* search. In this section, we explore a potentially powerful alternative heuristic to {\\\\logiclearner} AI. Deep metric learning \\\\cite{kaya2019deep} aims to embed data in high-dimensional metric space such that desirable relationships between data points are preserved w.r.t. a chosen metric. Here, we embed Boolean expressions in vector space with the aim of preserving semantic similarity as a metric between expression embeddings. We leverage \\\\textbf{PROOF\\\\_GEN} (Algorithm \\\\ref{alg:qgen}) to produce datasets for neural network training, and train small Siamese encoder-decoder neural networks on two pre-training $k$-way classification tasks, rule prediction and proof length estimation (Table \\\\ref{tab:nnproxy}) with a 70-30 train-test split. We restrict the proof length to be at most 3 steps to reduce the impact of sub-optimal generated proofs. Since there is an infinite vocabulary of Boolean expressions, we tokenize and learn fixed vocabulary embeddings at a character level. We train a Gated Recurrent Unit (GRU) \\\\cite{cho2014learning} encoder and a fully-connected multilayer perceptron (MLP) decoder for 350 epochs on a T4 GPU in Google Colab\\\\footnote{Google. (2024). Google Colaboratory: \\\\url{https://colab.research.google.com/}}. The networks achieve high scores in pre-training despite their simplicity, as shown in Figures \\\\ref{fig:ruleTrain}, \\\\ref{fig:distTrain} and Table \\\\ref{tab:nnproxy}.\\n\\n\\\\begin{figure}[ht!]\\n\\\\centering\\n\\\\begin{minipage}{.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/rule_pred_loss.jpg}\\n   \\\\caption{Training loss: rule prediction}\\n   \\\\label{fig:ruleTrain}\\n\\\\end{minipage}\\n\\\\begin{minipage}{.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/cat_step_loss.jpg}\\n   \\\\caption{Training loss: proof length}\\n   \\\\label{fig:distTrain}\\n\\\\end{minipage}\\n\\\\end{figure}\\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{Neural Network on Proxy Tasks}\\\\label{tab:nnproxy}\\n\\\\begin{tabular}{@{}lllccccc@{}}\\n\\\\toprule\\nPre-training & Input & Target & Dataset & Num. & \\\\multicolumn{3}{c}{Top-1 Accuracy (\\\\)} \\\\\\\\\\nTask & & & Size & Class & Random & Train & Test \\\\\\\\\\n\\\\midrule\\nRule Prediction & $(e1, e2)$ & $r\\\\in R: e2\\\\in r(e1)$& 3128 & 16 & 6.25 & 83.10 & 81.04   \\\\\\\\\\nProof Length & $(e1, e2)$ & Proof length $l\\\\le 3$ & 16797 & 4 & 25 & 86.17 & 80.58 \\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\nPost training on proxy tasks, we use the cosine similarity between GRU encoder outputs as the heuristic for A* search on our human-curated question bank. As a baseline, we use a randomly initialized GRU encoder without pre-training. As an ablation study on our choice of tokenizer and training data, we also use CANINE-s \\\\cite{DBLP:journals/corr/abs-2103-06874} embeddings as a heuristic. CANINE-s is a Transformer-based language model that does not require an explicit tokenizer, bypassing the limitations of fixed-vocabulary models in parsing Boolean expressions.\\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{Neural Network on Proxy Tasks}\\\\label{tab:nnresults}\\n\\\\begin{tabular}{@{}llccc@{}}\\n\\\\toprule\\nModel & Language & {\\\\logiclearner} & Timeout & Score \\\\\\\\\\n & Pre-training & Pre-training & (s) & (33) \\\\\\\\\\n\\\\midrule\\n\\\\multirow{6}{*}{GRU Encoder} & \\\\multirow{6}{*}{None} & \\\\multirow{2}{*}{Rule Prediction} & 5 & 7 \\\\\\\\\\n & & & 15 & \\\\textbf{13} \\\\\\\\\\n & & \\\\multirow{2}{*}{Proof Length} & 5 & 6 \\\\\\\\\\n & & & 15 & 9 \\\\\\\\\\n & & \\\\multirow{2}{*}{None} & 5 & 7   \\\\\\\\\\n & & & 15 & 10 \\\\\\\\\\n\\\\midrule\\n\\\\multirow{2}{*}{CANINE-s} & \\\\multirow{2}{*}{Multilingual Wikipedia \\\\cite{DBLP:journals/corr/abs-1810-04805}} & \\\\multirow{2}{*}{None} & 5 & 2 \\\\\\\\\\n & & & 15 & 2   \\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\nTable \\\\ref{tab:nnresults} shows the results of our neural network study. While pre-training on proof length prediction does not help in this case, rule prediction improves the performance at greater search depths. Our ablations show that much of the performance comes from the domain-specific tokenization and network architecture, as CANINE-s---a complex language model with extensive multilingual pre-training---performs significantly worse than our baseline. Additionally, all models except CANINE-s outperform ChatGPT. However, all models are outperformed by our ensemble in production. \\nWe emphasize again that this is merely a \\\\textbf{proof-of-concept} for potential future works that could leverage {\\\\logiclearner}'s proof-generation and AI structure to build powerful automated logic proof solvers.\\n\\n\\\\section{ChatGPT Struggles with Logic Proofs}\\\\label{sec:apxChat}\\n\\nTo evaluate ChatGPT as a logic proof solver, we tune the prompts to produce answers that resemble step-by-step proofs using the rules of propositional logic\\\\footnote{Full prompt tuning transcript: \\\\url{https://chat.openai.com/share/9fc940a7-e258-4512-967e-c0d8e780ba8b}}. We do not use complex compositional prompts as our end users (undergraduate students) are unlikely to be familiar with the mathematical prompt tuning literature. Without explicitly asking for rules, ChatGPT generates a truth table to solve proofs. Our final prompt is of the form described in Section \\\\ref{subsec:r2}. With this simple prompt, we ask ChatGPT all questions in our human-curated question bank\\\\footnote{Full proof-evaluation transcript: \\\\url{https://chat.openai.com/share/7b7aa7f5-fb66-4fd3-ae41-172e2bf257d2}}.\\n\\nWhile ChatGPT exhibits an astonishing ability to parse input and perform few-step reasoning, it exhibits several error modes with our simple but realistic prompt, detailed in Section \\\\ref{subsec:r2}. ChatGPT interprets the term `rules of logic' to beyond equivalence relations include operations such as Modus Ponens, but we do not count it against the results if the proof is logically correct. We ask it each question once as a naive user will not retry in case of an incorrect answer. By our (lenient) analysis of the results, ChatGPT only solves the 5 questions in Table \\\\ref{tab:gptcorrect} correctly. A majority of these are one-step questions, showing that ChatGPT with minimal prompt tuning can can only reason at very shallow depths.\\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{Questions correctly solved by ChatGPT}\\\\label{tab:gptcorrect}\\n\\\\begin{tabular}{@{}c|ll|l@{}}\\n\\\\toprule\\nQuestion & Premise & Target & Comments \\\\\\\\\\n\\\\midrule\\n1 & $\\\\neg(\\\\neg p)$ & $p$ & One-step Double Negation \\\\\\\\\\n3 & $p\\\\to (q\\\\to r)$ & $(p\\\\land q) \\\\to r$ & Uses Modus Ponens cases \\\\\\\\\\n6 & $\\\\neg p \\\\land \\\\neg q$ & $\\\\neg (p\\\\lor q)$ & One-step De Morgan's law \\\\\\\\\\n7 & $\\\\neg(p\\\\land \\\\neg q)\\\\lor q$ & $\\\\neg p \\\\lor q$ & De Morgan's $\\\\rightarrow$ Idempotence \\\\\\\\\\n9 & $(p\\\\lor q)\\\\land (p\\\\lor r)$ & $p\\\\lor (q\\\\land r)$ & One-step Distributivity \\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\n\\\\end{appendices}\\n\\n\\\\end{document}\\n\"}],\n",
       "  'main': 'sn-article.tex',\n",
       "  'title': 'LogicLearner: A Tool for the Guided Practice of Propositional Logic Proofs'},\n",
       " '2510.07746': {'id': '2510.07746',\n",
       "  'files': [{'name': '1_false_positives.tex',\n",
       "    'text': '\\\\todo[inline]{Rewrite so that it is not just the abstract restated.}\\nSuccess in practice and heuristic arguments, like the above, have inspired a widespread belief that t-SNE visualizations preserve the cluster structure of the input. To the contrary, we prove that the strength of the input clustering cannot be reliably inferred from the low-dimensional visualization.\\n\\n\\n\\nHeuristic arguments, like the above, and success in practice and have inspired a widespread belief that the cluster structure present in t-SNE visualizations is in correspondence with the cluster structure of the input. \\nThe practical success and appeal of t-SNE has inspired a b\\n\\nSuppose you have two datasets, and their t-SNE plots strongly suggest not only that the datasets are clustered but that they obey similar clusterings.\\n\\nPrevious works by \\\\citet{linderman2019clustering} and \\\\citet{arora2018analysis} have identified that clustered inputs induce clustered t-SNE visualizations in a suitable sense. A key question for practitioners left unanswered by these analyses is: when does a clustered output imply a clustered input? \\nDoes a t-SNE output appear clustered if and only if the input is clustered? \\nMore generally, what information can be deduced about the input given a visualization? We answer this question by providing theoretical and practical evidence that the strength of cluster structure in the input cannot be reliably inferred from the low-dimensional visualization. In particular, we prove that (i) similarly clustered t-SNE visualizations do not imply similarly clustered inputs, and (ii) distinctly clustered visualizations do not imply distinctly different inputs. \\n\\nsimilarly clustered outputs /> similarly strong clustering in input space.\\n\\ndifferently clustered outputs /> far apart inputs.\\n\\n\\nCentral to the widespread use of t-SNE and related methods is the conviction that they produce visualizations whose cluster structure roughly matches that of the input. To the contrary, we prove that the strength of the input clustering cannot be reliably inferred from the low-dimensional visualization.\\n\\n\\\\todo[inline]{Add the definitions of the cluster indexes to appendix.}\\nTo quantify the strength of the cluster structure in a dataset, we study\\nemploy\\nwell-known cluster indices such as the average silhouette score \\\\citep{rousseeuw1987silhouettes}, the Calinski-Harabasz index \\\\citep{Caliński01011974}, and the Dunn index \\\\citep{DunnIndex1974}. \\nAt their core, these indices measure the cluster partition\\'s salience by producing some ratio of inter-cluster and   intra-cluster distances. \\nFor sake of readability, we focus on presenting our results with respect to the average silhouette score. Our results hold identically for the other indices as well (see Appendix \\\\ref{sec:appendix_false_pos}). \\n\\n, we focus on the Average Silhouette Score for the sake of presentation. The formal definitions and associated theorems of the other two indices can be found in Appendix \\\\ref{sec:appendix_false_pos}. \\n\\n\\\\todo[inline]{Finish adding other two indices to appendix.}\\n\\nFor an $n$-point dataset $\\\\mathcal{X}   \\\\{x_1, \\\\dots, x_n\\\\} \\\\subset \\\\mathbb{R}^{n-1}$ and a partition of the dataset into clusters $C_1 \\\\sqcup C_2 \\\\sqcup \\\\dots \\\\sqcup C_k   [n]$\\\\footnote{In alignment with our usage of the cluster indexes and to avoid division by zero, we assume that for all $m \\\\in [k], |C_m|>1$. {\\\\color{red} Rewrite proof so this can be removed.}}, we denote the three indexes as $\\\\bar{\\\\mathcal{S}}(\\\\mathcal{X}; C_{m\\\\in[k]})$, -, and - for the Average Silhouette Score, the Calinski-Harabasz index, and the Davies-Bouldin index, respectively. You can see the range of each index in Table \\\\ref{table:demo}.\\n\\n\\\\begin{definition}\\n   Given a partition $C_1 \\\\sqcup C_2 \\\\sqcup \\\\dots \\\\sqcup C_k   [n]$ of $n$ points $\\\\{x_1,\\\\ldots,x_n\\\\}   X$, the \\\\textbf{silhouette score} of a point $x_i$ (w.r.t.\\\\ the partition), denoted \\\\(\\\\mathcal{S}(i)\\\\), is the normalized difference between the average within- and the closest across-cluster distances from $x_i$: \\n$$\\n\\\\mathcal{S}(i) : \\\\frac{b(i)-a(i)}{\\\\max \\\\{b(i),a(i)\\\\}} \\n\\\\hspace{0.25in}\\na(i) : \\\\sum_{j \\\\in C^{(i)}} \\\\frac{\\\\lVert x_i - x_j \\\\rVert}{|C^{(i)}|-1} \\n\\\\hspace{0.25in}\\nb(i) : \\\\min_{\\n\\\\substack{m \\\\in [k] \\\\\\\\ C_m \\\\neq C^{(i)}}} \\\\sum_{j \\\\in C_m} \\\\frac{\\\\lVert x_i - x_j \\\\rVert}{|C_m|},\\n$$\\nwhere $C^{(i)}$ is the cluster to which $i$ belongs. Note that if $|C^{(i)}|1$, then $\\\\mathcal{S}(i)$ is defined to be zero.\\nThe \\\\textbf{average silhouette score} then is simply the average across all points in $X$:\\n$$\\\\bar{\\\\mathcal{S}}(X; C_{m\\\\in[k]}) : \\\\frac{1}{n} \\\\sum_{i \\\\in [n]} \\\\mathcal{S}(i).$$\\n\\\\end{definition}\\nObserve that the (average) silhouette score ranges from $-1$ to $1$ with a score of $1$ being assigned to maximally clustered data, $-1$ to incorrectly clustered data, and $0$ to unclustered data.\\n\\n\\n\\\\iffalse\\nFor an $n$-point dataset $X   \\\\{x_1, \\\\dots, x_n\\\\} \\\\subset \\\\mathbb{R}^{n-1}$ and its partition into clusters $C_1 \\\\sqcup C_2 \\\\sqcup \\\\dots \\\\sqcup C_k   [n]$, the Silhouette Score of a point $x_i$ measures the difference between the average distance from $x_i$ to points in same cluster and $x_i$ to points in another cluster. Specifically, for $i\\\\in [n]$, let $C(i) \\\\in \\\\{C_1,\\\\ \\\\dots,\\\\ C_k\\\\}$ be the cluster to which $i$ belongs $(\\\\textup{i.e. } i \\\\in C(i))$. Then, for $|C(i)|>1$, the average distance of $x_i$ to its own cluster is:\\n $$a(i) : \\\\frac{1}{|C(i)|-1} \\\\sum_{j \\\\in C(i)} \\\\lVert x_i - x_j \\\\rVert^2,$$\\nthe average distance to the nearest distinct cluster is:\\n$$b(i) : \\\\min_{m \\\\in [k] : C_m \\\\neq C(i)} \\\\frac{1}{|C_m|} \\\\sum_{j \\\\in C_m} \\\\lVert x_i - x_j \\\\rVert^2,$$\\nand the Silhouette Score of $i \\\\in [n]$ is:\\n $$\\\\mathcal{S}(i) \\\\equiv \\\\frac{b(i)-a(i)}{\\\\max \\\\{b(i),a(i)\\\\}}.$$\\nIf $|C(i)|1$, then $\\\\mathcal{S}(i)$ is defined to be 0.\\nTo measure the prevalence of cluster structure across the entire dataset, we define the Average Silhouette Score of $\\\\mathcal{X}$ with respect to $C_1, \\\\dots, C_k$ as:\\n\\n$$\\\\bar{\\\\mathcal{S}}(\\\\mathcal{X}; C_{m\\\\in[k]}) \\\\equiv \\\\frac{1}{n} \\\\sum_{i \\\\in [n]} \\\\mathcal{S}(i).$$\\n\\n$\\\\bar{\\\\mathcal{S}}(\\\\mathcal{X}; C_{m\\\\in[k]})$ ranges from $-1$ to $1$ with a score of $1$ being assigned to maximally clustered data, $-1$ to incorrectly clustered data, and $0$ to unclustered data. \\n\\n\\\\fi\\n\\n\\n\\nDefining the strength of a clustering with respect to this cluster index, we show that any stationary t-SNE output can be produced by an arbitrarily unclustered input:\\nThis result can be generalized to any visualization produced by t-SNE or UMAP:\\n\\n\\\\begin{restatable}{theorem}{UnclustHammer} \\n\\\\begin{theorem}\\n\\\\label{thm:unclustHammer}\\n   Fix any $n > k > 1$, and $n$-point dataset $X \\\\subset \\\\mathbb{R}^{n-1}$ with partition $C_1 \\\\sqcup \\\\cdots \\\\sqcup C_k   [n]$ such that $|C_{m\\\\in[k]}| > 1$ and $\\\\bar{\\\\mathcal{S}}(X; C_{m\\\\in[k]})$ is well defined. For all $0 < \\\\epsilon \\\\leq 1$, there exists $n$-point dataset $X_\\\\epsilon \\\\subset \\\\mathbb{R}^{n-1}$ such that \\n   $$\\\\bar{\\\\mathcal{S}}(X_\\\\epsilon; C_{m\\\\in[k]})   \\\\epsilon \\\\cdot \\\\bar{\\\\mathcal{S}}(X; C_{m\\\\in[k]}), $$\\n   yet, for any $\\\\rho \\\\in (1, n-1)$:\\n   $${\\\\TSNE}_\\\\rho(X)   {\\\\TSNE}_\\\\rho(X_\\\\epsilon).$$\\n\\\\end{theorem}\\n\\\\end{restatable}\\nIt is important to understand the implications of this result. For any high-dimensional dataset $X$ (regardless of how clustered it is), we can find an arbitrarily unclustered impostor dataset $X_\\\\epsilon$ such that \\\\emph{all} t-SNE stationary points (local as well as global) of $X$ and $X_\\\\epsilon$ match perfectly! In other words it is \\\\emph{impossible} to distinguish between $X$ and $X_\\\\epsilon$ based on the low-dimensional t-SNE visualization.\\n\\n\\\\begin{figure}[t!]\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/rna/MAIN-single-cell.png}\\n   \\\\vspace{-0.15in}\\n   \\\\caption{\\n   \\\\small\\n   Visualizations of single-cell data (top row) versus an arbitrarily unclustered impostor dataset (bottom row).\\n   Based on the 2D t-SNE visualization (left column), it is difficult do distinguish which dataset (real or impostor) may have produced the plot. Plotting the high-dimensional interpoint distances (right column) confirms that the imposter dataset is unclustered in some sense. As a reference we also plot the 2D PCA visualization (center column) to indicate that this issue does not occur with other methods. The numbers on the bottom left of each figure shows the cluster salience in terms of the average silhouette score for the 2D t-SNE plot (left), 2D PCA plot (center), and high-dimensional input (right) for the real dataset (top) and the impostor dataset (bottom).\\n   The top row of plots are generated from a preprocessed version of the PBMC3k dataset, which consists of $2638$ points in $\\\\mathbb{R}^{50}$. The colored clustering in all plots corresponds to a DBSCAN clustering of the top left plot. The bottom row describes a different dataset which is much closer to a regular simplex, yet yields a remarkably similar t-SNE plot. Note the distinction between the similarity of the t-SNE plots and the difference of the PCA plots. \\n   Note that the color coding in all of the scatter plots corresponds to a DBSCAN clustering \\\\citep{ester1996density} of the top left t-SNE plot.} \\n   \\\\vspace{-0.1in}\\n   \\\\label{fig:single_cell_false_pos}\\n\\\\end{figure}\\n\\n\\nAs a consequence, the same maximally clustered visualization can be produced by a sequence of impostor datasets ranging from maximally clustered to arbitrarily unclustered,\\n\\\\begin{restatable}{corollary}{TwoClusterNUnclustered} \\n\\\\begin{corollary}\\n\\\\label{cor:twoCluster_n_Unclustered}\\n   For all $n \\\\geq 4$ even, and partition $C_1 \\\\sqcup C_2   [n]$ such that $|C_1||C_2|\\\\frac{n}{2}$.\\n   \\\\todo{Since t-SNE/UMAP is going into 2D, can maybe get away with k3 and even clusters. This may also open questions about up to how many clusters this holds true for tho.}\\n   There exist a sequence of $n$-point datasets in $\\\\mathbb{R}^{n-1}$, $\\\\{ X_\\\\epsilon\\\\}_{0 < \\\\epsilon \\\\leq 1}$, with \\n   $$\\\\bar{\\\\mathcal{S}}(X_\\\\epsilon; C_1,C_2)   \\\\epsilon$$\\n   such that for any $\\\\rho \\\\in (1, n-1)$, we have $Y\\\\in \\\\bigcap_{0<\\\\epsilon \\\\leq 1}{\\\\TSNE}_\\\\rho(X_\\\\epsilon)$ with\\n   $$\\\\bar{\\\\mathcal{S}}(Y; C_1,C_2)   1.$$ \\n\\\\end{corollary}\\n\\\\end{restatable}\\nThe above shows that $Y$, a perfectly clustered visualization, is a local (and   global, see the proof in Appendix) \\\\footnote{See proof of Corollary \\\\ref{cor:twoCluster_n_Unclustered}.} \\nminimizer for any member of a set of inputs that range from being maximally clustered to being arbitrarily unclustered. Thus, even from a \\\\emph{perfectly} clustered visualization, the strength of the input\\'s cluster structure cannot be inferred. \\n\\n\\n\\n\\n\\nNote that the existence of an impostor $X_\\\\epsilon$ is not just theoretical; it can be constructed practically as well (see Appendix \\\\ref{app:impostor_construction} for an explicit construction). Hence this phenomenon can be demonstrated in real-world scenarios, see Figure \\\\ref{fig:single_cell_false_pos}. \\nIn this case, we select a preprocessed version of the well-known PBMC3k single-cell genomics dataset ($2638$ points, $50$ dimensions; \\\\citet{single_cell}) as $X$. We show that there is an arbitrary unclustered impostor dataset $X_\\\\epsilon$ that is essentially indistinguishable from the real dataset in terms of its 2D t-SNE visualization. In short, similarity in t-SNE visualization does not necessarily imply similarity in the input space.\\n\\n\\nThis fact is by no means limited to hand-picked datasets used in proofs.\\n\\\\textcolor{red}{TALK ABOUT PRACTICAL DATA AND FIG 1.}\\nThe phenomenon can be observed in natural high-dimensional data such as that generated by a mixture of Gaussians and Student\\'s t-distributions (see Figure \\\\ref{fig:higher_dim_tighter_clusters}). In the figure, the visualizations of the two inputs appear equally clustered in spite of the fact that the Average Silhouette Score of the mixture of Student\\'s t-distributions is 10 times that of the mixture of Gaussians. In short, similarity in t-SNE visualizations does not guarantee similarity in the input space.\\n\\n\\n\\n\\n\\n\\n\\n\\nSymmetrically, similarity in the input space does not guarantee similarity in the t-SNE visualizations. In fact, any two drastically different visualizations can be produced by arbitrarily close inputs:\\n\\\\begin{restatable}{theorem}{Perturbhammer}\\n\\\\label{thm:perturbhammer}\\n   Fix any $n > 2$ and $\\\\rho \\\\in (1, n-1)$. For all $\\\\epsilon>0$ and all $Y, Y\\' \\\\in {\\\\IMTSNE}$, there exists $n$-point datasets $X   \\\\{x_1,\\\\ldots,x_n \\\\}$ and $ X\\'\\\\{x\\'_1,\\\\ldots,x\\'_n\\\\} \\\\subset \\\\mathbb{R}^{n-1}$ such that $\\\\forall i\\\\neq j$\\n   $$1 - \\\\epsilon \\\\leq \\\\frac{\\\\lVert x_i - x_j \\\\rVert^2}{\\\\lVert x\\'_i - x\\'_j \\\\rVert^2} \\\\leq 1 + \\\\epsilon,$$\\n yet $Y \\\\in {\\\\TSNE}_\\\\rho(X)$ and \\n   $Y\\' \\\\in {\\\\TSNE}_\\\\rho(X\\').$\\n\\\\end{restatable}\\nThus even minor perturbations of the input dataset can develop into massive changes in the visualization. Figure \\\\ref{fig:sameIn_diffOut} demonstrates this phenomenon quite clearly. We start with a dataset $X$ that is a regular unit simplex (all pairwise distances are unit length). By systematically perturbing the input $X$ ever so slightly ($\\\\epsilon\\\\leq 0.01$), t-SNE produces strikingly different outputs. \\n\\nThe key observation behind our main Theorems \\\\ref{thm:unclustHammer} and \\\\ref{thm:perturbhammer} is the simple yet counter-intuitive fact\\\\footnote{To the best of our knowledge, no theory or practical work on t-SNE has studied this observation formally.} that t-SNE is not only invariant under multiplicative scaling of the input squared distances, but also additive scaling of these distances. Specifically given a dataset $X\\\\{x_1,\\\\ldots,x_n\\\\}$, for any dataset $X\\'\\\\{x\\'_1,\\\\ldots,x\\'_n\\\\}$ and $C \\\\in \\\\R$ such that, $ \\\\lVert x\\'_i-x_j\\' \\\\rVert^2   \\\\lVert x_i-x_j \\\\rVert^2+C \\\\geq 0$ for $i \\\\neq j$, we have $\\\\TSNE_\\\\rho(X)   \\\\TSNE_\\\\rho(X\\')$ (see Lemma \\\\ref{lem:tSNE_additive_invariance} for a formal statement). As a consequence, for any input dataset, we can simply pump up the interpoint distances and construct an impostor dataset which has the same visualization profile but is arbitrarily close to a regular simplex (and hence is arbitrarily unclustered)\\\\footnote{See Algorithm \\\\ref{alg:impostor} for a formalization of this process.}.   entire t-SNE stationary point (hence the visualization) profile. \\nThis observation also leads to the following seemingly bizarre fact. a surprising fact about the set of all t-SNE outputs.\\nidea is formalized in a key lemma below which is also utilized to prove our main Theorems \\\\ref{thm:unclustHammer} and \\\\ref{thm:perturbhammer}.\\n\\n\\\\begin{figure}\\n   \\\\centering\\n   \\\\includegraphics[width1\\\\linewidth]{images/Same_input_diff_out_0.008_9_21_25_wAmongus.png}\\n   Similar inputs with vastly different visualizations.}\\n   \\\\vspace{-0.15in}\\n   \\\\caption{\\n   \\\\small\\n   Myriad 2D t-SNE visualizations, all produced by small perturbations of the same 200-point input dataset. \\\\textcolor{red}{FIX THIS} high dimensional Gaussian.\\n   Similar inputs with vastly different visualizations. \\n   The t-SNE visualization of five different \\n   Each perturbation satisfies the conditions of Theorem \\\\ref{thm:perturbhammer} for $\\\\epsilon   0.01$.}\\n   \\\\vspace{-0.1in}\\n   \\\\label{fig:sameIn_diffOut}\\n\\\\end{figure}\\n\\n\\n\\n\\n\\n\\\\begin{lemma}\\\\label{lem:image_of_tSNE}\\n   Fix any $n > 2$ and $\\\\rho \\\\in (1, n-1)$. For any $\\\\epsilon>0$, define the set of $\\\\epsilon$-perturbations of a unit simplex as $\\\\Delta_\\\\epsilon : \\\\{ X\\\\{x_1, \\\\dots, x_n\\\\} \\\\subset \\\\mathbb{R}^{n-1}: \\\\forall i\\\\neq j, \\\\lVert x_i - x_j \\\\rVert^2 \\\\in [1-\\\\epsilon, 1+\\\\epsilon]\\\\}$. Then, for all $\\\\epsilon>0$\\n   $${\\\\IMTSNE}   {\\\\TSNE}_\\\\rho(\\\\Delta_\\\\epsilon).$$\\n\\\\end{lemma}\\n\\\\todo[inline]{Corollary of this section is the perturbation result: As a consequence of this ``cluster exaggeration\\'\\', highly clustered visualizations can easily be perturbed by small changes in the input.}\\nThe key to understanding why this lemma holds is the \\\\emph{additive invariance} property of t-SNE. \\nIn other words, there is a set of datasets \\\\(\\\\Delta_\\\\epsilon\\\\) arbitrarily close to a regular unit simplex that generates \\\\textit{all} possible stationary t-SNE outputs! The instability of t-SNE on such datasets (c.f.\\\\ Figure \\\\ref{fig:sameIn_diffOut}) has real-world consequences since many high-dimensional datasets fall into this regime \\\\citep{conc_hd_dist_1, aggarwal2001surprising} due to the concentration of measure phenomenon \\\\citep{ledoux2001concentration}. In particular, such datasets are susceptible to single-point adversarial attacks. Consider a dataset $X$ sampled from a mixture of two high-dimensional Gaussians. t-SNE, as expected, reveals the two underlying clusters (c.f.\\\\ Figure \\\\ref{fig:one_pt_perturb}, first panel). However, we can add just a single ``poison\" point to $X$ and destroy the clustered visualization (see Figure \\\\ref{fig:one_pt_perturb} second panel). This failure mode of t-SNE is also observed on a real high-dimensional datasets (see Figure \\\\ref{fig:outliers_real_world} left vs.\\\\ center).\\n\\nThe success of the poison point attack can be attributed to additive invariance. Given an input dataset in \\\\(\\\\Delta_\\\\epsilon\\\\) from a clustered, high-dimensional distribution, the set of interpoint distances occupy a tight band between $1-\\\\epsilon$ and $1+\\\\epsilon$. Since t-SNE is invariant under additive scaling, the dataset appears identically as if all the distances are in the range $[0, 2\\\\epsilon].$ Thus, from t-SNE\\'s perspective, the variation between inter-cluster distance ($\\\\approx 2\\\\epsilon$) and intra-cluster ($\\\\approx 0$) is large. However, when the poison point is added at the mean, the minimum distance from any point to the rest of the set is approximately halved. As a result, almost all distances remain in the range $[1-\\\\epsilon, 1+\\\\epsilon],$ but, as t-SNE sees it, the effective inter-cluster ($\\\\approx (1+\\\\epsilon) - \\\\frac{1}{2}(1-\\\\epsilon)   \\\\frac{1}{2}+\\\\frac{3}{2}\\\\epsilon$) and intra-cluster ($\\\\approx (1-\\\\epsilon) - \\\\frac{1}{2}(1-\\\\epsilon)   \\\\frac{1}{2}-\\\\frac{1}{2}\\\\epsilon$) gap has been reduced, causing the cluster structure to go unrecognized in some cases.\\n\\nIn the next section, we explore this phenomenon on a real-world dataset (Figure \\\\ref{fig:outliers_real_world}), where we contrast it with t-SNE\\'s strikingly indifferent response to the injection of outlier points. \\n\\nwe can think of t-SNE as constructing a visualization based on a set of interpoint distances occupying a tight band, say between \\\\(1-\\\\epsilon\\\\) and \\\\(1+\\\\epsilon\\\\). If the dataset is clustered, there are \\n\\n\\nue to additive invariance, it is effectively looking at a shifted version of this set, ranging from \\\\(0\\\\) to \\\\(2\\\\epsilon\\\\). Injecting a poison point changes this calculus, by lowering the minimum interpoint distance to say \\\\(0.5\\\\). The effective range becomes \\\\([0, 0.5 + 2\\\\epsilon]\\\\). The interpoint distances In turn, naturally occurring structure in the visualization can be washed-out by adding a single ``poison point\\'\\'. \\n\\nsubstantially \\\\emph{lowers} the minimum interpoint distance, say from \\\\(1\\\\) down to \\\\(0.5\\\\), then the effective range becomes $0$ to $0.501$. In turn, naturally occurring structure in the visualization can be washed-out by adding a single ``poison point\\'\\'. \\n\\nanalyzing its tight band of interpoint distances, say between \\\\(1\\\\) to \\\\(1 + \\\\epsilon\\\\), and, due to additive invariance, can be treated as if they ranged from \\\\(0\\\\) to \\\\(\\\\epsilon\\\\). Injecting a poison point substantially   \\\\emph{lowers} the minimum interpoint distance, say from \\\\(1\\\\) down to \\\\(0.5\\\\), then the effective range becomes $0$ to $0.501$. In turn, naturally occurring structure in the visualization can be washed-out by adding a single ``poison point\\'\\'.   \\nWe can begin to explain the outsize influence of the poison point from the perspective of additive invariance. The basic idea here is that t-SNE Consider how t-SNE processes a low-aspect ratio dataset. Given some set of interpoint distances in a narrow band, say \\\\([1, 1+\\\\epsilon]\\\\), it processes these distances as if they belonged to \\\\([0,\\\\epsilon]\\\\), via additive invariance. We explore this phenomenon further in the next section, where we contrast it with t-SNE\\'s strikingly indifferent response to the injection of outlier points . \\n\\n\\n\\nGiven an input from such high-dimensional data, we can think of t-SNE as analyzing its tight band of interpoint distances, say between \\\\(1\\\\) to \\\\(1.001\\\\), and treating it as if it ranged fo \\\\(0\\\\) to \\\\(0.001\\\\) due to additive invariance. This suggests a weakness of t-SNE to a certain kind of adversarial perturbation: if one injects a point into this dataset which substantially \\\\emph{lowers} the minimum interpoint distance, say from \\\\(1\\\\) down to \\\\(0.5\\\\), then the effective range becomes $0$ to $0.501$. In turn, naturally occurring structure in the visualization can be washed-out by adding a single ``poison point\\'\\'. \\n\\n\\nsuch that t-SNE outputs a single cluster.   However an introduction of just a single adversarial point to alter interpoint distance results in a dramatic collapse of the global structre (figure middle) \\n\\n\\nOne should one that most high-dimensional datasets occupy this unstable \\\\(\\\\Delta_\\\\epsilon\\\\) region. This idea is well-studied \\nin previous work in database theory \\\\citep{conc_hd_dist_1, aggarwal2001surprising} and high-dimensional statistics \\\\citep{ledoux2001concentration}.\\n\\n\\nSuch datasets constitute an unstable part of the t-SNE input landscape, per Figure \\\\ref{fig:sameIn_diffOut}.\\n\\nThis is quite a practical observation insofar as many real-world datasets belong to \\\\(\\\\Delta_\\\\epsilon\\\\).\\n\\nThe t-SNE visualization of such datasets in $\\\\Delta_\\\\epsilon$ are also susceptible to single-point perturbations. Consider a $X \\\\in \\\\Delta_\\\\epsilon$ such that $\\\\TSNE(X)$ displays two clear clusters. We claim that one can add a single point to this dataset which renders the t-SNE output un-clustered! \\n\\nHow is this possible? To see this, think of t-SNE as operating on just the interpoint distances. Given the aforementioned \\\\(X\\\\), it takes in a band of distances between say \\\\(1-\\\\epsilon\\\\) and \\\\(1+\\\\epsilon\\\\) and by additive invariance \\n\\n\\n\\nConsider, for instance, introducing a single adversarial point. This point can be chosen to significantly alter the distribution of interpoint distances, having an outsize effect on the t-SNE output. \\n\\n\\nThis small ball \\\\(\\\\Delta_\\\\epsilon\\\\) is also an unstable part of the input landscape on multiple levels. As shown in Figure \\\\ref{fig:sameIn_diffOut}, an adversary making small changes to each of the interpoint distances can change t-SNE plots . We also observe \\n\\n\\nThis is good news for an adversary, in the sense that this small ball of inputs \\\\(\\\\Delta_\\\\epsilon\\\\) is unstable with respect to small perturbations of the points, per Figure \\\\ref{fig:DiffIn_SameOut}. \\n\\n\\nAs seen in Figure \\\\ref{fig:DiffIn_SameOut}, an adversary\\n\\n\\n\\nIt stands to reason that even adding a single point to such a dataset\\n\\nMoreover, even single-point changes to such inputs can have outsize effects. \\n\\n\\n\\n\\n\\nFor instance, consider adding a point at the center of a sample of a mixture of two high-dimensional Gaussians. \\n\\n\\n\\nsuch inputs are susceptible to an eve, more insidious kind of adversarial perturbation: namely, the injection of a single point. \\nThis means inputs to t-SNE which are close to a simplex are particularly unstable: small changes in the t-SNE input can yield radically different-looking outputs. This is in contrast to say, PCA, which depicts such datasets\\nThe additive invariance property and its consequences have an interesting interplay with the fact that most high-dimensional datasets do in fact reside in \\\\(\\\\Delta_\\\\epsilon\\\\) for small \\\\(\\\\epsilon\\\\).\\nIt stands to reason that slight perturbations of datasets residing in \\\\(\\\\Delta_\\\\epsilon\\\\) can potentially have outsize effects on the t-SNE. Consider, for instance, adding a single point. \\nGiven an input from \\\\(\\\\Delta_\\\\epsilon\\\\), we can think of t-SNE as analyzing its tight band of interpoint distances, say between \\\\(1\\\\) to \\\\(1.001\\\\), and treating it as if it ranged fo \\\\(0\\\\) to \\\\(0.001\\\\) due to additive invariance. This suggests a weakness of t-SNE to a certain kind of adversarial perturbation: if one injects a point into this dataset which substantially \\\\emph{lowers} the minimum interpoint distance, say from \\\\(1\\\\) down to \\\\(0.5\\\\), then the effective range becomes $0$ to $0.501$. In turn, naturally occurring structure in the visualization can be washed-out by adding a single ``poison point\\'\\'. this may change the calculus dramatically.\\n\\\\sout{We observe {\\\\color{red} this phenomena} in both synthetic and real data that the introduction of such a ``poison point\\'\\' can have an outsize effect on t-SNE output}. \\n\\nThis is perhaps most dramatic in Figure \\\\ref{fig:one_pt_perturb}, where injecting a single ``poison\\'\\' point at the mean of a mixture of two high-dimensional Gaussians destroys the cluster separation of the t-SNE output entirely. We explore this phenomenon further in the next section, where we contrast it with t-SNE\\'s strikingly indifferent response to the injection of outlier points. \\n\\n\\n\\nOne should one that most high-dimensional datasets occupy this unstable \\\\(\\\\Delta_\\\\epsilon\\\\) region. This idea is well-studied \\nin previous work in database theory \\\\citep{conc_hd_dist_1, aggarwal2001surprising} and high-dimensional statistics \\\\citep{ledoux2001concentration}.\\n\\n\\n\\nphenomenon in the theory of high-dimensional probability, closely related to the concentration of measure phenomenon (e.g.\\\\ some illustrative example about gaussians) \\n\\\\citep{ledoux2001concentration}. (citation that highD data is appox simplex)\\n\\n\\nGiven a high-dimensional dataset, t-SNE is agnostic to the \\\\textit{magnitude} of the minimum interpoint distance. This may allow t-SNE to zero in some structure that \\n\\nIt also suggests an Achilles heel: perturbing the moinimum interpoint distance using an adversarial injection point.\\n\\n\\n\\nAnother curious effect of additive invariance is t-SNE\\'s sensitivity to changes in the minimum interpoint distance by adding/moving? a single point.   can stop t-SNE from recognizing the cluster structure at all. This is most dramatically shown in Figure \\\\ref{fig:one_pt_perturb} where injecting a single ``poison\\'\\' point at the mean of a mixture of two Gaussians destroys the cluster structure entirely. We explore this phenomenon further in the next section, where we contrast it with t-SNE\\'s significantly more indifferent response to the injection of far away outliers.\\n\\n \\nThis fact reveals that t-SNE is un\\n\\n whose image under t-SNE is surjective to the \\\\emph{entire} t-SNE image for all possible inputs! \\n\\n\\\\todo[inline]{READ ABOVE}First, we introduce some notation:\\n\\n\\n\\nLemma \\\\ref{lem:X+C_exists} guarantees that $X_{+C}$ always exists. We say that a data visualization algorithm is \\\\emph{invariant to additive scaling} if for all $C \\\\geq 0$ and $n$-point inputs, $X \\\\subset \\\\mathbb{R}^{n-1}$, the algorithm produces the same output on $X$ and $X_{+C}.$ Using additive invariance, we can make any input arbitrarily unclustered while preserving the output visualization. Indeed, by increasing the strength of the additive scaling, the ratio between any two distinct distances in the input goes to 1. \\nIn turn, the cluster structure loses its prominence while the output is maintained. This is the key insight behind Theorem \\\\ref{thm:unclustHammer}. Theorem \\\\ref{thm:additiveInvariance} also shows that Theorem \\\\ref{thm:unclustHammer} actually holds for a large variety of algorithms.\\n\\nThis observation also puts forth an alternative interpretation of these algorithms. Because they are invariant under additive scaling, any input, $\\\\mathcal{X}$, registers to the algorithm as a condensed version where the minimum distance is subtracted off, i.e., with some mild abuse of notation,\\n$$\\\\text{t-SNE}_\\\\rho(\\\\mathcal{X})   \\\\text{t-SNE}_\\\\rho(\\\\mathcal{X}_{-\\\\min_{x \\\\neq x\\'\\\\in \\\\mathcal{X}} \\\\lVert x-x\\'\\\\rVert^2}).$$\\n\\nThe additive invariance property can make the input seem more clustered than it actually is, especially if the aspect ratio is close to 1. \\nIt is worth noting The NLP dataset from Figure \\\\ref{fig:higher_dim_tighter_clusters} exemplify this trend. As the dimension of the sample increases, concentration of measure causes the inter-cluster and intra-cluster distances to converge to their expectations. Since the intra-cluster distances will all be close to the minimum distance, the algorithm perceives the input as if the clusters are contracted. Hence, seemingly less clustered inputs, i.e. close to a simplex, can get more clustered outputs.\\n\\nThe additive invariance property of t-SNE has an interesting interplay with the \\\\textit{concentration of measure phenomenon}, which, broadly speaking, tells us that high-dimensional data tends to look approximately like a regular simplex. From a practical perspective, \\n\\n\\nAdditionally, perturbing the minimum distance by adding a single outlier can stop t-SNE from recognizing the cluster structure at all. This is most dramatically shown in Figure \\\\ref{fig:one_pt_perturb} where injecting a single ``poison\\'\\' point at the mean of a mixture of two Gaussians destroys the cluster structure entirely.\\n\\n\\\\begin{figure}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/outlier_figs/one_pt_perturb.png}\\n   \\\\vspace{-0.15in}\\n   \\\\caption{   \\\\small\\n   t-SNE versus PCA plots in response to the injection of a single ``poison\\'\\' point in the input dataset. The original dataset, visualized in panels 1 and 3, consists of \\\\(400\\\\) points sampled from a mixture of two well-separated Gaussians in \\\\(\\\\mathbb{R}^{2000}\\\\). The poison point is then placed at the mean of the previously sampled points; the resulting $401$-point dataset is visualized in panels 2 and 4. \\n   }\\\\label{fig:one_pt_perturb}\\n   \\\\vspace{-0.1in}\\n\\\\end{figure}\\n\\n\\n\\n\\n\\n\\n\\n\\\\todo[inline]{Add discussions of expansions of Theorem \\\\ref{thm:unclustHammer} to other data visualization methods and cluster indices due to the widespread applicability of the proof technique.}\\n\\n'},\n",
       "   {'name': '2_outliers.tex',\n",
       "    'text': \"\\n\\nMost analysis on t-SNE, including the previous section, is concerned with whether it faithfully depicts global structure, specifically cluster structure. In this section, we consider how t-SNE represents points that drastically deviate from the global structure: namely, \\\\textit{outliers}. It is natural to hope that data visualization methods can enable the identification of outliers. Unfortunately, we find that t-SNE cannot fulfill this desideratum, as it arbitrarily suppresses the severity of outliers in its depiction of certain datasets.\\nThe previous section studied that t-SNE is invariant under certain \\\\textit{global} transformations of the input dataset. This \\n\\n\\nIn particular, we prove that stationary t-SNE plots are liable to dramatically misrepresent the structure of data when it comes to \\\\textit{outliers}, i.e. points lying significantly far away from the rest of the point cloud. {\\\\color{red} Strengthen the above and give more introduction before u start explaining what is going on.}\\nOne of the difficult truths of studying real-world data is the presence of outliers, i.e.\\\\ data points which do not conform to the behavior of the bulk of the data. Geometrically, one can define an outlier as a point lying significantly far away from the rest of the point cloud. It is natural to hope that data visualization techniques would visualize outliers as such.\\n\\nAn intuitive explanation of this phenomenon can be made based on the asymmetry of the input and output affinity matrices of t-SNE. Roughly speaking, the input affinity behaves like a normalized, symmetrized nearest neighbor graph, where the log of the perplexity roughly corresponds to the number of neighbors. Meanwhile, the output affinity behaves more like a radius neighborhood graph, at least in the sense that each point's neighborhood scale is the same. This means the output affinity is optimized to represent the outlier point in close proximity with at least some points, even if it was extremely far from those points in the input. This means the input affinity records an outlier–––even a very extreme one–––as having some set of neighbors. The output affinity is optimized to respect this, and hence places the outlier close to some points. \\n\\nTo begin to formalize this observation, we provide a geometric definition of an outlier.\\n\\n\\\\begin{definition}\\\\label{def:alpha_outlier}(\\\\(\\\\alpha\\\\)-outlier configuration) \\nFix \\\\(X \\\\subset \\\\mathbb{R}^D\\\\), \\\\(x_0\\\\in \\\\mathbb{R}^D\\\\), and \\\\(\\\\alpha \\\\in \\\\mathbb{R}_+\\\\). We say \\\\(X\\\\) is an \\\\textbf{\\\\((\\\\alpha, x_0)\\\\)-outlier configuration} if there exists a hyperplane separating \\\\(x_0\\\\) and \\\\(X\\\\setminus \\\\{x_0\\\\}\\\\) with margin width at least \\n   \\\\begin{equation*}\\\\label{eq:alpha_margin}\\n   \\\\alpha \\\\cdot \\\\max\\\\{1, \\\\diam(X\\\\setminus\\\\{x_0\\\\}) \\\\},\\n   \\\\end{equation*}\\ni.e., there exists a unit vector \\\\(v \\\\in \\\\mathbb{R}^D\\\\) such that for all \\\\(x\\\\in X\\\\setminus\\\\{x_0\\\\}\\\\), \\\\((v-x_0)\\\\cdot (x-x_0)\\\\) is at least the above.   \\n   \\\\textcolor{red}{use max}\\n   \\\\textcolor{red}{do a plus 1. this will make the proof much easier. avoids issues with diameter   0.}\\n Define the \\\\textbf{outlier number} of a dataset\\\\todo{Measure might be an overloaded word in a math context?}\\n , denoted \\\\(\\\\alpha(X)\\\\), as the largest \\\\(\\\\alpha\\\\) for which there exists \\\\(x_0 \\\\in X\\\\) such that \\\\(X\\\\) is an \\\\((\\\\alpha, x_0)\\\\)-outlier configuration.\\n\\\\end{definition}\\n\\nThis definition can be generalized to accommodate more than one outlier, but for the purposes of theoretical analysis we consider just one. Note that the outlier extremity \\\\(\\\\alpha\\\\) is defined relative to the diameter of the rest of the points, unless that diameter is below \\\\(1\\\\). The choice of a threshold here is important and intuitive: it allows us to have a suitable notion of outlier in extreme cases such as when \\\\(\\\\diam(X\\\\setminus\\\\{x_0\\\\})   0\\\\).\\n\\nOur main theorem establishes that any stationary t-SNE output, \\\\textit{regardless of its input}, is incapable of depicting extreme outliers. , in the sense that the \\\\(\\\\alpha\\\\) number is bounded by a small constant. , in the sense that the \\\\(\\\\alpha\\\\) outlier measure of any stationary t-SNE output is upper-bounded by a small universal constant. We show this by analyzing the gradient on the outlier point. The following lemma will be useful. For simplicity, we work in one dimension, and we make the assumption that\\n\\n\\n\\\\begin{figure}[t!]\\n   \\\\centering\\n\\\\includegraphics[width0.49\\\\linewidth]{./iclr2026/images/outlier_figs/illustrative_plot_.png}\\n\\\\includegraphics[width\\\\linewidth]{images/outlier_figs/credit_card_full.png}\\n   \\\\includegraphics[width\\\\linewidth]{images/outlier_figs/OUTLIER_MAIN1.png}\\n\\\\includegraphics[width\\\\linewidth]{images/outlier_figs/OUTLIER_MAIN2.png}\\n\\\\includegraphics[width0.707\\\\linewidth]{./iclr2026/images/outlier_figs/OUTLIER_REAL2.png}\\n\\\\vspace{-0.15in}\\n   \\\\caption{\\n   \\\\small\\n   t-SNE's versus PCA's response to \\\\(\\\\alpha\\\\)-outliers. Top row: on a dataset that tracks financial activity, around \\\\(1\\\\\\\\) of which is fraudulent, t-SNE fails while PCA largely succeeds at separating fraudulent (red) from non-fraudulent (black) points. Note that each of the fraudulent data points is an \\\\((\\\\alpha > 0)\\\\)-outlier with respect to the non-fraudulent group; the top right figure shows how t-SNE and PCA register those \\\\(\\\\alpha\\\\)-values in their output. Middle row: a similar analysis on a synthetic dataset comprised of a Gaussian sample plus a single $\\\\alpha$-outlier, with varying values of $\\\\alpha$. Bottom row: mixture of two Gaussians plus 1, 10, and 100 $\\\\alpha$-outliers. Despite a large gap ($\\\\alpha > 1$) between the outliers and the two clusters, t-SNE is unable to separate them.\\n   In the t-SNE plots, the outliers are not separated   cluster structure, whereas in the PCA plots the outliers overtake the structure of the embedding. In the top row, we compare t-SNE and PCA's visualization of a dataset regarding credit card fraud detection; in particular, we plot \\\\(\\\\alpha(X,x_0)\\\\). In the middle row, we run a similar analysis on a synthetic data model of a set of Gaussian samples plus an outlier. \\n   } \\n   \\\\label{fig:outliers1}\\n   \\\\vspace{-0.1in}\\n\\\\end{figure}\\n\\n\\n\\n\\n\\\\begin{restatable}{theorem}{OutlierAbs} \\n\\\\label{thm:outlier_abs}\\nFix \\\\(n > 2\\\\) \\\\todo{$n\\\\geq 2$ otherwise $\\\\rho$ is not defined.} we're actually woprking with n+1 points, so I updated rho. \\nand \\\\(\\\\rho \\\\in (1,n-1)\\\\). Let \\\\(Y   \\\\{y_0, y_1,\\\\ldots,y_{n-1}\\\\} \\\\in {\\\\IMTSNE} \\\\) be a stationary t-SNE embedding. Without loss of generality let \\\\(y_0\\\\) be the outlier point. Then we have:\\n\\\\[\\\\alpha(Y)   \\\\alpha(Y,y_0) \\\\leq \\\\sqrt{1 + \\\\Big(1 + \\\\frac{2}{n-2}\\\\Big) \\\\Big(\\\\frac{8}{1 + \\\\sum_{i1}^{n-1} P_{0|i}(X)}\\\\Big)}   3 + o(1)\\\\]\\nfor all \\\\(X   \\\\{x_0, x_1,\\\\ldots,x_{n-1}\\\\}\\\\) such that \\\\(Y\\\\in \\\\TSNE_{\\\\rho}(X)\\\\).\\nand \\\\(\\\\rho \\\\in [1,n]\\\\). Let \\\\(Y   \\\\{y_0, y_1,\\\\ldots,y_{n}\\\\} \\\\in {\\\\IMTSNE} \\\\) be a stationary t-SNE embedding. Without loss of generality let \\\\(y_0\\\\) be the outlier point. Then we have:\\n\\\\[\\\\alpha(Y)   \\\\alpha(Y,y_0) \\\\leq \\\\sqrt{1 + \\\\Big(1 + \\\\frac{2}{n-1}\\\\Big) \\\\Big(\\\\frac{8}{1 + \\\\sum_{i1}^n P_{0|i}}\\\\Big)}   3 + o_n(1)\\\\]\\nfor all \\\\(P(X)   P(\\\\{x_1,\\\\ldots,x_n\\\\})\\\\) such that \\\\(Y\\\\in \\\\TSNE_{\\\\rho}(X)\\\\).\\n\\\\end{restatable}\\n\\nThe result is proven via analysis of the t-SNE gradient: we argue that if the outlier is too far away, its gradient is nonzero, thus violating stationarity. Key to this analysis is a comparison between the aggregate behavior of the outlier point's affinities in the input versus the output; in other words, the comparison between \\\\(\\\\sum_{i1}^{n-1} P_{i0}\\\\) and \\\\(\\\\sum_{i1}^{n-1} Q_{i0}\\\\). This is where the fundamental asymmetry of t-SNE comes in. While the latter is dependent on the position of the outlier point \\\\(y_0\\\\), per Lemma \\\\ref{lem:Qsum_bound}, the former has a lower bound of \\\\(1/(2n)\\\\) due to the normalization of the conditional affinity probabilities. \\nThe key fact behind this result is the normalization of the \\\\(P\\\\) matrix. \\n\\n\\nThe input-agnostic nature of this result is striking: even if the input is an extreme outlier configuration, a t-SNE output cannot depict its extremity past roughly \\\\(\\\\alpha   3\\\\). This behavior stands in stark contrast to that of principal component analysis (PCA), as shown in Figure \\\\ref{fig:outliers1} on both real and synthetic data models. PCA tends to preserve the \\\\(\\\\alpha\\\\) outlier number, while t-SNE seldom depicts outliers past \\\\(\\\\alpha > 0.2\\\\) in practice, and often depicts them as within the convex hull of the rest of the points (hence \\\\(\\\\alpha   0\\\\)). Furthermore, when faced with multiple outliers, (Figure \\\\ref{fig:outliers1}, bottom) t-SNE gracefully accommodates them into the global structure of the bulk of the data. We make this comparison in practice, where we find, somewhat strikingly, that \\\\(\\\\alpha\\\\) tends to hover quite close to \\\\(0\\\\). Visually, in the plots, outliers are often unnoticeable. The values of \\\\(\\\\alpha\\\\) we observe from t-SNE outputs hover well below \\\\(1\\\\)\\n\\nOur result suggests that t-SNE is an inappropriate tool to use in situations involving outlier detection. Consider, for instance, a dataset of financial transactions where the goal is to detect fraudulent user, studied by \\\\citet{dalpozzolo2015_calib}. In this dataset, only \\\\(0.172\\\\)\\\\ percent of the points ($492$ out of $284,807$) are fraudulent and by many standard statistical metrics register as outliers. Comparing the t-SNE and PCA plots on a   random representative subset of this data ($5050$ points, of which $50$ are fraudulent), we see that t-SNE mixes the frauds with the bulk of the points hides \\\\textcolor{red}{mixes?} the frauds\\nwhile PCA keeps them separated for the most partdepicts most of them as far away \\\\textcolor{red}{keeps them separated?}\\n, see Figure \\\\ref{fig:outliers1}, top row.\\n\\nFinally, note the distinction between t-SNE's muted response to outliers and its dramatic sensitivity to poison points. We illustrate this distinction on a dataset of BBC news articles \\\\citep{bbc_dataset}, see Figure \\\\ref{fig:outliers_real_world}. Given RoBERTa \\\\citep{liu2019roberta} sentence embeddings of these articles (\\\\(n2225, D1024\\\\)), we find that injecting $220$ poison points (see Appendix \\\\ref{sec:appendix_outliers_experiment} for the explicit construction) can halve the silhouette score of the t-SNE embedding with respect to the ground-truth labelling, whereas injecting $1100$ large-$\\\\alpha$-outliers slightly improves the silhouette score. Figure \\\\ref{fig:bbc_appendix}.\\\\textcolor{red}{refer to fig 5? cite bbc dataset?}\\n\\n\\n\\\\begin{theorem}\\n   For all \\\\(\\\\alpha> 1\\\\), there exists a perplexity \\\\(\\\\rho\\\\) and a dataset \\\\(X_\\\\alpha\\\\) which is an \\\\((\\\\alpha, x_0)\\\\)-outlier configuration with affinity matrix \\\\( PP_\\\\rho(X_\\\\alpha)\\\\) such that   \\\\[\\\\]\\n\\\\end{theorem}\\n\\n\\n\\n\\n\\\\begin{figure}\\n   \\\\centering\\n   \\\\includegraphics[width0.283\\\\linewidth]{./iclr2026/images/outlier_figs/OUTLIER_REAL1.png}\\n\\\\includegraphics[width\\\\linewidth]{images/BBC_POISON.png}\\n\\\\vspace{-0.15in}\\n   \\\\caption{   \\\\small\\n\\\\textcolor{red}{FIX CAPTION} \\nt-SNE's response to the injection of poison points (middle) and $\\\\alpha$-outliers (right) on the BBC News Article dataset. Middle: injecting poison points (red) to the original dataset (black) significantly disrupts the underlying cluster structure. Right: while injecting $(\\\\alpha > 1)$-outliers (red) does not disrupt the underlying cluster structure (black), the extreme outliers themselves are not well separated. The bottom left label in each plot denotes silhouette score of the t-SNE projected original points (without the injected points) with respect to the true labels (business, entertainment, politics, sport, tech).   Outlier behavior on real-world data. Left: given data monitoring financial activity (\\\\(n5050\\\\), \\\\(D30\\\\)) where one percent of users are committing fraud, PCA succeeds and t-SNE fails in visualizing the frauds as outliers. Right: using RoBERTa transformer embeddings of BBC news article titles (\\\\(n1191, D768\\\\)), we find that its t-SNE clustering can be destroyed by injecting a relatively small number of ``poison'' points; meanwhile, the clustering structure is remarkably robust to the injection of faraway \\\\((\\\\alpha\\\\gg 1)\\\\)-outliers\\n}\\n   \\\\label{fig:outliers_real_world}\\n\\\\vspace{-0.1in}\\n\\\\end{figure}\\n\\n\\n\\n\\n\\n\\n\\n\\n\"},\n",
       "   {'name': 'abstract.tex',\n",
       "    'text': \"\\nt-distributed stochastic neighbor embedding\\n\\nTo what extent can we rely on the data visualizations generated by t-distributed stochastic neighbor embedding (t-SNE)? We show that the strength of the cluster structure one may infer from a t-SNE output may be far from representative of the input. \\n\\nproduces visualizations whose cluster structure roughly matches that of the input\\nreflects the salient structure in the input\\n\\\\vspace{-0.06in}\\nCentral to the widespread use of t-distributed stochastic neighbor embedding \\\\mbox{(t-SNE)} is the conviction that it produces visualizations whose structure roughly matches that of the input. To the contrary, we prove that (1) the strength of the input clustering, and (2) the extremity of outlier points, \\\\emph{cannot} be reliably inferred from the t-SNE output. We demonstrate the prevalence of these failure modes in practice as well.\\n\\\\vspace{-0.06in}\\n\\nWe study these failure modes both theoretically and experimentally.\\n\\n\\n\\n's low-dimensional visualization produced by produced by t-SNE. We study these failure modes in both real and synthetic datasets. \\n\\n(in practice, often subsumed into the ambient sclu).\\n\\nurthermore, optimal t-SNE embeddings are incapable of depicting extreme outliers. \\n\\n\\nstronger than what is present in the input. \\n\\n\\n\\nBy identifying distinct geometric characteristics of stationary t-SNE outputs\\n\\nthe strength of cluster structure in the input dataset\\n\\nshould be taken with a grain of salt. We show that t-SNE outputs can arbitrarily exaggerate the strength of cluster structure in the input dataset. \\n\\nand that optimal t-SNE embeddings are incapable of depicting extreme outliers. We study these characteristics on real-world and \\n\\ntl;dr optimal t-SNE outputs are biased towards overemphasizing cluster structure\\n\\nfind that optimal t-SNE embeddings can be arbitrarily more clustered than their inputs per standard metrics of cluster saliency such as the silhouette score. We also prove that optimal t-SNE embeddings are prone to misrepresent extreme outliers. We present experiments which corroborate these findings in practical settings. \\n this trend that t-SNE is prone \\n\\nare liable to exaggerate cluster structure and misrepresent outlier points as though they neatly belong to such clusters.\\n\\n and misrepresent outliers \\n\\n\"},\n",
       "   {'name': 'appendix.tex',\n",
       "    'text': \"\\\\section{Appendix: Misrepresentation of Cluster Structure}\\\\label{sec:appendix_false_pos}\\n\\n\\n\\\\subsection{Additional Experiments}\\n\\\\todo[inline]{Make Figure 6 appear here.}\\n\\\\iffalse\\n\\\\begin{figure}\\n   \\\\centering\\n   \\\\includegraphics[width0.49\\\\linewidth]{images/Same_Output_Diff_Input_in_same_dim_9_19_25.png}\\n   \\\\caption{Mixture of Gaussians of Increasing Dimension.} \\\\label{fig:DiffIn_SameOut}\\n   \\\\todo[inline]{Update caption.}\\n   The t-SNE visualization and interpoint distance matrix of 50 points sampled from $N\\\\left(\\\\vec{0}, \\\\frac{1}{d^{3/4}}I_d\\\\right)$ and 50 points sampled from $N\\\\left(\\\\vec{e}_1,   \\\\frac{1}{d^{3/4}}I_d\\\\right)$ for $d \\\\in [100, 1000, 10000].$ Notice that as the dimension increases, the interpoint distance matrix of the input approaches a simplex but the t-SNE visualization becomes more clustered.\\n\\\\end{figure}\\n\\\\fi\\n\\nIn Figure \\\\ref{fig:higher_dim_tighter_clusters}, we plot a sample from a mixture of two Gaussians in 250, 500, 1000, 2000, and 4000 dimensions. Notice that as the dimension of the Gaussian increases, the interpoint distance matrix of the input points (bottom) approaches a simplex but the t-SNE corresponding visualization (top) remains qualitatively unchanged. \\n\\n\\\\begin{figure}[h!]\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/appendix_figs/Same_Output_Diff_Input_9_22_25.png}\\n   \\\\caption{\\\\small t-SNE's interplay with Gaussian concentration of measure. \\n   \\\\todo[inline]{Decide whether to keep or remove this figure.}]\\n}\\\\label{fig:higher_dim_tighter_clusters}\\n\\\\end{figure}\\n\\n\\n\\\\begin{figure}[h!]\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{./iclr2026/images/appendix_figs/dist_hist.png}\\n   \\\\caption{\\\\small Higher dimensional data looks more like a regular simplex; an example in NLP. We plot the histogram of interpoint distances of a term frequency-inverse document frequency (tf-idf) vectorization of the BBC news dataset as we increase the number of features used. }\\\\label{fig:NLP_simplex}\\n\\\\end{figure}\\n\\n\\\\textcolor{red}{rework}\\n It is also an intuitive property in practice: collecting more measurements and continually normalizing the resulting vector of measurements tends to push the minimum and maximum interpoint distances between data points closer together, see Figure \\\\ref{fig:NLP_simplex} for a manifestation of this phenomenon in the NLP domain. \\n\\n\\n\\\\subsection{Calinski-Harabasz Index}\\\\label{sec:CHindex}\\nFor an $n$-point dataset $X   \\\\{x_1, \\\\dots, x_n\\\\} \\\\subset \\\\mathbb{R}^{n-1}$ and a partition of the dataset into clusters $C_1 \\\\sqcup C_2 \\\\sqcup \\\\dots \\\\sqcup C_k   [n]$ with $n > k > 1$, the Calinski-Harabasz Index is defined as the ratio of the distance between cluster centers to the   internal distance to a cluster's center. Let $E$ be the function sending $S \\\\subseteq [n]$ to $\\\\mathbb{R}^{n-1}$ such that:\\n$$E(S)   \\\\frac{1}{|S|}\\\\sum_{i \\\\in S} x_i.$$\\nThen the Calinski-Harabasz Index is defined as\\\\footnote{If the denominator and numerator are $0$, then $\\\\text{CH}(X; C_{m \\\\in [k]}) : 1$. If only the denominator is 0, then $\\\\text{CH}(X; C_{m \\\\in [k]}) : \\\\infty$.}:\\n$$\\\\text{CH}(X; C_{m \\\\in [k]})   \\\\frac{\\\\frac{1}{k-1}\\\\sum_{m \\\\in [k]} |C_m|\\\\cdot \\\\lVert E(C_m) - E([n])\\\\rVert^2}{\\\\frac{1}{n-k}\\\\sum_{m \\\\in [k]} \\\\sum_{i \\\\in C_m} \\\\lVert x_i - E(C_m)\\\\rVert^2}.$$\\n\\nIt ranges from $0$ to $\\\\infty$ with a score of $\\\\infty$ being assigned to perfectly clustered data, 1 to unclustered data and 0 to incorrectly clustered data. \\n\\nNow we provide an analogue to Theorem \\\\ref{thm:unclustHammer} with respect to the Calinski-Harabasz Index:\\n\\n\\n\\\\begin{theorem}\\n\\\\label{thm:unclustHammerCH}\\n   Fix any $n > k > 1$, and $n$-point dataset ${X} \\\\subset \\\\mathbb{R}^{n-1}$ with partition $C_1 \\\\sqcup \\\\cdots \\\\sqcup C_k   [n]$ such that $\\\\textup{CH}({X}; C_{m\\\\in[k]}) > 1$. For all $1 < \\\\epsilon \\\\leq \\\\textup{CH}({X}; C_{m\\\\in[k]})$, there exists $n$-point dataset ${X}_\\\\epsilon \\\\subset \\\\mathbb{R}^{n-1}$ such that \\n   $$\\\\textup{CH}({X}_\\\\epsilon; C_{m\\\\in[k]})   \\\\epsilon, $$\\n   yet, for any $\\\\rho \\\\in (1, n-1)$:\\n   $${\\\\TSNE}_\\\\rho({X})   {\\\\TSNE}_\\\\rho({X}_\\\\epsilon).$$\\n\\\\end{theorem}\\n\\n\\\\begin{corollary}\\n\\\\label{cor:twoCluster_n_UnclusteredCH}\\n   For all $n \\\\geq 4$ even, and partition $C_1 \\\\sqcup C_2   [n]$ such that $|C_1||C_2|\\\\frac{n}{2}$. There exist a sequence of $n$-point datasets in $\\\\mathbb{R}^{n-1}$, $\\\\{ {X}_\\\\epsilon\\\\}_{1 < \\\\epsilon \\\\leq \\\\infty}$, with \\n   $$\\\\textup{CH}({X}_\\\\epsilon; C_1, C_2)   \\\\epsilon$$\\n   such that for any $\\\\rho \\\\in (1, n-1)$, $\\\\bigcap_{1<\\\\epsilon \\\\leq \\\\infty}\\\\text{t-SNE}_\\\\rho({X}_\\\\epsilon)$ contains $n$-point dataset ${Y} \\\\subseteq \\\\mathbb{R}^2$ with\\n   $$\\\\textup{CH}(Y; C_1,C_2)   \\\\infty.$$ \\n\\\\end{corollary}\\n\\n\\\\begin{proof}[\\\\textbf{Proof of Theorem \\\\ref{thm:unclustHammerCH}}]\\n   \\\\textcolor{red}{for clarity you should call it \\\\(g_X(C)\\\\), also maybe use a different letter than \\\\(C\\\\), even little \\\\(c\\\\) is better}\\n   First, let us assume that $\\\\text{CH}(X; C_{m \\\\in [k]}) < \\\\infty$. Let $g$ be the function from Corollary \\\\ref{cor:g_exists}, and let $f(C)   \\\\text{CH}(g(C); C_{m \\\\in [k]})$. Note that $f$ is continuous whenever the denominator of $\\\\text{CH}(\\\\ \\\\cdot\\\\ ; C_{m \\\\in [k]})$ \\\\textcolor{red}{maybe give the denominator a name}\\n   is non-zero which is always the case for $C \\\\in [0,1]$. Therefore, the image of $f$ on $[0,1)$ contains the interval \\\\textcolor{red}{you did not establish it is a decreasing function of \\\\(C\\\\)} \\n   $(f(1), f(0)]   (1, \\\\text{CH}(X ; C_{m \\\\in [k]})]$. \\\\textcolor{red}{at least show \\\\(f(1)   1\\\\)} \\n   Thus, for all $\\\\epsilon \\\\in (1, \\\\text{CH}(X ; C_{m \\\\in [k]})]$, there exists $C \\\\in [0,1)$ such that $X_\\\\epsilon   g(C)$ satisfies the hypothesis.\\n\\n   If $\\\\text{CH}(X; C_{m \\\\in [k]})   \\\\infty,$ then $f$ is continuous on $(0,1)$ only. Thus for all $\\\\epsilon \\\\in (1, \\\\text{CH}(X ; C_{m \\\\in [k]}))$, there exists $C \\\\in (0,1)$ such that $X_\\\\epsilon   g(C)$ satisfies the hypothesis and for $\\\\epsilon \\\\text{CH}(X ; C_{m \\\\in [k]}))$, $X_\\\\epsilon   X$ satisfies the hypothesis.\\n\\\\end{proof}\\n\\nFor proof of Corollary \\\\ref{cor:twoCluster_n_UnclusteredCH} see Appendix \\\\ref{sec:proofs_clust}.\\n\\n\\\\subsection{Dunn Index}\\nFor an $n$-point dataset $X   \\\\{x_1, \\\\dots, x_n\\\\} \\\\subset \\\\mathbb{R}^{n-1}$ and a partition of the dataset into clusters $C_1 \\\\sqcup C_2 \\\\sqcup \\\\dots \\\\sqcup C_k   [n]$ with $|C_{m\\\\in[k]}| > 1$, the Dunn index measures the ratio between the minimum inter-cluster distance and maximum intra-cluster distance. Specifically, the Dunn index is given by the expression\\\\footnote{If the denominator and numerator are $0$, then $\\\\text{DI}(X; C_{m \\\\in [k]}) : 1$. If only the denominator is 0, then $\\\\text{DI}(X; C_{m \\\\in [k]}) : \\\\infty$.}\\n$$\\\\text{DI}(X; C_{m \\\\in [k]})   \\\\frac{\\\\min_{m,l \\\\in [k], m\\\\neq l, i \\\\in C_m, j \\\\in C_l} \\\\lVert x_i - x_j \\\\rVert}{\\\\max_{m \\\\in [k], i,j \\\\in C_m} \\\\lVert x_i - x_j \\\\rVert}.$$\\n\\n\\nIt ranges from $0$ to $\\\\infty$ with a score of $0$ being assigned to incorrectly clustered data, $1$ to unclustered data, and $\\\\infty$ to perfectly clustered data.\\n\\nNow we provide an analogue to Theorem \\\\ref{thm:unclustHammer} with respect to the Dunn Index:\\n\\n\\\\begin{theorem}\\n\\\\label{thm:unclustHammerDunn}\\n   Fix any $n > k > 1$, and $n$-point dataset ${X} \\\\subset \\\\mathbb{R}^{n-1}$ with partition $C_1 \\\\sqcup \\\\cdots \\\\sqcup C_k   [n]$ such that $|C_{m\\\\in[k]}| > 1$ and $\\\\textup{DI}({X}; C_{m\\\\in[k]}) > 1$. For all $1 < \\\\epsilon \\\\leq \\\\textup{DI}({X}_\\\\epsilon; C_{m\\\\in[k]})$, there exists $n$-point dataset ${X}_\\\\epsilon \\\\subset \\\\mathbb{R}^{n-1}$ such that \\n   $$\\\\textup{DI}({X}_\\\\epsilon; C_{m\\\\in[k]})   \\\\epsilon, $$\\n   yet, for any $\\\\rho \\\\in (1, n-1)$:\\n   $${\\\\TSNE}_\\\\rho({X})   {\\\\TSNE}_\\\\rho({X}_\\\\epsilon).$$\\n\\\\end{theorem}\\n\\n\\\\begin{corollary}\\n\\\\label{cor:twoCluster_n_UnclusteredDunn}\\n   For all $n \\\\geq 4$ even, and partition $C_1 \\\\sqcup C_2   [n]$ such that $|C_1||C_2|\\\\frac{n}{2}$. There exist a sequence of $n$-point datasets in $\\\\mathbb{R}^{n-1}$, $\\\\{ {X}_\\\\epsilon\\\\}_{1 < \\\\epsilon \\\\leq \\\\infty}$, with \\n   $$\\\\textup{DI}({X}_\\\\epsilon; C_1, C_2)   \\\\epsilon$$\\n   such that for any $\\\\rho \\\\in (1, n-1)$, $\\\\bigcap_{1<\\\\epsilon \\\\leq \\\\infty}\\\\text{t-SNE}_\\\\rho({X}_\\\\epsilon)$ contains $n$-point dataset ${Y} \\\\subseteq \\\\mathbb{R}^2$ with\\n   $$\\\\textup{DI}(Y; C_1,C_2)   \\\\infty.$$ \\n\\\\end{corollary}\\n\\n\\\\begin{proof}[\\\\textbf{Proof of Theorem \\\\ref{thm:unclustHammerDunn}}]\\n   First, let us assume that $\\\\text{DI}(X; C_{m \\\\in [k]}) < \\\\infty.$ Let $g$ be the function from Corollary \\\\ref{cor:g_exists}, and $f(C)   \\\\text{DI}(g(C); C_{m \\\\in [k]}).$ Fix $i,j \\\\in [n]$ such that:\\n   $$\\\\min_{m,l \\\\in [k], m\\\\neq l, i' \\\\in C_m, j' \\\\in C_l} \\\\lVert x_{i'} - x_{j'} \\\\rVert   \\\\lVert x_i - x_j \\\\rVert,$$\\n   and $t,r \\\\in [n]$ such that:\\n   $$\\\\max_{m \\\\in [k], i',j' \\\\in C_m} \\\\lVert x_{i'} - x_{j'} \\\\rVert   \\\\lVert x_{r} - x_{t} \\\\rVert,$$\\n   Then:   \\\\textcolor{red}{you should emphaszie that the \\\\(g\\\\) transformation doesn't change the identity of max and min; DI is NOT continuous in X because of the max/miin   }\\n   $$f(C)   \\\\frac{ \\\\sqrt{(1-C)\\\\cdot\\\\lVert x_i - x_j \\\\rVert+C}}{ \\\\sqrt{(1-C)\\\\cdot\\\\lVert x_r - x_t \\\\rVert+C}},$$ \\n   \\\\textcolor{red}{note the continuity}\\n   since $g$ preserves the ordering of the distances.   from min to max.\\n   Thus, $f$ is continuous on $[0,1)$ and the image of $f$ on $[0,1)$ is $(f(0), f(1)]   (1, \\\\text{DI}(X; C_{m \\\\in [k]})]$. Therefore, for all $\\\\epsilon \\\\in (1,\\\\text{DI}(X; C_{m \\\\in [k]})]$, there exists $C \\\\in [0,1)$ such that $X_\\\\epsilon   g(C)$ satisfies the hypothesis.\\n\\n   If $\\\\text{DI}(X; C_{m \\\\in [k]})   \\\\infty,$ then $f$ is continuous on $(0,1)$ only. Thus for all $\\\\epsilon \\\\in (1, \\\\text{DI}(X; C_{m \\\\in [k]})),$ there exist $C \\\\in (0,1)$ such that $X_\\\\epsilon   g(C)$ satisfies the hypothesis and for $\\\\epsilon   \\\\text{DI}(X; C_{m \\\\in [k]}),$ $X_\\\\epsilon   X$ satisfies the hypothesis.\\n\\\\end{proof}\\n\\nFor proof of Corollary \\\\ref{cor:twoCluster_n_UnclusteredDunn} see Appendix \\\\ref{sec:proofs_clust}.\\n\\n\\\\iffalse\\n\\\\subsection{Davies–Bouldin index}\\nFor an $n$-point dataset $X   \\\\{x_1, \\\\dots, x_n\\\\} \\\\subset \\\\mathbb{R}^{n-1}$ and a partition of the dataset into clusters $C_1 \\\\sqcup C_2 \\\\sqcup \\\\dots \\\\sqcup C_k   [n]$ with $|C_{m\\\\in[k]}| \\\\geq 1$, the Davies–Bouldin index measures the average ratio of the distance between a cluster and a cluster centroid to the distance to the next closet cluster centroid. Let $E$ be as perviously defined in Appendix \\\\ref{sec:CHindex} and fix $p,q \\\\geq 1$. For $m,l \\\\in [k]$ define:\\n\\n$$S_m : \\\\left( \\\\frac{1}{|C_m|} \\\\sum_{j \\\\in C_m} \\\\lVert x_j - E(C_m) \\\\rVert_2^q\\\\right)^{1/q}$$\\n\\nand\\n\\n$$M_{ml}   \\\\lVert E(C_m) - E(C_l) \\\\rVert_p.$$\\n\\nThen the Davies–Bouldin index is:\\\\footnote{If the denominator and numerator are 0, then $\\\\frac{S_m+S_l}{M_{lm}} : ??$. If only the denominator is 0, then\\n$\\\\frac{S_m+S_l}{M_{lm}} : \\\\infty$.}\\n\\n$$\\\\bar{R}_{p,q}(X; C_{m\\\\in[k]})   \\\\frac{1}{k} \\\\sum_{m \\\\in [k]} \\\\max_{l \\\\neq m} \\\\frac{S_m +S_l}{M_{lm}}.$$\\n\\nThe index ranges from 0 to $\\\\infty$ with a score of $0$ being assigned to perfectly clustered data, and $\\\\infty$ to misclustered data. Let $\\\\Dela \\\\subset \\\\mathbb{R}^{n-1}$ be a regular simplex of any side length. Then, the \\n\\nNow we restate Theorem \\\\ref{thm:perturbhammer} with respect to the Davies-Bouldin index:\\n\\n\\\\fi\\n\\n\\n\\n\\n\\n\\n\\n\\n\\\\subsection{Proofs}\\\\label{sec:proofs_clust} \\n\\n\\n\\nThe main effort of this section will be to prove Lemma \\\\ref{lem:image_of_tSNE} which gives us Theorems \\\\ref{thm:unclustHammer} and \\\\ref{thm:perturbhammer}. We first introduce a number of technical lemmas that collectively show that t-SNE is invariant under additive and multiplicative scaling of the input.\\n\\n\\\\begin{lemma}\\\\label{lem:unique_neighborhood} Let \\\\(H(\\\\cdot)\\\\) denote the entropy function. For any $n>2$, \\\\(X   \\\\{x_1,\\\\ldots,x_n\\\\}\\\\subset \\\\mathbb{R}^{n-1}\\\\) and \\\\(\\\\rho \\\\in (1,n-1)\\\\), there is a unique \\\\(\\\\sigma_i \\\\geq 0\\\\) that minimizes\\n\\\\[\\\\Big|   H(P_{\\\\cdot |i}(X; \\\\sigma_i) ) - \\\\log_2 \\\\rho \\\\Big|.\\\\]\\n\\\\end{lemma}\\n\\\\begin{proof}\\n   This follows easily from the fact that \\\\(H(P_{\\\\cdot |i}(X; \\\\sigma))\\\\) is a continuous, strictly increasing function of \\\\(\\\\sigma\\\\) (see e.g.\\\\ Lemma 4.2 of \\\\citet{jeong2024convergence}), where \\\\(\\\\lim_{\\\\sigma\\\\to \\\\infty}H(P_{\\\\cdot |i}(X; \\\\sigma))   \\n   \\\\log_2(n-1)\\\\) and \\\\(H(P_{\\\\cdot |i}(X; 0)) \\\\in (0,\\\\log_2(n-1))\\\\). If \\\\(\\\\log_2 \\\\rho \\\\in (h, n-1)\\\\), then there exists a unique \\\\(\\\\sigma_i\\\\) achieving zero gap. Otherwise, the \\n\\\\end{proof}\\n\\n\\\\begin{definition}\\n   For any $n \\\\geq 1$, dataset $X   \\\\{x_1, \\\\dots, x_n\\\\} \\\\subset \\\\mathbb{R}^{n-1},$ and $C \\\\geq 0$, define ${X_{+C}   \\\\{x'_1, \\\\dots, x'_n\\\\} \\\\subset \\\\mathbb{R}^{n-1}}$ such that for all $i\\\\neq j$ \\n   $$\\\\lVert x'_i-x_j' \\\\rVert^2   \\\\lVert x_i-x_j \\\\rVert^2 + C.$$\\n\\\\end{definition}\\n\\n\\\\begin{lemma}\\\\label{lem:X+C_exists}\\n   Fix any $n \\\\geq 1$. For all $n$-point datasets $X   \\\\{x_1, \\\\dots, x_n\\\\} \\\\subset \\\\mathbb{R}^{n-1}$ and $C \\\\geq 0$, there exists $X_{+C}   \\\\{x'_1, \\\\dots, x'_n\\\\} \\\\subset \\\\mathbb{R}^{n-1}$ such that for all $i\\\\neq j$, $\\\\lVert x'_i-x_j' \\\\rVert^2   \\\\lVert x_i-x_j \\\\rVert^2 + C$.\\n\\\\end{lemma}\\n\\\\begin{proof}\\n   Let $D$ be the squared inter-point distance matrix of $X$. Thus, the inter-point squared distance matrix of $X_{+C}$ is $D_{+C}   D + C\\\\cdot (11^T-I_{n})$. By a famous theorem by \\\\citet{schoenberg1938metric}, $X_{+C}$ is isometrically embeddable in $\\\\mathbb{R}^{n-1}$ with respect to $\\\\ell_2$ metric if and only if $\\\\forall u \\\\in \\\\mathbb{R}^n$ with $u^T \\\\vec{1}   0$, $u^T D_{+C} u \\\\leq 0$ holds. Indeed,\\n   $$u^T D_{+C} u   u^T D u + C\\\\cdot(u^T\\\\vec{1})(\\\\vec{1}^T u) - C \\\\cdot u^T u   u^T D u - C \\\\cdot \\\\lVert u \\\\rVert^2 \\\\leq 0,$$\\n   where the final inequality uses the fact that $D$ is embeddable.\\n\\\\end{proof}\\n\\n\\\\begin{lemma}\\\\label{lem:tSNE_additive_invariance}\\n   Fix any $n > 2$. For all $n$-point datasets $X \\\\subset \\\\mathbb{R}^{n-1}$, $\\\\rho \\\\in (1, n-1)$, and $C \\\\geq 0$:\\n   $${\\\\TSNE}_\\\\rho(X)   {\\\\TSNE}_\\\\rho(X_{+C}).$$\\n\\\\end{lemma}\\n\\\\begin{proof}\\n   It is sufficient to show that the input affinity matrices for $X$ and $X_{+C}$ are identical. Indeed, for all $i,j \\\\in [n], i\\\\neq j$ and for all \\\\(\\\\sigma_i > 0\\\\)\\n   \\\\begin{align*}\\n   P_{j|i}(X ; \\\\sigma_i) & \\\\frac{\\\\exp\\\\left(-\\\\lVert x_i - x_j \\\\rVert^2_2 / (2\\\\sigma_i^2)\\\\right)}{\\\\sum_{k \\\\neq i} \\\\exp\\\\left(-\\\\lVert x_i - x_k \\\\rVert^2_2 / (2\\\\sigma_i^2)\\\\right)} \\\\\\\\\\n   & \\\\frac{\\\\exp\\\\left(-(\\\\lVert x_i - x_j \\\\rVert^2_2+C) / (2\\\\sigma_i^2)\\\\right)}{\\\\sum_{k \\\\neq i} \\\\exp\\\\left(-(\\\\lVert x_i - x_k \\\\rVert^2_2+C) / (2\\\\sigma_i^2)\\\\right)}   P_{j|i}(X_{+C}; \\\\sigma_i).\\n   \\\\end{align*}\\n   If \\\\(\\\\sigma_i   0\\\\), then \\\\(P_{j|i}(X)\\\\) is purely a function of the ordering of the squared interpoint distances, which is unaffected by adding a constant.\\n\\\\end{proof}\\n\\n\\n\\n\\\\begin{lemma}\\\\label{lem:tSNE_multiplicative_invariance}\\n   Fix any $n > 2$. For all $n$-point datasets $X \\\\subset \\\\mathbb{R}^{n-1}$, $\\\\rho \\\\in (1, n-1)$, and $C > 0$:\\n   $${\\\\TSNE}_\\\\rho(X)   {\\\\TSNE}_\\\\rho(C \\\\cdot X).$$\\n\\\\end{lemma}\\n\\n\\\\begin{proof} \\n   First note that for any dataset $X$ and its scaling $C\\\\cdot X$, and all $\\\\sigma_i\\\\geq 0$, we have the following:\\n   It suffices to show that the input affinity matrices for $X$ and $C \\\\cdot X$ are identical. Note that, for all datasets \\\\(X\\\\) and \\\\(\\\\sigma_i > 0\\\\):\\n   \\\\iffalse For all \\\\(\\\\sigma_i>   0\\\\), the entropy of \\\\(P_{\\\\cdot |i}(X; \\\\sigma_i )\\\\) is given by:Let \\\\(\\\\sigma_i\\\\) be any value achieving the perplexity condition, and let \\\\(H(P_{\\\\cdot | i})\\\\) be the corresponding entropy of the conditional affinities. Observe:\\n   \\\\begin{align*}\\n   &\\\\log_2(\\\\rho) \\n   &H_i   \\n   &\\\\sum_{j 1, j\\\\neq i}^n P_{j|i} \\\\log_2(1/P_{j|i}) \\\\\\\\\\n   &\\n   -\\\\sum_{j 1, j\\\\neq i}^n \\\\frac{\\\\exp(-\\\\lVert x_i-x_j\\\\rVert^2/(2\\\\sigma_i^2))}{\\\\sum_{k1, k\\\\neq j}^n \\\\exp(-\\\\lVert x_i-x_k\\\\rVert^2/(2\\\\sigma_i^2))} \\\\log_2\\\\left(\\\\frac{\\\\exp(-\\\\lVert x_i-x_j\\\\rVert^2/(2\\\\sigma_i^2))}{\\\\sum_{k1, k\\\\neq j}^n \\\\exp(-\\\\lVert x_i-x_k\\\\rVert^2/(2\\\\sigma_i^2))}\\\\right).\\n   \\\\end{align*}\\n   Since \\\\cite{jeong2024convergence} showed the above uniquely defines $\\\\sigma_i$ (see Lemma 4.2), it must be the case that for $C\\\\cdot X$, the optimal neighborhood size for $i$ is $C \\\\cdot \\\\sigma_i$. \\\\fi\\n   Therefore for any $j \\\\in [n]\\\\setminus \\\\{i\\\\}$:\\n   \\\\begin{align*}\\n   P_{j|i}(C\\\\cdot X; C\\\\cdot \\\\sigma_i)   \\\\frac{\\\\exp(-C^2\\\\cdot\\\\lVert x_i-x_j\\\\rVert^2/(2C^2\\\\cdot\\\\sigma_i^2))}{\\\\sum_{k1, k\\\\neq j}^n \\\\exp(-C^2\\\\cdot\\\\lVert x_i-x_k\\\\rVert^2/(2C^2\\\\cdot\\\\sigma_i^2))}   P_{j|i}(X; \\\\sigma_i).\\n   \\\\end{align*}\\nLet \\\\(H(\\\\cdot)\\\\) denote the entropy function. By the above, \\\\({H(P_{\\\\cdot | i}(X;\\\\sigma_i))   H(P_{\\\\cdot | i}(C\\\\cdot X;C\\\\cdot \\\\sigma_i))}\\\\). Note that \\\\(H(P_{\\\\cdot|i}(X; \\\\sigma_i))\\\\) is a strictly decreasing function of \\\\(\\\\sigma_i > 0\\\\) as shown in \\\\citep{jeong2024convergence} Lemma 4.2 (and correspondingly for \\\\(H(P_{\\\\cdot|i}(C\\\\cdot X; \\\\gamma_i))\\\\)).\\nLet $\\\\sigma^*_i$ and correspondingly $\\\\gamma^*_i$ be the (unique, per Lemma \\\\ref{lem:unique_neighborhood}) neighborhood scalings that satisfy the perplexity condition for \\\\(X\\\\) and \\\\(C\\\\cdot X\\\\) respectively (see Section \\\\ref{sec:prelims}). \\nFor any input perplexity value $\\\\rho \\\\in (1,n-1)$, let $\\\\sigma^*_i$ (and correspondingly $\\\\gamma^*_i)$ be the (unique) neighborhood scalings that minimize the gap between $H(P_{\\\\cdot | i}(X;\\\\sigma_i))$ and $\\\\log \\\\rho$ (c.f.\\\\ Section \\\\ref{sec:prelims}) (correspondingly $H(P_{\\\\cdot | i}(C\\\\cdot X;\\\\gamma_i))$). \\nThen \\\\(\\\\gamma_i^*   C\\\\cdot \\\\sigma_i^*\\\\).\\n\\nTherefore \\\\(P_{\\\\cdot |i}(C\\\\cdot X; \\\\gamma_i^*)   P_{\\\\cdot |i}(C\\\\cdot X; C\\\\cdot \\\\sigma_i^*) P_{\\\\cdot |i}(X; \\\\sigma_i^*) \\\\), yielding the result. Clearly,\\n\\\\[ \\\\min | \\\\log \\\\rho - H()|\\\\]\\nThat is,\\n\\\\begin{align*}\\n\\\\Sigma^*_i &: \\\\argmin_{\\\\sigma > 0} \\\\Big| \\\\log \\\\rho - H(P_{\\\\cdot | i}(X;\\\\sigma)_i) \\\\Big|   \\\\\\\\\\n\\\\Gamma^*_i &: \\\\argmin_{\\\\gamma > 0} \\\\Big| \\\\log \\\\rho - H(P_{\\\\cdot | i}(C\\\\cdot X;\\\\gamma_i)) \\\\Big|.   \\n\\\\end{align*}\\nWe will show \\\\(\\\\Sigma_i^*   \\\\Gamma_i^*\\\\)\\nFor all \\\\(\\\\gamma_i \\\\in \\\\Gamma_i^*\\\\), there exists \\\\(\\\\sigma_i \\\\in \\\\Sigma_i^*\\\\) such that \\\\(\\\\gamma_i^*   C\\\\cdot \\\\sigma_i^*\\\\). If this were not the case, we would have a contradiction: \\nBy \\\\citep{jeong2024convergence} Lemma 4.2, there exists a unique \\\\(\\\\sigma_i^*\\\\) that determines \\\\(P_{j|i}(X)\\\\) and a unique \\\\(\\\\gamma_i^*\\\\) that determines \\\\(P_{j|i}(C\\\\cdot X)\\\\). Therefore \\\\(\\\\gamma_i^*   C\\\\sigma_i\\\\) and hence \\\\(P_{\\\\cdot|i}(C\\\\cdot X)   P_{\\\\cdot|i}(X) \\\\).\\n   Let \\\\(H(\\\\cdot)\\\\)By \\\\citep{jeong2024convergence} Lemma 4.2\\n\\\\end{proof}\\n\\nUsing the additive and multiplicative invariance of t-SNE, we now prove Lemma \\\\ref{lem:image_of_tSNE}:\\n\\n\\\\todo[inline]{Redo labeling / maybe restatable?}\\n\\n\\\\begin{lemma}\\n   Fix any $n \\\\geq 2$ and $\\\\rho \\\\in [1, n-1]$. For all $\\\\epsilon > 0$, Let $S_\\\\epsilon   \\\\{ \\\\mathcal{X}\\\\{x_1, \\\\dots, x_n\\\\} \\\\subset \\\\mathbb{R}^{n-1}: \\\\forall i,j \\\\in [n], \\\\lVert x_i - x_j \\\\rVert^2 \\\\in [1-\\\\epsilon, 1+\\\\epsilon]\\\\}$, then:\\n   $${\\\\TSNE}_\\\\rho(n)   {\\\\TSNE}_\\\\rho(S_\\\\epsilon).$$\\n\\\\end{lemma}\\n\\n\\\\begin{proof}[\\\\textbf{Proof of Lemma \\\\ref{lem:image_of_tSNE}}] \\n   Fix any $\\\\epsilon>0$. It suffices to show that ${\\\\IMTSNE} \\\\subseteq {\\\\TSNE}_\\\\rho(\\\\Delta_\\\\epsilon).$ Fix any $Y \\\\in {\\\\IMTSNE}$, there exists a $n$-point dataset $X   \\\\{x_1,\\\\ldots,x_n\\\\} \\\\subset \\\\mathbb{R}^{n-1}$ such that:\\n   $$Y \\\\in {\\\\TSNE}_\\\\rho(X).$$\\n   Using additive and multiplicative invariance, we will manipulate $X$ such that it is in $\\\\Delta_\\\\epsilon$ which by Lemma \\\\ref{lem:tSNE_additive_invariance} and Lemma \\\\ref{lem:tSNE_multiplicative_invariance} will not change the output. Let $D_{\\\\max}   \\\\max_{i\\\\neq j}\\\\lVert x_i - x_ j \\\\rVert^2$ and $D_{\\\\min}   \\\\min_{i,j \\\\in [n], i\\\\neq j}\\\\lVert x_i - x_ j \\\\rVert^2$. WLOG, assume that $D_{\\\\max} \\\\neq 0$ otherwise $X_{+1} \\\\in \\\\Delta_\\\\epsilon.$ Set $A   \\\\frac{1}{2\\\\epsilon}\\\\big \\\\lvert (1-\\\\epsilon)D_{\\\\max} - (1+\\\\epsilon)D_{\\\\min} \\\\big \\\\rvert$   and $B   \\\\frac{1+\\\\epsilon}{D_{\\\\max} + A}.$ Note that since $A \\\\geq 0$ and $D_{\\\\max}>0$, $B$ is well defined and strictly greater than 0. Then the dataset $B \\\\cdot (X_{+A})   \\\\{x'_1, \\\\dots, x'_n\\\\}$ exists by Lemma \\\\ref{lem:X+C_exists} and is such that:\\n   $$\\\\text{t-SNE}_\\\\rho(X)   \\\\text{t-SNE}_\\\\rho(B \\\\cdot (X_{+A}))$$\\n   by Lemma \\\\ref{lem:tSNE_additive_invariance} and Lemma \\\\ref{lem:tSNE_multiplicative_invariance}. Moreover, for all $i \\\\neq j$:\\n   \\\\begin{align*}\\n   \\\\lVert x'_i - x'_j \\\\rVert^2 & \\\\frac{1+\\\\epsilon}{D_{\\\\max}+A} \\\\cdot (\\\\lVert x_i - x_j \\\\rVert^2+A) \\\\leq   1+\\\\epsilon,\\n   \\\\end{align*}\\n   and\\n   \\\\begin{align*}\\n   \\\\lVert x'_i - x'_j \\\\rVert^2 &\\\\geq (1+\\\\epsilon)\\\\frac{D_{\\\\min}+A}{D_{\\\\max}+A}\\\\\\\\\\n   &\\\\geq (1+\\\\epsilon)\\\\frac{D_{\\\\min}+\\\\frac{1}{2\\\\epsilon}\\\\big ( (1-\\\\epsilon)D_{\\\\max} - (1+\\\\epsilon)D_{\\\\min} \\\\big )}{D_{\\\\max}+\\\\frac{1}{2\\\\epsilon}\\\\big ( (1-\\\\epsilon)D_{\\\\max} - (1+\\\\epsilon)D_{\\\\min} \\\\big )}\\\\\\\\\\n   &\\\\geq (1+\\\\epsilon)\\\\frac{(1-\\\\epsilon)(D_{\\\\max} - D_{\\\\min})}{(1+\\\\epsilon)(D_{\\\\max} - D_{\\\\min})}   1-\\\\epsilon,\\n   \\\\end{align*}\\n\\n   where the second inequality follows because $A \\\\geq \\\\frac{1}{2\\\\epsilon}\\\\big ( (1-\\\\epsilon)D_{\\\\max} - (1+\\\\epsilon)D_{\\\\min} \\\\big ) > -D_{\\\\max}.$\\n\\\\end{proof}\\n\\nThe above lemmas give us the following useful corollary that will allow us to prove Theorem \\\\ref{thm:unclustHammer}, Theorem \\\\ref{thm:unclustHammerCH}, and Theorem \\\\ref{thm:unclustHammerDunn}.\\n\\n\\\\begin{corollary}\\\\label{cor:g_exists}\\n   Fix any $n > 2$, and $X   \\\\{x_1, \\\\dots, x_n\\\\} \\\\subset \\\\mathbb{R}^{n-1}.$ There exists a well-defined, continuous function $g: [0,1] \\\\to \\\\mathbb{R}^{n \\\\times n-1}$ such that:\\n   $$C \\\\mapsto ((1-C)\\\\cdot X)_{+C},$$\\n   and for all $\\\\rho \\\\in (1, n-1)$ and $C \\\\in [0,1):$\\n   $${\\\\TSNE}_\\\\rho(X)   {\\\\TSNE}_\\\\rho(g(C)).$$\\n   Moreover, $g$ is such that for all $i \\\\neq j, l \\\\neq m \\\\in [n]:$\\n\\n   $$\\\\lVert x_i - x_j \\\\rVert \\\\leq \\\\lVert x_l - x_k \\\\rVert \\\\implies \\\\lVert g(C)_i - g(C)_j \\\\rVert \\\\leq \\\\lVert g(C)_l - g(C)_k \\\\rVert$$\\n\\\\end{corollary}\\n\\n\\\\begin{proof}\\n   $g$ is well defined by Lemma \\\\ref{lem:X+C_exists} and WLOG continuous since it continuously transforms the distances in $X$:\\n   $$\\\\forall i,j \\\\in [n], i\\\\neq j, \\\\hspace{1cm} \\\\lVert g(C)_i - g(C)_j \\\\rVert   \\\\sqrt{(1-C) \\\\cdot \\\\lVert x_i - x_j \\\\rVert^2 + C}.$$\\n   Moreover, by Lemmas \\\\ref{lem:tSNE_additive_invariance} and \\\\ref{lem:tSNE_multiplicative_invariance}, for all $C \\\\in [0,1):$\\n   $${\\\\TSNE}_\\\\rho(X)   {\\\\TSNE}_\\\\rho(g(C)).$$\\n\\\\end{proof}\\n\\nUsing the above lemmas, Theorem \\\\ref{thm:unclustHammer}, Corollary \\\\ref{cor:twoCluster_n_Unclustered}, and Theorem \\\\ref{thm:perturbhammer} are proven.\\n\\n\\\\UnclustHammer*\\n\\n\\\\begin{proof}[\\\\textbf{Proof of Theorem \\\\ref{thm:unclustHammer}}]\\n   Let $g$ be the function from Corollary \\\\ref{cor:g_exists}, and $f(C)   \\\\bar{\\\\mathcal{S}}(g(C); C_{m \\\\in [k]}).$ Note that $f$ is continuous for $C \\\\in [0,1]$ since $g$ is continuous, and $\\\\bar{\\\\mathcal{S}}(\\\\ \\\\cdot\\\\ ; C_{m \\\\in [k]})$ is continuous whenever for all $i\\\\in[n]$, $a(i), b(i) \\\\neq 0$ which follows from $\\\\bar{\\\\mathcal{S}}(X ; C_{m \\\\in [k]})$ being well-defined and the definition of $g$. Therefore, the image of $f$ on $[0,1)$ contains the interval $(f(1), f(0)]   (0, \\\\bar{\\\\mathcal{S}}(X ; C_{m \\\\in [k]})]$ (or if $\\\\bar{\\\\mathcal{S}}(X ; C_{m \\\\in [k]}) \\\\leq 0,$ $[\\\\bar{\\\\mathcal{S}}(X ; C_{m \\\\in [k]}), 0)$). Thus, for all $\\\\epsilon \\\\in (0,1]$, there exists $C \\\\in [0,1)$ such that $X_\\\\epsilon   g(C)$ satisfies the hypothesis.\\n\\\\end{proof}\\n\\nNow we can prove Corollary \\\\ref{cor:twoCluster_n_Unclustered}, Corollary \\\\ref{cor:twoCluster_n_UnclusteredCH}, and Corollary \\\\ref{cor:twoCluster_n_UnclusteredDunn} simultaneously: \\n\\n\\\\iffalse\\n\\\\todo[inline]{fix numbering of this corollary.}\\n\\\\TwoClusterNUnclustered*\\n\\\\fi\\n\\n\\\\begin{proof}[\\\\textbf{Proof of Corollaries \\\\ref{cor:twoCluster_n_Unclustered}, \\\\ref{cor:twoCluster_n_UnclusteredCH}, and \\\\ref{cor:twoCluster_n_UnclusteredDunn}}]\\n   The proof proceeds by showing a dataset and its output who have an average silhouette score of 1, Calinski-Harabasz index of $\\\\infty$, and Dunn index of $\\\\infty$, and then applies Theorem \\\\ref{thm:unclustHammer}, Theorem \\\\ref{thm:unclustHammerCH}, and Theorem \\\\ref{cor:twoCluster_n_UnclusteredDunn} respectively. WLOG fix partition $C_1 \\\\sqcup C_2   [n]$ with $C_1[1,n/2]$ and $C_2   [n/2+1,n]$. Consider the $n$-point dataset, $\\\\mathcal{X}\\\\{x_1, \\\\dots, x_n\\\\} \\\\subseteq \\\\mathbb{R}^{n-1}$, such that for all $i \\\\in C_1$, $x_i   \\\\vec{0}$, and for all $i \\\\in C_2$, $x_i   \\\\vec{e_1}$.\\n   \\n   Routine calculations show that the conditional input affinities are:\\n\\n   $$P_{i|j}   \\\\begin{cases} \\n   \\\\frac{1}{\\\\frac{n}{2}-1+\\\\frac{n}{2}\\\\exp\\\\left(-\\\\frac{1}{2\\\\sigma^2_j} \\\\right)} & i \\\\in C(j), i \\\\neq j \\\\\\\\\\n   \\\\frac{\\\\exp\\\\left(-\\\\frac{1}{2\\\\sigma^2_j} \\\\right)}{\\\\frac{n}{2}-1+\\\\frac{n}{2}\\\\exp\\\\left(-\\\\frac{1}{2\\\\sigma^2_j} \\\\right)} & i \\\\not\\\\in C(j)\\\\\\\\\\n   0 & i   j.\\n   \\\\end{cases}$$\\n\\n   By symmetry, $\\\\sigma_j   \\\\sigma_i$ for all $i,j \\\\in [n].$ Hence, let $\\\\sigma$ be the neighborhood size for all $j \\\\in [n]$ which is non-zero and well defined for $\\\\rho \\\\in [1, n-1].$ Thus the symmetrized input affinities are:\\n\\n   $$P_{ij}   \\\\begin{cases} \\n   \\\\frac{1}{\\\\frac{n^2}{2}-n+\\\\frac{n^2}{2}\\\\exp\\\\left(-\\\\frac{1}{2\\\\sigma^2} \\\\right)} & i \\\\in C(j), i\\\\neq j \\\\\\\\\\n   \\\\frac{\\\\exp\\\\left(-\\\\frac{1}{2\\\\sigma^2}\\\\right)}{\\\\frac{n^2}{2}-n+\\\\frac{n^2}{2}\\\\exp\\\\left(-\\\\frac{1}{2\\\\sigma^2} \\\\right)} & i \\\\in C_1, j \\\\in C_2 \\\\\\\\\\n   0 & i   j.\\n   \\\\end{cases}$$\\n\\n   Any set $\\\\mathcal{Y}   \\\\{ y_1, \\\\dots, y_n\\\\} \\\\subseteq \\\\mathbb{R}$ is a global minimizer if $P_{ij}   Q_{ij}$ for all $i,j \\\\in [n]$. In this case, this is achieved if $y_{i \\\\in C_1}   0$ and $y_{i \\\\in C_2}   \\\\sqrt{\\\\exp\\\\left( \\\\frac{1}{2\\\\sigma^2}\\\\right) - 1}.$ Furthermore, since $\\\\mathcal{Y}$ can be isometrically embedded in $\\\\mathbb{R}^d$ for all $d \\\\geq 1$, this result holds for t-SNE embeddings of all dimensions. \\n\\n   To finish the proof note that for all $i \\\\in [n]$, $a(i)0$ when defined with respect to $\\\\mathcal{Y}$ and partition $C_1 \\\\sqcup C_2.$\\n\\\\end{proof}\\n\\n\\\\Perturbhammer*\\n\\\\begin{proof}[\\\\textbf{Proof of Theorem \\\\ref{thm:perturbhammer}}]\\n   The proof is immediate by application of Lemma \\\\ref{lem:image_of_tSNE}.   \\n\\\\end{proof}\\n\\n\\\\newpage\\n\\n\\\\subsection{Impostor dataset construction}\\n\\\\label{app:impostor_construction}\\n\\nThe construction of an impostor dataset based on an input dataset is done as follows.\\n\\n\\\\begin{algorithm}[H]\\n\\\\caption{Impostor Dataset Creation}\\n\\\\label{alg:impostor}\\n\\\\begin{algorithmic}[1]\\n\\\\Require Dataset $X   \\\\{x_1, \\\\ldots, x_n\\\\}$ with at least two distinct points, and tolerance $\\\\epsilon > 0$\\n\\\\Return Dataset $X_\\\\epsilon   \\\\{x_1', \\\\ldots, x_n'\\\\}$ such that $\\\\mathrm{t\\\\text{-}SNE}(X_\\\\epsilon)   \\\\mathrm{t\\\\text{-}SNE}(X)$ and aspect ratio$(X_\\\\epsilon) < 1+\\\\epsilon$\\n\\\\State Construct squared interpoint distance matrix of $X$, denote it by $D$\\n\\\\State Form $D' \\\\gets \\\\frac{\\\\epsilon}{\\\\max_{i,j} D_{ij}} \\\\cdot D + (11^\\\\top - I_n)$\\n\\\\State Run classical multidimensional scaling on $D'$ to obtain its Euclidean embedding $$X_\\\\epsilon   \\\\{x_1', \\\\ldots, x_n'\\\\} \\\\subset \\\\mathbb{R}^{n-1}.$$\\n\\\\State \\\\Return $X_\\\\epsilon$\\n\\\\end{algorithmic}\\n\\\\end{algorithm}\\n\\n\\n\\\\newpage\\n\\n\\\\section{Appendix: Misrepresentation of Outliers}\\\\label{sec:appendix_outliers}\\n\\n\\n\\\\subsection{Additional Experiments}\\\\label{sec:appendix_outliers_experiment}\\nWe provide a comparison of t-SNE and PCA on the BBC news dataset. For ease of presentation, we take a three-cluster, \\\\((n   1204)\\\\)-size subset (business, sports, tech) and we analyze what happens under injection of $120$ poison points versus $120$ far outliers. \\n\\n\\\\begin{figure}[h!]\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/appendix_figs/bbc_appendix.png}\\n   \\\\caption{\\\\small t-SNE vs.\\\\ PCA on poison points versus outlier points on a three-cluster subset of the BBC news dataset. The label on the bottom left is silhouette score of the plot (sans injected points) with respect to their ground-truth labels.}\\n   \\\\label{fig:bbc_appendix}\\n\\\\end{figure}\\n\\n\\nIn both Figure \\\\ref{fig:outliers_real_world} and Figure \\\\ref{fig:bbc_appendix}, poison points are picked as follows: first we run a $k$-means algorithm on the original dataset; then, for each poison point, we pick one of the these means and $10$ random points of the dataset, and we average these two quantities (the idea is to connect the points in a way that contradicts the ground-truth three-clustering). We found \\\\(k2\\\\) worked well. We pick outlier points as normal vectors centered at the mean of the dataset with variance \\\\(32\\\\) (the diameter of the original dataset is roughly \\\\(1.5\\\\)).\\n\\n\\\\subsection{Proofs}\\n\\n\\\\begin{lemma}\\\\label{lem:Qsum_bound} Fix $n\\\\geq 2$ and \\\\(Y   \\\\{y_0,\\\\ldots,y_{n-1}\\\\} \\\\subset \\\\mathbb{R}^d\\\\). Let ${\\\\beta : \\\\diam(Y\\\\setminus \\\\{y_0\\\\})}$ and ${\\\\gamma : \\\\min_{j\\\\in [n]} \\\\|y_0-y_j\\\\|}$. Then\\n\\\\begin{align*}\\n   \\\\sum_{i1}^n Q_{0i} &\\\\leq \\\\frac{1}{2 + (n-2)\\\\cdot \\\\frac{1+\\\\gamma^2}{1+\\\\beta^2}} . \\\\\\\\\\n   & \\\\frac{1 +   D_Y^2}{2(1+D_Y^2 ) + (n-1)(1+\\\\alpha^2 D_Y^2)} \\n\\\\end{align*}\\n\\\\end{lemma}\\n\\\\begin{proof}\\n   Let \\\\(Z_0   \\\\sum_{i1}^n \\\\frac{1}{1+\\\\|y_i-y_0\\\\|^2}\\\\) and \\\\(Z_{1:n}   \\\\sum_{i, j: i\\\\neq j}   \\\\frac{1}{1+\\\\|y_i-y_j\\\\|^2}\\\\). Then\\n   \\\\[\\\\sum_{i1}^n Q_{0i}   \\\\frac{Z_0}{2Z_0 + Z_{1:n}}   \\\\frac{1}{2 + Z_{1:n}/Z_0}.\\\\]\\nNow observe that\\n\\\\begin{align*}\\n   \\\\frac{Z_{1:n}}{Z_0} & \\\\frac{\\\\sum_{i,j:i\\\\neq j} (1 + \\\\|y_i-y_j\\\\|^2)^{-1}}{\\\\sum_{j1}^n (1 + \\\\|y_0 -y_j\\\\|^2)^{-1}}   \\\\\\\\\\n &\\\\geq \\\\frac{(n-1)(n-2)(1+\\\\max_{i,j \\\\in[n]} \\\\|y_i-y_j\\\\|^2)^{-1}}{(n-1)(1+\\\\min_{j \\\\in[n]} \\\\|y_0-y_j\\\\|^2)^{-1}}   \\\\\\\\\\n   & \\\\frac{(n-2)(1+\\\\gamma^2)}{1+\\\\beta^2}.   \\\\\\\\\\n   & \\\\frac{(n^2-n)(1+\\\\beta^2)^{-1}}{n(1+\\\\alpha^2)^{-1}}   \\\\\\\\\\n   & (n-1)\\\\cdot \\\\frac{1 + \\\\alpha^2}{1 + \\\\beta^2} .\\n   &\\\\geq \\\\frac{(n^2-n)(1+ D_Y^2)}{n( 1 + \\\\alpha^2 D_Y^2)}\\\\geq (n-1)/\\\\alpha^2 \\n\\\\end{align*}\\nNote that \\\\(\\\\min_{j\\\\in [n]}\\\\|y_0-y_j\\\\|^2   \\\\geq \\\\alpha\\\\max\\\\{\\\\beta,1\\\\}\\\\).\\n{\\\\color{red} Under new definition (margin-based) of $\\\\alpha$-outlier, it is not obvious that $\\\\alpha^2   \\\\min_{j \\\\in [n]} \\\\lVert y_0 - y_j \\\\rVert^2$. Maybe do a case by case thing on whether $\\\\diam > 1$. Also doesn't this margin definition add a factor $2$ somewhere to the inequality?}\\nPlugging this back into the previous equation gives the statement. \\n\\\\end{proof}\\n\\n\\\\textcolor{red}{UNDER CONSTRUCTION}\\n\\n\\\\begin{lemma}\\\\label{lem:r_split}\\n   Fix \\\\(n\\\\geq 2\\\\) and \\\\(Y   \\\\{y_0,y_1,...,y_{n-1}\\\\} \\\\subset \\\\mathbb{R}^d\\\\). If \\\\(Y\\\\) is a \\\\((\\\\alpha, y_0)\\\\)-outlier configuration such that $\\\\alpha   \\\\alpha(Y)$, then there exists $v \\\\in \\\\mathbb{R}^d$ such that for all $i \\\\in [n]$:\\n   \\\\[\\\\|y_i-y_0\\\\| \\\\cdot \\\\frac{ \\\\alpha}{\\\\sqrt{1+\\\\alpha^2}} \\\\leq (y_i - y_0)\\\\cdot v \\\\leq \\\\|y_i-y_0\\\\|.\\\\]\\n\\\\end{lemma}\\n\\n\\\\begin{proof}\\n   Fix $i \\\\in [n]$, let ${\\\\beta : \\\\diam(Y\\\\setminus \\\\{y_0\\\\})}$, and WLOG let $y_0   0.$ Take $v$ as in Definition \\\\ref{def:alpha_outlier}. Then by Cauchy-Schwarz, $(y_i-y_0) \\\\cdot v \\\\leq \\\\| y_i-y_0 \\\\|.$ To prove the other side of the inequality, we only need to lower bound the cosine of the angle between $y_i-y_0$ and $v$:\\n   $$(y_i - y_0)\\\\cdot v   \\\\| y_i - y_0 \\\\| \\\\cdot \\\\cos(\\\\angle({v, y_i})).$$\\n   Since $v$ is the maximum-margin hyperplane between $y_00$ and $Y\\\\setminus \\\\{y_0\\\\}$, it holds that $u   v \\\\cdot (\\\\alpha \\\\max\\\\{1, \\\\beta\\\\})$ is in the convex hull of $Y\\\\setminus \\\\{y_0\\\\}.$ Indeed, $\\\\| u \\\\|   \\\\inf_{y \\\\in \\\\conv(Y\\\\setminus \\\\{y_0\\\\})} \\\\| y \\\\|.$ Thus, we know that the closed ball $\\\\overline{B_\\\\beta (u)}$ contains $\\\\conv(Y\\\\setminus\\\\{y_0\\\\}).$ Therefore, there exists $t \\\\in \\\\mathbb{R}^d$ such that $\\\\| t \\\\| \\\\leq \\\\beta, u + t   y_i,$ and $u\\\\cdot t \\\\geq 0$. Hence\\n   $$\\\\cos(\\\\angle(v, y_i))   \\\\frac{v \\\\cdot y_i}{\\\\lVert y_i \\\\rVert}   \\\\frac{v\\\\cdot(u+t)}{\\\\sqrt{\\\\lVert u \\\\rVert^2 + \\\\lVert t\\\\rVert^2 -2u \\\\cdot t}} \\\\geq \\\\frac{\\\\alpha \\\\max(\\\\beta, 1)}{\\\\sqrt{\\\\alpha^2 \\\\max(\\\\beta, 1)^2 + \\\\beta^2}} \\\\geq \\\\frac{\\\\alpha}{\\\\sqrt{1+\\\\alpha^2}},$$\\n   completing the proof.\\n\\\\end{proof}\\n\\n\\n\\\\OutlierAbs*\\n\\\\begin{proof}\\nFix $Y \\\\in \\\\IMTSNE$ and define \\\\(\\\\gamma   \\\\min_i \\\\|y_i-y_0\\\\|\\\\). WLOG, let \\\\(y_0\\\\) be the outlier point and assume $\\\\gamma > 0$ otherwise the hypothesis goes through trivially.\\n   Since \\\\(Y\\\\) is stationary, \\\\(\\\\frac{\\\\partial L}{\\\\partial y_0}   0\\\\). Pick \\\\(v\\\\) as in Lemma \\\\ref{lem:r_split} and observe:\\n   \\\\begin{align*}\\n   0   \\\\frac{\\\\partial \\\\mathcal{L}}{\\\\partial y_0}\\\\cdot v & \\\\sum_{i1}^{n-1} \\\\frac{(P_{i0} - Q_{i0})(y_0-y_i)\\\\cdot v}{1 + \\\\|y_0-y_i\\\\|^2} \\\\\\\\\\n   &\\\\geq \\\\frac{\\\\alpha}{\\\\sqrt{1 + \\\\alpha^2}}\\\\sum_{i1}^{n-1}   P_{i0} \\\\frac{\\\\|y_0-y_i\\\\|}{1 + \\\\|y_0-y_i\\\\|^2}- \\\\sum_{i1}^{n-1} Q_{i0} \\\\frac{\\\\|y_0-y_i\\\\|}{1 + \\\\|y_0-y_i\\\\|^2}\\\\\\\\\\n   &\\\\geq \\\\frac{\\\\alpha}{\\\\sqrt{1 + \\\\alpha^2}}\\\\sum_{i1}^{n-1}   P_{i0} \\\\frac{\\\\|y_0-y_i\\\\|}{1 + \\\\|y_0-y_i\\\\|^2}- \\\\sum_{i1}^{n-1} Q_{i0} \\\\frac{\\\\|y_0-y_i\\\\|}{1 + \\\\|y_0-y_i\\\\|^2}\\\\\\\\\\n   &\\\\geq \\\\frac{\\\\alpha}{\\\\sqrt{\\\\alpha^2 + 1}}   \\\\frac{\\\\gamma}{1+(\\\\gamma+\\\\beta)^2}\\\\sum_{i1}^{n-1} P_{i0}\\n   - \\\\frac{\\\\gamma + \\\\beta}{1 + \\\\gamma^2}\\\\sum_{i1}^{n-1} Q_{i0} \\\\\\\\\\n   &\\\\geq   \\\\frac{\\\\alpha}{\\\\sqrt{1 + \\\\alpha^2}}\\\\frac{\\\\gamma}{1+(\\\\gamma+\\\\beta)^2}\\\\frac{1 + \\\\sum_{i1}^{n-1} P_{0|i}}{2n}\\n   - \\\\frac{\\\\gamma + \\\\beta}{1 + \\\\gamma^2}\\\\frac{1}{2 + (n-2)\\\\frac{1+\\\\gamma^2}{1 + \\\\beta^2}}\\n   \\\\end{align*}\\nwhere, in the third line, we use Lemma \\\\ref{lem:Qsum_bound} and the fact that \\\\(\\\\sum_{i1}^n P_{i|0}   1\\\\). Multiplying by $\\\\frac{1+\\\\gamma^2}{\\\\gamma+\\\\beta}\\\\cdot \\\\frac{2n}{1 + \\\\sum_{i1}^{n-1} P_{0|i}} > 0$ and rearranging, we get that:\\n   \\nIf the above expression is positive, this yields a contradiction with the stationary condition \\\\(\\\\partial \\\\mathcal{L} / \\\\partial y_0   0\\\\). Therefore, the above expression must be non-positive. This implies We can manipulate the expression as follows:\\n\\n\\n\\\\begin{align*}\\n   \\\\frac{\\\\alpha}{\\\\sqrt{1 + \\\\alpha^2}} \\\\cdot \\\\frac{1+\\\\gamma^2}{\\\\gamma+\\\\beta} \\\\cdot \\\\frac{\\\\gamma}{1+(\\\\gamma+\\\\beta)^2} \\n   &\\\\leq \\\\frac{1}{2 + (n-2)\\\\cdot \\\\frac{1+\\\\gamma^2}{1+\\\\beta^2}}\\\\cdot \\\\frac{2n}{1 + \\\\sum_{i1}^{n-1} P_{0|i}} \\\\\\\\\\n   &\\\\leq \\\\frac{1+\\\\beta^2}{(n-2)(1+\\\\gamma^2)}\\\\cdot \\\\frac{2n}{1 + \\\\sum_{i1}^{n-1} P_{0|i}} \\\\\\\\\\n   & \\\\frac{1+\\\\beta^2}{1+\\\\gamma^2}\\\\cdot \\\\Big(1 + \\\\frac{2}{n-2}\\\\Big)\\\\cdot \\\\frac{2}{1+\\\\sum_{i1}^{n-1} P_{0|i}}.\\n\\\\end{align*}\\nRecall, by definition of \\\\(\\\\alpha\\\\)-outlier configuration, that $\\\\gamma \\\\geq \\\\alpha \\\\cdot \\\\max\\\\{\\\\beta,1\\\\}$. Rearranging, we have:\\n\\\\begin{align*}\\n   \\\\Big(1 + \\\\frac{2}{n-2}\\\\Big)\\\\cdot \\\\frac{2}{1+\\\\sum_{i1}^{n-1} P_{0|i}} &\\\\geq \\\\frac{\\\\alpha}{\\\\sqrt{1 + \\\\alpha^2}} \\\\cdot \\\\frac{\\\\gamma}{\\\\gamma+\\\\beta} \\\\cdot \\\\frac{1+\\\\gamma^2}{1+(\\\\gamma+\\\\beta)^2} \\\\cdot \\\\frac{1+\\\\gamma^2}{1+\\\\beta^2} \\\\\\\\ \\n   &\\\\geq \\\\frac{\\\\alpha}{\\\\sqrt{1 + \\\\alpha^2 }}\\\\frac{\\\\gamma^3}{(\\\\gamma+\\\\beta)^3}\\\\frac{1 + \\\\gamma^2}{1+\\\\beta^2} \\\\\\\\ \\n   &\\\\geq \\\\frac{\\\\alpha}{\\\\sqrt{1 + \\\\alpha^2 }}\\\\frac{\\\\alpha^3\\\\max\\\\{\\\\beta,1\\\\}^3}{(\\\\alpha\\\\max\\\\{\\\\beta,1\\\\}+\\\\beta)^3}\\\\frac{1 + \\\\alpha^2\\\\max\\\\{\\\\beta,1\\\\}^2}{1+\\\\beta^2} \\\\\\\\\\n   &\\\\geq \\\\frac{\\\\alpha}{\\\\sqrt{1 + \\\\alpha^2 }}\\\\frac{\\\\alpha^3}{(\\\\alpha+\\\\frac{\\\\beta}{\\\\max\\\\{\\\\beta,1\\\\}})^3}\\\\frac{1 + \\\\alpha^2}{2} \\\\\\\\\\n   &\\\\geq \\\\frac{\\\\alpha}{\\\\sqrt{1 + \\\\alpha^2 }}\\\\frac{\\\\alpha^3}{(1+\\\\alpha)^3}\\\\frac{1 + \\\\alpha^2}{2}\\\\\\\\\\n   & \\\\frac{\\\\alpha^4\\\\sqrt{1 + \\\\alpha^2 }}{2(1+\\\\alpha)^3}.\\n\\\\end{align*}\\n\\nAssume $\\\\alpha \\\\geq 3$ (or else the hypothesis holds trivially), then the above is lower-bounded by ${(\\\\alpha^2 - 1)/4}$. Solving for $\\\\alpha$, we find \\\\(\\\\alpha \\\\leq \\\\sqrt{1 + \\\\Big(1 + \\\\frac{2}{n-2}\\\\Big)\\\\cdot \\\\Big( \\\\frac{8}{1+\\\\sum_{i1}^{n-1} P_{0|i}}\\\\Big) }\\\\).\\n\\n\\\\end{proof}\"},\n",
       "   {'name': 'discussion.tex',\n",
       "    'text': \"Our \\\\sout{mathematical} \\nstudy of t-SNE has established in considerable generality that one cannot infer the \\\\textit{degree} of cluster structure or the \\\\textit{extremity} of outliers from a t-SNE plot, see Theorems \\\\ref{thm:unclustHammer}, \\\\ref{thm:perturbhammer}, and \\\\ref{thm:outlier_abs}. The proofs and intuitions behind these statements guided us to the surprising empirical observation that one cannot even infer the \\\\textit{existence} of clusters or outliers. In particular, the injection of a small subset of adversarially chosen points can largely mask the cluster structure, while sizable injections of outlier points are masked within the cluster structure, \\nsee Figures \\\\ref{fig:one_pt_perturb}, \\\\ref{fig:outliers1}, \\\\ref{fig:outliers_real_world}, and \\\\ref{fig:bbc_appendix}. Further work should seek to formalize these latter set of empirical observations. C\\n\\nThese behaviors are more pronounced as data becomes more high-dimensional: the damage done by an adversarially chosen poison point is largest on high-aspect-ratio data, while more faraway outliers can fly under the radar in the high-dimensional regime.   t-SNE on high-dimensional data is susceptible to certain adversarial attacks and \\n\\nWe have identified two properties of t-SNE that give rise to these idiosyncratic behaviors: (1) additive invariance with respect to the squared interpoint distances, and (2) the asymmetry between the input and output affinity matrices.   (in that the former probes nearest-neighbors and the latter probes radius-neighbors)\\n While we have uncovered significant false positive failure modes that arise from these properties, we cannot completely rule out their utility. Additive invariance, while brittle under certain adversarial perturbations, may be robust to certain random perturbations. Indeed, adding random noise to a dataset is approximately equivalent to adding a constant to the interpoint distances due to concentration of measure. Additive invariance effectively allows t-SNE to ignore such noise, see Figure \\\\ref{fig:higher_dim_tighter_clusters}. This phenomenon is worthy of further study.\\n \\n while additive invariance is susceptible to adversarial perturbations, it is perhaps very robust to random perurbation\\n \\n additive invariance has desirable denoising properties, considering the concentration effects of high-dimensional noise (see Figure \\\\ref{fig:higher_dim_tighter_clusters}).   Indeed, Gaussian noise \\n\\nThe holy grail of d\\nThe gold standard result for data visualization is a guarantee that clustered output implies clustered input in a suitable sense.\\n\\n\\n\\nt-SNE belongs to a larger collection of practical data visualization techniques \\\\citep{mcinnes2018umap, jacomy2014forceatlas2, tang2016visualizing, amid2019trimap}. Beyond characterizing the intrinsic strengths and limitations of t-SNE, it would be instructive to taxonomize this zoo of techniques.\\n\\nt-SNE belongs to a wide selection of recently developed \\ndata visualization techniques that are yet to be understood fully \\\\citep{mcinnes2018umap, jacomy2014forceatlas2, tang2016visualizing, amid2019trimap}. Our hope is that this work inspires the reader to explore this fascinating landscape further and pursue the essential question: what can be provably deduced from a visualization?\\n\\nOur hope is that this work inspires the reader to explore this fertile frontier further and quest after the quintessential question: what can be provably deduced from a visualization?\\n\\nfor future work is to systematically chart this landscape. to explore this landscape further.\\n\\nwhose false positives remain largely unstudied, both in theory and practice.\\n\\nwe would like to provide a taxonomy to understand this zoo of modern techniques.\\n\\n Our broader vision is to taxonomize the array \\n \\n understand the intrinsic strengths and limitations of \\n\\ns compared to other methods;–––the gold standard here is separation, not just non sequitir performance guarantee\\n \\n taxonomize of other data visualization techniques. Can \\n\\n dissect the favorable and unfavorable qualities\\n\\n\\nt-SNE is one of many visualization methods that, in practice, rely purely on a gradient-based optimization of the output points. Related methods in this regard include UMAP \\\\citep{mcinnes2018umap}, ForceAtlas \\\\citep{jacomy2014forceatlas2}, LargeVis \\\\citep{tang2016visualizing}, and TriMap \\\\citep{amid2019trimap}. This is in contrast to an earlier generations of data visualization methods, from PCA and classical MDS   to \\\\citet{tenenbaum2000global} and Laplacian Eigenmaps \\\\citet{belkin2003laplacian}, which usually reduce to eigenvalue problems). Future work should focus on understanding this landscape of ``force-based'' visualization methods and their seemingly unreasonable effectiveness.\\n\\n \\nIt is understood on some heuristic level, for instance, that the affinity matrix asymmetry helps alleviate the ``crowding problem.'' Likewise, \\n\\nLikewise, there is reason to believe that additive invariance has \\n\\nThe grand vision is an axiomatic approach to data visualization, analogous to impossibility theorem for data visualization. Can we rigorously define certain desiderate of a data visualization method a la Arora\\n\\n\\nCan the benefits of additive invariance be rigorously understood from a denoising perspective?   we leave it open to future work to quantify the strengths. \\n\\nFORMALIZE POISON POINTS\\n\\nIt would be instructive to extend our false positive and outlier type studies to other data visualization methods. \\n\\n\\\\textcolor{red}{why the glitch?} \\nOne of the distinctive features of t-SNE that arose in our analysis is the property of additive invariance. While we have explored how this property relates to the exaggeration of cluster structure and the sensitivity of t-SNE to poison points, there is reason to believe that additive invariance is beneficial in the processing of high-dimensional, simplex-like data. Additive invariance data processing removes this high-dimensional signature and zeros in on the interrelations between interpoint distances. It is worth noting that UMAP \\\\citep{mcinnes2018umap} have a feature similar (but not quite identical) to t-SNE's additive invariance, involving the subtraction of the minimum interpoint distance.   This is because high-dimensional data tends to be close to a simplex. This is a well-known and easily observed principle in practice, see Figure \\\\ref{fig:NLP_simplex}, as well as in theory, from the perspective of concentration of measure. It is possible that additive-invariance is key to designing good data visualization algorithms. Indeed, it is strikingly prevalent among popular visualization algorithms such as t-SNE, UMAP, LargeVis, Laplacian Eigenmaps, and Diffusion Maps. \\n\\\\todo[inline]{Add discussion of: $\\\\E_{x'\\\\sim N(x,I_d), y'\\\\sim N(y,I_d)}[\\\\lVert x' - y' \\\\rVert_2^2]   \\\\lVert x - y \\\\rVert_2^2 + C $ phenomenon.}\\n\\n\\\\todo[inline]{Include in one form or another in discussion:\\n\\\\begin{theorem}\\\\label{thm:additiveInvariance}\\n   Laplacian Eigenmaps, LargeVis, SNE, t-SNE, and diffusion maps with Gaussian kernel all are invariant under additive scaling.\\n\\\\end{theorem}}\\n\\nIndeed, this property may be useful for other endeavors involving high-dimensional data such as measuring cluster significance. Commonly used measures of cluster significance, such as Silhouette Score, rely on comparing the gap between inter-clusters and intra-cluster distances ``multiplicatively'' (i.e. comparing these distances as a ratio). Therefore, high-dimensional datasets which tend to have close to 1 aspect ratios will not present as highly clustered to these measures even though the data may have a far more statistically significant clustering than similar datasets in low dimensions. Our hope is that comparing inter- and intra-cluster distances ``additively'' would overcome this issue and provide a computationally-friendly way to measure cluster significance that is more aligned with the nature of high-dimensional data.\\n\\n[additive invariance paragraph?]\\n\\n\\n\\nt-SNE is one of many visualization methods that, in practice, rely purely on a gradient-based optimization of the output points. Related methods in this regard include UMAP \\\\citep{mcinnes2018umap}, ForceAtlas \\\\citep{jacomy2014forceatlas2}, LargeVis \\\\citep{tang2016visualizing}, and TriMap \\\\citep{amid2019trimap}. This is in contrast to an earlier generations of data visualization methods, from PCA and classical MDS   to \\\\citet{tenenbaum2000global} and Laplacian Eigenmaps \\\\citet{belkin2003laplacian}, which usually reduce to eigenvalue problems). Future work should focus on understanding this landscape of ``force-based'' visualization methods and their seemingly unreasonable effectiveness.\\n\\n\\n\\\\textcolor{red}{i want a sentence name-dropping all the ``force-based'' methods- trimap \\\\citep{amid2019trimap}- umap \\\\citep{mcinnes2018umap}- forceatlas \\\\citep{jacomy2014forceatlas2}- largevis \\\\citep{tang2016visualizing}}\\n\\n\\n More generally, what information can be deduced about the input given a visualization? \"},\n",
       "   {'name': 'introduction.tex',\n",
       "    'text': \"\\n\\n\\nt-SNE and related data visualization methods have become staples in modern exploratory data analysis. They just seem to work: practitioners find that these techniques effortlessly tease out interesting cluster structures in datasets. Consequently they are now used ubiquitously in a wide array of fields, ranging from single-cell genomics to language model interpretability \\\\citep{kobak2019art, petukhova2025text}. The practical success of these techniques has naturally piqued some interest in the theoretical computer science community as well. \\\\citet{arora2018analysis} and \\\\citet{linderman2019clustering} for instance show that t-SNE does indeed have the capability to reveal the latent clusters present in the input data. For UMAP XYZ shows blah. thus corroborating? the impirics.\\n\\nWhile such studies answer the true positive discovery question, \\nthe false postive discovery question remains un.\\n\\na are crucial in understanding how t-SNE and UMAP works, \\n\\n\\nthey visualize the right clusters on many labeled dataset (say, MNIST) and they produce convincing clusters on many unlabeled datasets (say, single-cell genomics readings). As a result of their appealing behavior and relatively efficient implementations, these methods have gained a substantial user base throughout the sciences, and particularly in molecular biology \\\\citep{kobak2019art}, as tools for hypothesis generation and exploratory data analysis. \\n\\nThere is some theoretical evidence that t-SNE \\n\\n\\\\textcolor{red}{should glorify more, include positive theory results, THEN go negative, talk about our goal of stress-testing, ``there are many good questions to still ask.'' }\\n\\nOn the other hand, there is much to question about these methods. For one, they are both produced via gradient-based optimization of (reasonable, yet) complicated objective functions. It is extremely unlikely that t-SNE or UMAP outputs actually constitute globally optimal solutions to their respective optimization problems\\\\footnote{In fact, considering that standard t-SNE and UMAP implementations typically run for a fixed number of steps rather than until convergence, we don't even know if most outputs are local minima!}. Even if we did have access to optimal outputs, it is not at all obvious what one can definitely conclude from them. it is not at all obvious that the solution to this objective function should give us something \\nThese visualizations are simply not as ``interpretable'' as other unsupervised learning methods, such as, say, a principal component analysis (PCA) projection, where we can exactly quantify the amount of variance preserved in the picture, or a hierarchical clustering, where we can compare (both quantitatively and visually) the persistence of clustering patterns at different scales. \\n\\nThese are just some of the concerns that motivate a more careful theoretical analysis of t-SNE and UMAP, aimed at identifying performance guarantees or a lack thereof.\\n\\n\\n\\nData visualization is a controversial enterprise. On one hand, there is so much demand to \\\\textit{see} data. This is evidenced by the sheer popularity of t-distributed stochastic neighbor embedding (t-SNE) and uniform manifold approximation (UMAP), which have become standard exploratory analysis tools in various fields, but particularly single-cell biology \\\\citep{kobak2019art}. are widely used for visualizing cluster structure in high-dimensional data, These methods, and the enterprise they represent,rank among the most widely-used yet under-studied data analysis methods. It makes sense: in exploratory data analysis, there is a distinct desire to \\\\textit{see} the data.\\n These methods have received relatively little theoretical attention. These methods, in a word, are non-linear embedding techniques, typically   These methods produce non-linear metric embeddings into two or three dimensional space by optimizing\\nThese methods are essentially non-linear metric embeddings which arise as the solution to non-convex optimization problems. As such, they are difficult to analyze mathematically and there is relatively little theoretical understanding of each method, especially as compared to other unsupervised learning models like spectral clustering and \\\\(k\\\\)-means. \\n\\nExisting analysis of t-SNE\\n(to the author's knowledge, UMAP has received essentially no such attention) \\n has established that, given high-dimensional data with spherical, well-separated cluster structure, t-SNE outputs a visualization which preserves that cluster structure   \\\\citep{arora2018analysis,linderman2019clustering}. In other words, t-SNE is provably good at generating \\\\textit{true positives} in its visualization of clusters. Curiously, t-SNE's susceptibility to generate \\\\textit{false positives}, i.e.\\\\ fabricated clusters in the output visualization, has remained largely unstudied. One should note that this is not a purely academic curiosity, since the interpretation of t-SNE outputs have important consequences downstream in the sciences, influencing hypothesis generation, experimental design, and scientific conclusions.   and which experiments to run next. Our results show that one should avoid taking t-SNE clusterings at face value. \\\\textbf{There is a unique danger to false positives.}\\n\\n\\nAs an illustration of the danger of false positives, consider the 2D t-SNE visualization of a $100$-point dataset residing in \\\\(\\\\mathbb{R}^{100}\\\\) (depicted on the right). \\n\\n\\\\begin{wrapfigure}{r}{0.4\\\\textwidth}\\n\\\\begin{figure}[h!]\\n   \\\\centering\\n   \\n   \\\\vspace{-0.15in}\\n   \\\\includegraphics[width0.35\\\\textwidth]\\n   {images/tsne_plot.png}\\n   \\\\vspace{-0.2in}\\\\includegraphics[width0.4\\\\linewidth]{images/umap_plot.png}\\n   \\\\includegraphics[width0.3\\\\linewidth]{images/umap_demo.png} \\\\label{fig:tsneumap_demo}\\n\\\\end{figure}\\n\\\\end{wrapfigure}\\n\\nThe following are true:\\n\\\\begin{itemize}\\n   \\\\item \\\\(k\\\\)-means on the original dataset produces a decidedly poor clustering by standard metrics, \\n   \\\\item \\\\(k\\\\)-means on the t-SNE and UMAP projections produce the same partition of points. \\n\\\\end{itemize}\\n\\n\\nBased on this plot, it is tempting to conclude that the input dataset obviously contains two distinct clusters. In this case, one understandably, any practitioner \\nwould likely design their subsequent data analysis workflow guided by these two salient clusters. However a closer examination of the original (high-dimensional) dataset reveals that the situation perhaps may not be as clear-cut. By standard cluster saliency metrics, for instance, the input dataset appears quite poorly clustered according to the partition that t-SNE so strongly suggests, see Table \\\\ref{table:demo}.\\n\\nthat perhaps the 100-point dataset contains two distinct clusters. In this case, understandably, our unsuspecting naive data analyst would design their subsequent data analysis workflow guided by the two salient clusters they see. But alas even a basic cluster saliency test of the original (high-dimensional) dataset reveals that these clusters are spurious.\\n\\n\\n\\\\begin{table}[h!]\\\\label{table:demo}\\n\\\\centering\\n\\\\caption{\\\\small Clustering scores (with respect to $k$-means) according to various popular cluster saliency metrics. The range in the first column specifies the possible values that can be attained. A higher value indicates data being highly clustered.   }\\nThe direction indicator in the first column denotes what score values (higher or lower) imply data being highly clustered and the range indicates the possible values given the cluster partition is optimal.}\\n\\\\vspace{0.3cm}\\n\\\\begin{tabular}{lccc}\\n\\\\hline\\n\\\\textbf{Cluster Score (range)} & \\\\textbf{t-SNE (2D)} &   \\\\textbf{Original Data (100D)} \\\\\\\\\\n\\\\hline\\nSilhouette $ [-1,1]$ & .918 & .006 \\\\\\\\\\nCalinski-Harabasz $ [0,\\\\infty]$   & 5590 .40\\n& 1.61 \\\\\\\\\\nDunn Index $[0,\\\\infty]$   & 3.65 & .998 \\\\\\\\\\n Davies–Bouldin   $\\\\downarrow (0,\\\\infty)$   & 0.1188 & 7.79 \\\\\\\\\\n\\\\hline\\n\\\\vspace{0.05cm}\\n\\\\end{tabular}\\n\\\\end{table}\\n\\n\\\\begin{wrapfigure}{r}{0.5\\\\textwidth}\\n\\\\begin{figure}[h!]\\n   \\\\centering\\n   \\\\vspace{-0.1in}\\n   \\\\includegraphics[width0.45\\\\textwidth]{images/demo/DEMO_pca_tsne_umap_distances.png}\\n   \\\\includegraphics[width0.3\\\\linewidth]{images/umap_distances.png}\\n   \\\\hspace{-0.5in}\\n   \\\\includegraphics[width0.32\\\\textwidth]{images/original_distances.png}\\n   \\\\vspace{-0.3in}\\n\\\\end{figure}\\n\\\\end{wrapfigure}\\n\\n\\nThe interpoint distance matrix plots on the right further elucidate this discrepancy. t-SNE's two-dimensional visualization (right) features a sizable separation between small intra-cluster and large inter-cluster distances. This separation is not present in the original input data (left), where interpoint distances are near-uniform.\\n\\n\\nwhereas in the original dataset, the points are all roughly equidistant from one another. \\na clear partition of the points is present\\n\\n. In the high-dimensional dataset, all the points are roughly equidistant from one another. This stands in stark contrast with the interpoint distance matrix of the two-dimensional visualizations, where the partition of points is very obvious.\\n\\n\\n\\nThe behavior that t-SNE and UMAP exhibit in this case are the result of a particular invaraince property. \\n\\nAre t-SNE and UMAP completely fabricating this cluster structure? Not quite. Not quite. However, there is a sense in which t-SNE and UMAP are very much exaggerating the cluster structure in this dataset.\\n\\\\vspace{0.09in}\\n\\nOur work formalizes this phenomenon and other ``cluster-happy'' behaviors exhibited by t-SNE. Our theoretical analysis, suffused with experiments, shows that one should take positively clustered outputs with a grain of salt. Our contributions are as follows:\\n\\nMore broadly, we are interested in to what extent we can rely on the outputs generated by such techniques. \\n\\n\\\\begin{itemize}\\n\\n   \\\\item \\\\textbf{Misrepresentation of clusters:} We prove that both highly-clustered and arbitrarily \\\\textit{un}-clustered datasets \\\\footnote{According to standard metrics such as silhouette score.} \\n   can produce the same maximally clustered visualization,\\n   We find that input data which is arbitrarily \\\\textit{un}-clustered (according to standard metrics such as silhouette score) can have maximally clustered visualizations\\n   see Theorem \\\\ref{thm:unclustHammer} and Corollary \\\\ref{cor:twoCluster_n_Unclustered}. Indeed, any visualization produced by t-SNE can be produced by an arbitrarily un-clustered dataset. \\n   Moreover, we prove that arbitrarily close inputs can have vastly distinct visualizations, see Theorem \\\\ref{thm:perturbhammer}. We identify the peculiar property of t-SNE that explains these behaviors. We use this understanding to design a targeted adversarial attack , the injection of ``poison'' points, \\n   that disrupts cluster structure in the output, see Figure \\\\ref{fig:one_pt_perturb}.\\n   \\\\item \\\\textbf{Misrepresentation of outliers}: We prove that, regardless of input structure, the resulting t-SNE output is incapable of depicting extreme outliers, in the sense of depicting one point as substantially far away from all the others, see Theorem \\\\ref{thm:outlier_abs}. In practice, on both synthetic and real datasets, we observe a more concerning phenomenon that faraway outliers are often subsumed into the cluster structure of the bulk of points, see Figures \\\\ref{fig:outliers1} and \\\\ref{fig:outliers_real_world}. We then show that, if the input to t-SNE consists of a mixture of Gaussians plus some number of extreme outliers, the output will depict these outliers as part of the Gaussians.\\n\\\\end{itemize}\\n\\nIt is worth emphasizing that these behaviors are somewhat peculiar to t-SNE, UMAP, and related methods. \\n\\nWhile there has been some work investigating the shortcomings of t-SNE in various practical settings (see Section \\\\ref{sec:related_work_limitations} for a detailed discussion of the relevant literature), to the best of our knowledge this is the first work which theoretically analyzes some of the key limitations of t-SNE. \\n\\n\\ninformal theorem 1: take high-dimensional Gaussian noise, just a small perturbation gives you ANY bi-partition; OR, more generally, un-clustered, make it clustered (ACCORDING TO WELL-ESTABLISHED cluster metrics like rand index, dunn index, silhouette score)\\n\\nFIGURE 1: datasets which are epsilon apart but produce extremely different outputs\\n\\nFIGURE 2: clustered and highly unclustered producing the same output\\n\\n\\ninformation theorem 2: arbitrarily extreme outliers are visualized very close to the bulk of the data\\n\\n\\n\\n\"},\n",
       "   {'name': 'main.tex',\n",
       "    'text': \"\\n\\\\documentclass{article}   For LaTeX2e\\n\\\\usepackage{iclr2026_conference,times}\\n\\\\usepackage[normalem]{ulem}\\n\\n Optional math commands from https://github.com/goodfeli/dlbook_notation.\\n\\\\input{math_commands.tex}\\n\\n\\\\usepackage{hyperref}\\n\\\\usepackage{algorithm}\\n\\\\usepackage{algorithmic}\\n\\\\usepackage{algpseudocode}\\n\\\\usepackage{todonotes}\\n\\\\usepackage{url}\\n\\\\usepackage{wrapfig}\\n\\\\usepackage{thm-restate}\\n\\\\usepackage{amsmath,amssymb,amsthm}\\n\\n Define theorem-like environments\\n\\\\newtheorem{theorem}{Theorem}[section]\\n\\\\newtheorem{lemma}[theorem]{Lemma}\\n\\\\newtheorem{corollary}[theorem]{Corollary}\\n\\\\newtheorem{proposition}[theorem]{Proposition}\\n\\\\newtheorem{observation}[theorem]{Observation}\\n\\\\newtheorem{definition}[theorem]{Definition}\\n\\\\newtheorem{example}[theorem]{Example}\\n\\n\\n\\\\DeclareMathOperator{\\\\indicate}{1 \\\\kern -4pt 1}\\n\\\\DeclareMathOperator*{\\\\V}{\\\\ell}{\\\\mathbb{V}}\\n\\\\DeclareMathOperator*{\\\\vis}{\\\\sf{vis}}\\n\\\\DeclareMathOperator*{\\\\dc}{\\\\sf{dc}}\\n\\\\DeclareMathOperator*{\\\\Rand}{Rand}\\n\\\\DeclareMathOperator*{\\\\Mirk}{Mirk}\\n\\\\DeclareMathOperator*{\\\\sph}{sph}\\n\\\\DeclareMathOperator*{\\\\diam}{diam}\\n\\\\DeclareMathOperator*{\\\\conv}{conv}\\n\\\\DeclareMathOperator*{\\\\TSNE}{t-SNE}\\n\\\\DeclareMathOperator*{\\\\IMTSNE}{\\\\mathsf{Im}({\\\\TSNE}_{\\\\rho,n})}\\n\\\\DeclareMathOperator*{\\\\core}{C}\\n\\\\DeclareMathOperator*{\\\\diag}{diag}\\n\\\\DeclareMathOperator*{\\\\degree}{deg}\\n\\\\DeclareMathOperator*{\\\\clusters}{\\\\sf{clusters}}\\n\\\\DeclareMathOperator*{\\\\PP}{\\\\mathtt{PP}}\\n\\n\\\\renewcommand{\\\\sout}[1]{\\n   \\\\bgroup\\n   \\\\markoverwith{\\\\textcolor{red}{\\\\rule[0.5ex]{2pt}{0.5pt}}}\\n   \\\\ULon{\\\\textcolor{red}{#1}}\\n   \\\\egroup\\n}\\n\\n\\\\title{t-SNE exaggerates clusters, provably}\\n\\n Authors must not appear in the submitted version. They should be hidden\\n as long as the \\\\iclrfinalcopy macro remains commented out below.\\n Non-anonymous submissions will be rejected without review.\\n\\n\\\\author{Noah Bergam\\\\textsuperscript{\\\\textdagger}, Szymon Snoeck\\\\textsuperscript{*}, \\\\& Nakul Verma\\\\textsuperscript{\\\\textdagger} \\\\\\\\\\n\\\\textsuperscript{\\\\textdagger}Computer Science Department; \\\\textsuperscript{*}Applied Mathematics Department\\\\\\\\\\nColumbia University \\\\\\\\\\nNew York, NY 10027\\\\\\\\\\n\\\\texttt{\\\\{njb2154,sgs2179\\\\}@columbia.edu, verma@cs.columbia.edu} \\\\\\\\\\n}\\n\\n The \\\\author macro works with any number of authors. There are two commands\\n used to separate the names and addresses of multiple authors: \\\\And and \\\\AND.\\n\\n Using \\\\And between authors leaves it to \\\\LaTeX{} to determine where to break\\n the lines. Using \\\\AND forces a linebreak at that point. So, if \\\\LaTeX{}\\n puts 3 of 4 authors names on the first line, and the last on the second\\n line, try using \\\\AND instead of \\\\And before the third author name.\\n\\n\\\\newcommand{\\\\fix}{\\\\marginpar{FIX}}\\n\\\\newcommand{\\\\new}{\\\\marginpar{NEW}}\\n\\n\\\\iclrfinalcopy   Uncomment for camera-ready version, but NOT for submission.\\n\\\\begin{document}\\n\\n\\n\\\\maketitle\\n\\n\\\\begin{abstract}\\n\\\\input{abstract}\\n\\\\end{abstract}\\n\\n\\\\section{Introduction}\\n\\\\input{introduction}\\n\\n\\\\section{Related Work}\\n\\\\input{related_work}\\n\\n\\\\section{Preliminaries}\\\\label{sec:prelims}\\n\\\\input{preliminaries}\\n\\n\\\\section{Misrepresentation of Cluster Structure}\\n\\\\input{1_false_positives}\\n\\n\\\\section{Misrepresentation of Outliers}\\n\\\\input{2_outliers}\\n\\n\\\\section{Discussion}\\n\\\\input{discussion}\\n\\n\\n\\\\subsubsection*{Author Contributions}\\nIf you'd like to, you may include   a section for author contributions as is donein many journals. This is optional and at the discretion of the authors.\\n\\n\\\\subsubsection*{Acknowledgments}\\nNB was supported by the Irving Institute for Cancer Research and ONR N00014-24-1-2700.\\n\\nAcknowledgments are not included to maintain anonymity during the review process.\\n\\n\\n\\\\bibliography{iclr2026_conference}\\n\\\\bibliographystyle{iclr2026_conference}\\n\\n\\\\newpage\\n\\\\appendix\\n\\\\input{appendix.tex}\\n\\n\\n\\n\\\\end{document}\\n\"},\n",
       "   {'name': 'math_commands.tex',\n",
       "    'text': \" NEW MATH DEFINITIONS \\n\\n\\\\usepackage{amsmath,amsfonts,bm}\\n\\n Mark sections of captions for referring to divisions of figures\\n\\\\newcommand{\\\\figleft}{{\\\\em (Left)}}\\n\\\\newcommand{\\\\figcenter}{{\\\\em (Center)}}\\n\\\\newcommand{\\\\figright}{{\\\\em (Right)}}\\n\\\\newcommand{\\\\figtop}{{\\\\em (Top)}}\\n\\\\newcommand{\\\\figbottom}{{\\\\em (Bottom)}}\\n\\\\newcommand{\\\\captiona}{{\\\\em (a)}}\\n\\\\newcommand{\\\\captionb}{{\\\\em (b)}}\\n\\\\newcommand{\\\\captionc}{{\\\\em (c)}}\\n\\\\newcommand{\\\\captiond}{{\\\\em (d)}}\\n\\n Highlight a newly defined term\\n\\\\newcommand{\\\\newterm}[1]{{\\\\bf #1}}\\n\\n\\n Figure reference, lower-case.\\n\\\\def\\\\figref#1{figure~\\\\ref{#1}}\\n Figure reference, capital. For start of sentence\\n\\\\def\\\\Figref#1{Figure~\\\\ref{#1}}\\n\\\\def\\\\twofigref#1#2{figures \\\\ref{#1} and \\\\ref{#2}}\\n\\\\def\\\\quadfigref#1#2#3#4{figures \\\\ref{#1}, \\\\ref{#2}, \\\\ref{#3} and \\\\ref{#4}}\\n Section reference, lower-case.\\n\\\\def\\\\secref#1{section~\\\\ref{#1}}\\n Section reference, capital.\\n\\\\def\\\\Secref#1{Section~\\\\ref{#1}}\\n Reference to two sections.\\n\\\\def\\\\twosecrefs#1#2{sections \\\\ref{#1} and \\\\ref{#2}}\\n Reference to three sections.\\n\\\\def\\\\secrefs#1#2#3{sections \\\\ref{#1}, \\\\ref{#2} and \\\\ref{#3}}\\n Reference to an equation, lower-case.\\n\\\\def\\\\eqref#1{equation~\\\\ref{#1}}\\n Reference to an equation, upper case\\n\\\\def\\\\Eqref#1{Equation~\\\\ref{#1}}\\n A raw reference to an equation---avoid using if possible\\n\\\\def\\\\plaineqref#1{\\\\ref{#1}}\\n Reference to a chapter, lower-case.\\n\\\\def\\\\chapref#1{chapter~\\\\ref{#1}}\\n Reference to an equation, upper case.\\n\\\\def\\\\Chapref#1{Chapter~\\\\ref{#1}}\\n Reference to a range of chapters\\n\\\\def\\\\rangechapref#1#2{chapters\\\\ref{#1}--\\\\ref{#2}}\\n Reference to an algorithm, lower-case.\\n\\\\def\\\\algref#1{algorithm~\\\\ref{#1}}\\n Reference to an algorithm, upper case.\\n\\\\def\\\\Algref#1{Algorithm~\\\\ref{#1}}\\n\\\\def\\\\twoalgref#1#2{algorithms \\\\ref{#1} and \\\\ref{#2}}\\n\\\\def\\\\Twoalgref#1#2{Algorithms \\\\ref{#1} and \\\\ref{#2}}\\n Reference to a part, lower case\\n\\\\def\\\\partref#1{part~\\\\ref{#1}}\\n Reference to a part, upper case\\n\\\\def\\\\Partref#1{Part~\\\\ref{#1}}\\n\\\\def\\\\twopartref#1#2{parts \\\\ref{#1} and \\\\ref{#2}}\\n\\n\\\\def\\\\ceil#1{\\\\lceil #1 \\\\rceil}\\n\\\\def\\\\floor#1{\\\\lfloor #1 \\\\rfloor}\\n\\\\def\\\\1{\\\\bm{1}}\\n\\\\newcommand{\\\\train}{\\\\mathcal{D}}\\n\\\\newcommand{\\\\valid}{\\\\mathcal{D_{\\\\mathrm{valid}}}}\\n\\\\newcommand{\\\\test}{\\\\mathcal{D_{\\\\mathrm{test}}}}\\n\\n\\\\def\\\\eps{{\\\\epsilon}}\\n\\n\\n Random variables\\n\\\\def\\\\reta{{\\\\textnormal{$\\\\eta$}}}\\n\\\\def\\\\ra{{\\\\textnormal{a}}}\\n\\\\def\\\\rb{{\\\\textnormal{b}}}\\n\\\\def\\\\rc{{\\\\textnormal{c}}}\\n\\\\def\\\\rd{{\\\\textnormal{d}}}\\n\\\\def\\\\re{{\\\\textnormal{e}}}\\n\\\\def\\\\rf{{\\\\textnormal{f}}}\\n\\\\def\\\\rg{{\\\\textnormal{g}}}\\n\\\\def\\\\rh{{\\\\textnormal{h}}}\\n\\\\def\\\\ri{{\\\\textnormal{i}}}\\n\\\\def\\\\rj{{\\\\textnormal{j}}}\\n\\\\def\\\\rk{{\\\\textnormal{k}}}\\n\\\\def\\\\rl{{\\\\textnormal{l}}}\\n rm is already a command, just don't name any random variables m\\n\\\\def\\\\rn{{\\\\textnormal{n}}}\\n\\\\def\\\\ro{{\\\\textnormal{o}}}\\n\\\\def\\\\rp{{\\\\textnormal{p}}}\\n\\\\def\\\\rq{{\\\\textnormal{q}}}\\n\\\\def\\\\rr{{\\\\textnormal{r}}}\\n\\\\def\\\\rs{{\\\\textnormal{s}}}\\n\\\\def\\\\rt{{\\\\textnormal{t}}}\\n\\\\def\\\\ru{{\\\\textnormal{u}}}\\n\\\\def\\\\rv{{\\\\textnormal{v}}}\\n\\\\def\\\\rw{{\\\\textnormal{w}}}\\n\\\\def\\\\rx{{\\\\textnormal{x}}}\\n\\\\def\\\\ry{{\\\\textnormal{y}}}\\n\\\\def\\\\rz{{\\\\textnormal{z}}}\\n\\n Random vectors\\n\\\\def\\\\rvepsilon{{\\\\mathbf{\\\\epsilon}}}\\n\\\\def\\\\rvtheta{{\\\\mathbf{\\\\theta}}}\\n\\\\def\\\\rva{{\\\\mathbf{a}}}\\n\\\\def\\\\rvb{{\\\\mathbf{b}}}\\n\\\\def\\\\rvc{{\\\\mathbf{c}}}\\n\\\\def\\\\rvd{{\\\\mathbf{d}}}\\n\\\\def\\\\rve{{\\\\mathbf{e}}}\\n\\\\def\\\\rvf{{\\\\mathbf{f}}}\\n\\\\def\\\\rvg{{\\\\mathbf{g}}}\\n\\\\def\\\\rvh{{\\\\mathbf{h}}}\\n\\\\def\\\\rvu{{\\\\mathbf{i}}}\\n\\\\def\\\\rvj{{\\\\mathbf{j}}}\\n\\\\def\\\\rvk{{\\\\mathbf{k}}}\\n\\\\def\\\\rvl{{\\\\mathbf{l}}}\\n\\\\def\\\\rvm{{\\\\mathbf{m}}}\\n\\\\def\\\\rvn{{\\\\mathbf{n}}}\\n\\\\def\\\\rvo{{\\\\mathbf{o}}}\\n\\\\def\\\\rvp{{\\\\mathbf{p}}}\\n\\\\def\\\\rvq{{\\\\mathbf{q}}}\\n\\\\def\\\\rvr{{\\\\mathbf{r}}}\\n\\\\def\\\\rvs{{\\\\mathbf{s}}}\\n\\\\def\\\\rvt{{\\\\mathbf{t}}}\\n\\\\def\\\\rvu{{\\\\mathbf{u}}}\\n\\\\def\\\\rvv{{\\\\mathbf{v}}}\\n\\\\def\\\\rvw{{\\\\mathbf{w}}}\\n\\\\def\\\\rvx{{\\\\mathbf{x}}}\\n\\\\def\\\\rvy{{\\\\mathbf{y}}}\\n\\\\def\\\\rvz{{\\\\mathbf{z}}}\\n\\n Elements of random vectors\\n\\\\def\\\\erva{{\\\\textnormal{a}}}\\n\\\\def\\\\ervb{{\\\\textnormal{b}}}\\n\\\\def\\\\ervc{{\\\\textnormal{c}}}\\n\\\\def\\\\ervd{{\\\\textnormal{d}}}\\n\\\\def\\\\erve{{\\\\textnormal{e}}}\\n\\\\def\\\\ervf{{\\\\textnormal{f}}}\\n\\\\def\\\\ervg{{\\\\textnormal{g}}}\\n\\\\def\\\\ervh{{\\\\textnormal{h}}}\\n\\\\def\\\\ervi{{\\\\textnormal{i}}}\\n\\\\def\\\\ervj{{\\\\textnormal{j}}}\\n\\\\def\\\\ervk{{\\\\textnormal{k}}}\\n\\\\def\\\\ervl{{\\\\textnormal{l}}}\\n\\\\def\\\\ervm{{\\\\textnormal{m}}}\\n\\\\def\\\\ervn{{\\\\textnormal{n}}}\\n\\\\def\\\\ervo{{\\\\textnormal{o}}}\\n\\\\def\\\\ervp{{\\\\textnormal{p}}}\\n\\\\def\\\\ervq{{\\\\textnormal{q}}}\\n\\\\def\\\\ervr{{\\\\textnormal{r}}}\\n\\\\def\\\\ervs{{\\\\textnormal{s}}}\\n\\\\def\\\\ervt{{\\\\textnormal{t}}}\\n\\\\def\\\\ervu{{\\\\textnormal{u}}}\\n\\\\def\\\\ervv{{\\\\textnormal{v}}}\\n\\\\def\\\\ervw{{\\\\textnormal{w}}}\\n\\\\def\\\\ervx{{\\\\textnormal{x}}}\\n\\\\def\\\\ervy{{\\\\textnormal{y}}}\\n\\\\def\\\\ervz{{\\\\textnormal{z}}}\\n\\n Random matrices\\n\\\\def\\\\rmA{{\\\\mathbf{A}}}\\n\\\\def\\\\rmB{{\\\\mathbf{B}}}\\n\\\\def\\\\rmC{{\\\\mathbf{C}}}\\n\\\\def\\\\rmD{{\\\\mathbf{D}}}\\n\\\\def\\\\rmE{{\\\\mathbf{E}}}\\n\\\\def\\\\rmF{{\\\\mathbf{F}}}\\n\\\\def\\\\rmG{{\\\\mathbf{G}}}\\n\\\\def\\\\rmH{{\\\\mathbf{H}}}\\n\\\\def\\\\rmI{{\\\\mathbf{I}}}\\n\\\\def\\\\rmJ{{\\\\mathbf{J}}}\\n\\\\def\\\\rmK{{\\\\mathbf{K}}}\\n\\\\def\\\\rmL{{\\\\mathbf{L}}}\\n\\\\def\\\\rmM{{\\\\mathbf{M}}}\\n\\\\def\\\\rmN{{\\\\mathbf{N}}}\\n\\\\def\\\\rmO{{\\\\mathbf{O}}}\\n\\\\def\\\\rmP{{\\\\mathbf{P}}}\\n\\\\def\\\\rmQ{{\\\\mathbf{Q}}}\\n\\\\def\\\\rmR{{\\\\mathbf{R}}}\\n\\\\def\\\\rmS{{\\\\mathbf{S}}}\\n\\\\def\\\\rmT{{\\\\mathbf{T}}}\\n\\\\def\\\\rmU{{\\\\mathbf{U}}}\\n\\\\def\\\\rmV{{\\\\mathbf{V}}}\\n\\\\def\\\\rmW{{\\\\mathbf{W}}}\\n\\\\def\\\\rmX{{\\\\mathbf{X}}}\\n\\\\def\\\\rmY{{\\\\mathbf{Y}}}\\n\\\\def\\\\rmZ{{\\\\mathbf{Z}}}\\n\\n Elements of random matrices\\n\\\\def\\\\ermA{{\\\\textnormal{A}}}\\n\\\\def\\\\ermB{{\\\\textnormal{B}}}\\n\\\\def\\\\ermC{{\\\\textnormal{C}}}\\n\\\\def\\\\ermD{{\\\\textnormal{D}}}\\n\\\\def\\\\ermE{{\\\\textnormal{E}}}\\n\\\\def\\\\ermF{{\\\\textnormal{F}}}\\n\\\\def\\\\ermG{{\\\\textnormal{G}}}\\n\\\\def\\\\ermH{{\\\\textnormal{H}}}\\n\\\\def\\\\ermI{{\\\\textnormal{I}}}\\n\\\\def\\\\ermJ{{\\\\textnormal{J}}}\\n\\\\def\\\\ermK{{\\\\textnormal{K}}}\\n\\\\def\\\\ermL{{\\\\textnormal{L}}}\\n\\\\def\\\\ermM{{\\\\textnormal{M}}}\\n\\\\def\\\\ermN{{\\\\textnormal{N}}}\\n\\\\def\\\\ermO{{\\\\textnormal{O}}}\\n\\\\def\\\\ermP{{\\\\textnormal{P}}}\\n\\\\def\\\\ermQ{{\\\\textnormal{Q}}}\\n\\\\def\\\\ermR{{\\\\textnormal{R}}}\\n\\\\def\\\\ermS{{\\\\textnormal{S}}}\\n\\\\def\\\\ermT{{\\\\textnormal{T}}}\\n\\\\def\\\\ermU{{\\\\textnormal{U}}}\\n\\\\def\\\\ermV{{\\\\textnormal{V}}}\\n\\\\def\\\\ermW{{\\\\textnormal{W}}}\\n\\\\def\\\\ermX{{\\\\textnormal{X}}}\\n\\\\def\\\\ermY{{\\\\textnormal{Y}}}\\n\\\\def\\\\ermZ{{\\\\textnormal{Z}}}\\n\\n Vectors\\n\\\\def\\\\vzero{{\\\\bm{0}}}\\n\\\\def\\\\vone{{\\\\bm{1}}}\\n\\\\def\\\\vmu{{\\\\bm{\\\\mu}}}\\n\\\\def\\\\vtheta{{\\\\bm{\\\\theta}}}\\n\\\\def\\\\va{{\\\\bm{a}}}\\n\\\\def\\\\vb{{\\\\bm{b}}}\\n\\\\def\\\\vc{{\\\\bm{c}}}\\n\\\\def\\\\vd{{\\\\bm{d}}}\\n\\\\def\\\\ve{{\\\\bm{e}}}\\n\\\\def\\\\vf{{\\\\bm{f}}}\\n\\\\def\\\\vg{{\\\\bm{g}}}\\n\\\\def\\\\vh{{\\\\bm{h}}}\\n\\\\def\\\\vi{{\\\\bm{i}}}\\n\\\\def\\\\vj{{\\\\bm{j}}}\\n\\\\def\\\\vk{{\\\\bm{k}}}\\n\\\\def\\\\vl{{\\\\bm{l}}}\\n\\\\def\\\\vm{{\\\\bm{m}}}\\n\\\\def\\\\vn{{\\\\bm{n}}}\\n\\\\def\\\\vo{{\\\\bm{o}}}\\n\\\\def\\\\vp{{\\\\bm{p}}}\\n\\\\def\\\\vq{{\\\\bm{q}}}\\n\\\\def\\\\vr{{\\\\bm{r}}}\\n\\\\def\\\\vs{{\\\\bm{s}}}\\n\\\\def\\\\vt{{\\\\bm{t}}}\\n\\\\def\\\\vu{{\\\\bm{u}}}\\n\\\\def\\\\vv{{\\\\bm{v}}}\\n\\\\def\\\\vw{{\\\\bm{w}}}\\n\\\\def\\\\vx{{\\\\bm{x}}}\\n\\\\def\\\\vy{{\\\\bm{y}}}\\n\\\\def\\\\vz{{\\\\bm{z}}}\\n\\n Elements of vectors\\n\\\\def\\\\evalpha{{\\\\alpha}}\\n\\\\def\\\\evbeta{{\\\\beta}}\\n\\\\def\\\\evepsilon{{\\\\epsilon}}\\n\\\\def\\\\evlambda{{\\\\lambda}}\\n\\\\def\\\\evomega{{\\\\omega}}\\n\\\\def\\\\evmu{{\\\\mu}}\\n\\\\def\\\\evpsi{{\\\\psi}}\\n\\\\def\\\\evsigma{{\\\\sigma}}\\n\\\\def\\\\evtheta{{\\\\theta}}\\n\\\\def\\\\eva{{a}}\\n\\\\def\\\\evb{{b}}\\n\\\\def\\\\evc{{c}}\\n\\\\def\\\\evd{{d}}\\n\\\\def\\\\eve{{e}}\\n\\\\def\\\\evf{{f}}\\n\\\\def\\\\evg{{g}}\\n\\\\def\\\\evh{{h}}\\n\\\\def\\\\evi{{i}}\\n\\\\def\\\\evj{{j}}\\n\\\\def\\\\evk{{k}}\\n\\\\def\\\\evl{{l}}\\n\\\\def\\\\evm{{m}}\\n\\\\def\\\\evn{{n}}\\n\\\\def\\\\evo{{o}}\\n\\\\def\\\\evp{{p}}\\n\\\\def\\\\evq{{q}}\\n\\\\def\\\\evr{{r}}\\n\\\\def\\\\evs{{s}}\\n\\\\def\\\\evt{{t}}\\n\\\\def\\\\evu{{u}}\\n\\\\def\\\\evv{{v}}\\n\\\\def\\\\evw{{w}}\\n\\\\def\\\\evx{{x}}\\n\\\\def\\\\evy{{y}}\\n\\\\def\\\\evz{{z}}\\n\\n Matrix\\n\\\\def\\\\mA{{\\\\bm{A}}}\\n\\\\def\\\\mB{{\\\\bm{B}}}\\n\\\\def\\\\mC{{\\\\bm{C}}}\\n\\\\def\\\\mD{{\\\\bm{D}}}\\n\\\\def\\\\mE{{\\\\bm{E}}}\\n\\\\def\\\\mF{{\\\\bm{F}}}\\n\\\\def\\\\mG{{\\\\bm{G}}}\\n\\\\def\\\\mH{{\\\\bm{H}}}\\n\\\\def\\\\mI{{\\\\bm{I}}}\\n\\\\def\\\\mJ{{\\\\bm{J}}}\\n\\\\def\\\\mK{{\\\\bm{K}}}\\n\\\\def\\\\mL{{\\\\bm{L}}}\\n\\\\def\\\\mM{{\\\\bm{M}}}\\n\\\\def\\\\mN{{\\\\bm{N}}}\\n\\\\def\\\\mO{{\\\\bm{O}}}\\n\\\\def\\\\mP{{\\\\bm{P}}}\\n\\\\def\\\\mQ{{\\\\bm{Q}}}\\n\\\\def\\\\mR{{\\\\bm{R}}}\\n\\\\def\\\\mS{{\\\\bm{S}}}\\n\\\\def\\\\mT{{\\\\bm{T}}}\\n\\\\def\\\\mU{{\\\\bm{U}}}\\n\\\\def\\\\mV{{\\\\bm{V}}}\\n\\\\def\\\\mW{{\\\\bm{W}}}\\n\\\\def\\\\mX{{\\\\bm{X}}}\\n\\\\def\\\\mY{{\\\\bm{Y}}}\\n\\\\def\\\\mZ{{\\\\bm{Z}}}\\n\\\\def\\\\mBeta{{\\\\bm{\\\\beta}}}\\n\\\\def\\\\mPhi{{\\\\bm{\\\\Phi}}}\\n\\\\def\\\\mLambda{{\\\\bm{\\\\Lambda}}}\\n\\\\def\\\\mSigma{{\\\\bm{\\\\Sigma}}}\\n\\n Tensor\\n\\\\DeclareMathAlphabet{\\\\mathsfit}{\\\\encodingdefault}{\\\\sfdefault}{m}{sl}\\n\\\\SetMathAlphabet{\\\\mathsfit}{bold}{\\\\encodingdefault}{\\\\sfdefault}{bx}{n}\\n\\\\newcommand{\\\\tens}[1]{\\\\bm{\\\\mathsfit{#1}}}\\n\\\\def\\\\tA{{\\\\tens{A}}}\\n\\\\def\\\\tB{{\\\\tens{B}}}\\n\\\\def\\\\tC{{\\\\tens{C}}}\\n\\\\def\\\\tD{{\\\\tens{D}}}\\n\\\\def\\\\tE{{\\\\tens{E}}}\\n\\\\def\\\\tF{{\\\\tens{F}}}\\n\\\\def\\\\tG{{\\\\tens{G}}}\\n\\\\def\\\\tH{{\\\\tens{H}}}\\n\\\\def\\\\tI{{\\\\tens{I}}}\\n\\\\def\\\\tJ{{\\\\tens{J}}}\\n\\\\def\\\\tK{{\\\\tens{K}}}\\n\\\\def\\\\tL{{\\\\tens{L}}}\\n\\\\def\\\\tM{{\\\\tens{M}}}\\n\\\\def\\\\tN{{\\\\tens{N}}}\\n\\\\def\\\\tO{{\\\\tens{O}}}\\n\\\\def\\\\tP{{\\\\tens{P}}}\\n\\\\def\\\\tQ{{\\\\tens{Q}}}\\n\\\\def\\\\tR{{\\\\tens{R}}}\\n\\\\def\\\\tS{{\\\\tens{S}}}\\n\\\\def\\\\tT{{\\\\tens{T}}}\\n\\\\def\\\\tU{{\\\\tens{U}}}\\n\\\\def\\\\tV{{\\\\tens{V}}}\\n\\\\def\\\\tW{{\\\\tens{W}}}\\n\\\\def\\\\tX{{\\\\tens{X}}}\\n\\\\def\\\\tY{{\\\\tens{Y}}}\\n\\\\def\\\\tZ{{\\\\tens{Z}}}\\n\\n\\n Graph\\n\\\\def\\\\gA{{\\\\mathcal{A}}}\\n\\\\def\\\\gB{{\\\\mathcal{B}}}\\n\\\\def\\\\gC{{\\\\mathcal{C}}}\\n\\\\def\\\\gD{{\\\\mathcal{D}}}\\n\\\\def\\\\gE{{\\\\mathcal{E}}}\\n\\\\def\\\\gF{{\\\\mathcal{F}}}\\n\\\\def\\\\gG{{\\\\mathcal{G}}}\\n\\\\def\\\\gH{{\\\\mathcal{H}}}\\n\\\\def\\\\gI{{\\\\mathcal{I}}}\\n\\\\def\\\\gJ{{\\\\mathcal{J}}}\\n\\\\def\\\\gK{{\\\\mathcal{K}}}\\n\\\\def\\\\gL{{\\\\mathcal{L}}}\\n\\\\def\\\\gM{{\\\\mathcal{M}}}\\n\\\\def\\\\gN{{\\\\mathcal{N}}}\\n\\\\def\\\\gO{{\\\\mathcal{O}}}\\n\\\\def\\\\gP{{\\\\mathcal{P}}}\\n\\\\def\\\\gQ{{\\\\mathcal{Q}}}\\n\\\\def\\\\gR{{\\\\mathcal{R}}}\\n\\\\def\\\\gS{{\\\\mathcal{S}}}\\n\\\\def\\\\gT{{\\\\mathcal{T}}}\\n\\\\def\\\\gU{{\\\\mathcal{U}}}\\n\\\\def\\\\gV{{\\\\mathcal{V}}}\\n\\\\def\\\\gW{{\\\\mathcal{W}}}\\n\\\\def\\\\gX{{\\\\mathcal{X}}}\\n\\\\def\\\\gY{{\\\\mathcal{Y}}}\\n\\\\def\\\\gZ{{\\\\mathcal{Z}}}\\n\\n Sets\\n\\\\def\\\\sA{{\\\\mathbb{A}}}\\n\\\\def\\\\sB{{\\\\mathbb{B}}}\\n\\\\def\\\\sC{{\\\\mathbb{C}}}\\n\\\\def\\\\sD{{\\\\mathbb{D}}}\\n Don't use a set called E, because this would be the same as our symbol\\n for expectation.\\n\\\\def\\\\sF{{\\\\mathbb{F}}}\\n\\\\def\\\\sG{{\\\\mathbb{G}}}\\n\\\\def\\\\sH{{\\\\mathbb{H}}}\\n\\\\def\\\\sI{{\\\\mathbb{I}}}\\n\\\\def\\\\sJ{{\\\\mathbb{J}}}\\n\\\\def\\\\sK{{\\\\mathbb{K}}}\\n\\\\def\\\\sL{{\\\\mathbb{L}}}\\n\\\\def\\\\sM{{\\\\mathbb{M}}}\\n\\\\def\\\\sN{{\\\\mathbb{N}}}\\n\\\\def\\\\sO{{\\\\mathbb{O}}}\\n\\\\def\\\\sP{{\\\\mathbb{P}}}\\n\\\\def\\\\sQ{{\\\\mathbb{Q}}}\\n\\\\def\\\\sR{{\\\\mathbb{R}}}\\n\\\\def\\\\sS{{\\\\mathbb{S}}}\\n\\\\def\\\\sT{{\\\\mathbb{T}}}\\n\\\\def\\\\sU{{\\\\mathbb{U}}}\\n\\\\def\\\\sV{{\\\\mathbb{V}}}\\n\\\\def\\\\sW{{\\\\mathbb{W}}}\\n\\\\def\\\\sX{{\\\\mathbb{X}}}\\n\\\\def\\\\sY{{\\\\mathbb{Y}}}\\n\\\\def\\\\sZ{{\\\\mathbb{Z}}}\\n\\n Entries of a matrix\\n\\\\def\\\\emLambda{{\\\\Lambda}}\\n\\\\def\\\\emA{{A}}\\n\\\\def\\\\emB{{B}}\\n\\\\def\\\\emC{{C}}\\n\\\\def\\\\emD{{D}}\\n\\\\def\\\\emE{{E}}\\n\\\\def\\\\emF{{F}}\\n\\\\def\\\\emG{{G}}\\n\\\\def\\\\emH{{H}}\\n\\\\def\\\\emI{{I}}\\n\\\\def\\\\emJ{{J}}\\n\\\\def\\\\emK{{K}}\\n\\\\def\\\\emL{{L}}\\n\\\\def\\\\emM{{M}}\\n\\\\def\\\\emN{{N}}\\n\\\\def\\\\emO{{O}}\\n\\\\def\\\\emP{{P}}\\n\\\\def\\\\emQ{{Q}}\\n\\\\def\\\\emR{{R}}\\n\\\\def\\\\emS{{S}}\\n\\\\def\\\\emT{{T}}\\n\\\\def\\\\emU{{U}}\\n\\\\def\\\\emV{{V}}\\n\\\\def\\\\emW{{W}}\\n\\\\def\\\\emX{{X}}\\n\\\\def\\\\emY{{Y}}\\n\\\\def\\\\emZ{{Z}}\\n\\\\def\\\\emSigma{{\\\\Sigma}}\\n\\n entries of a tensor\\n Same font as tensor, without \\\\bm wrapper\\n\\\\newcommand{\\\\etens}[1]{\\\\mathsfit{#1}}\\n\\\\def\\\\etLambda{{\\\\etens{\\\\Lambda}}}\\n\\\\def\\\\etA{{\\\\etens{A}}}\\n\\\\def\\\\etB{{\\\\etens{B}}}\\n\\\\def\\\\etC{{\\\\etens{C}}}\\n\\\\def\\\\etD{{\\\\etens{D}}}\\n\\\\def\\\\etE{{\\\\etens{E}}}\\n\\\\def\\\\etF{{\\\\etens{F}}}\\n\\\\def\\\\etG{{\\\\etens{G}}}\\n\\\\def\\\\etH{{\\\\etens{H}}}\\n\\\\def\\\\etI{{\\\\etens{I}}}\\n\\\\def\\\\etJ{{\\\\etens{J}}}\\n\\\\def\\\\etK{{\\\\etens{K}}}\\n\\\\def\\\\etL{{\\\\etens{L}}}\\n\\\\def\\\\etM{{\\\\etens{M}}}\\n\\\\def\\\\etN{{\\\\etens{N}}}\\n\\\\def\\\\etO{{\\\\etens{O}}}\\n\\\\def\\\\etP{{\\\\etens{P}}}\\n\\\\def\\\\etQ{{\\\\etens{Q}}}\\n\\\\def\\\\etR{{\\\\etens{R}}}\\n\\\\def\\\\etS{{\\\\etens{S}}}\\n\\\\def\\\\etT{{\\\\etens{T}}}\\n\\\\def\\\\etU{{\\\\etens{U}}}\\n\\\\def\\\\etV{{\\\\etens{V}}}\\n\\\\def\\\\etW{{\\\\etens{W}}}\\n\\\\def\\\\etX{{\\\\etens{X}}}\\n\\\\def\\\\etY{{\\\\etens{Y}}}\\n\\\\def\\\\etZ{{\\\\etens{Z}}}\\n\\n The true underlying data generating distribution\\n\\\\newcommand{\\\\pdata}{p_{\\\\rm{data}}}\\n The empirical distribution defined by the training set\\n\\\\newcommand{\\\\ptrain}{\\\\hat{p}_{\\\\rm{data}}}\\n\\\\newcommand{\\\\Ptrain}{\\\\hat{P}_{\\\\rm{data}}}\\n The model distribution\\n\\\\newcommand{\\\\pmodel}{p_{\\\\rm{model}}}\\n\\\\newcommand{\\\\Pmodel}{P_{\\\\rm{model}}}\\n\\\\newcommand{\\\\ptildemodel}{\\\\tilde{p}_{\\\\rm{model}}}\\n Stochastic autoencoder distributions\\n\\\\newcommand{\\\\pencode}{p_{\\\\rm{encoder}}}\\n\\\\newcommand{\\\\pdecode}{p_{\\\\rm{decoder}}}\\n\\\\newcommand{\\\\precons}{p_{\\\\rm{reconstruct}}}\\n\\n\\\\newcommand{\\\\laplace}{\\\\mathrm{Laplace}}   Laplace distribution\\n\\n\\\\newcommand{\\\\E}{\\\\mathbb{E}}\\n\\\\newcommand{\\\\Ls}{\\\\mathcal{L}}\\n\\\\newcommand{\\\\R}{\\\\mathbb{R}}\\n\\\\newcommand{\\\\emp}{\\\\tilde{p}}\\n\\\\newcommand{\\\\lr}{\\\\alpha}\\n\\\\newcommand{\\\\reg}{\\\\lambda}\\n\\\\newcommand{\\\\rect}{\\\\mathrm{rectifier}}\\n\\\\newcommand{\\\\softmax}{\\\\mathrm{softmax}}\\n\\\\newcommand{\\\\sigmoid}{\\\\sigma}\\n\\\\newcommand{\\\\softplus}{\\\\zeta}\\n\\\\newcommand{\\\\KL}{\\\\mathrm{KL}}{D_{\\\\mathrm{KL}}}\\n\\\\newcommand{\\\\Var}{\\\\mathrm{Var}}\\n\\\\newcommand{\\\\standarderror}{\\\\mathrm{SE}}\\n\\\\newcommand{\\\\Cov}{\\\\mathrm{Cov}}\\n Wolfram Mathworld says $L^2$ is for function spaces and $\\\\ell^2$ is for vectors\\n But then they seem to use $L^2$ for vectors throughout the site, and so does\\n wikipedia.\\n\\\\newcommand{\\\\normlzero}{L^0}\\n\\\\newcommand{\\\\normlone}{L^1}\\n\\\\newcommand{\\\\normltwo}{L^2}\\n\\\\newcommand{\\\\normlp}{L^p}\\n\\\\newcommand{\\\\normmax}{L^\\\\infty}\\n\\n\\\\newcommand{\\\\parents}{Pa}   See usage in notation.tex. Chosen to match Daphne's book.\\n\\n\\\\DeclareMathOperator*{\\\\argmax}{arg\\\\,max}\\n\\\\DeclareMathOperator*{\\\\argmin}{arg\\\\,min}\\n\\n\\\\DeclareMathOperator{\\\\sign}{sign}\\n\\\\DeclareMathOperator{\\\\Tr}{Tr}\\n\\\\let\\\\ab\\\\allowbreak\\n\"},\n",
       "   {'name': 'preliminaries.tex',\n",
       "    'text': '\\n\\nLet \\\\(\\\\Delta_X   \\\\Delta(x_1,...,x_n) : \\\\max \\\\|x_i-x_j\\\\| / \\\\min \\\\|x_i-x_j\\\\|\\\\) denote the aspect ratio of a point set in Euclidean space. For \\\\(X \\\\in \\\\mathbb{R}^{n\\\\times d}\\\\) a matrix of points, let \\\\(D_X \\\\in \\\\mathbb{R}^{n\\\\times n}\\\\) denote its corresponding interpoint distance matrix. \\n\\nThe problem of producing a t-SNE plot proceeds as follows: \\n\\\\paragraph{Basic Notation}\\n\\n\\\\paragraph{Notation} Unless otherwise specified \\\\(\\\\|\\\\cdot\\\\|\\\\) refers to the Euclidean norm; big-O notation is with respect to \\\\(n\\\\), the number of points; \\n\\n\\\\paragraph{t-SNE} \\nGiven an input dataset\\\\footnote{Without loss of generality, we shall assume that the input dimension $Dn-1$.} \\\\(X   \\\\{x_1,\\\\ldots, x_n\\\\}\\\\subset \\\\mathbb{R}^{D}\\\\), the goal of t-SNE is to find an embedding \\\\(Y   \\\\{y_1,\\\\ldots,y_n\\\\}\\\\subset \\\\mathbb{R}^{d}\\\\) (where \\\\(d\\\\ll D\\\\), typically \\\\(d2\\\\)) that approximately maintains the neighborhood structure in $X$. t-SNE accomplishes this by assigning affinities to input data points which encode how likely an input point is to be a neighbor to a given point. The goal then is to find a configuration of the embedded points $Y$ that induces a similar neighborhood affinity. \\nSpecifically, for \\\\(n>2\\\\), let \\\\(P   P(X) \\\\in \\\\mathbb{R}_+^{n\\\\times n}\\\\) and \\\\(Q   Q(Y) \\\\in \\\\mathbb{R}_+^{n\\\\times n}\\\\) be the input and embedded\\n\\\\textit{affinity matrices} describing the pairwise neighborhood similarities in the input and the output, respectively.\\nan \\\\emph{distribution} that   on the neighbors minimize some joint function of \\\\(X\\\\) and \\\\(Y\\\\). This objective function operates not directly on \\\\(X\\\\) and \\\\(Y\\\\) but instead on \\\\textit{affinity matrices} \\\\(P   P(X) \\\\in \\\\mathbb{R}_+^{n\\\\times n}\\\\) and \\\\(Q   Q(Y) \\\\in \\\\mathbb{R}_+^{n\\\\times n}\\\\) describing the pairwise similarities in the input and output, respectively.\\nt-SNE was introduced as a modification to a method called stochastic neighbor embedding (SNE), developed by \\\\citet{hinton2002stochastic}. The general principle of stochastic neighbor embedding is to a construct kernel similarity matrices over the input and output and then iteratively update the output points such that their affinities match those of the input. t-SNE notably uses a Gaussian-based kernel on the input points and a t-distribution-based kernel on the output points (hence the name t-SNE).   The design choices of t-SNE worked significantly better in practice, and the impressive user base followed.\\nt-SNE constructs \\\\(P\\\\) \\nby first computing the affinities for each point $i$ defined as (for any $j\\\\neq i$)\\\\footnote{When $X$ and $\\\\sigma_i^*$ are clear from context, we will often drop it from the notation.}\\nbased on a symmetrized Gaussian kernel with varying neighborhood sizes \\\\(\\\\{\\\\sigma_i: i\\\\in [n]\\\\}\\\\) for the input affinities. For any input data point $i$, define the neighborhood affinities (with respect to $i$) as\\nconditional Let us first define for distinct \\\\(i,j \\\\in [n]\\\\) the \\\\textit{conditional Gaussian kernel} \\n\\\\begin{equation}\\n   P_{j|i}(X; \\\\sigma_i) : \\\\frac{\\\\exp(-\\\\lVert x_i-x_j\\\\rVert^2/(2\\\\sigma_i^2))}{\\\\sum_{k\\\\neq i} \\\\exp(-\\\\lVert x_i-x_k\\\\rVert^2/(2\\\\sigma_i^2))} \\\\hspace{0.5in} P_{i|i} : 0,\\n\\\\end{equation}\\nwhere $\\\\sigma_i \\\\geq 0$ encodes the (point-dependent) neighborhood scalings\\\\footnote{We define \\\\(P_{j|i}(X; 0) : \\\\lim_{\\\\sigma_i \\\\to 0} P_{j|i}(X; \\\\sigma_i)\\\\).}. It is worth noting that $P_{\\\\cdot|i}$ is a valid probability distribution over $[n]$.\\nThe matrix \\\\(P\\\\) is then constructed based on a crucial parameter called the \\\\emph{perplexity}, which is denoted by \\\\(\\\\rho \\\\in (1,n-1)\\\\) and can be viewed as a proxy for effective number of neighbors, as follows.   (denoted as \\\\(\\\\rho\\\\) and taking values in \\\\([1,n-1]\\\\)), as follows: \\\\footnote{Notably, \\\\citet{jeong2024convergence} established that the procedure is only well-defined for \\\\(\\\\rho\\\\in [1,n-1]\\\\)} \\n\\\\begin{itemize}\\n   \\\\item[(1)] For each $i \\\\in [n]$, select the (unique, see Lemma \\\\ref{lem:unique_neighborhood}) neighborhood scale \\\\(\\\\sigma_i^* \\\\geq 0\\\\) that minimizes the gap between the entropy of \\\\(P_{\\\\cdot | j}(X;\\\\sigma^*_i)\\\\) and \\\\(\\\\log_2 \\\\rho\\\\)., (for any \\\\(\\\\rho \\\\subset [1,n-1]\\\\) for which the entropy is achievable).\\n   \\\\item[(2)] Define \\\\(P   [P_{ij}]_{i,j\\\\in [n]}\\\\) where \\\\(P_{ij} : \\\\frac{1}{2n}(P_{i|j}(\\\\sigma^*_j) + P_{j|i}(\\\\sigma^*_i))\\\\) if \\\\(i\\\\neq j\\\\) and zero otherwise.\\n\\\\end{itemize}\\nThe above procedure is well-defined for \\\\(\\\\rho \\\\in [1, n-1]\\\\).\\nLet \\\\(P_X\\\\) be the set of \\\\(\\\\rho\\\\) for which the above procedure is defined.\\n\\nTo avoid the so-called \\\\emph{the crowding problem} (see \\\\cite{van2008visualizing} for details), the output affinity matrix \\\\(Q\\\\) is computed based on a t-distribution. Specifically, for \\\\(i\\\\neq j\\\\)\\n\\\\begin{equation}\\n   Q_{ij}(Y) : \\\\frac{(1 + \\\\|y_i -y_j\\\\|^2)^{-1}}{\\\\sum_{k,l ; k\\\\neq l} (1 + \\\\|y_k-y_l\\\\|^2)^{-1}} \\\\hspace{0.5in}\\n   Q_{ii} :0.\\n\\\\end{equation}\\nAs indicated before, the objective then is to minimize the gap between the input and the output affinities. This is accomplished by penalizing the relative entropy (KL-divergence) from \\\\(P\\\\) to \\\\(Q\\\\), where these affinity matrices are viewed as probability distributions.   \\\\[\\\\textup{minimize}_Y \\\\; \\\\mathcal{L}_X(Y): \\\\KL(P(X)\\\\|Q(Y))   \\\\sum_{\\\\substack{i,j\\\\\\\\ i\\\\neq j}} P_{ij}(X) \\\\log\\\\Big( \\\\frac{P_{ij}(X) }   {Q_{ij}(Y)}\\\\Big).\\\\]\\n\\\\todo[inline]{L(Y) needs to have a reference to $X$}\\nThis highly non-convex objective is usually optimized by initializing at a good starting point via an \\\\emph{early exaggeration phase}, followed by performing standard gradient descent methods and returning an embedding $Y$ that corresponds to a local minimum of the objective. Our central task is to study the nature of the these (local minimum) embeddings returned by t-SNE and their relation to the space of input datasets.\\nWe can now define the main object of interest in this paper: optimal t-SNE embeddings.\\n\\\\begin{definition}\\n   For an $(n>2)$-point dataset $X \\\\subset \\\\mathbb{R}^{n-1}$ and perplexity parameter $\\\\rho \\\\in (1,n-1)$, define\\n   $${\\\\TSNE}_{\\\\rho}(X) : \\\\{ Y \\\\subset \\\\mathbb{R}^d : \\\\nabla_Y \\\\mathcal{L}_X(Y)   0 \\\\} $$\\n   as the set of outputs $Y\\\\subset \\\\mathbb{R}^d$ that are stationary to the t-SNE objective on a given input $X$.\\n   \\n   Furthermore, for a set of $n$-point datasets $\\\\mathcal{X}_n$, we define ${\\\\TSNE}_{\\\\rho}(\\\\mathcal{X}_n)   \\\\bigcup_{X \\\\in \\\\mathcal{X}_n}{\\\\TSNE}_{\\\\rho}(X).$\\n   If $\\\\mathcal{X}_n$ is the set of \\\\emph{all} $n$-point datasets, we denote ${\\\\TSNE}_\\\\rho(\\\\mathcal{X}_n)$ as ${\\\\IMTSNE}$.\\n\\\\end{definition}\\nAll the supporting proofs for our formal statements can be found in the Appendix, and the code related to our empirical demonstrations is available on Github at \\n\\\\href{https://github.com/njbergam/tsne-exaggerates-clusters}{\\\\texttt{https://github.com/njbergam/tsne-exaggerates-clusters}}.\\\\href{https://github.com/anon594/iclr26_submission8125}{\\\\texttt{https://github.com/anon594/iclr26\\\\_submission8125}}.\\n\\n\\\\paragraph{Code} The code related to our experimental demonstrations is publicly available on \\\\href{https://github.com/anon594/iclr26_submission8125/commits/main/}{GitHub}.\\n\\n'},\n",
       "   {'name': 'related_work.tex',\n",
       "    'text': '\\n\\nThere are two notably distinct ways of contextualizing t-SNE, the data visualization method introduced by \\\\citet{van2008visualizing}. In a chronological sense, \\n\\n\\n\\nConfidence in the data visualizations produced by t-SNE and related methods is a somewhat contentious subject in data science \\\\citep{marx2024seeing}. Some works argue that these methods have merit in terms of preserving cluster structure, while others warn us about the fundamental issues with them and the broader goal of data visualization. \\n\\n\\n\\\\subsection{Data Visualization}\\n\\n\\\\subsection{Performance Guarantees and Analysis of t-SNE}\\n\\n\\\\citet{shaham2017stochastic} were among the first to provide a guarantee on the visualization produced by optimal SNE embeddings of well-clustered data. Works by\\n\\\\citet{linderman2019clustering} and \\n\\\\citet{arora2018analysis} refined and extended this analysis, showing that t-SNE outputs produced using gradient descent yield well-clustered visualizations so long as the input is sufficiently well-clustered. The latter work established this guarantee in considerable generality, including cases where the input is sampled from a mixture of well-separated log-concave distributions.\\n\\nAlong with these algorithmic performance guarantee results, there is a line of work that seeks to establish a more fundamental understanding of t-SNE as an optimization problem. \\\\citet{cai2022theoretical}, for instance, characterized the distinct phases of gradient-based optimization of t-SNE, and proved an asymptotic equivalence between the early exaggeration phase and spectral clustering. \\\\citet{auffinger2023equilibrium} proved a consistency result for a continuous analogue of t-SNE, viewing the optimization problem as producing a map between distributions rather than just point sets. \\\\citet{jeong2024convergence} and \\\\citet{weinkove2024stochastic} studied the gradient flow of t-SNE. The former showed mild assumptions under which optima exist, and the latter showed that, even in cases where the gradient flow diverges the relative interpoint distances stabilize in the limit.\\\\todo{Pretty sure that result was actually established by Jeong and Wu. Weinkove showed SNE does not have this property and bounded the rate of divergence.}.\\n\\nuncovered examples where the t-SNE gradient flow pushes the points infinitely far apart. \\\\textcolor{red}{the relative inerpoint distances stabilize, mention gradient flow} \\n\\\\todo{fix}\\n\\nshowed existence of data distributions and output initializations for which the t-SNE output via gradient descent may diverge. \\n\\n\\\\citet{auffinger2023equilibrium} proved the existence of a t-SNE optimizer in an asymptotic sense: if the input points are sampled from a measure then the t-SNE optimal approaches some unique measure. \\\\citet{jeong2024convergence} use gradient flow techniques to prove the existence and boundedness of t-SNE optimal embeddings under minimal assumptions on the input. \\\\citet{weinkove2024stochastic} showed that \\n\\n\\\\citet{auffinger2023equilibrium} were the first to provide a sort of consistency result, guaranteeing that t-SNE embeddings generated by i.i.d.\\\\ samples from a fixed probability distribution converge in the large sample limit. Furthermore, this limit is related to an optimal measure in the output space: a dimension-reduced version of the original probability measure. The result only shows the existence of this so-called \\\\textit{equilibrium measure}; it is up to future investigation to compute or approximate this measure explicitly.\\n\\n\\\\citet{jeong2024convergence} established the existence of a minimizer for the t-SNE optimization problem   under mild assumptions. They do so by establishing the existence of a bound on the diameter for optimal t-SNE embeddings, though they fall short of giving numerical estimates for this diameter. Their approach hinges on looking at the t-SNE update as a gradient flow. Some of their interesting intermediate results include: (1) the centroid of t-SNE points under a gradient flow does not change, and (2) if one interpoint distance escapes to infinity in a t-SNE gradient flow, then all interpoint distances escape to infinity, at roughly the same rate. \\n\\n\\\\subsection{Weaknesses and Criticisms}\\n\\\\label{sec:related_work_limitations}\\nAs t-SNE, UMAP (developed circa 2008 and 2018, respectively), and related methods have gained widespread usage, they have also attracted a fair share of criticism. \\n\\nBunte, K., Haase, S., Biehl, M., & Villmann, T. (2012). Stochastic neighbor embedding (SNE) for dimension reduction and visualization using arbitrary divergences. Neurocomputing, 90, 23-45\\n\\n\\\\citet{bunte_aribrary_divergences} were among the first to investigate the potential shortcomings of using KL-divergence in a t-SNE visualization and proposed a generalization to other divergences that may be better suited for specific datasets and user needs.\\nBuilding upon the precision-recall framework of \\\\citet{venna2010information},\\n\\\\citet{im2018stochastic} extended this result and explored specific intrinsic structures within data that may be less suited for t-SNE. They concluded that while t-SNE is more attuned to reveal intrinsic cluster structure, it usually fails to reveal intrinsic manifold structure. \\n\\nIn terms of analyzing cluster structure specifically, \\\\citet{yang2021t} provided empirical evidence that t-SNE visualizations are prone to \\\\emph{false negatives}. They presented a selection of well-clustered real-world datasets which t-SNE embeddings, even with reasonable parameter-tuning, do not seem to faithfully represent. They also showed that these practical datasets do not abide by the theoretical cluster separation conditions that are required by \\\\citet{arora2018analysis} analysis. clustered visualization guarantee to go through.\\n\\\\citet{chari2023specious} argued that t-SNE and UMAP are unreliable tools for exploratory data analysis. Taking single-cell genomic data as an important real-world example, they provided systematic empirical evidence that these embeddings suffer high distortion, and often misrepresent neighborhood and cluster structure. Curiously, to the best of our knowledge, there is no systematic theoretical study investigating false positive behavior of t-SNE.\\n\\nMore recently, \\\\citet{snoeck_incompressibility} provided   theoretical evidence that, not just t-SNE, but any embedding technique that attempts to visualize data in constant dimensions is bound to misrepresent the neighborhood structure in most datasets. This work focuses exclusively on how misrepresentations induced by t-SNE visualizations can lead to false conclusions in terms of data analysis.\\n\\nis bound to be misrepresented in not just t-SNE or UMAP visualizations but any low-dimensional metric embedding of realistic data. \\n\\n\\nIn light of the inherent limitations of data visualization procedures, several techniques have sprung up to enhance or test their efficacy. \\n\\n\\\\citet{xia2024statistical} create a statistical test which designates whether embedded points are faithfully represented or\\n\\n\\npreserving distances and neighborhoods suffices for clusters, but not necessary\\n\\nFrom a theoretical perspective, the tradeoff between interpoint distance distortion and dimension of an embedding is fairly well-understood \\\\citep{matousek2013lectures}. [Snoeck et al, 2025] \\n\\n\\\\citep{}\\n\\n\\\\citep{kobak2019art}, for instance, advocated for a protocol for using t-SNE on single-cell data, which included PCA initialization, a high learning rate, and multi-scale similarity kernels. They acknowledge that \\n\\n\\\\citep{chari2023specious} showed that t-SNE is simply not robust to changes in the parameters.\\n\\n\\nFALSE NEGATIVES\\\\citep{yang2021t}\\n\\n\\nThere has been a wave of criticism about t-SNE and UMAP in recent years.\\n\\n\\n\\nHow to even define neighborhood preservation? jaccard, . then cite our own paper, theory treatment.\\n\\n\\\\citep{marx2024seeing} discusses a lab that developed sc-DEED which compared t-SNE and UMAP to understand whether these visualizations are actually useful. This is a bit misguided as it implicitly seems to come from the point of view that t-SNE and UMAP are independent demonstrations of the structure of the data when in fact they are very correlated.'}],\n",
       "  'main': 'main.tex',\n",
       "  'title': 't-SNE exaggerates clusters, provably'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to /Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/logiclearner_a_tool_for_the_guided_practice_of_propositional_logic_proofs.json\n",
      "Saving to /Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__1_false_positives.json\n",
      "Saving to /Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__2_outliers.json\n",
      "Saving to /Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__abstract.json\n",
      "Saving to /Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__appendix.json\n",
      "Saving to /Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__discussion.json\n",
      "Saving to /Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__introduction.json\n",
      "Saving to /Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__main.json\n",
      "Saving to /Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__math_commands.json\n",
      "Saving to /Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__preliminaries.json\n",
      "Saving to /Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__related_work.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'2503.19280': ['/Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/logiclearner_a_tool_for_the_guided_practice_of_propositional_logic_proofs.json'],\n",
       " '2510.07746': ['/Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__1_false_positives.json',\n",
       "  '/Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__2_outliers.json',\n",
       "  '/Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__abstract.json',\n",
       "  '/Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__appendix.json',\n",
       "  '/Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__discussion.json',\n",
       "  '/Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__introduction.json',\n",
       "  '/Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__main.json',\n",
       "  '/Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__math_commands.json',\n",
       "  '/Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__preliminaries.json',\n",
       "  '/Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/t_sne_exaggerates_clusters_provably__related_work.json']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_save_texsource_jsons(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to /Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/logiclearner_a_tool_for_the_guided_practice_of_propositional_logic_proofs.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/Akseldkw/coding/Columbia/UML-Project/data/scientific/logiclearner_a_tool_for_the_guided_practice_of_propositional_logic_proofs.json']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_texsource_jsons(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Version 3 December 2023\\n See section 11 of the User Manual for version history\\n\\n\\n   \\n Please do not use \\\\input{...} to include other tex files.   \\n Submit your LaTeX manuscript as one .tex document.   \\n   \\n All additional figures and files should be attached   \\n separately and not embedded in the \\\\TeX\\\\ document itself.   \\n   \\n\\n\\n\\\\documentclass[referee,sn-basic]{sn-jnl} referee option is meant for double line spacing\\n\\n\\n to print line numbers in the margin use lineno option \\n\\n\\n\\\\documentclass[lineno'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src[\"files\"][0][\"text\"][:500]  # first 500 chars of main tex file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sn-article.tex'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src[\"files\"][0][\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2503.19280',\n",
       " 'files': [{'name': 'sn-article.tex',\n",
       "   'text': \"Version 3 December 2023\\n See section 11 of the User Manual for version history\\n\\n\\n   \\n Please do not use \\\\input{...} to include other tex files.   \\n Submit your LaTeX manuscript as one .tex document.   \\n   \\n All additional figures and files should be attached   \\n separately and not embedded in the \\\\TeX\\\\ document itself.   \\n   \\n\\n\\n\\\\documentclass[referee,sn-basic]{sn-jnl} referee option is meant for double line spacing\\n\\n\\n to print line numbers in the margin use lineno option \\n\\n\\n\\\\documentclass[lineno,sn-basic]{sn-jnl} Basic Springer Nature Reference Style/Chemistry Reference Style\\n\\n\\n to compile with pdflatex/xelatex use pdflatex option \\n\\n\\n\\\\documentclass[pdflatex,sn-basic]{sn-jnl} Basic Springer Nature Reference Style/Chemistry Reference Style\\n\\n\\nNote: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove \\x93Numbered\\x94 in the optional parenthesis. \\nThe option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst   \\n \\n\\\\documentclass[pdflatex,sn-nature]{sn-jnl} Style for submissions to Nature Portfolio journals\\n\\\\documentclass[pdflatex,sn-basic]{sn-jnl} Basic Springer Nature Reference Style/Chemistry Reference Style\\n\\\\documentclass[pdflatex,sn-mathphys-num]{sn-jnl} Math and Physical Sciences Numbered Reference Style \\n\\\\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl} Math and Physical Sciences Author Year Reference Style\\n\\\\documentclass[pdflatex,sn-aps]{sn-jnl} American Physical Society (APS) Reference Style\\n\\\\documentclass[pdflatex,sn-vancouver,Numbered]{sn-jnl} Vancouver Reference Style\\n\\\\documentclass[pdflatex,sn-apa]{sn-jnl} APA Reference Style \\n\\\\documentclass[pdflatex,sn-chicago]{sn-jnl} Chicago-based Humanities Reference Style\\n\\n Standard Packages\\n<additional latex packages if required can be included here>\\n\\n\\\\usepackage{graphicx}\\n\\\\usepackage{multirow}\\n\\\\usepackage{amsmath,amssymb,amsfonts}\\n\\\\usepackage{amsthm}\\n\\\\usepackage{mathrsfs}\\n\\\\usepackage[title]{appendix}\\n\\\\usepackage{xcolor}\\n\\\\usepackage{textcomp}\\n\\\\usepackage{manyfoot}\\n\\\\usepackage{booktabs}\\n\\\\usepackage{algorithm}\\n\\\\usepackage{algorithmicx}\\n\\\\usepackage{algpseudocode}\\n\\\\usepackage{listings}\\n\\\\usepackage{subcaption}\\n\\\\usepackage[export]{adjustbox}\\n\\n\\n\\n as per the requirement new theorem styles can be included as shown below\\n\\\\theoremstyle{thmstyleone}\\n\\\\newtheorem{theorem}{Theorem}   meant for continuous numbers\\n\\\\newtheorem{theorem}{Theorem}[section] meant for sectionwise numbers\\n optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition\\n\\\\newtheorem{proposition}[theorem]{Proposition} \\n\\\\newtheorem{proposition}{Proposition} to get separate numbers for theorem and proposition etc.\\n\\n\\\\theoremstyle{thmstyletwo}\\n\\\\newtheorem{example}{Example}\\n\\\\newtheorem{remark}{Remark}\\n\\n\\\\theoremstyle{thmstylethree}\\n\\\\newtheorem{definition}{Definition}\\n\\n\\\\newtheorem{lemma}[theorem]{Lemma}\\n\\n\\\\newcommand{{\\\\logiclearner}}{\\\\textsc{LogicLearner}}\\n\\n\\\\raggedbottom\\n\\\\unnumbered uncomment this for unnumbered level heads\\n\\n\\\\begin{document}\\n\\n\\\\title[Article Title]{LogicLearner: A Tool for the Guided Practice of Propositional Logic Proofs}\\n\\n\\\\author*[1]{\\\\fnm{Amogh} \\\\sur{Inamdar}}\\\\email{amogh.inamdar@columbia.edu}\\n\\n\\\\author[1,3]{\\\\fnm{Uzay} \\\\sur{Macar}}\\\\email{uzay@aiphabet.org}\\n\\n\\\\author[1]{\\\\fnm{Michel} \\\\sur{Vazirani}}\\\\email{mvv2114@columbia.edu}\\n\\n\\\\author[2]{\\\\fnm{Michael} \\\\sur{Tarnow}}\\\\email{m.tarnow@columbia.edu}\\n\\n\\\\author[2]{\\\\fnm{Zarina} \\\\sur{Mustapha}}\\\\email{zarina@columbia.edu}\\n\\n\\\\author[2]{\\\\fnm{Natalia} \\\\sur{Dittren}}\\\\email{nd2664@columbia.edu}\\n\\n\\\\author[2]{\\\\fnm{Sam} \\\\sur{Sadeh}}\\\\email{ss6316@columbia.edu}\\n\\n\\\\author[1]{\\\\fnm{Nakul} \\\\sur{Verma}}\\\\email{verma@cs.columbia.edu}\\n\\n\\\\author[1,3]{\\\\fnm{Ansaf} \\\\sur{Salleb-Aouissi}}\\\\email{ansaf@cs.columbia.edu}\\n\\n\\\\affil[1]{\\\\orgdiv{Computer Science}, \\\\orgname{Columbia University}, \\\\country{United States}}   , \\\\orgaddress{\\\\street{500 W 120 St.}, \\\\city{New York City}, \\\\postcode{10027}, \\\\state{New York}, \\\\country{United States}}}\\n\\n\\\\affil[2]{\\\\orgdiv{Center for Teaching and Learning}, \\\\orgname{Columbia University}, \\\\country{United States}} , \\\\orgaddress{\\\\street{Lewisohn Hall, 2970 Broadway}, \\\\city{New York City}, \\\\postcode{10027}, \\\\state{New York}, \\\\country{United States}}}\\n\\n\\\\affil[3]{\\\\orgdiv{}\\\\orgname{Aiphabet, Inc.}, \\\\country{United States}} , \\\\orgaddress{\\\\street{}, \\\\city{}, \\\\postcode{}, \\\\state{}, \\\\country{}}}\\n\\n\\\\abstract{The study of propositional logic---fundamental to the theory of computing---is a cornerstone of the undergraduate computer science curriculum. Learning to solve logical proofs requires repeated guided practice, but undergraduate students often lack access to on-demand tutoring in a judgment-free environment. In this work, we highlight the need for guided practice tools in undergraduate mathematics education and outline the desiderata of an effective practice tool. We accordingly develop {\\\\logiclearner}\\\\footnote{https://logiclearner.ctl.columbia.edu/}, a web application for guided logic proof practice. {\\\\logiclearner} consists of an interface to attempt logic proofs step-by-step and an automated proof solver to generate solutions on the fly, allowing users to request guidance as needed. We pilot {\\\\logiclearner} as a practice tool in two semesters of an undergraduate discrete mathematics course and receive strongly positive feedback for usability and pedagogical value in student surveys. To the best of our knowledge, {\\\\logiclearner} is the only learning tool that provides an end-to-end practice environment for logic proofs with immediate, judgment-free feedback.}\\n\\n\\\\keywords{Logic, Proof, Education, Mathematics, Artificial Intelligence, Pedagogy}\\n\\n\\\\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}\\n\\n\\\\maketitle\\n\\n\\\\section{Introduction}\\\\label{sec:intro}\\n\\n\\\\begin{figure}[t!]\\n\\\\centering\\n\\\\includegraphics[width\\\\textwidth]{images/introFigDemo.jpg}\\n\\\\caption{{\\\\logiclearner} is a holistic environment for guided logic proof practice.}\\\\label{fig:intro}\\n\\\\end{figure}\\n\\nThe ability to think critically and reason quantitatively is an important skill for all Computer Science (CS) students. CS students generally acquire such quantitative problem-solving skills in foundational undergraduate proof-based CS courses such as discrete mathematics. However, learning mathematics can be difficult. Students tend to find dealing with proofs challenging, and can seldom construct and communicate a long sequence of logical arguments effortlessly. Students often skip proof steps, leading to logical leaps or incorrect conclusions in their arguments. In order to promote student learning, effective course instructors identify gaps in student understanding and provide timely, high-quality feedback. Student who receive such feedback are able to build confidence, master the material, and develop valuable quantitative reasoning skills \\\\citep{evans2013making}. However, a common issue across educational institutions and grading platforms is that instructors lack the resources to provide timely and informative feedback that promotes student learning. Once the time comes for formal assessments of learning (such as graded homeworks and exams), it is usually too late for the student to master the primary objectives of such courses. This issue is exacerbated in undergraduate-level proof-based courses such as discrete mathematics. There is a wide range in the mathematical maturity of students, and feedback on assignments and quizzes is usually received after several weeks. Additionally, per our exploration of the state-of-the-art, there are currently no off-the-shelf automated grading systems for student-level mathematical proofs to speed up the feedback process. This lack of timely feedback, coupled with a general level of student anxiety in dealing with proofs, translates directly into decreased student performance---our study of exam results in an undergraduate discrete mathematics course found that exam performance was \\\\emph{10\\\\ lower on proof questions} than other types of questions.\\n\\nInteractive practice in undergraduate-level education is often limited to periodic tutorial sessions with instructors or teaching assistants (TAs). While TAs can do an excellent job in helping students, their limited availability creates a challenge in large undergraduate classes. Additionally, students may fear judgment from peers and instructors over the quality and frequency of their doubts in group settings, leading to reticence in seeking help. To alleviate these shortcomings, we propose the development and use of web-based tools for the guided practice of quantitative reasoning problems. We identify the following desiderata for such practice tools.\\n\\\\begin{itemize}\\n   \\\\item \\\\textbf{An effective practice tool is accessible and user-friendly.} Even an excellent practice tool is of little use to students who cannot access it. Tools that are difficult to use or hidden behind paywalls tend to isolate students with limited resources, despite such students being likely to benefit the most from them. \\n   \\\\item \\\\textbf{A practice tool must be a source of truth.} As a trusted authority on the subject matter, a teaching aid that provides incorrect information is especially detrimental to the learning process. Ideally, such tools would allow external experts to independently verify the correctness of these tools. We encourage open-source development as a strategy to ensure that pedagogical tools are both accessible and transparent.\\n   \\\\item \\\\textbf{An effective practice tool is non-judgmental.} While expert feedback is invaluable, a student who fears judgement from peers and instructors will likely be more willing to explore ideas in a self-guided setting. This is why a good practice tool should provide constructive feedback and encouragement to students. Overly critical feedback or high-stakes settings may discourage use and affect the learning process.\\n   \\\\item \\\\textbf{An effective practice tool provides only relevant guidance.} Effective instructors know that feedback must be catered to the problem setting and the type of mistake made. A tool that inundates the student with options or provides very basic feedback (e.g., only grading an answer as right or wrong) is likely to be less effective than one with specific, cogent, and relevant guidance.\\n\\\\end{itemize}\\n\\nWith these desiderata in mind, we tackle the challenge of improving the pedagogical experience of learning to solve logic proofs. We develop {\\\\logiclearner} (Figure \\\\ref{fig:intro}), an open-source web application that provides a holistic environment for logic proof practice. {\\\\logiclearner} minimizes the feedback turnaround time for logic proof practice, leading to greater student engagement with course materials related to theorems and proofs. Using tools from AI and machine learning, {\\\\logiclearner} validates student solutions at each steps, identifies mistakes, and solves the problem on-the-fly to provide personalized hints in a judgment-free environment. {\\\\logiclearner} is intended to supplement, not replace, the invaluable efforts of teaching assistants and tutorial sessions in a course that covers propositional logic. {\\\\logiclearner} is a free and open-source project, managed by the Center for Teaching and Learning (CTL) at Columbia University. Visuals of the application are presented in Appendix \\\\ref{sec:apxVisual}. The source code is available at \\\\url{https://github.com/ccnmtl/logiclearnertools}. \\n\\\\\\\\\\n\\nOur contributions in this work can be summarized as follows:\\n\\\\begin{itemize}\\n   \\\\item We identify a gap in the availability of on-demand, judgment-free guidance in undergraduate computer science curricula. \\n   \\\\item We highlight the need for practice tools that provide instantaneous, high-quality feedback, and identify the desiderata of these tools for pedagogical impact.\\n   \\\\item We develop {\\\\logiclearner}, a web-based application for logic proof practice with instant guidance that is mathematically sound and judgment free. At the time of writing, {\\\\logiclearner} is the only application for the end-to-end practice of logic proofs with guidance.\\n   \\\\item Through user studies across two semesters of an undergraduate discrete mathematics course, we show that {\\\\logiclearner} reduces student anxiety and improves their confidence in tackling logic proofs.\\n   \\\\item We assess the drawbacks of AI question-answering tools (a popular alternative among students for pedagogical guidance) and show that {\\\\logiclearner} provides superior pedagogical value over such tools in multiple ways.\\n\\\\end{itemize}\\n\\n\\\\section{Related Work}\\\\label{sec:related}\\n\\nOur work combines the advances in automated problem solving and in software for mathematics/logic education. In this section, we analyze the state of the art in these areas though a pedagogical lens.\\n\\n\\\\subsection{Automated logic problem solvers}\\\\label{subsec:rel1}\\n\\nTo compensate for a lack of on-demand, judgment-free pedagogical guidance, students are increasingly turning to AI question-answering systems powered by Large Language Models (LLMs), like ChatGPT\\\\footnote{OpenAI (2023). ChatGPT (Mar 14 version); GPT 3.5 backend. \\\\url{https://openai.com/blog/chatgpt}}, for pedagogical feedback. LLMs \\\\cite{achiam2023gpt, touvron2023llama, vicuna2023} are neural networks with billions of parameters that are trained for text generation on internet-scale datasets. With previously unseen prowess on natural language tasks, LLMs are now the basis of applications across domains ranging from medicine to finance \\\\cite{thirunavukarasu2023large, singhal2022large, cui2023chatlaw, webersinke2022climatebert, wu2023bloomberggpt}. However, using an LLM-based application as a learning tool for mathematics has several drawbacks. LLMs require the use of specialized prompting techniques to score well on mathematical reasoning tasks. Chain-of-Thought prompting \\\\cite{wei2022chain} formulates step-by-step LLM prompts to emulate human reasoning, improving on question-only prompts for simple arithmetic and logic problems. Extensions such as Graph of Thoughts \\\\cite{besta2023graph} and MathPrompter \\\\cite{imani2023mathprompter} develop increasingly complex input- and early-layer techniques for sophisticated users. Drori et al. \\\\cite{drori2022neural} leverage the OpenAI Codex LLM \\\\cite{chen2021evaluating} to produce impressive question-answering performance on undergraduate mathematics courses, but explicitly state that the model cannot solve ``questions with solutions that require proofs''. Datasets that have been developed to evaluate reasoning ability \\\\citep{ontanon2022logicinference, cobbe2021training, hendrycks2021measuring} primarily consist of problems with simple, few-step solutions. Additionally, LLMs have been shown to hallucinate factual information \\\\cite{huang2023survey} and are susceptible to prompt manipulation \\\\cite{cohen2024comes}. Hence, students relying on LLMs for assistance with proofs are at risk of receiving \\\\emph{confident but incorrect} answers that stunt the learning process.\\n\\nAnother focus in the literature is on tool-assisted human problem solving. Automated theorem-provers such as Isabelle \\\\cite{nipkow2002isabelle} have been used to develop human-friendly representations of logic problems \\\\cite{Villadsen_2022} and other mathematical structures \\\\cite{fuenmayor2022formalising}. Other tools---such as the popular mathematical software Wolfram \\\\cite{weisstein}---simplify or evaluate Boolean expressions using truth tables, but are not directly useful for the development of proof-solving skill.\\n\\nWhile these works make impressive strides in AI mathematical reasoning, they are not yet advanced enough to solve problems requiring several steps of reasoning. Most importantly, these systems \\\\emph{are not designed for pedagogical use}. They do not provide interfaces that benefit problem-solving practice and produce full answers, preventing students from reasoning by themselves with only subtle hints when needed. Using such systems as practice tools compounds students' frustrations with the learning process. To address this important gap and facilitate practice, we develop {\\\\logiclearner} as purpose-built learning tool that is both mathematically sound and easy to use. \\n\\n\\\\subsection{The pedagogy of propositional logic}\\\\label{subsec:rel2}\\n\\nPropositional logic (and discrete mathematics at large) are important topics of study---not only for computer science students, but also for general mathematical maturity \\\\citep{sandefur2022teaching, greefrath2022mathematical}.\\nThe literature largely asserts that these are challenging topics to learn. Multiple prior works have attempted to model student behavior in learning to solve proofs. Dawkins and Roh \\\\cite{dawkins2022aspects} posit that students have a twofold process to understanding a logical property, first generating examples and then evaluating which examples satisfy the property. This generate-and-test process is analogous to many fundamental computer search algorithms. EvoLogic \\\\cite{galafassi2020evologic, galafassi2022evologic} takes an agent-based approach to model student responses to 10 simple logic exercises. These works do not attempt to improve the learning process, as we do in this paper. \\n\\nLogical concepts also form the basis of a few learning games. \\\\textit{Proplog} and \\\\textit{Syllog} \\\\cite{ohrstrom2019teaching} require players to evaluate the logical soundness of propositions and syllogisms respectively. TrueBiters \\\\cite{de2019truebiters} aims to gamify the process of learning truth tables by representing bitwise operators as monsters that `eat' bits and return the results, with the goal of reducing a bit string to a specified target bit. We are not aware of any games or practice tools that directly aim to build proof-solving skills, as {\\\\logiclearner} does.\\n\\n\\\\section{Methods}\\\\label{sec:method}\\n\\n{\\\\logiclearner} is a web application with a gamified interface that is purpose-built for learning, backed by a parser to process Boolean expressions and an AI proof solver to provide hints when requested. In this section, we describe the working of each of its components in detail. Starting with the user interface, each subsequent sub-section describes a module that is increasingly abstracted away from the student. We reiterate that {\\\\logiclearner} is open-source and encourage community contributions to its improvement.\\n\\n\\\\subsection{User Experience}\\\\label{subsec:m1}\\n\\nAs described in Section \\\\ref{sec:intro}, ease of use is essential to a good pedagogical tool. We design {\\\\logiclearner} as a web application with an attractive but simple interface that allows students to focus on solving the problems. For first-time users, {\\\\logiclearner} contains a tutorial section with 6 simple steps describing usability. Students then attempt logic problems of varying difficulty. As they go through the learning process, {\\\\logiclearner} tracks their progress and enables them to review and re-attempt previously attempted proofs. As a web application, {\\\\logiclearner} \\\\textbf{works well on both computers and mobile devices}, allowing for practice on the go.\\n\\n\\\\subsubsection{User Interface}\\\\label{subsubsec:m11}\\n\\n\\\\begin{figure}[ht!]\\n   \\\\centering\\n   \\\\includegraphics[width0.8\\\\linewidth]{images/methodsUserFlow.jpg}\\n   \\\\caption{The user flow diagram of {\\\\logiclearner} shows that students have a linear decision making process, enabling ease of use.}\\\\label{fig:uflow}\\n\\\\end{figure}\\n\\nAs seen in Figure \\\\ref{fig:uflow}, students can choose the difficulty of the proof they will attempt. As a student attempts a proof, {\\\\logiclearner} finds solutions from the steps they have already taken and produces hints when requested to unblock their progress. Once a solution has been found (or requested), the student is able to review the full proof and attempt the next question if desired. Detailed screenshots of the application, including the tutorial section, can be found in Appendix \\\\ref{sec:apxVisual}.\\n\\nStudents use buttons to navigate the application. At each proof step, they select a logic rule to apply via a drop-down menu and enter the corresponding statement as free-form text. If incorrect, students are not explained their mistakes to allow them to reason again. Upon requesting a hint, students are provided with the correct logic rule to apply. Another hint request leads to the correct expression for the next step being provided. We use this two-step hints process to allow students to attempt problems with only partial information, just as an instructor would nudge a student in the right direction. To prevent student frustration when stuck, we also allow them to view the full solution without penalty if requested. Students are able to reset their progress and re-attempt the questions \\\\emph{ad infinitum}. The user interface of {\\\\logiclearner} is implemented as a Django\\\\footnote{\\\\url{https://www.djangoproject.com/}} web server in Python.\\n\\n\\\\subsubsection{Database}\\\\label{subsubsec:m12}\\n\\n{\\\\logiclearner} maintains a PostgreSQL\\\\footnote{\\\\url{https://www.postgresql.org/}} database as the back end of the user experience. Students do not need to explicitly log in to the application, but their progress is still preserved across sessions by tracking and storing site accesses. This provides a seamless user experience where students are not prompted to repeat proofs they have already attempted unless explicitly requested. {\\\\logiclearner} is a fully open source application, and source code is available in public repositories on GitHub\\\\footnote{UX: \\\\url{https://github.com/ccnmtl/logiclearner}, Back-end: \\\\url{https://github.com/ccnmtl/logiclearnertools}}. The application server and database are actively managed by Center for Teaching and Learning at Columbia University.\\n\\n\\n\\\\subsection{Application Back-end and Proof Solving}\\\\label{subsec:m2}\\n\\n{\\\\logiclearner} performs several computations upon receiving user input. When a student provides a logic rule and an expression as the next step of a proof, the application must \\n\\\\begin{itemize}\\n   \\\\item validate the syntax of the input expression,\\n   \\\\item check for logical entailment from the current state, and\\n   \\\\item find the subsequent steps of a full solution for hints.\\n\\\\end{itemize}\\n\\nThis process---described as a data flow diagram in Figure \\\\ref{fig:methods}---requires a computational model of Boolean expressions and logical proofs. We model Boolean expressions with a Context-Free Grammar (CFG) and describe the logic proof as a graph search problem between expressions. We then use AI search techniques to solve these proofs and provide hints to students in real time. Below, we describe this `business logic' of {\\\\logiclearner} in detail.\\n\\n\\\\begin{figure*}[ht!]\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/LLmethodsFig.jpg}\\n   \\\\caption{{\\\\logiclearner} generates a `frontier' of all possible next steps from the current expression, and subsequently a full proof for hints. User input is checked for syntax and membership in the frontier. The user receives feedback on incorrect inputs and can request a hint at any time.}\\\\label{fig:methods}\\n\\\\end{figure*}\\n\\nWe use the following notation in this and subsequent sections: $X   \\\\{x, y, z,\\\\ldots\\\\}$ is a set of Boolean variables. $E$ denotes the set containing every Boolean expression $e$ formed by any number of variables from $X$ and the constants $\\\\{T, F\\\\}$, which denote \\\\textbf{True} and \\\\textbf{False} respectively. $R$ denotes the set of logic rules, which are transformations according to the named logical equivalences between expressions (Idempotence, Associativity, etc., Table \\\\ref{tab:rules}). Every $r\\\\in R$ transforms expression $e$ into one of a set of expressions $\\\\{e_1, e_2,\\\\ldots\\\\}$, where each $e_i$ is formed by applying $r$ to a different position in $e$. If $r$ cannot be applied to $e$ then $r(e)\\\\emptyset$. To fully represent an equivalence relation, we define the inverse of a rule $r$ to be $r^{-1}\\\\in R$ such that $e\\\\in r^{-1}\\\\circ r(e)$. For example, we can apply Idempotence ($r$) to the expression $p\\\\lor q \\\\lor q$ to derive $p\\\\lor q$, and likewise apply Idempotence ($r^{-1}$) to $p\\\\lor q$ to derive $p\\\\lor q \\\\lor q$. We phrase our problems as `\\\\textit{Prove that \\\\textbf{source} is logically equivalent to \\\\textbf{target}.}', where \\\\textbf{source} and \\\\textbf{target} are Boolean expressions. When \\\\textbf{target} is \\\\textbf{True}/\\\\textbf{False}, the question is phrased `\\\\textit{---is a Tautology}'/`\\\\textit{---is a Fallacy}' respectively. Every problem is guaranteed to have a solution.\\n\\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{The rules of propositional logic}\\\\label{tab:rules}\\n\\\\begin{tabular}{@{}lll@{}}\\n\\\\toprule\\nRule & Equivalence 1 & Equivalence 2   \\\\\\\\\\n\\\\midrule\\nAbsorption   &   $p \\\\lor (p\\\\land q) \\\\equiv p$ & $p \\\\land (p\\\\lor q) \\\\equiv p$ \\\\\\\\\\nAssociativity   &   $p \\\\lor (q\\\\lor r) \\\\equiv (p \\\\lor q) \\\\lor r$ & $p \\\\land (q\\\\land r) \\\\equiv (p \\\\land q) \\\\land r$ \\\\\\\\\\nCommutativity   & $p \\\\lor q \\\\equiv q \\\\lor p$ & $p \\\\land q \\\\equiv q \\\\land p$ \\\\\\\\\\nDe Morgan's Law & $\\\\neg(p \\\\lor q) \\\\equiv \\\\neg p\\\\land \\\\neg q$ & $\\\\neg(p \\\\land q) \\\\equiv \\\\neg p\\\\lor \\\\neg q$ \\\\\\\\\\nDistributivity   & $p \\\\lor (q\\\\land r) \\\\equiv (p \\\\lor q) \\\\land (p \\\\lor r)$ & $p \\\\land (q\\\\lor r) \\\\equiv (p \\\\land q) \\\\lor (p \\\\land r)$ \\\\\\\\\\n & $p \\\\lor (q\\\\lor r) \\\\equiv (p \\\\lor q) \\\\lor (p \\\\lor r)$ & $p \\\\land (q\\\\land r) \\\\equiv (p \\\\land q) \\\\land (p \\\\land r)$ \\\\\\\\\\nDomination   &   $p \\\\lor T \\\\equiv T$ & $p \\\\land F \\\\equiv F$ \\\\\\\\\\nIdempotence   &   $p \\\\lor p \\\\equiv p$ & $p \\\\land p \\\\equiv p$ \\\\\\\\\\nIdentity   & $p \\\\lor F \\\\equiv p$ & $p \\\\land T \\\\equiv p$   \\\\\\\\\\nIff as Implication   & $p \\\\leftrightarrow q \\\\equiv (p \\\\to q) \\\\land (q \\\\to p)$ & -- \\\\\\\\\\nImplication as Disjunction &   $p \\\\to q \\\\equiv \\\\neg p \\\\lor q$ & -- \\\\\\\\\\nNegation   & $p \\\\lor \\\\neg p \\\\equiv T$ & $p \\\\land \\\\neg p \\\\equiv F$ \\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\n\\\\subsubsection{Validating user input}\\\\label{subsubsec:m21}\\n\\nTo take a step towards the solution from their current state $e_t$, a user selects a logic rule $r$ and inputs a Boolean expression $e_{t+1}$. The input is `valid' if it is a syntactically correct Boolean expression that is entailed by the selected rule of logic, i.e., $e_{t+1}\\\\in r(e_t)$. \\n\\nWe use a left-recursive context-free grammar for Boolean expressions and use the Lark\\\\footnote{\\\\url{https://pypi.org/project/lark-parser}} parser-generator to validate user syntax. The grammar accounts for variations in token representation (such as `1', `T', or `True' for the \\\\textit{true} truth value) and requires a maximum look-ahead of 1, enabling the use of Lark's fast and efficient LALR(1) parser. Each parse tree node is annotated the start and end positions of its token span.\\n\\n\\\\subsubsection{Modeling the logic proof}\\\\label{subsubsec:m23}\\n\\nSolving logic problems is challenging to students who first encounter them. To guide students who are stuck, we develop a two-level hints feature that can be triggered to provide the correct rule and subsequently the correct expression for the next step. To do this, {\\\\logiclearner} must find a path to the target expression $e_t$ from the user's current expression $e_c$. The size of the Boolean state space and unpredictable user input make pre-computed or brute-force solutions infeasible. Here, we model logic proofs as a graph search problem and use A* search \\\\cite{Hart1968} to dynamically find solutions. We define some properties and assumptions of our search graph below: \\n\\\\begin{enumerate}[1.]\\n\\\\item Every node represents a Boolean expression $e$. For a given proof, the student must derive expression $e_t$ starting from expression $e_s$.\\n\\\\item Every edge represents a transition between nodes brought about by applying a logic rule $r_i$. That is, $\\\\exists$ edge $(e_p,e_q)$ $\\\\forall i, p, q$ s.t. $r_i(e_p) \\\\supseteq e_q$.\\n\\\\item Since every rule $r_i\\\\in R$ is an equivalence relation, every edge $(e_p, e_q)$ is bidirectional.\\n\\\\item We only focus on `solvable' problems of the type \\\\textit{`Prove that $e_s$ is logically equivalent to $e_t$'}, so that at least one path (proof) from $e_s$ to $e_t$ is known to exist.\\n\\\\end{enumerate}\\n\\nThese properties ensure that every attempt of a {\\\\logiclearner} problem always has a path to success.\\n\\n\\\\begin{lemma}\\\\label{lemma:1}\\n In every {\\\\logiclearner} problem, the target expression $e_t$ is always reachable from any expression $e_c$ that a student derives from the premise $e_s$.   \\n\\\\end{lemma}\\n\\\\begin{proof}\\nSuppose the student applied logic rules $r_1, r_2,\\\\ldots,r_n$ in sequence to start expression $e_s$ to reach $e_c$ (i.e., $e_c\\\\in r_n\\\\circ r_{n-1}\\\\circ\\\\cdots\\\\circ r_1(e_s)$). Property 3 lets us apply their corresponding inverses to $e_c$ to obtain $e_s$ (i.e., $e_s\\\\in r^{-1}_1\\\\circ r^{-1}_2\\\\circ\\\\cdots\\\\circ r^{-1}_n(e_c)$). By Property 4, $e_t$ is reachable from $e_s$.\\n\\\\end{proof}\\n\\nThus, solving a logic proof is equivalent to searching a connected graph containing $e_s$ and $e_t$. Since rules like Idempotence and Absorption can be chained indefinitely, this graph has an infinite depth. Note that the method in our proof may not produce the shortest path from $e_c$ to $e_s$. Our AI search strategy is also motivated by the fact that hints that backtrack to a start state make for a poor user experience. \\n\\n\\n\\\\subsubsection{Finding a search frontier}\\\\label{subsubsec:m22}\\n\\nThe search frontier $F(e)$ of a Boolean expression $e$ is the set of all expressions obtained applying a logical equivalence to $e$, i.e., $F(e)   \\\\{e' | \\\\exists r\\\\in R: e'\\\\in r(e)\\\\}$. \\\\textbf{FRONTIER\\\\_GEN}, our efficient frontier-generation algorithm, uses Lark's \\\\textbf{Transformer} feature to traverse up the expression's annotated parse tree from leaves to root, replacing each node by the set of possible transforms at that node. This culminates at the root as a set of all potential next-steps via logical substitution. \\\\textbf{FRONTIER\\\\_GEN} visits each subtree of the parse tree exactly once and applies each logic rule to the token span at that root, keeping all valid transformations.\\n\\nA sub-expression of an expression $e$ is a contiguous span of $e$'s tokens that forms a syntactically correct expression, appearing as a sub-tree in $e$'s parse tree. The smallest sub-expression, a single literal (Boolean variable or constant), forms a leaf node in the parse tree. Thus, an expression $e$ containing $|e|$ literals ($\\\\le$ num. leaves) has at most $2|e|$ sub-expressions ($\\\\le$ num. subtrees). The number of logic rules $|R|$ is constant and we implement each rule in $\\\\mathcal{O}(1)$ operations. Hence, the size of the search frontier $F(e)$ is $\\\\mathcal{O}(|e|)$ and \\\\textbf{FRONTIER\\\\_GEN} finds this frontier in $\\\\mathcal{O}(|R|\\\\cdot|E|)\\\\mathcal{O}(|E|)$ operations, which is worst-case optimal.\\n\\n\\\\begin{algorithm}[hbt!]\\n\\\\caption{FRONTIER\\\\_GEN}\\\\label{alg:cap}\\n\\\\begin{algorithmic}\\n\\\\Require Annotated parse tree $T(e)$ of Boolean expression $e$, Rule set $R$\\n\\\\Ensure Search frontier $F(e)$\\n\\\\State $F \\\\gets \\\\emptyset$ \\n\\\\While{Transform(e)}   \\\\Comment{Lark Transformer iterates leaf-to-root}\\n   \\\\State $n \\\\gets \\\\verb|get_current_node|()$\\n   \\\\State $t_s,t_e \\\\gets \\\\verb|get_token_span|(n)$\\n   \\\\State $S_n \\\\gets \\\\verb|apply_logic_rules|(R, n)$   \\\\Comment{Set of next-step sub-expressions}\\n   \\\\While{$S_n \\\\neq \\\\emptyset$}\\n   \\\\State $s \\\\gets \\\\verb|get_element|(S_n)$\\n   \\\\State $F \\\\gets F\\\\bigcup \\\\verb|concat|(e[:t_s],s,e[t_e:])$   \\\\Comment{Sub. rule result into token span}\\n   \\\\State $S_n \\\\gets S_n - \\\\{s\\\\}$\\n   \\\\EndWhile\\n\\\\EndWhile\\n\\\\end{algorithmic}\\n\\\\end{algorithm}\\n\\n\\\\subsubsection{Solving proofs with A* Search}\\\\label{subsubsec:m24}\\n\\nThe A* graph search algorithm aims to find the lowest path between two nodes on a weighted graph using heuristic approximations of unknown path costs. A `consistent' heuristic---one that obeys the Triangle inequality---guarantees that A* search finds an optimal path between two nodes \\\\citep{Hart1968}. This makes it well-suited to solving logic proofs when structured as a Boolean graph search problem. However, we are not aware of an efficient heuristic that consistently estimates the semantic similarity (shortest path) between arbitrary Boolean expressions. Instead, we focus our efforts on a variety of heuristics that approximate the path length between expressions. \\n\\n\\\\begin{figure*}[ht!]\\n   \\\\centering\\n   \\\\begin{subfigure}[t]{0.8\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/userHintsDemo.jpg}\\n   \\\\caption{}\\n   \\\\end{subfigure}\\n   \\n   \\\\begin{subfigure}[t]{0.9\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/LLtreeFig.jpg}\\n   \\\\caption{}\\n   \\\\end{subfigure}\\n   \\\\caption{(a) A users attempts a logic proof on {\\\\logiclearner} and requests hints when stuck. (b) Search trees are calculated at every step of the proof attempt. The trees for the two requested hints in part (a) are as depicted here as numbered.}\\\\label{fig:proofTrees}\\n\\\\end{figure*}\\n\\nAs a first approximation, we use a weighted linear combination of measures of similarity between string representations of Boolean expressions (Table \\\\ref{tab:heur}). Since logic rules are not applied with uniform probability, we also consider the rule used to generate a search node. Since computing a gradient over our objective (number of questions solved) is infeasible, we optimize this combination with a genetic algorithm \\\\cite{holland1973genetic} which `evolves' a population of weighted combinations of heuristics (candidate solutions). Candidate fitness is evaluated on the training problem set and the next generation is chosen via fitness-proportionate selection with elitism. The weight ranges, crossover and mutation probabilities, and degree of elitism are determined empirically. Our genetic algorithm produces a marked improvement in performance compared to a randomly weighted ensemble, but we do not argue that the resulting heuristic is consistent.\\n\\nIn production, we implement a time-bound, depth-limited A* path-finding algorithm that searches the Boolean graph space for the target node beginning from the start node. We use the above ensemble, optimized on our question bank, as our heuristic of choice. Since {\\\\logiclearner} provides real-time hints, we constrain its search time to a few seconds and return the path to the lowest-cost node if the solution isn't found. Limiting the search depth compensates for inconsistent heuristics by preventing meandering search paths. As an illustration, stylized search trees corresponding to hints requested on the interface are shown in Figure \\\\ref{fig:proofTrees}. Details about our training methodology, hyperparameter search, and production heuristic can be found in Appendix \\\\ref{sec:apxAblate}.\\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{A* search heuristics}\\\\label{tab:heur}\\n\\\\begin{tabular}{@{}ll@{}}\\n\\\\toprule\\nHeuristic & Description (computed on a transform $(e_1, r, e_2)$ where $e_2\\\\in r(e_1)$) \\\\\\\\\\n\\\\midrule\\nUnitary function & Returns 1 regardless of input \\\\\\\\\\nLevenshtein distance & The number of single-character edits required to transform $e_1$ to $e_2$ \\\\\\\\\\nVariable mismatch & The number of variables that appear either only in $e_1$ or only in $e_2$ \\\\\\\\\\nLength difference & Absolute difference in string length between $e_1$ and $e_2$ \\\\\\\\\\nRule weight (for each $r\\\\in R$) & A prior on rule $r$ proportional to its frequency in training data \\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\n\\\\subsection{Extensions: question generation and neural embeddings}\\\\label{subsec:m3}\\n\\nWhile our production heuristic is efficient and effective, it approximates the complex semantics of Boolean algebra with only a few surface-level comparisons. A natural extension to this is to integrate \\\\emph{semantic} information into our heuristic ensemble by modeling the space of Boolean expressions in a way that allows for efficient approximations of similarity. As a proof-of-concept, we develop a neural network to learn the semantics of Boolean expressions for proof-solving though auxiliary tasks.\\n\\n\\nNeural networks are powerful machine learning models that are able learn complex relationships present in large datasets. Since these models require more data than can be manually curated, we develop \\\\textbf{PROOF\\\\_GEN} (Algorithm \\\\ref{alg:qgen}) to automatically generate logic proofs. Starting from a given target expression $e_t$, \\\\textbf{PROOF\\\\_GEN} generates a search frontier and randomly selects an expression $e_{t-1}$ from it. This procedure is repeated to obtain $e_{t-2},\\\\ldots,e_{t-k+1}e_s$. Reversed, this is now a $k$-step proof $e_s\\\\rightarrow\\\\cdots\\\\rightarrow e_t$. This also provides us with a way to expand {\\\\logiclearner}'s question bank---\\\\textbf{PROOF\\\\_GEN} does not guarantee elegance, but cherry-picking interesting proofs from its output is an easier task than manually composing novel proofs.\\n\\n\\\\begin{algorithm}[hbt!]\\n\\\\caption{PROOF\\\\_GEN}\\\\label{alg:qgen}\\n\\\\begin{algorithmic}\\n\\\\Require Boolean expression $e$, Number of proof steps $N$\\n\\\\Ensure Logic proof $P(e',e)$\\n\\\\State $P \\\\gets [\\\\ ]$ \\n\\\\State $i \\\\gets 0$\\n\\\\While{$i < N$}\\n   \\\\State $F \\\\gets \\\\verb|FRONTIER_GEN|(e)$\\n   \\\\State $e' \\\\gets \\\\verb|random_select|(F)$\\n   \\\\State $P \\\\gets \\\\verb|append|(P, e')$\\n\\\\EndWhile\\n\\\\State $P \\\\gets \\\\verb|reverse|(P)$\\n\\\\end{algorithmic}\\n\\\\end{algorithm}\\n\\nWe use \\\\textbf{PROOF\\\\_GEN} generate large amounts of data for learning Boolean semantics, and train encoder-decoder neural networks on two tasks: rule prediction and proof length prediction. To evaluate the potential of neural networks to embed Boolean expressions in metric space while preserving semantic similarity, we use the cosine similarity between encoded Boolean expressions as a heuristic to A* search on our human-curated question bank. We also evaluate an untrained baseline and a language model pre-trained on a multilingual text corpus. Details of these experiments and their results are shown in Appendix \\\\ref{sec:apxNN}. \\\\textbf{PROOF\\\\_GEN} does not have graphical interface at the time of writing, but can be accessed via the \\\\verb|logictools| library API.\\n\\n\\\\section{Results}\\\\label{sec:result}\\n\\nTo evaluate its effectiveness, we piloted {\\\\logiclearner} as an optional practice tool over two semesters of the undergraduate discrete mathematics class (COMS 3203) at Columbia University. We surveyed students on their confidence in solving logic proofs before and after covering the propositional logic unit of the course. We also recorded their assessment of {\\\\logiclearner}'s features after learning. We present these results in Section \\\\ref{subsec:r1}. In Section \\\\ref{subsec:r2}, we compare the performance of {\\\\logiclearner} with ChatGPT, an online question-answering tool powered by a Large Language Model (refer to Section \\\\ref{subsec:rel1} for an overview). Analyzing ChatGPT's most common failure modes, we show that LLMs are not currently suited for proof solving and pedagogy. We also present technical results on the performance of {\\\\logiclearner}'s AI proof solver and justify our choice of A* heuristic.\\n\\n\\\\subsection{User Study: {\\\\logiclearner} as a practice tool}\\\\label{subsec:r1}\\n\\nTo evaluate {\\\\logiclearner} as a tool for guided practice, we conducted a user study and analyzed feedback from the students of COMS 3203, the undergraduate discrete mathematics course at Columbia University, across two semesters. Before and after the propositional logic unit, we surveyed students on their confidence in solving logic proofs in various scenarios. Confidence was self-assessed along the 7-point Likert scale \\\\cite{likert1932technique}, ranging from `Not at all confident' to `Completely confident'. Scenarios ranged from \\\\emph{writing a proof with assistance} to \\\\emph{writing a proof in an exam setting}. As seen in Figure \\\\ref{fig:aconf}, student confidence in \\\\emph{writing a logical proof completely and correctly} improves dramatically after lessons in propositional logic and practice with {\\\\logiclearner}. In particular, students gained confidence in solving proofs in an exam setting, the hardest scenario in our survey. This was also reflected in the course outcomes---adjusted for overall difficulty, we observed that the mean student performance on logic proof questions in the COMS 3203 exams \\\\textbf{increased by roughly 5\\\\} after the introduction of LogicLearner as an optional practice tool. This is a notable improvement as the class sizes are large (300+) and overall performance on these exams is around 80\\\\ on average, making it unlikely that large changes in results can be observed. \\n\\n\\\\begin{figure}[ht!]\\n\\\\includegraphics[width\\\\textwidth]{images/aggregated_likert.png}\\n\\\\caption{Confidence scores for solving logic problems under various scenarios, aggregated across two semesters of a discrete mathematics course}\\\\label{fig:aconf}\\n\\\\end{figure}\\n\\nPost-learning, students were prompted to rate the user-friendliness of {\\\\logiclearner} on a scale of 1 to 5 and answer long-form questions on the application's performance. Figure \\\\ref{fig:afeat} shows students rate {\\\\logiclearner}'s highly, with every feature scoring a mean of approximately 4 out of 5 with a standard deviation of less than 1. We analyze the student responses to long-form questions for positive/negative intent using a sentiment classification pipeline from Hugging Face\\\\footnote{\\\\url{https://huggingface.co/}} (Figure \\\\ref{fig:asentiment}). Two of the questions are negative leading questions (`Missing features/improvements' and `Ease of use/challenges in usage'), and negative responses are expected. Negative responses mostly comprise of suggestions for improvement and positive responses indicate where students found nothing lacking. The two open-ended questions (`(Would you) Recommend to others' and `(Would you) Use it to practice') show overwhelmingly positive responses, showing that respondents found {\\\\logiclearner} to be a useful tool for logic practice. The phrasing of the long-form questions and full respondent statistics can be found in Appendix \\\\ref{sec:apxSurvey}.\\n\\n\\\\begin{figure}[ht!]\\n\\\\centering\\n\\\\begin{minipage}{.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width0.9\\\\linewidth]{images/aggregated_feature.png}\\n   \\\\caption{Aggregated mean feature ratings}\\n   \\\\label{fig:afeat}\\n\\\\end{minipage}\\n\\\\begin{minipage}{.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width0.8\\\\linewidth]{images/aggregate_sentiment.png}\\n   \\\\caption{Sentiment to long-form questions}\\n   \\\\label{fig:asentiment}\\n\\\\end{minipage}\\n\\\\end{figure}\\n\\nFor brevity, we only present the aggregate results of the surveys in these figures. Individual surveys (Appendix \\\\ref{sec:apxSurvey}) are highly consistent, emphasizing the significance of our results. \\n\\n\\\\subsection{Performance and AI results}\\\\label{subsec:r2}\\n\\nWe use a genetic algorithm to optimize a weighted linear ensemble of heuristics (Table \\\\ref{tab:heur}) for {\\\\logiclearner} to solve logic proofs with A* search. The details of the optimization and ablation studies involved in our heuristic selection can be found in Appendix \\\\ref{sec:apxAblate}. We compare {\\\\logiclearner} with ChatGPT, a publicly available conversational interface to the GPT series of Large Language Models (LLMs) trained for text generation on an internet-scale dataset. ChatGPT is the most easily accessible AI question-answering tool to end users, and is increasingly being used in an academic setting. However, ChatGPT is not designed as a pedagogical tool, and the user experience varies significantly from that of {\\\\logiclearner}. We find that it is also unable to produce mathematically consistent answers, and this \\\\emph{confident but incorrect} behavior could lead to maladaptation from students learning incorrect information. Figure \\\\ref{fig:flowCompare} describes one potential workflow of a student using ChatGPT for proof solving. In contrast with the {\\\\logiclearner} user flow (Figure \\\\ref{fig:uflow}), even when ChatGPT provides correct answers, it will not gradually nudge the user to a solution or take the user's progress into account unless prompted. Re-attempting and review are also not possible with such LLM-based QA systems.\\n\\n\\\\begin{figure}[ht!]\\n\\\\centering\\n\\\\includegraphics[width0.8\\\\textwidth]{images/chatGptUserFlow.jpg}\\n\\\\caption{A potential student workflow with ChatGPT for proof solving.}\\\\label{fig:flowCompare}\\n\\\\end{figure}\\n\\n\\\\subsubsection{Proof-solving performance}\\\\label{subsubsec:r21}\\n\\nWe evaluate {\\\\logiclearner} on the application's human-curated question bank of 33 logic proofs, as well as two QUESTION\\\\_GEN (Algorithm \\\\ref{alg:qgen}) generated datasets with 198 3-step proofs and 66 10-step proofs respectively. We also train and evaluate a simple neural network heuristic as a \\\\emph{proof-of-concept} as described in Section \\\\ref{subsec:m3}, and detail our experiments in Appendix \\\\ref{sec:apxNN}. Due to resource constraints, we evaluate neural models (including ChatGPT) only on our human-curated dataset. We provide the following prompt to ChatGPT: \\\\emph{`Using the rules of propositional logic, prove that \\\\textbf{premise} is logically equivalent to \\\\textbf{target}. Make sure to explain each step.'}. This prompt is manually tuned, and encourages some Chain-of-Thought reasoning \\\\cite{wei2022chain}, though we do not claim to fully use this method. The LLM's responses are well-framed, but rarely correct. A student is likely not sophisticated enough to catch these technical mistakes, making ChatGPT infeasible as a learning aid for proof solving.\\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{AI performance on logic proof question banks.}\\\\label{tab:aiRes}\\n\\\\begin{tabular}{@{}llcccc@{}}\\n\\\\toprule\\nMethod & Heuristic & Eval time & \\\\multicolumn{3}{c}{Score on Dataset (questions)} \\\\\\\\\\n & & (seconds) & Curated & \\\\multicolumn{2}{c}{Generated} \\\\\\\\ \\n & & & (33 mixed) & (198 small) & (66 mixed) \\\\\\\\\\n\\\\midrule\\n\\\\multirow{2}{*}{{\\\\logiclearner}} & Comparator ensemble & 3 & \\\\textbf{28} & \\\\textbf{143} & \\\\textbf{43} \\\\\\\\\\n & Neural Network & 15 & 13 & - & - \\\\\\\\\\n\\\\midrule\\nChatGPT & Relevant prompt & 15 - 30 & 5 & - & -\\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\nAs seen in Table \\\\ref{tab:aiRes}, {\\\\logiclearner} is able to solve almost every question in our question bank in real time, providing a seamless user experience to students requesting hints. The {\\\\logiclearner} heuristic is also able to generalize well to previously unseen and computer-generated questions of varying lengths, reinforcing its strength as a heuristic. In contrast, ChatGPT is unable to solve any proofs that are longer than 1-2 steps. Our inspection of ChatGPT's proofs shows some recurring error modes:\\n\\\\begin{itemize}\\n   \\\\item ChatGPT does not model the semantics of Boolean variables, resulting in false equivalences. For example, it does not distinguish between $q$ and $r$ in the bottom of Figure \\\\ref{fig:chatFails}(a).\\n   \\\\item It often hallucinates variable or rule names, even for correct equivalences. Figure \\\\ref{fig:chatFails}(b) shows instances of ChatGPT confusing the Absorption and Domination laws, and incorrectly spelling `Commutative'.\\n   \\\\item As seen in Figure \\\\ref{fig:chatFails}(c), ChatGPT fails to correctly perform parity-dependent operations like negation and matching parentheses. Parity matching is known to be a hard problem for Transformer \\\\cite{vaswani2017attention} based architectures like LLMs \\\\cite{Hahn_2020}, and is one of the most frequent failure modes we observed.\\n   \\\\item ChatGPT cannot accurately parse long expression sequences or maintain context over the span of a long answer, resulting in it hallucinating a conclusion to an incorrect target (Figure \\\\ref{fig:chatFails}(d)).\\n\\\\end{itemize} \\n\\nAdditional details of our ChatGPT analysis (such as prompt selection) are presented in Appendix \\\\ref{sec:apxChat}. We hypothesize that using {\\\\logiclearner}'s proofs for Retrieval-Augmented Generation \\\\citep{lewis2020retrieval} to LLMs could produce well-explained, mathematically sound proofs, but we leave this to future work.\\n\\n\\\\begin{figure*}[ht!]\\n   \\\\centering\\n   \\\\begin{subfigure}[t]{0.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/chatBadVar.png}\\n   \\\\caption{}\\n   \\\\end{subfigure}\\n   ~ \\n   \\\\begin{subfigure}[t]{0.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/chatBadLaw.png}\\n   \\\\caption{}\\n   \\\\end{subfigure}\\n\\n   \\\\begin{subfigure}[t]{0.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/chatBadNeg.png}\\n   \\\\caption{}\\n   \\\\end{subfigure}\\n   ~ \\n   \\\\begin{subfigure}[t]{0.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/chatWrongTarget.png}\\n   \\\\caption{}\\n   \\\\end{subfigure}\\n   \\\\caption{A variety of ChatGPT's failure modes on logic proofs.}\\\\label{fig:chatFails}\\n\\\\end{figure*}\\n\\n\\\\section{Limitations and future work}\\\\label{sec:limit}\\n\\nOur surveys across two semesters of undergraduate discrete mathematics show a strongly positive response to {\\\\logiclearner}. However, voluntary student surveys are susceptible to only receiving responses from conscientious students who tend to perform well regardless of training method. Additionally, it is challenging to disentangle the benefits of {\\\\logiclearner} from unaided student learning, since denying access to a practice tool a control group of students is unfair to that group. More concrete results would likely require a multi-year, multi-university effort. While {\\\\logiclearner} is guaranteed to be mathematically precise, we use a simple AI heuristic that does not guarantee accuracy. Future work could explore the potential of large models as heuristics to {\\\\logiclearner}'s graph search framework for proof solving---we explore such ideas in Appendix \\\\ref{sec:apxNN}. {\\\\logiclearner} is open-source, and we encourage the development of more sophisticated search methods. We also note that such improvements may make for interesting student projects.\\n\\n\\\\section{Conclusion}\\\\label{sec:conclusion}\\n\\nThe study of propositional logic benefits from judgment-free guidance during practice. Building on existing work on the inability of Large Language Models (LLMs) to perform deep reasoning, we analyze the failure modes of ChatGPT, an LLM-based application that is increasingly used for pedagogical guidance. We outline the requirements of an effective practice tool for proof solving, and accordingly develop {\\\\logiclearner}, an interactive application to practice propositional logic proofs with real-time an AI proof solver for real-time pedagogical guidance. We pilot {\\\\logiclearner} as a practice tool over two semesters of the undergraduate discrete mathematics course at Columbia University and receive strongly positive feedback on its design and utility in student surveys. To the best of our knowledge, {\\\\logiclearner} is the first and only application for fully-automated guided proof-solving practice at the time of writing. {\\\\logiclearner} is free and open-source, and we look forward to the continued development of tools that democratize access to mathematical understanding.\\n\\n\\\\backmatter\\n\\n\\\\bmhead{Acknowledgements}\\n\\nWe thank the Center for Teaching and Learning (CTL) at Columbia University for lending their expertise in designing {\\\\logiclearner} as an effective pedagogical tool, and for continuing to manage the {\\\\logiclearner} application server and its open-source code base.\\n\\n\\\\section*{Declarations}\\n\\n\\\\begin{itemize}\\n\\\\item \\\\textbf{Funding} This work was supported by a generous grant from the Columbia University Provost's Faculty Committee on Educational Innovation. \\n\\\\item \\\\textbf{Competing interests} All authors declare that they have no conflicts of interest in relation to this work.\\n\\\\item \\\\textbf{Ethics approval and consent to participate} Student surveys were approved by the Institutional Review Board. Approval number: IRB-AAAU0354.\\n\\\\item \\\\textbf{Consent for publication} All authors consent to the publication of this manuscript.\\n\\\\item \\\\textbf{Data availability} Student survey data is presented in aggregate form in Appendix \\\\ref{sec:apxSurvey}. Full experimental results are in Appendix \\\\ref{sec:apxAblate}.\\n\\\\item \\\\textbf{Materials availability} Not applicable.\\n\\\\item \\\\textbf{Code availability} {\\\\logiclearner} is a free and open-source application under the GNU GPLv3 license. Source code is available at \\\\url{https://github.com/ccnmtl/logiclearnertools}.\\n\\\\item \\\\textbf{Author contribution} AI implemented the business logic (Parser, Search, API), designed and conducted the technical experiments, and authored the manuscript. MV and UM designed and implemented an earlier version of the {\\\\logiclearner} parser. The staff at the CTL (MT, ZM, ND, SS) designed the {\\\\logiclearner} web application user experience and database. The application server and git repositories are managed by the CTL. As the PIs of this project, NV and AS developed the vision for {\\\\logiclearner}, oversaw the entirety of its design and implementation, and mentored all student contributors who worked on the application.\\n\\\\end{itemize}\\n \\n\\n\\n\\\\clearpage\\n\\n\\\\bibliography{sn-bibliography} common bib file\\n if required, the content of .bbl file can be included here once bbl is generated\\n\\\\input sn-article.bbl\\n\\n\\\\clearpage\\n\\n\\n\\\\begin{appendices}\\n\\n\\\\section{Visuals of the LogicLearner Web Application}\\\\label{sec:apxVisual}\\n\\nWe designed the user interface and user experience of {\\\\logiclearner} in collaboration with the Columbia University Center for Teaching and Learning\\\\footnote{\\\\url{https://ctl.columbia.edu}} (CTL). The CTL specializes in designing the user experience of educational tools, and hosts the {\\\\logiclearner} application and database on its server. \\n\\n\\\\begin{figure}[ht!]\\n\\\\includegraphics[width0.99\\\\textwidth, frame]{images/introScreen.png}\\n\\\\caption{The {\\\\logiclearner} home screen.}\\\\label{fig:appIntro}\\n\\\\end{figure}\\n\\n\\\\begin{figure}[ht!]\\n\\\\includegraphics[width0.99\\\\textwidth, frame]{images/levelsScreen.png}\\n\\\\caption{Level selection according to proof difficulty.}\\\\label{fig:appLevel}\\n\\\\end{figure}\\n\\n\\n\\\\begin{figure}[ht!]\\n\\\\includegraphics[width0.99\\\\textwidth, frame]{images/guideScreen.png}\\n\\\\caption{A tutorial on using {\\\\logiclearner}.}\\\\label{fig:appGuide}\\n\\\\end{figure}\\n\\n\\n\\\\begin{figure}[ht!]\\n\\\\includegraphics[width0.99\\\\textwidth, frame]{images/noviceScreen.png}\\n\\\\caption{Selecting questions in the Novice level.}\\\\label{fig:appNovice}\\n\\\\end{figure}\\n\\n\\\\begin{figure*}[t!]\\n   \\\\centering\\n   \\\\begin{subfigure}[t]{0.48\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/oneHintScreen.png}\\n   \\\\caption{One hint requested.}\\n   \\\\end{subfigure}\\n   ~ \\n   \\\\begin{subfigure}[t]{0.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/twoHintScreen.png}\\n   \\\\caption{Two hints requested}\\n   \\\\end{subfigure}\\n   \\\\caption{Requesting hints during a question attempt.}\\n\\\\end{figure*}\\n\\n\\\\begin{figure}[ht!]\\n\\\\includegraphics[width0.99\\\\textwidth]{images/completedHintsScreen.png}\\n\\\\caption{A completed logic proof.}\\\\label{fig:appCompleted}\\n\\\\end{figure}\\n\\n\\\\section{AI Experiments and Ablations}\\\\label{sec:apxAblate}\\n\\nWe train a genetic algorithm to optimize the weights of a linear combination of our heuristics defined in Table \\\\ref{tab:heur}. Weights were constrained to floating point numbers in $[-10,10]$, which produced the best results empirically. The hyperparameters explored were the per-question time $\\\\tau$, number of training questions $|Q|$, elitism $\\\\epsilon$, and the crossover and mutation probabilities $P_c,P_m$. A score (questions solved) was calculated for three datasets: human-curated $(S_v)$, short generated questions ($S_t$), and mixed-length generated questions $(S_l)$. We choose a small subset of our human-curated question bank to train our algorithm, and present the final performance on all questions in \\\\ref{tab:ablate1}. The weights in our production heuristic are presented in Table \\\\ref{tab:prodheur}. Note that this heuristic was committed to production prior to some of the later AI improvements. \\n\\nExperiments were conducted on modest hardware with a limited search depth, and better hardware may lead to better performance. Despite this, our genetic algorithm shows a considerable improvement over the random baseline.\\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{Genetic algorithm ablation trials}\\\\label{tab:ablate1}\\n\\\\begin{tabular}{@{}llllllllll@{}}\\n\\\\toprule\\nPopulation & Total & $\\\\tau$ & $|Q|$ & $\\\\epsilon$ & $P_c$ & $P_m$ & $S_v$ & $S_t$ & $S_l$ \\\\\\\\\\nSize & Generations & (s) & & & & & (33) & (198) & (66) \\\\\\\\ \\n\\\\midrule\\n5 & 5 & 3 & 5 & 1 & 0.8 & 0.2 & 22 & 119 & 29 \\\\\\\\\\n10 & 10 & 3 & 10 & 1 & 0.8 & 0.2 & 21 & 114 & 32 \\\\\\\\\\n10 & 10 & 3 & 20 & 1 & 0.8 & 0.2 & 27 & 129 & 38 \\\\\\\\\\n10 & 10 & 3 & 33 & 1 & 0.8 & 0.2 & 27 & 141 & 43 \\\\\\\\\\n10 & 10 & 5 & 33 & 1 & 0.8 & 0.2 & 26 & 135 & 40 \\\\\\\\\\n10 & 10 & 3 & 33 & 3 & 0.8 & 0.2 & \\\\textbf{29} & 137 & 42 \\\\\\\\\\n10 & 20 & 3 & 33 & 3 & 0.6 & 0.2 & 28 & \\\\textbf{143} & \\\\textbf{43} \\\\\\\\\\n20 & 10 & 3 & 33 & 1 & 0.8 & 0.2 & 28 & 140 & 43 \\\\\\\\\\n20 & 20 & 3 & 33 & 1 & 0.8 & 0.2 & 28 & 133 & 43 \\\\\\\\\\n20 & 10 & 33 & 30 & 3 & 0.8 & 0.2 & 27 & 127 & 40 \\\\\\\\\\n\\\\midrule\\nProduction & & & & & & & & &   \\\\\\\\\\n\\\\midrule\\n20 & 8 & 1 & 7 & 3 & 0.8 & 0.5 & 27 & 129 & 37 \\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{A* search heuristics}\\\\label{tab:prodheur}\\n\\\\begin{tabular}{@{}ll|ll@{}}\\n\\\\toprule\\nHeuristic & Weight & Heuristic & Weight \\\\\\\\\\n\\\\midrule\\nLevenshtein distance & 3.36 & Distributivity & 3.94 \\\\\\\\\\nUnitary function & 3.76 & Domination & 4.09 \\\\\\\\\\nVariable Mismatch & 6.09 & Idempotence & -7.03 \\\\\\\\\\nLength difference & 1.53 & Identity & -9.85 \\\\\\\\\\nAbsorption & -3.88 & Iff as Implication & -4.20 \\\\\\\\\\nAssociativity & 1.94 & Implication as Disjunction & 6.92 \\\\\\\\\\nCommutativity & -8.07 & Negation & -0.55 \\\\\\\\\\nDe Morgan's Law & 3.71 & Start state & 1.44 \\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\nWe also trained our model on our dataset of 66 randomly generated solutions of depths 2-10 evaluated performance on the original question bank. These results, presented in Table \\\\ref{tabR2}, show that our heuristic generalizes from our randomly generated dataset to the human-generated question bank.\\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{Train on randomly generated questions}\\\\label{tabR2}\\n\\\\begin{tabular}{@{}lllllllllll@{}}\\n\\\\toprule\\nPopulation & Total & $\\\\tau$ & $|Q|$ & $\\\\epsilon$ & $\\\\tau_\\\\mathrm{eval}$ & $P_c$ & $P_m$ & $S_v$ & $S_t$ & $S_l$ \\\\\\\\\\nSize & Generations & (s) & & & (s) & & & (33) & (198) & (66) \\\\\\\\ \\n\\\\midrule\\n5 & 10 & 3 & 5 & 1 & 1 & 0.8 & 0.3 & 25 & 136 & 41 \\\\\\\\\\n5 & 10 & 3 & 5 & 1 & 3 & 0.8 & 0.3 & 27 & 143 & 42 \\\\\\\\\\n10 & 10 & 3 & 10 & 1 & 1 & 0.8 & 0.3 & 28 & 140 & 43 \\\\\\\\\\n10 & 10 & 3 & 10 & 1 & 3 & 0.8 & 0.3 & 29 & 147 & 46 \\\\\\\\\\n10 & 10 & 3 & 10 & 1 & 1 & 0.8 & 0.5 & 26 & 141 & 45 \\\\\\\\\\n10 & 10 & 3 & 10 & 1 & 3 & 0.8 & 0.5 & 27 & 143 & 45 \\\\\\\\\\n10 & 10 & 3 & 10 & 1 & 1 & 0.9 & 0.3 & 27 & 126 & 41 \\\\\\\\\\n10 & 10 & 3 & 10 & 1 & 3 & 0.9 & 0.3 & 29 & 138 & 42 \\\\\\\\\\n20 & 5 & 3 & 66 & 1 & 1 & 0.8 & 0.3 & 27 & 131 & 44 \\\\\\\\\\n20 & 5 & 3 & 66 & 1 & 3 & 0.8 & 0.3 & 29 & 139 & 45 \\\\\\\\\\n20 & 10 & 3 & 66 & 1 & 1 & 0.8 & 0.3 & 28 & 142 & 41 \\\\\\\\\\n20 & 10 & 3 & 66 & 1 & 3 & 0.8 & 0.3 & 29 & 147 & 44 \\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\n\\\\section{Student Survey Results}\\\\label{sec:apxSurvey}\\n\\n\\\\begin{figure}[ht!]\\n\\\\includegraphics[width\\\\textwidth]{images/survey1_likert.png}\\n\\\\caption{Survey 1: confidence scores}\\\\label{fig:s1conf}\\n\\\\end{figure}\\n\\n\\\\begin{figure}[ht!]\\n\\\\includegraphics[width\\\\textwidth]{images/survey2_likert.png}\\n\\\\caption{Survey 2: confidence scores}\\\\label{fig:s2conf}\\n\\\\end{figure}\\n\\nAs seen in Figures \\\\ref{fig:s1conf} and \\\\ref{fig:s2conf}, the confidence scores are highly similar across semesters. There were no changes to the {\\\\logiclearner} application in production between evaluations.\\n\\n\\\\begin{figure}[ht!]\\n\\\\centering\\n\\\\begin{minipage}{.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width0.9\\\\linewidth]{images/survey1_feature.png}\\n   \\\\caption{Survey 1: feature ratings}\\n   \\\\label{fig:s1feat}\\n\\\\end{minipage}\\n\\\\begin{minipage}{.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width0.8\\\\linewidth]{images/survey1_sentiment.png}\\n   \\\\caption{Survey 1: long-form sentiment}\\n   \\\\label{fig:s1sentiment}\\n\\\\end{minipage}\\n\\\\end{figure}\\n\\n\\\\begin{figure}[ht!]\\n\\\\centering\\n\\\\begin{minipage}{.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width0.9\\\\linewidth]{images/survey2_feature.png}\\n   \\\\caption{Survey 2: feature ratings}\\n   \\\\label{fig:s2feat}\\n\\\\end{minipage}\\n\\\\begin{minipage}{.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width0.8\\\\linewidth]{images/survey2_sentiment.png}\\n   \\\\caption{Survey 2: long-form sentiment}\\n   \\\\label{fig:s2sentiment}\\n\\\\end{minipage}\\n\\\\end{figure}\\n\\nThe feature scores and long-form sentiment (Figures \\\\ref{fig:s1feat}, \\\\ref{fig:s2feat}, \\\\ref{fig:s1sentiment}, \\\\ref{fig:s2sentiment}) show a similar consistency across surveyed groups, bolstering our confidence in these results.\\n\\nTable \\\\ref{tab:longform} shows the number of respondents per question, with the long-form questions written as queried. The number of respondents is significantly higher in the pre-learning, but there are enough respondents (especially when aggregated) to show significant results for the post-learning evaluation. \\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{Survey questions and number of respondents}\\\\label{tab:longform}\\n\\\\begin{tabular}{@{}llll@{}}\\n\\\\toprule\\nQuestion & Survey 1 & Survey 2 & Total \\\\\\\\\\n\\\\midrule\\nPre-learning confidence (6 questions) & 65 & 213 & 278 \\\\\\\\\\nPost-learning confidence (6 questions) & 28 & 52 & 80 \\\\\\\\\\n\\\\midrule\\nFeature ratings (scale of 1-5) & 27 & 43 & 70 \\\\\\\\\\n\\\\midrule\\nWhat features are we missing? & 10 & 13 & 23 \\\\\\\\ \\nHow could we improve our app? & & & \\\\\\\\\\n\\\\midrule\\nHow easy is it to use our app? & & & \\\\\\\\ \\nWhat were some challenges you faced & 14 & 16 & 30 \\\\\\\\\\nin using our app? & & & \\\\\\\\\\n\\\\midrule\\nWould you recommend Logic Learner & & & \\\\\\\\ \\nto others for practicing propositional logic? & 14 & 20 & 34 \\\\\\\\\\nWhy or why not? & & & \\\\\\\\\\n\\\\midrule\\nWould you use this app for practicing & 15 & 21 & 36 \\\\\\\\ \\nfor an exam or homework? & & & \\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\n\\\\section{Deep Boolean Metric Learning}\\\\label{sec:apxNN}\\n\\n{\\\\logiclearner}'s hints feature requires a robust and efficient heuristic for proof solving with A* search. In this section, we explore a potentially powerful alternative heuristic to {\\\\logiclearner} AI. Deep metric learning \\\\cite{kaya2019deep} aims to embed data in high-dimensional metric space such that desirable relationships between data points are preserved w.r.t. a chosen metric. Here, we embed Boolean expressions in vector space with the aim of preserving semantic similarity as a metric between expression embeddings. We leverage \\\\textbf{PROOF\\\\_GEN} (Algorithm \\\\ref{alg:qgen}) to produce datasets for neural network training, and train small Siamese encoder-decoder neural networks on two pre-training $k$-way classification tasks, rule prediction and proof length estimation (Table \\\\ref{tab:nnproxy}) with a 70-30 train-test split. We restrict the proof length to be at most 3 steps to reduce the impact of sub-optimal generated proofs. Since there is an infinite vocabulary of Boolean expressions, we tokenize and learn fixed vocabulary embeddings at a character level. We train a Gated Recurrent Unit (GRU) \\\\cite{cho2014learning} encoder and a fully-connected multilayer perceptron (MLP) decoder for 350 epochs on a T4 GPU in Google Colab\\\\footnote{Google. (2024). Google Colaboratory: \\\\url{https://colab.research.google.com/}}. The networks achieve high scores in pre-training despite their simplicity, as shown in Figures \\\\ref{fig:ruleTrain}, \\\\ref{fig:distTrain} and Table \\\\ref{tab:nnproxy}.\\n\\n\\\\begin{figure}[ht!]\\n\\\\centering\\n\\\\begin{minipage}{.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/rule_pred_loss.jpg}\\n   \\\\caption{Training loss: rule prediction}\\n   \\\\label{fig:ruleTrain}\\n\\\\end{minipage}\\n\\\\begin{minipage}{.5\\\\textwidth}\\n   \\\\centering\\n   \\\\includegraphics[width\\\\linewidth]{images/cat_step_loss.jpg}\\n   \\\\caption{Training loss: proof length}\\n   \\\\label{fig:distTrain}\\n\\\\end{minipage}\\n\\\\end{figure}\\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{Neural Network on Proxy Tasks}\\\\label{tab:nnproxy}\\n\\\\begin{tabular}{@{}lllccccc@{}}\\n\\\\toprule\\nPre-training & Input & Target & Dataset & Num. & \\\\multicolumn{3}{c}{Top-1 Accuracy (\\\\)} \\\\\\\\\\nTask & & & Size & Class & Random & Train & Test \\\\\\\\\\n\\\\midrule\\nRule Prediction & $(e1, e2)$ & $r\\\\in R: e2\\\\in r(e1)$& 3128 & 16 & 6.25 & 83.10 & 81.04   \\\\\\\\\\nProof Length & $(e1, e2)$ & Proof length $l\\\\le 3$ & 16797 & 4 & 25 & 86.17 & 80.58 \\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\nPost training on proxy tasks, we use the cosine similarity between GRU encoder outputs as the heuristic for A* search on our human-curated question bank. As a baseline, we use a randomly initialized GRU encoder without pre-training. As an ablation study on our choice of tokenizer and training data, we also use CANINE-s \\\\cite{DBLP:journals/corr/abs-2103-06874} embeddings as a heuristic. CANINE-s is a Transformer-based language model that does not require an explicit tokenizer, bypassing the limitations of fixed-vocabulary models in parsing Boolean expressions.\\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{Neural Network on Proxy Tasks}\\\\label{tab:nnresults}\\n\\\\begin{tabular}{@{}llccc@{}}\\n\\\\toprule\\nModel & Language & {\\\\logiclearner} & Timeout & Score \\\\\\\\\\n & Pre-training & Pre-training & (s) & (33) \\\\\\\\\\n\\\\midrule\\n\\\\multirow{6}{*}{GRU Encoder} & \\\\multirow{6}{*}{None} & \\\\multirow{2}{*}{Rule Prediction} & 5 & 7 \\\\\\\\\\n & & & 15 & \\\\textbf{13} \\\\\\\\\\n & & \\\\multirow{2}{*}{Proof Length} & 5 & 6 \\\\\\\\\\n & & & 15 & 9 \\\\\\\\\\n & & \\\\multirow{2}{*}{None} & 5 & 7   \\\\\\\\\\n & & & 15 & 10 \\\\\\\\\\n\\\\midrule\\n\\\\multirow{2}{*}{CANINE-s} & \\\\multirow{2}{*}{Multilingual Wikipedia \\\\cite{DBLP:journals/corr/abs-1810-04805}} & \\\\multirow{2}{*}{None} & 5 & 2 \\\\\\\\\\n & & & 15 & 2   \\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\nTable \\\\ref{tab:nnresults} shows the results of our neural network study. While pre-training on proof length prediction does not help in this case, rule prediction improves the performance at greater search depths. Our ablations show that much of the performance comes from the domain-specific tokenization and network architecture, as CANINE-s---a complex language model with extensive multilingual pre-training---performs significantly worse than our baseline. Additionally, all models except CANINE-s outperform ChatGPT. However, all models are outperformed by our ensemble in production. \\nWe emphasize again that this is merely a \\\\textbf{proof-of-concept} for potential future works that could leverage {\\\\logiclearner}'s proof-generation and AI structure to build powerful automated logic proof solvers.\\n\\n\\\\section{ChatGPT Struggles with Logic Proofs}\\\\label{sec:apxChat}\\n\\nTo evaluate ChatGPT as a logic proof solver, we tune the prompts to produce answers that resemble step-by-step proofs using the rules of propositional logic\\\\footnote{Full prompt tuning transcript: \\\\url{https://chat.openai.com/share/9fc940a7-e258-4512-967e-c0d8e780ba8b}}. We do not use complex compositional prompts as our end users (undergraduate students) are unlikely to be familiar with the mathematical prompt tuning literature. Without explicitly asking for rules, ChatGPT generates a truth table to solve proofs. Our final prompt is of the form described in Section \\\\ref{subsec:r2}. With this simple prompt, we ask ChatGPT all questions in our human-curated question bank\\\\footnote{Full proof-evaluation transcript: \\\\url{https://chat.openai.com/share/7b7aa7f5-fb66-4fd3-ae41-172e2bf257d2}}.\\n\\nWhile ChatGPT exhibits an astonishing ability to parse input and perform few-step reasoning, it exhibits several error modes with our simple but realistic prompt, detailed in Section \\\\ref{subsec:r2}. ChatGPT interprets the term `rules of logic' to beyond equivalence relations include operations such as Modus Ponens, but we do not count it against the results if the proof is logically correct. We ask it each question once as a naive user will not retry in case of an incorrect answer. By our (lenient) analysis of the results, ChatGPT only solves the 5 questions in Table \\\\ref{tab:gptcorrect} correctly. A majority of these are one-step questions, showing that ChatGPT with minimal prompt tuning can can only reason at very shallow depths.\\n\\n\\\\begin{table}[ht!]\\n\\\\begin{center}\\n\\\\begin{minipage}{\\\\textwidth}\\n\\\\caption{Questions correctly solved by ChatGPT}\\\\label{tab:gptcorrect}\\n\\\\begin{tabular}{@{}c|ll|l@{}}\\n\\\\toprule\\nQuestion & Premise & Target & Comments \\\\\\\\\\n\\\\midrule\\n1 & $\\\\neg(\\\\neg p)$ & $p$ & One-step Double Negation \\\\\\\\\\n3 & $p\\\\to (q\\\\to r)$ & $(p\\\\land q) \\\\to r$ & Uses Modus Ponens cases \\\\\\\\\\n6 & $\\\\neg p \\\\land \\\\neg q$ & $\\\\neg (p\\\\lor q)$ & One-step De Morgan's law \\\\\\\\\\n7 & $\\\\neg(p\\\\land \\\\neg q)\\\\lor q$ & $\\\\neg p \\\\lor q$ & De Morgan's $\\\\rightarrow$ Idempotence \\\\\\\\\\n9 & $(p\\\\lor q)\\\\land (p\\\\lor r)$ & $p\\\\lor (q\\\\land r)$ & One-step Distributivity \\\\\\\\\\n\\\\botrule\\n\\\\end{tabular}\\n\\\\end{minipage}\\n\\\\end{center}\\n\\\\end{table}\\n\\n\\\\end{appendices}\\n\\n\\\\end{document}\\n\"}],\n",
       " 'main': 'sn-article.tex',\n",
       " 'title': 'LogicLearner: A Tool for the Guided Practice of Propositional Logic Proofs'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kret_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
